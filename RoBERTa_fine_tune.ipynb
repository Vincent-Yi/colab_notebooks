{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RoBERTa_fine_tune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNEu7vcAWxK8",
        "outputId": "9f83aaeb-7c61-4b2f-aae1-3fe53bf90dce"
      },
      "source": [
        "!pip install transformers tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.2)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 34.9 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0sLA0Qv3xEW"
      },
      "source": [
        "## 1 Data Preparation\n",
        "## 1.1 Base\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.isdir('/content/data/'):\n",
        "    !mkdir /content/data/\n",
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_train.txt /content/data\n",
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_dev.txt /content/data\n",
        "!mv /content/data/combined_train.txt /content/data/train.txt\n",
        "!mv /content/data/combined_dev.txt /content/data/dev.txt\n",
        "\n",
        "!split -n 15 /content/data/train.txt\n",
        "\n",
        "!mv /content/xaa /content/data/train00.txt\n",
        "!mv /content/xab /content/data/train01.txt\n",
        "!mv /content/xac /content/data/train02.txt\n",
        "!mv /content/xad /content/data/train03.txt\n",
        "!mv /content/xae /content/data/train04.txt\n",
        "!mv /content/xaf /content/data/train05.txt\n",
        "!mv /content/xag /content/data/train06.txt\n",
        "!mv /content/xah /content/data/train07.txt\n",
        "!mv /content/xai /content/data/train08.txt\n",
        "!mv /content/xaj /content/data/train09.txt\n",
        "!mv /content/xak /content/data/train10.txt\n",
        "!mv /content/xal /content/data/train11.txt\n",
        "!mv /content/xam /content/data/train12.txt\n",
        "!mv /content/xan /content/data/train13.txt\n",
        "!mv /content/xao /content/data/train14.txt\n",
        "\n",
        "\n",
        "!split -n 15 /content/data/dev.txt\n",
        "\n",
        "!mv /content/xaa /content/data/dev00.txt\n",
        "!mv /content/xab /content/data/dev01.txt\n",
        "!mv /content/xac /content/data/dev02.txt\n",
        "!mv /content/xad /content/data/dev03.txt\n",
        "!mv /content/xae /content/data/dev04.txt\n",
        "!mv /content/xaf /content/data/dev05.txt\n",
        "!mv /content/xag /content/data/dev06.txt\n",
        "!mv /content/xah /content/data/dev07.txt\n",
        "!mv /content/xai /content/data/dev08.txt\n",
        "!mv /content/xaj /content/data/dev09.txt\n",
        "!mv /content/xak /content/data/dev10.txt\n",
        "!mv /content/xal /content/data/dev11.txt\n",
        "!mv /content/xam /content/data/dev12.txt\n",
        "!mv /content/xan /content/data/dev13.txt\n",
        "!mv /content/xao /content/data/dev14.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTniAQic5Wyb",
        "outputId": "06df705f-334e-4400-c581-359c609b146c"
      },
      "source": [
        "## 1.2 Domain\n",
        "\n",
        "!mkdir -p /content/data/biomed\n",
        "!mkdir -p /content/data/cs\n",
        "!mkdir -p /content/data/review\n",
        "!mkdir -p /content/data/news\n",
        "\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt    # 4561839\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt  # 35265731\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt      # 712923\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt  # 905793\n",
        "\n",
        "TRAIN_SIZE = int(712923 * 0.8)\n",
        "VAL_SIZE = 712923 - TRAIN_SIZE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4561839 /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt\n",
            "35265731 /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt\n",
            "712923 /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt\n",
            "905793 /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JJ-2YH-DNBr"
      },
      "source": [
        "TRAIN_SIZE =  570338#@param {type:\"integer\"}\n",
        "VAL_SIZE =  142585#@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTOLQ8QpDd6O"
      },
      "source": [
        "## Every domain specific corpus has 570338 lines\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt) > /content/data/cs/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt) > /content/data/cs/dev.txt\n",
        "\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt) > /content/data/biomed/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt) > /content/data/biomed/dev.txt\n",
        "\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt) > /content/data/review/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt) > /content/data/review/dev.txt\n",
        "\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt) > /content/data/news/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt) > /content/data/news/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtV6vxRtEKds"
      },
      "source": [
        "## 2 Train tokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "corpus = '/content/drive/MyDrive/RoBERTa_base_data/combined_corpus.txt'\n",
        "tokenizer = ByteLevelBPETokenizer\n",
        "tokenizer.train(files=corpus,\n",
        "                vocab_size=50265,\n",
        "                min_frequency=2,\n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "if not os.path.isdir('/content/models/roberta/'):\n",
        "    !mkdir -p \"/content/models/roberta\"\n",
        "tokenizer.save('/content/models/roberta/tokenizer.json', pretty=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8ba6RJJEKbz"
      },
      "source": [
        "## 2.1 Copy Trained Tokenizer\n",
        "import os\n",
        "if not os.path.isdir('/content/models/roberta'):\n",
        "    !mkdir -p \"/content/models/roberta\"\n",
        "!cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO-v-oLHFOMT",
        "outputId": "6d207ca8-fe06-4db8-8fc0-530c313809f6"
      },
      "source": [
        "## 3 run_language_model.py\n",
        "%%writefile run_lm.py\n",
        "\n",
        "### THIS FILE IS COPIED FROM THE HUGGINGFACE REPOSITORY FOR CONVENIENCE.\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm.auto import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "\n",
        "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size) + \"_\" + filename\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "            for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"wb\") as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)\n",
        "\n",
        "\n",
        "class LineByLineTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "        # Here, we do not cache the features, operating under the assumption\n",
        "        # that we will soon use fast multithreaded tokenizers from the\n",
        "        # `tokenizers` repo everywhere =)\n",
        "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return LineByLineTextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
        "    else:\n",
        "        return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
        "    ordering_and_checkpoint_path = []\n",
        "\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
        "\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    return checkpoints_sorted\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def mask_tokens(inputs: torch.Tensor, tokenizer: PreTrainedTokenizer, args) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "\n",
        "    if tokenizer.mask_token is None:\n",
        "        raise ValueError(\n",
        "            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "        )\n",
        "\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
        "    special_tokens_mask = [\n",
        "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "    ]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
        "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate\n",
        "    )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if (\n",
        "        args.model_name_or_path\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
        "        # try:\n",
        "        #     # set global_step to gobal_step of last saved checkpoint from model path\n",
        "        #     checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "        #     global_step = int(checkpoint_suffix)\n",
        "        #     epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "        #     steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            # steps_trained_in_current_epoch = 0\n",
        "\n",
        "        #     logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "        #     logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "        #     logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "        #     logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        # except ValueError:\n",
        "        #     logger.info(\"  Starting fine-tuning.\")\n",
        "        checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "        global_step = int(checkpoint_suffix)\n",
        "        epochs_trained = 0\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for epoch in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "        if args.local_rank != -1:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            \n",
        "            # Skip past any already trained steps if resuming training\n",
        "            # if steps_trained_in_current_epoch > 0:\n",
        "            #     steps_trained_in_current_epoch -= 1\n",
        "            #     continue\n",
        "\n",
        "            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                # GPU:\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                    os.makedirs(output_dir, exist_ok=True)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prefix=\"\") -> Dict:\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir, exist_ok=True)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate\n",
        "    )\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "    print(\"Eval loss = \" + str(eval_loss))\n",
        "\n",
        "    result = {\"perplexity\": perplexity}\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--train_data_file\", default=None, type=str, required=True, help=\"The input training data file (a text file).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_type\", type=str, required=True, help=\"The model architecture to be trained or fine-tuned.\",\n",
        "    )\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--eval_data_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--line_by_line\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--should_continue\", action=\"store_true\", help=\"Whether to continue from latest checkpoint in output_dir\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--mlm\", action=\"store_true\", help=\"Train with masked-language modeling loss instead of language modeling.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mlm_probability\", type=float, default=0.15, help=\"Ratio of tokens to mask for masked language modeling loss\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--config_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional directory to store the pre-trained models downloaded from s3 (instead of the default one)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--block_size\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"Optional input sequence length after tokenization.\"\n",
        "        \"The training dataset will be truncated in block of this size for training.\"\n",
        "        \"Default to the model max input length for single sentence inputs (take into account special tokens).\",\n",
        "    )\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=1.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--save_total_limit\",\n",
        "        type=int,\n",
        "        default=None,\n",
        "        help=\"Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not args.mlm:\n",
        "        raise ValueError(\n",
        "            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
        "            \"flag (masked language modeling).\"\n",
        "        )\n",
        "    if args.eval_data_file is None and args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "    # if args.should_continue:\n",
        "    sorted_checkpoints = _sorted_checkpoints(args)\n",
        "    if len(sorted_checkpoints) == 0:\n",
        "        raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
        "    else:\n",
        "        args.model_name_or_path = sorted_checkpoints[-1]\n",
        "        print(\"Load checkpoint from \" + args.model_name_or_path)\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "        and not args.should_continue\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        # try: ## Setup tpu\n",
        "        #     device = xm.xla_device()\n",
        "        #     args.n_gpu = 1\n",
        "        # except:\n",
        "        #     device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        #     args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    if args.config_name:\n",
        "        config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    elif args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
        "    else:\n",
        "        # When we release a pip version exposing CONFIG_MAPPING,\n",
        "        # we can do `config = CONFIG_MAPPING[args.model_type]()`.\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --config_name\"\n",
        "        )\n",
        "\n",
        "    if args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    elif args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --tokenizer_name\"\n",
        "        )\n",
        "\n",
        "    if args.block_size <= 0:\n",
        "        # args.block_size = tokenizer.max_len\n",
        "        args.block_size = 512\n",
        "        # Our input block size will be the max possible for the model\n",
        "    else:\n",
        "        # args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "        args.block_size = min(args.block_size, 512)\n",
        "\n",
        "    if args.model_name_or_path:\n",
        "        model = AutoModelWithLMHead.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            config=config,\n",
        "            cache_dir=args.cache_dir,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        # Create output directory if needed\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_lm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRaGaTJ9Ddqm"
      },
      "source": [
        "## 4 Config\n",
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxAfm1pEGbM_"
      },
      "source": [
        "## 5 Model Parameters\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"/content/models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"/content/models/roberta/output_base\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"/content/data/train.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"/content/data/dev.txt\" #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx0xKb3SEKhl"
      },
      "source": [
        "## 6 Base Command\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 8 \\\n",
        "    --per_gpu_eval_batch_size 8 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 100 \\\n",
        "    --logging_steps 5 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/base_output_5k.txt\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_S4-1JTEKZJ"
      },
      "source": [
        "## 7 Training Parameters\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"num_train_epochs\": 1.0,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUHIxOZy9gQA",
        "outputId": "00d6c361-d36e-4273-9054-c0dcaccf5c9b"
      },
      "source": [
        "## Setup TPU\n",
        "import tensorflow as tf\n",
        "\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(tf.config.list_logical_devices(\"TPU\"))\n",
        "\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "\n",
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.94.29.114:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.94.29.114:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.94.29.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.94.29.114:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n",
            "Collecting torch-xla==1.9\n",
            "  Using cached https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl (149.9 MB)\n",
            "Requirement already satisfied: cloud-tpu-client==0.10 in /usr/local/lib/python3.7/dist-packages (0.10)\n",
            "Requirement already satisfied: google-api-python-client==1.8.0 in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (1.8.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.34.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.26.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.53.0)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2018.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.4.7)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH0KeNucNucr"
      },
      "source": [
        "# Copy last trained model to MODEL_DIR\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base5/models /content/\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base5/runs /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cupj9DJJOjut"
      },
      "source": [
        "!cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ky5K-BjmWSdJ"
      },
      "source": [
        "!rm -r /content/models/roberta/output_base/checkpoint-163600\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-163700\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-163800\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-163900\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164000\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164100\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164200\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164300\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164400\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164500\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164600\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164700\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164800\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-164900\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165000\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165100\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165200\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165300\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165400\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165500\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165600\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165700\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165800\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-165900\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166000\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166100\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166200\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166300\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166400\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166500\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166600\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166700\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166800\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-166900\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-167000\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-167100\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-167200\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-167300\n",
        "!rm -r /content/models/roberta/output_base/checkpoint-167400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbkYvxBDcemK"
      },
      "source": [
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-163600\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-163700\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-163800\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-163900\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164000\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164100\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164200\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164300\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164400\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164500\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164600\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164700\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164800\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-164900\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165000\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165100\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165200\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165300\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165400\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165500\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165600\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165700\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165800\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-165900\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166000\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166100\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166200\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166300\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166400\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166500\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166600\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166700\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166800\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-166900\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-167000\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-167100\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-167200\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-167300\n",
        "!rm -r /content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base1/models/roberta/output_base/checkpoint-167400\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XfXFM8RXGkgL",
        "outputId": "594b72fc-bd78-4c98-aba1-5e47be09c15e"
      },
      "source": [
        "## 8.1 Train RoBERTa base\n",
        "\n",
        "for num_corpus in range(6, 15):\n",
        "    if num_corpus < 10:\n",
        "        print(\"__________START \" + f\"train0{num_corpus}__________\")\n",
        "        train_file = f\"train0{num_corpus}.txt\"\n",
        "        dev_file = f\"dev0{num_corpus}.txt\"\n",
        "    else:\n",
        "        print(\"__________START \" + f\"train{num_corpus}__________\")\n",
        "        train_file = f\"train{num_corpus}.txt\"\n",
        "        dev_file = f\"dev{num_corpus}.txt\"\n",
        "    \n",
        "    # Model Parameters\n",
        "    MODEL_TYPE = \"roberta\"\n",
        "    MODEL_DIR = \"/content/models/roberta\"\n",
        "    OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "    TRAIN_PATH = \"/content/drive/MyDrive/Adaptive_pretrain/data/\"+train_file\n",
        "    EVAL_PATH = \"/content/drive/MyDrive/Adaptive_pretrain/data/\"+dev_file\n",
        "\n",
        "    train_params = {\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "        \"model_type\": MODEL_TYPE,\n",
        "        \"config_name\": MODEL_DIR,\n",
        "        \"tokenizer_name\": MODEL_DIR,\n",
        "        \"train_path\": TRAIN_PATH,\n",
        "        \"eval_path\": EVAL_PATH,\n",
        "        \"num_train_epochs\": 1.0,\n",
        "        \"do_eval\": \"--do_eval\",\n",
        "        \"evaluate_during_training\": \"\",\n",
        "        \"line_by_line\": \"--line_by_line\",\n",
        "        \"should_continue\": \"\",\n",
        "        \"model_name_or_path\": \"\",\n",
        "    }\n",
        "\n",
        "    # if num_corpus != 0:\n",
        "    #     train_params['should_continue'] = \"--should_continue\"\n",
        "    \n",
        "    !{cmd.format(**train_params)}\n",
        "\n",
        "    save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_5k_base{num_corpus}/'\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    !cp -R /content/models \"$save_dir\"\n",
        "    !cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration:   5%|▌         | 15520/306948 [23:36<6:32:05, 12.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15522/306948 [23:37<6:36:32, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15524/306948 [23:37<7:27:31, 10.85it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15526/306948 [23:37<6:35:10, 12.29it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15528/306948 [23:37<6:22:54, 12.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15530/306948 [23:37<6:06:17, 13.26it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15532/306948 [23:37<6:51:17, 11.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15534/306948 [23:38<6:30:42, 12.43it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15536/306948 [23:38<6:52:14, 11.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15538/306948 [23:38<6:14:31, 12.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15540/306948 [23:38<6:42:44, 12.06it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15542/306948 [23:38<6:19:39, 12.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15544/306948 [23:38<6:37:13, 12.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15546/306948 [23:38<6:01:13, 13.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15548/306948 [23:39<6:23:18, 12.67it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15550/306948 [23:39<6:46:04, 11.96it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15552/306948 [23:39<6:27:52, 12.52it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15554/306948 [23:39<6:12:36, 13.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15556/306948 [23:39<6:19:54, 12.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15558/306948 [23:39<6:08:59, 13.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15560/306948 [23:40<7:16:28, 11.13it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15562/306948 [23:40<6:50:24, 11.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15564/306948 [23:40<7:07:59, 11.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15566/306948 [23:40<6:31:43, 12.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15568/306948 [23:40<6:28:31, 12.50it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15570/306948 [23:40<6:20:40, 12.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15572/306948 [23:41<6:25:43, 12.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15574/306948 [23:41<6:22:48, 12.69it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15576/306948 [23:41<6:20:03, 12.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15578/306948 [23:41<6:44:49, 12.00it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15580/306948 [23:41<6:43:45, 12.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15582/306948 [23:41<6:21:55, 12.71it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15584/306948 [23:42<6:29:25, 12.47it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15586/306948 [23:42<6:15:19, 12.94it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15588/306948 [23:42<6:14:40, 12.96it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15590/306948 [23:42<6:11:16, 13.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15592/306948 [23:42<6:18:12, 12.84it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15594/306948 [23:42<5:52:10, 13.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15596/306948 [23:42<5:57:37, 13.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15598/306948 [23:43<5:53:03, 13.75it/s]\u001b[A09/17/2021 05:56:49 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615000\n",
            "09/17/2021 05:56:49 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614000] due to args.save_total_limit\n",
            "09/17/2021 05:56:53 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615000\n",
            "\n",
            "Iteration:   5%|▌         | 15600/306948 [23:48<67:24:28,  1.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15602/306948 [23:48<48:51:05,  1.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15604/306948 [23:48<36:11:28,  2.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15606/306948 [23:48<27:10:54,  2.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15608/306948 [23:48<21:01:29,  3.85it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15610/306948 [23:49<16:16:19,  4.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15612/306948 [23:49<13:55:52,  5.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15614/306948 [23:49<11:35:18,  6.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15616/306948 [23:49<10:05:58,  8.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15618/306948 [23:49<8:50:24,  9.15it/s] \u001b[A\n",
            "Iteration:   5%|▌         | 15620/306948 [23:49<8:06:35,  9.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15622/306948 [23:50<7:26:19, 10.88it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15624/306948 [23:50<7:01:44, 11.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15626/306948 [23:50<6:41:18, 12.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15628/306948 [23:50<6:45:15, 11.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15630/306948 [23:50<6:24:25, 12.63it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15632/306948 [23:50<6:30:25, 12.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15634/306948 [23:50<5:59:18, 13.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15636/306948 [23:51<6:46:46, 11.94it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15638/306948 [23:51<6:26:17, 12.57it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15640/306948 [23:51<6:29:20, 12.47it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15642/306948 [23:51<6:15:18, 12.94it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15644/306948 [23:51<6:31:51, 12.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15646/306948 [23:51<6:14:24, 12.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15648/306948 [23:52<6:11:18, 13.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15650/306948 [23:52<6:05:03, 13.30it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15652/306948 [23:52<6:11:59, 13.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15654/306948 [23:52<5:47:17, 13.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15656/306948 [23:52<5:52:03, 13.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15658/306948 [23:52<6:10:46, 13.09it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15660/306948 [23:52<6:23:21, 12.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15662/306948 [23:53<6:03:01, 13.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15664/306948 [23:53<6:32:20, 12.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15666/306948 [23:53<6:16:51, 12.88it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15668/306948 [23:53<6:03:17, 13.36it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15670/306948 [23:53<6:07:03, 13.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15672/306948 [23:53<6:11:20, 13.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15674/306948 [23:53<6:09:16, 13.15it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15676/306948 [23:54<6:36:08, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15678/306948 [23:54<6:17:23, 12.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15680/306948 [23:54<6:21:47, 12.71it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15682/306948 [23:54<6:35:48, 12.26it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15684/306948 [23:54<6:25:35, 12.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15686/306948 [23:54<6:12:00, 13.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15688/306948 [23:55<7:13:09, 11.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15690/306948 [23:55<6:29:34, 12.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15692/306948 [23:55<7:03:34, 11.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15694/306948 [23:55<6:29:52, 12.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15696/306948 [23:55<7:14:13, 11.18it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15698/306948 [23:55<6:38:46, 12.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15700/306948 [23:56<6:45:04, 11.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15702/306948 [23:56<7:11:37, 11.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15704/306948 [23:56<6:41:23, 12.09it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15706/306948 [23:56<6:15:51, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15708/306948 [23:56<6:39:34, 12.15it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15710/306948 [23:56<6:42:48, 12.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15712/306948 [23:57<6:26:42, 12.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15714/306948 [23:57<6:32:32, 12.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15716/306948 [23:57<6:38:04, 12.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15718/306948 [23:57<6:35:14, 12.28it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15720/306948 [23:57<6:40:45, 12.11it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15722/306948 [23:57<6:25:47, 12.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15724/306948 [23:58<6:39:40, 12.14it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15726/306948 [23:58<6:01:10, 13.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15728/306948 [23:58<6:37:19, 12.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15730/306948 [23:58<5:58:39, 13.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15732/306948 [23:58<6:24:00, 12.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15734/306948 [23:58<5:47:24, 13.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15736/306948 [23:59<6:28:08, 12.50it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15738/306948 [23:59<7:00:19, 11.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15740/306948 [23:59<7:30:55, 10.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15742/306948 [23:59<7:02:45, 11.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15744/306948 [23:59<6:58:05, 11.61it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15746/306948 [23:59<6:36:03, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15748/306948 [24:00<7:06:36, 11.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15750/306948 [24:00<6:22:00, 12.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15752/306948 [24:00<6:38:45, 12.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15754/306948 [24:00<6:20:45, 12.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15756/306948 [24:00<6:44:48, 11.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15758/306948 [24:00<6:22:02, 12.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15760/306948 [24:01<6:22:52, 12.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15762/306948 [24:01<6:24:30, 12.62it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15764/306948 [24:01<6:33:20, 12.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15766/306948 [24:01<6:07:51, 13.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15768/306948 [24:01<6:25:52, 12.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15770/306948 [24:01<6:01:10, 13.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15772/306948 [24:01<6:12:43, 13.02it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15774/306948 [24:02<6:08:59, 13.15it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15776/306948 [24:02<6:05:49, 13.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15778/306948 [24:02<5:53:23, 13.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15780/306948 [24:02<6:04:49, 13.30it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15782/306948 [24:02<6:29:48, 12.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15784/306948 [24:02<6:27:11, 12.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15786/306948 [24:03<6:20:45, 12.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15788/306948 [24:03<6:07:44, 13.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15790/306948 [24:03<6:15:48, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15792/306948 [24:03<7:04:21, 11.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15794/306948 [24:03<6:54:38, 11.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15796/306948 [24:03<6:28:29, 12.49it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15798/306948 [24:03<5:57:23, 13.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15800/306948 [24:04<6:17:48, 12.84it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15802/306948 [24:04<6:03:39, 13.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15804/306948 [24:04<6:19:50, 12.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15806/306948 [24:04<6:10:52, 13.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15808/306948 [24:04<6:11:16, 13.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15810/306948 [24:04<6:45:17, 11.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15812/306948 [24:05<7:18:10, 11.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15814/306948 [24:05<6:47:45, 11.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15816/306948 [24:05<7:30:23, 10.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15818/306948 [24:05<7:29:25, 10.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15820/306948 [24:05<7:20:13, 11.02it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15822/306948 [24:06<6:54:33, 11.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15824/306948 [24:06<7:12:55, 11.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15826/306948 [24:06<6:48:34, 11.88it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15828/306948 [24:06<6:58:40, 11.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15830/306948 [24:06<6:52:22, 11.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15832/306948 [24:06<7:11:21, 11.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15834/306948 [24:07<7:05:36, 11.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15836/306948 [24:07<6:49:58, 11.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15838/306948 [24:07<6:34:50, 12.29it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15840/306948 [24:07<6:33:26, 12.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15842/306948 [24:07<6:10:20, 13.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15844/306948 [24:07<6:48:57, 11.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15846/306948 [24:08<6:34:50, 12.29it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15848/306948 [24:08<6:54:38, 11.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15850/306948 [24:08<6:25:43, 12.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15852/306948 [24:08<6:02:01, 13.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15854/306948 [24:08<6:14:03, 12.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15856/306948 [24:08<6:40:50, 12.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15858/306948 [24:08<6:21:44, 12.71it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15860/306948 [24:09<6:20:27, 12.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15862/306948 [24:09<6:18:12, 12.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15864/306948 [24:09<6:40:23, 12.12it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15866/306948 [24:09<7:12:09, 11.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15868/306948 [24:09<7:44:17, 10.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15870/306948 [24:10<7:04:09, 11.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15872/306948 [24:10<6:52:16, 11.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15874/306948 [24:10<6:13:24, 12.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15876/306948 [24:10<6:15:53, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15878/306948 [24:10<6:04:59, 13.29it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15880/306948 [24:10<6:08:17, 13.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15882/306948 [24:10<5:47:12, 13.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15884/306948 [24:11<6:19:41, 12.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15886/306948 [24:11<6:04:28, 13.31it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15888/306948 [24:11<6:17:10, 12.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15890/306948 [24:11<5:52:37, 13.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15892/306948 [24:11<6:23:41, 12.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15894/306948 [24:11<6:08:36, 13.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15896/306948 [24:12<8:10:40,  9.89it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15898/306948 [24:12<7:54:52, 10.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15900/306948 [24:12<7:09:13, 11.30it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15902/306948 [24:12<6:27:36, 12.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15904/306948 [24:12<6:17:02, 12.87it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15906/306948 [24:12<5:55:59, 13.63it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15908/306948 [24:12<5:47:02, 13.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15910/306948 [24:13<6:04:19, 13.31it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15912/306948 [24:13<5:54:26, 13.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15914/306948 [24:13<5:37:19, 14.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15916/306948 [24:13<5:42:30, 14.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15918/306948 [24:13<5:58:51, 13.52it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15920/306948 [24:13<6:00:46, 13.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15922/306948 [24:14<6:39:58, 12.13it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15924/306948 [24:14<6:30:57, 12.41it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15926/306948 [24:14<5:54:43, 13.67it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15928/306948 [24:14<6:35:21, 12.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15930/306948 [24:14<6:12:37, 13.02it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15932/306948 [24:14<6:07:31, 13.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15934/306948 [24:14<5:38:21, 14.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15936/306948 [24:15<6:16:03, 12.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15938/306948 [24:15<6:19:00, 12.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15940/306948 [24:15<6:32:55, 12.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15942/306948 [24:15<6:06:45, 13.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15944/306948 [24:15<6:20:49, 12.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15946/306948 [24:15<6:09:51, 13.11it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15948/306948 [24:16<6:17:52, 12.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15950/306948 [24:16<6:08:01, 13.18it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15952/306948 [24:16<6:35:48, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15954/306948 [24:16<6:37:44, 12.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15956/306948 [24:16<6:52:42, 11.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15958/306948 [24:16<6:39:46, 12.13it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15960/306948 [24:17<6:53:39, 11.72it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15962/306948 [24:17<6:47:02, 11.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15964/306948 [24:17<6:23:49, 12.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15966/306948 [24:17<6:07:59, 13.18it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15968/306948 [24:17<6:38:13, 12.18it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15970/306948 [24:17<6:06:39, 13.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15972/306948 [24:17<6:07:23, 13.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15974/306948 [24:18<6:16:47, 12.87it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15976/306948 [24:18<6:32:46, 12.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15978/306948 [24:18<6:44:26, 11.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15980/306948 [24:18<6:58:49, 11.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15982/306948 [24:18<6:39:05, 12.15it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15984/306948 [24:18<6:15:54, 12.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15986/306948 [24:19<6:07:08, 13.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15988/306948 [24:19<6:50:04, 11.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15990/306948 [24:19<6:16:48, 12.87it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15992/306948 [24:19<6:22:17, 12.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15994/306948 [24:19<6:28:03, 12.50it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15996/306948 [24:19<6:30:46, 12.41it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 15998/306948 [24:20<6:05:06, 13.28it/s]\u001b[A09/17/2021 05:57:26 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615100\n",
            "09/17/2021 05:57:26 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614100] due to args.save_total_limit\n",
            "09/17/2021 05:57:30 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615100\n",
            "\n",
            "Iteration:   5%|▌         | 16000/306948 [24:25<66:23:00,  1.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16002/306948 [24:25<48:10:31,  1.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16004/306948 [24:25<35:47:09,  2.26it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16006/306948 [24:25<27:14:50,  2.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16008/306948 [24:25<21:10:56,  3.82it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16010/306948 [24:26<16:34:32,  4.88it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16012/306948 [24:26<13:51:14,  5.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16014/306948 [24:26<11:09:54,  7.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16016/306948 [24:26<9:46:56,  8.26it/s] \u001b[A\n",
            "Iteration:   5%|▌         | 16018/306948 [24:26<9:10:06,  8.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16020/306948 [24:26<8:24:33,  9.61it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16022/306948 [24:26<7:46:54, 10.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16024/306948 [24:27<7:26:56, 10.85it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16026/306948 [24:27<7:00:21, 11.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16028/306948 [24:27<6:48:43, 11.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16030/306948 [24:27<6:18:08, 12.82it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16032/306948 [24:27<6:26:20, 12.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16034/306948 [24:27<6:09:34, 13.12it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16036/306948 [24:28<5:55:08, 13.65it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16038/306948 [24:28<5:58:52, 13.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16040/306948 [24:28<6:13:06, 12.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16042/306948 [24:28<5:52:54, 13.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16044/306948 [24:28<5:52:57, 13.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16046/306948 [24:28<5:52:30, 13.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16048/306948 [24:28<6:03:10, 13.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16050/306948 [24:29<6:02:19, 13.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16052/306948 [24:29<6:12:41, 13.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16054/306948 [24:29<5:57:39, 13.56it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16056/306948 [24:29<6:56:18, 11.65it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16058/306948 [24:29<6:33:16, 12.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16060/306948 [24:29<6:32:15, 12.36it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16062/306948 [24:30<6:19:05, 12.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16064/306948 [24:30<6:10:18, 13.09it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16066/306948 [24:30<5:49:27, 13.87it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16068/306948 [24:30<5:49:51, 13.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16070/306948 [24:30<5:50:18, 13.84it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16072/306948 [24:30<5:46:01, 14.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16074/306948 [24:30<5:51:03, 13.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16076/306948 [24:31<6:00:47, 13.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16078/306948 [24:31<5:40:12, 14.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16080/306948 [24:31<6:18:45, 12.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16082/306948 [24:31<5:56:33, 13.60it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16084/306948 [24:31<6:25:36, 12.57it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16086/306948 [24:31<6:11:54, 13.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16088/306948 [24:31<6:28:54, 12.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16090/306948 [24:32<6:01:09, 13.42it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16092/306948 [24:32<6:03:55, 13.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16094/306948 [24:32<6:16:36, 12.87it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16096/306948 [24:32<5:56:07, 13.61it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16098/306948 [24:32<5:46:26, 13.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16100/306948 [24:32<6:21:47, 12.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16102/306948 [24:33<6:15:29, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16104/306948 [24:33<6:28:13, 12.49it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16106/306948 [24:33<6:03:10, 13.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16108/306948 [24:33<6:06:23, 13.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16110/306948 [24:33<5:56:39, 13.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16112/306948 [24:33<6:10:42, 13.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16114/306948 [24:33<5:45:20, 14.04it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16116/306948 [24:34<5:51:15, 13.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16118/306948 [24:34<5:29:05, 14.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16120/306948 [24:34<6:01:37, 13.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16122/306948 [24:34<6:01:56, 13.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16124/306948 [24:34<5:58:18, 13.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16126/306948 [24:34<5:43:42, 14.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16128/306948 [24:34<5:48:11, 13.92it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16130/306948 [24:35<5:57:13, 13.57it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16132/306948 [24:35<6:15:23, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16134/306948 [24:35<6:09:46, 13.11it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16136/306948 [24:35<6:52:12, 11.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16138/306948 [24:35<6:59:54, 11.54it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16140/306948 [24:35<6:52:51, 11.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16142/306948 [24:36<6:23:24, 12.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16144/306948 [24:36<6:41:41, 12.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16146/306948 [24:36<6:26:29, 12.54it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16148/306948 [24:36<6:31:24, 12.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16150/306948 [24:36<6:49:48, 11.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16152/306948 [24:36<6:51:37, 11.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16154/306948 [24:37<6:19:55, 12.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16156/306948 [24:37<6:35:58, 12.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16158/306948 [24:37<6:15:49, 12.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16160/306948 [24:37<6:35:42, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16162/306948 [24:37<6:14:13, 12.95it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16164/306948 [24:37<6:02:06, 13.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16166/306948 [24:38<6:27:18, 12.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16168/306948 [24:38<6:11:34, 13.04it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16170/306948 [24:38<6:07:08, 13.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16172/306948 [24:38<6:15:35, 12.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16174/306948 [24:38<5:57:43, 13.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16176/306948 [24:38<6:04:56, 13.28it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16178/306948 [24:38<6:03:14, 13.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16180/306948 [24:39<6:11:58, 13.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16182/306948 [24:39<6:06:10, 13.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16184/306948 [24:39<6:31:32, 12.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16186/306948 [24:39<6:14:09, 12.95it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16188/306948 [24:39<6:45:34, 11.95it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16190/306948 [24:39<6:42:36, 12.04it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16192/306948 [24:40<6:36:12, 12.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16194/306948 [24:40<6:04:55, 13.28it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16196/306948 [24:40<6:32:27, 12.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16198/306948 [24:40<6:32:10, 12.36it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16200/306948 [24:40<6:55:45, 11.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16202/306948 [24:40<6:20:46, 12.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16204/306948 [24:41<6:42:56, 12.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16206/306948 [24:41<6:11:05, 13.06it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16208/306948 [24:41<6:35:08, 12.26it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16210/306948 [24:41<6:03:50, 13.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16212/306948 [24:41<6:49:48, 11.82it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16214/306948 [24:41<6:36:25, 12.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16216/306948 [24:41<6:25:22, 12.57it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16218/306948 [24:42<6:00:09, 13.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16220/306948 [24:42<6:00:55, 13.43it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16222/306948 [24:42<5:39:03, 14.29it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16224/306948 [24:42<5:37:54, 14.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16226/306948 [24:42<5:37:43, 14.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16228/306948 [24:42<5:50:59, 13.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16230/306948 [24:42<5:40:20, 14.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16232/306948 [24:43<5:41:15, 14.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16234/306948 [24:43<5:40:30, 14.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16236/306948 [24:43<5:40:47, 14.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16238/306948 [24:43<5:43:46, 14.09it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16240/306948 [24:43<6:12:17, 13.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16242/306948 [24:43<6:11:58, 13.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16244/306948 [24:44<7:08:04, 11.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16246/306948 [24:44<6:49:41, 11.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16248/306948 [24:44<6:27:22, 12.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16250/306948 [24:44<6:36:54, 12.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16252/306948 [24:44<6:40:54, 12.09it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16254/306948 [24:44<6:08:16, 13.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16256/306948 [24:44<6:06:28, 13.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16258/306948 [24:45<6:24:29, 12.60it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16260/306948 [24:45<6:48:57, 11.85it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16262/306948 [24:45<6:29:29, 12.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16264/306948 [24:45<6:46:50, 11.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16266/306948 [24:45<6:28:11, 12.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16268/306948 [24:45<6:23:36, 12.63it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16270/306948 [24:46<6:13:36, 12.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16272/306948 [24:46<6:35:36, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16274/306948 [24:46<6:28:10, 12.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16276/306948 [24:46<6:39:55, 12.11it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16278/306948 [24:46<6:20:16, 12.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16280/306948 [24:46<6:32:01, 12.36it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16282/306948 [24:47<6:18:09, 12.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16284/306948 [24:47<6:04:49, 13.28it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16286/306948 [24:47<5:48:49, 13.89it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16288/306948 [24:47<5:56:23, 13.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16290/306948 [24:47<5:38:00, 14.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16292/306948 [24:47<6:17:08, 12.84it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16295/306948 [24:47<5:29:59, 14.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16297/306948 [24:48<6:07:52, 13.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16299/306948 [24:48<5:49:42, 13.85it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16301/306948 [24:48<5:55:00, 13.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16303/306948 [24:48<5:36:54, 14.38it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16305/306948 [24:48<5:58:08, 13.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16307/306948 [24:48<5:52:45, 13.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16309/306948 [24:49<6:24:43, 12.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16311/306948 [24:49<6:43:10, 12.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16313/306948 [24:49<6:33:13, 12.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16315/306948 [24:49<6:40:09, 12.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16317/306948 [24:49<6:36:34, 12.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16319/306948 [24:49<6:38:51, 12.14it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16321/306948 [24:50<6:44:31, 11.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16323/306948 [24:50<6:22:06, 12.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16325/306948 [24:50<6:44:47, 11.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16327/306948 [24:50<6:45:11, 11.95it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16329/306948 [24:50<6:48:47, 11.85it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16331/306948 [24:50<6:24:01, 12.61it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16333/306948 [24:51<6:53:02, 11.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16335/306948 [24:51<6:50:32, 11.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16337/306948 [24:51<6:45:28, 11.95it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16339/306948 [24:51<6:34:26, 12.28it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16341/306948 [24:51<6:27:44, 12.49it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16343/306948 [24:51<6:51:50, 11.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16345/306948 [24:52<6:38:23, 12.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16347/306948 [24:52<6:37:22, 12.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16349/306948 [24:52<6:48:23, 11.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16351/306948 [24:52<6:30:03, 12.42it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16353/306948 [24:52<6:41:13, 12.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16355/306948 [24:52<6:29:27, 12.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16357/306948 [24:53<6:33:52, 12.30it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16359/306948 [24:53<6:05:11, 13.26it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16361/306948 [24:53<5:57:54, 13.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16363/306948 [24:53<5:33:43, 14.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16365/306948 [24:53<5:52:28, 13.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16367/306948 [24:53<6:24:19, 12.60it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16369/306948 [24:53<6:36:57, 12.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16371/306948 [24:54<6:05:41, 13.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16373/306948 [24:54<6:19:42, 12.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16375/306948 [24:54<6:21:34, 12.69it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16377/306948 [24:54<6:11:05, 13.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16379/306948 [24:54<5:56:47, 13.57it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16381/306948 [24:54<5:54:24, 13.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16383/306948 [24:54<6:04:14, 13.30it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16385/306948 [24:55<5:59:32, 13.47it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16387/306948 [24:55<6:27:53, 12.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16389/306948 [24:55<6:11:15, 13.04it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16391/306948 [24:55<5:59:41, 13.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16393/306948 [24:55<6:21:46, 12.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16395/306948 [24:55<6:14:59, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16397/306948 [24:56<6:20:26, 12.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16399/306948 [24:56<6:43:24, 12.00it/s]\u001b[A09/17/2021 05:58:02 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615200\n",
            "09/17/2021 05:58:02 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614200] due to args.save_total_limit\n",
            "09/17/2021 05:58:06 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615200\n",
            "\n",
            "Iteration:   5%|▌         | 16401/306948 [25:01<64:21:46,  1.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16403/306948 [25:01<47:00:05,  1.72it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16405/306948 [25:01<35:01:23,  2.30it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16407/306948 [25:01<26:10:09,  3.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16409/306948 [25:01<20:21:21,  3.96it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16411/306948 [25:02<16:45:52,  4.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16413/306948 [25:02<13:53:37,  5.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16415/306948 [25:02<11:46:20,  6.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16417/306948 [25:02<10:40:20,  7.56it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16419/306948 [25:02<9:16:29,  8.70it/s] \u001b[A\n",
            "Iteration:   5%|▌         | 16421/306948 [25:02<8:31:21,  9.47it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16423/306948 [25:03<7:35:10, 10.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16425/306948 [25:03<8:02:39, 10.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16427/306948 [25:03<7:43:10, 10.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16429/306948 [25:03<7:24:20, 10.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16431/306948 [25:03<6:37:04, 12.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16433/306948 [25:03<6:27:02, 12.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16435/306948 [25:03<6:23:24, 12.63it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16437/306948 [25:04<6:32:50, 12.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16439/306948 [25:04<6:19:17, 12.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16441/306948 [25:04<6:32:47, 12.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16443/306948 [25:04<6:44:30, 11.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16445/306948 [25:04<6:24:55, 12.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16447/306948 [25:04<5:59:37, 13.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16449/306948 [25:05<6:09:06, 13.12it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16451/306948 [25:05<5:56:34, 13.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16453/306948 [25:05<7:16:20, 11.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16455/306948 [25:05<6:57:35, 11.59it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16457/306948 [25:05<6:55:12, 11.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16459/306948 [25:05<6:46:20, 11.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16461/306948 [25:06<7:02:14, 11.47it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16463/306948 [25:06<6:26:55, 12.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16465/306948 [25:06<6:26:22, 12.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16467/306948 [25:06<6:29:15, 12.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16469/306948 [25:06<6:34:33, 12.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16471/306948 [25:06<6:06:59, 13.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16473/306948 [25:07<6:07:54, 13.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16475/306948 [25:07<5:49:11, 13.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16477/306948 [25:07<5:50:45, 13.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16479/306948 [25:07<5:46:38, 13.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16481/306948 [25:07<6:00:24, 13.43it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16483/306948 [25:07<6:09:03, 13.12it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16485/306948 [25:07<5:59:57, 13.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16487/306948 [25:08<5:40:15, 14.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16489/306948 [25:08<6:02:03, 13.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16491/306948 [25:08<5:47:19, 13.94it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16493/306948 [25:08<6:07:02, 13.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16495/306948 [25:08<6:19:02, 12.77it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16497/306948 [25:08<7:26:57, 10.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16499/306948 [25:09<6:58:22, 11.57it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16501/306948 [25:09<6:48:56, 11.84it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16503/306948 [25:09<6:29:25, 12.43it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16505/306948 [25:09<6:41:42, 12.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16507/306948 [25:09<6:26:12, 12.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16509/306948 [25:09<6:19:59, 12.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16511/306948 [25:09<6:05:53, 13.23it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16513/306948 [25:10<6:21:48, 12.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16515/306948 [25:10<6:03:42, 13.31it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16517/306948 [25:10<6:34:30, 12.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16519/306948 [25:10<5:56:29, 13.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16521/306948 [25:10<6:11:28, 13.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16523/306948 [25:10<5:54:48, 13.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16525/306948 [25:11<6:22:43, 12.65it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16527/306948 [25:11<5:58:14, 13.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16529/306948 [25:11<5:51:52, 13.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16531/306948 [25:11<5:39:59, 14.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16533/306948 [25:11<6:01:30, 13.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16535/306948 [25:11<5:42:04, 14.15it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16537/306948 [25:11<5:59:38, 13.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16539/306948 [25:12<6:26:35, 12.52it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16541/306948 [25:12<6:44:04, 11.98it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16543/306948 [25:12<6:09:35, 13.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16545/306948 [25:12<6:53:41, 11.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16547/306948 [25:12<6:10:54, 13.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16549/306948 [25:12<6:14:00, 12.94it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16551/306948 [25:13<5:45:50, 13.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16553/306948 [25:13<5:30:26, 14.65it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16555/306948 [25:13<5:48:02, 13.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16557/306948 [25:13<6:15:34, 12.89it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16559/306948 [25:13<6:22:51, 12.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16561/306948 [25:13<6:05:39, 13.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16563/306948 [25:13<5:52:30, 13.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16565/306948 [25:14<6:14:55, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16567/306948 [25:14<5:53:02, 13.71it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16569/306948 [25:14<6:24:47, 12.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16571/306948 [25:14<5:58:29, 13.50it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16573/306948 [25:14<6:02:11, 13.36it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16575/306948 [25:14<5:55:26, 13.62it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16577/306948 [25:14<5:54:21, 13.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16579/306948 [25:15<6:07:29, 13.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16581/306948 [25:15<6:23:20, 12.62it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16583/306948 [25:15<5:51:45, 13.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16585/306948 [25:15<6:07:22, 13.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16587/306948 [25:15<5:48:46, 13.87it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16589/306948 [25:15<6:02:27, 13.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16591/306948 [25:16<5:52:18, 13.74it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16593/306948 [25:16<6:17:59, 12.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16595/306948 [25:16<6:14:18, 12.93it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16597/306948 [25:16<6:34:21, 12.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16599/306948 [25:16<6:46:15, 11.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16601/306948 [25:16<7:14:10, 11.15it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16603/306948 [25:17<7:02:30, 11.45it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16605/306948 [25:17<6:49:08, 11.83it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16607/306948 [25:17<6:26:14, 12.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16609/306948 [25:17<6:35:53, 12.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16611/306948 [25:17<6:13:23, 12.96it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16613/306948 [25:17<6:23:47, 12.61it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16615/306948 [25:17<6:06:28, 13.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16617/306948 [25:18<6:44:43, 11.96it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16619/306948 [25:18<6:45:37, 11.93it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16621/306948 [25:18<7:03:36, 11.42it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16623/306948 [25:18<6:31:58, 12.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16625/306948 [25:18<6:26:00, 12.54it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16627/306948 [25:18<6:12:24, 12.99it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16629/306948 [25:19<6:18:12, 12.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16631/306948 [25:19<6:07:16, 13.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16633/306948 [25:19<5:58:57, 13.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16635/306948 [25:19<5:35:10, 14.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16637/306948 [25:19<6:04:45, 13.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16639/306948 [25:19<6:03:16, 13.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16641/306948 [25:20<6:08:40, 13.12it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16643/306948 [25:20<6:14:23, 12.92it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16645/306948 [25:20<6:59:26, 11.54it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16647/306948 [25:20<6:38:49, 12.13it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16649/306948 [25:20<7:23:15, 10.92it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16651/306948 [25:20<6:26:29, 12.52it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16653/306948 [25:21<6:42:49, 12.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16655/306948 [25:21<6:09:12, 13.10it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16657/306948 [25:21<6:18:27, 12.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16659/306948 [25:21<5:50:53, 13.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16661/306948 [25:21<6:20:43, 12.71it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16663/306948 [25:21<6:17:33, 12.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16665/306948 [25:21<6:30:09, 12.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16667/306948 [25:22<6:49:23, 11.82it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16669/306948 [25:22<6:36:18, 12.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16671/306948 [25:22<6:27:11, 12.50it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16673/306948 [25:22<6:30:09, 12.40it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16675/306948 [25:22<6:11:48, 13.01it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16677/306948 [25:22<6:10:12, 13.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16679/306948 [25:23<7:12:06, 11.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16681/306948 [25:23<6:40:38, 12.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16683/306948 [25:23<6:50:33, 11.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16685/306948 [25:23<7:10:32, 11.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16687/306948 [25:23<6:36:18, 12.21it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16689/306948 [25:23<6:21:43, 12.67it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16692/306948 [25:24<5:59:30, 13.46it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16694/306948 [25:24<6:18:23, 12.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16696/306948 [25:24<7:00:10, 11.51it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16698/306948 [25:24<6:27:54, 12.47it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16700/306948 [25:24<7:07:22, 11.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16702/306948 [25:25<6:50:05, 11.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16704/306948 [25:25<6:36:48, 12.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16706/306948 [25:25<6:44:16, 11.97it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16708/306948 [25:25<6:50:48, 11.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16710/306948 [25:25<6:27:26, 12.48it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16712/306948 [25:25<6:31:45, 12.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16714/306948 [25:25<6:19:22, 12.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16716/306948 [25:26<6:34:16, 12.27it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16718/306948 [25:26<6:36:59, 12.18it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16720/306948 [25:26<6:32:47, 12.31it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16722/306948 [25:26<6:05:17, 13.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16724/306948 [25:26<6:32:29, 12.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16726/306948 [25:26<6:46:33, 11.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16728/306948 [25:27<6:38:29, 12.14it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16730/306948 [25:27<6:54:22, 11.67it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16732/306948 [25:27<6:46:04, 11.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16734/306948 [25:27<6:26:18, 12.52it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16736/306948 [25:27<6:19:06, 12.76it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16738/306948 [25:27<5:54:06, 13.66it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16740/306948 [25:28<6:02:30, 13.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16742/306948 [25:28<6:09:51, 13.08it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16744/306948 [25:28<6:47:42, 11.86it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16747/306948 [25:28<5:50:03, 13.82it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16749/306948 [25:28<6:02:44, 13.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16751/306948 [25:28<6:14:35, 12.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16753/306948 [25:29<6:10:07, 13.07it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16755/306948 [25:29<5:57:44, 13.52it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16757/306948 [25:29<5:52:45, 13.71it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16759/306948 [25:29<6:09:27, 13.09it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16761/306948 [25:29<6:03:16, 13.31it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16763/306948 [25:29<5:48:20, 13.88it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16765/306948 [25:29<5:50:35, 13.80it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16767/306948 [25:30<6:12:06, 13.00it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16769/306948 [25:30<8:33:31,  9.42it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16771/306948 [25:30<7:47:42, 10.34it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16773/306948 [25:30<7:37:05, 10.58it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16775/306948 [25:30<7:12:56, 11.17it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16777/306948 [25:31<7:06:12, 11.35it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16779/306948 [25:31<6:48:34, 11.84it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16781/306948 [25:31<6:44:47, 11.95it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16783/306948 [25:31<6:36:21, 12.20it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16785/306948 [25:31<6:30:19, 12.39it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16787/306948 [25:31<6:18:32, 12.78it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16789/306948 [25:32<6:23:46, 12.60it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16791/306948 [25:32<5:56:58, 13.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16793/306948 [25:32<6:53:16, 11.70it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16795/306948 [25:32<6:42:12, 12.02it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16797/306948 [25:32<6:28:48, 12.44it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16799/306948 [25:32<6:24:55, 12.56it/s]\u001b[A09/17/2021 05:58:39 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615300\n",
            "09/17/2021 05:58:39 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614300] due to args.save_total_limit\n",
            "09/17/2021 05:58:43 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615300\n",
            "\n",
            "Iteration:   5%|▌         | 16801/306948 [25:38<67:41:44,  1.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16803/306948 [25:38<49:20:11,  1.63it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16805/306948 [25:38<36:52:25,  2.19it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16807/306948 [25:38<27:32:09,  2.93it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16809/306948 [25:38<21:09:13,  3.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16811/306948 [25:38<16:50:21,  4.79it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16813/306948 [25:39<14:01:39,  5.75it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16815/306948 [25:39<11:38:25,  6.92it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16817/306948 [25:39<10:00:47,  8.05it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16819/306948 [25:39<9:17:00,  8.68it/s] \u001b[A\n",
            "Iteration:   5%|▌         | 16821/306948 [25:39<8:27:24,  9.53it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16823/306948 [25:39<7:32:42, 10.68it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16825/306948 [25:40<7:44:31, 10.41it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16827/306948 [25:40<7:34:32, 10.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16829/306948 [25:40<7:24:17, 10.88it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16831/306948 [25:40<7:07:26, 11.31it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16833/306948 [25:40<7:05:40, 11.36it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16835/306948 [25:40<6:58:46, 11.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16837/306948 [25:41<6:55:32, 11.64it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16839/306948 [25:41<7:22:16, 10.93it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16841/306948 [25:41<7:27:05, 10.81it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16843/306948 [25:41<6:58:43, 11.55it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16845/306948 [25:41<6:56:36, 11.61it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16847/306948 [25:41<7:09:17, 11.26it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16849/306948 [25:42<6:44:51, 11.94it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16851/306948 [25:42<6:05:36, 13.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16853/306948 [25:42<6:46:06, 11.91it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16855/306948 [25:42<6:32:23, 12.32it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16857/306948 [25:42<6:35:47, 12.22it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16859/306948 [25:42<6:46:08, 11.90it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16861/306948 [25:43<6:37:26, 12.16it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16863/306948 [25:43<6:46:30, 11.89it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16865/306948 [25:43<6:34:48, 12.25it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16867/306948 [25:43<6:08:43, 13.11it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16869/306948 [25:43<6:35:01, 12.24it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16871/306948 [25:43<6:02:42, 13.33it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16873/306948 [25:44<6:52:05, 11.73it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16875/306948 [25:44<6:11:00, 13.03it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16877/306948 [25:44<6:44:15, 11.96it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16879/306948 [25:44<7:05:11, 11.37it/s]\u001b[A\n",
            "Iteration:   5%|▌         | 16881/306948 [25:44<7:02:39, 11.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16883/306948 [25:44<6:29:33, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16885/306948 [25:45<6:30:04, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16887/306948 [25:45<6:08:02, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16889/306948 [25:45<6:14:57, 12.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16891/306948 [25:45<6:12:37, 12.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16893/306948 [25:45<10:10:21,  7.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16895/306948 [25:46<8:41:34,  9.27it/s] \u001b[A\n",
            "Iteration:   6%|▌         | 16897/306948 [25:46<7:57:42, 10.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16899/306948 [25:46<7:41:19, 10.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16901/306948 [25:46<7:00:55, 11.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16903/306948 [25:46<6:54:09, 11.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16905/306948 [25:46<7:08:07, 11.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16907/306948 [25:47<6:30:03, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16909/306948 [25:47<6:37:01, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16911/306948 [25:47<6:46:30, 11.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16913/306948 [25:47<7:04:21, 11.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16915/306948 [25:47<6:42:51, 12.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16917/306948 [25:47<6:49:21, 11.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16919/306948 [25:48<6:11:24, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16921/306948 [25:48<6:15:11, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16923/306948 [25:48<6:07:39, 13.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16925/306948 [25:48<6:31:27, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16927/306948 [25:48<6:14:07, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16929/306948 [25:48<6:39:07, 12.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16931/306948 [25:48<6:00:08, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16933/306948 [25:49<5:51:42, 13.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16935/306948 [25:49<5:41:58, 14.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16937/306948 [25:49<5:46:23, 13.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16939/306948 [25:49<5:32:01, 14.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16941/306948 [25:49<5:59:27, 13.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16943/306948 [25:49<5:59:36, 13.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16945/306948 [25:50<6:12:08, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16947/306948 [25:50<6:11:50, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16949/306948 [25:50<6:18:44, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16951/306948 [25:50<6:06:26, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16953/306948 [25:50<6:17:57, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16955/306948 [25:50<6:04:57, 13.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16957/306948 [25:50<6:18:04, 12.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16959/306948 [25:51<6:26:48, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16961/306948 [25:51<6:44:58, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16963/306948 [25:51<6:09:45, 13.07it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16965/306948 [25:51<6:00:34, 13.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16967/306948 [25:51<6:10:45, 13.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16969/306948 [25:51<6:14:57, 12.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16971/306948 [25:51<5:51:26, 13.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16973/306948 [25:52<6:30:09, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16975/306948 [25:52<6:33:01, 12.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16977/306948 [25:52<7:01:36, 11.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16979/306948 [25:52<6:39:39, 12.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16981/306948 [25:52<6:18:48, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16983/306948 [25:52<5:57:07, 13.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16985/306948 [25:53<6:18:49, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16987/306948 [25:53<5:53:59, 13.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16989/306948 [25:53<6:24:54, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16991/306948 [25:53<6:37:47, 12.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16993/306948 [25:53<6:50:14, 11.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16995/306948 [25:53<6:47:15, 11.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16997/306948 [25:54<6:48:42, 11.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 16999/306948 [25:54<6:32:28, 12.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17001/306948 [25:54<6:34:52, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17003/306948 [25:54<6:31:25, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17005/306948 [25:54<6:27:40, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17007/306948 [25:54<6:07:23, 13.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17009/306948 [25:55<6:25:35, 12.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17011/306948 [25:55<6:19:43, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17013/306948 [25:55<6:21:59, 12.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17015/306948 [25:55<6:16:07, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17017/306948 [25:55<6:13:08, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17019/306948 [25:55<6:29:05, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17021/306948 [25:56<6:41:45, 12.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17023/306948 [25:56<6:25:49, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17025/306948 [25:56<6:55:47, 11.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17027/306948 [25:56<6:52:38, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17029/306948 [25:56<7:19:20, 11.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17031/306948 [25:56<6:44:15, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17033/306948 [25:57<6:40:28, 12.07it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17035/306948 [25:57<6:03:20, 13.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17037/306948 [25:57<6:59:19, 11.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17039/306948 [25:57<6:25:10, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17041/306948 [25:57<6:45:55, 11.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17043/306948 [25:57<6:26:32, 12.50it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17045/306948 [25:58<6:36:08, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17047/306948 [25:58<6:29:41, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17049/306948 [25:58<6:23:58, 12.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17051/306948 [25:58<6:12:57, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17053/306948 [25:58<6:04:14, 13.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17055/306948 [25:58<6:02:50, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17057/306948 [25:58<6:22:54, 12.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17059/306948 [25:59<5:56:11, 13.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17061/306948 [25:59<5:59:55, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17063/306948 [25:59<5:35:24, 14.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17065/306948 [25:59<6:13:19, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17067/306948 [25:59<6:43:33, 11.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17069/306948 [25:59<6:49:14, 11.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17071/306948 [26:00<6:23:52, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17073/306948 [26:00<6:28:53, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17075/306948 [26:00<6:33:15, 12.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17077/306948 [26:00<6:33:54, 12.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17079/306948 [26:00<6:21:40, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17081/306948 [26:00<6:24:21, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17083/306948 [26:00<6:05:31, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17085/306948 [26:01<6:11:04, 13.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17087/306948 [26:01<6:18:50, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17089/306948 [26:01<6:19:31, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17091/306948 [26:01<6:04:18, 13.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17093/306948 [26:01<6:25:18, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17095/306948 [26:01<6:13:52, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17097/306948 [26:02<6:28:54, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17099/306948 [26:02<6:26:40, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17101/306948 [26:02<6:49:22, 11.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17103/306948 [26:02<6:15:50, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17105/306948 [26:02<6:15:55, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17107/306948 [26:02<5:50:44, 13.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17109/306948 [26:03<5:47:52, 13.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17111/306948 [26:03<5:22:02, 15.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17113/306948 [26:03<5:49:09, 13.83it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17115/306948 [26:03<5:39:16, 14.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17117/306948 [26:03<5:48:43, 13.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17119/306948 [26:03<6:33:53, 12.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17121/306948 [26:03<6:39:14, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17123/306948 [26:04<6:10:43, 13.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17125/306948 [26:04<6:13:58, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17127/306948 [26:04<5:58:27, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17129/306948 [26:04<6:49:39, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17131/306948 [26:04<7:02:52, 11.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17133/306948 [26:04<7:08:05, 11.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17135/306948 [26:05<6:52:22, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17137/306948 [26:05<7:21:49, 10.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17139/306948 [26:05<6:58:12, 11.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17141/306948 [26:05<7:22:48, 10.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17143/306948 [26:05<6:43:17, 11.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17145/306948 [26:05<6:49:19, 11.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17147/306948 [26:06<6:31:40, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17149/306948 [26:06<7:22:05, 10.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17151/306948 [26:06<7:10:31, 11.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17153/306948 [26:06<6:53:21, 11.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17155/306948 [26:06<6:15:54, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17157/306948 [26:06<6:19:40, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17159/306948 [26:07<5:57:09, 13.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17161/306948 [26:07<6:00:44, 13.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17163/306948 [26:07<6:11:01, 13.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17165/306948 [26:07<6:23:05, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17167/306948 [26:07<6:38:01, 12.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17169/306948 [26:07<6:42:07, 12.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17171/306948 [26:08<6:47:40, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17173/306948 [26:08<6:32:02, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17175/306948 [26:08<6:09:57, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17177/306948 [26:08<6:17:54, 12.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17179/306948 [26:08<5:57:39, 13.50it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17181/306948 [26:08<6:13:36, 12.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17183/306948 [26:08<6:13:16, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17185/306948 [26:09<6:52:34, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17187/306948 [26:09<6:17:58, 12.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17189/306948 [26:09<6:39:08, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17191/306948 [26:09<6:21:25, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17193/306948 [26:09<6:41:42, 12.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17195/306948 [26:09<6:24:57, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17197/306948 [26:10<6:55:08, 11.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17199/306948 [26:10<6:28:23, 12.43it/s]\u001b[A09/17/2021 05:59:16 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615400\n",
            "09/17/2021 05:59:16 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614400] due to args.save_total_limit\n",
            "09/17/2021 05:59:20 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615400\n",
            "\n",
            "Iteration:   6%|▌         | 17201/306948 [26:15<67:06:48,  1.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17203/306948 [26:15<48:37:31,  1.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17205/306948 [26:15<36:03:25,  2.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17207/306948 [26:15<26:37:26,  3.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17209/306948 [26:16<20:28:48,  3.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17211/306948 [26:16<15:57:41,  5.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17213/306948 [26:16<13:03:41,  6.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17215/306948 [26:16<10:57:47,  7.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17217/306948 [26:16<9:57:35,  8.08it/s] \u001b[A\n",
            "Iteration:   6%|▌         | 17219/306948 [26:16<8:56:29,  9.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17221/306948 [26:17<8:23:58,  9.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17223/306948 [26:17<7:58:22, 10.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17225/306948 [26:17<7:55:03, 10.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17227/306948 [26:17<7:11:36, 11.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17229/306948 [26:17<6:48:08, 11.83it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17231/306948 [26:17<6:12:28, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17233/306948 [26:17<6:23:14, 12.60it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17235/306948 [26:18<6:28:31, 12.43it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17237/306948 [26:18<6:09:25, 13.07it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17239/306948 [26:18<5:38:55, 14.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17241/306948 [26:18<6:01:48, 13.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17243/306948 [26:18<5:53:33, 13.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17245/306948 [26:18<6:01:09, 13.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17247/306948 [26:18<5:49:54, 13.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17249/306948 [26:19<6:45:35, 11.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17251/306948 [26:19<7:00:18, 11.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17253/306948 [26:19<7:07:43, 11.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17255/306948 [26:19<6:45:16, 11.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17257/306948 [26:19<6:31:31, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17259/306948 [26:20<6:18:51, 12.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17261/306948 [26:20<6:36:12, 12.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17263/306948 [26:20<6:24:00, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17265/306948 [26:20<6:11:12, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17267/306948 [26:20<5:52:48, 13.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17269/306948 [26:20<6:54:04, 11.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17271/306948 [26:21<6:35:12, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17273/306948 [26:21<6:47:34, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17275/306948 [26:21<6:57:07, 11.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17277/306948 [26:21<6:50:39, 11.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17279/306948 [26:21<6:15:36, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17281/306948 [26:21<6:41:40, 12.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17283/306948 [26:21<6:26:40, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17285/306948 [26:22<6:09:53, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17287/306948 [26:22<5:58:21, 13.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17289/306948 [26:22<5:55:08, 13.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17291/306948 [26:22<5:36:29, 14.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17293/306948 [26:22<5:49:06, 13.83it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17295/306948 [26:22<5:48:02, 13.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17297/306948 [26:22<6:07:12, 13.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17299/306948 [26:23<6:12:02, 12.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17301/306948 [26:23<6:43:52, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17303/306948 [26:23<6:28:58, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17305/306948 [26:23<6:31:24, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17307/306948 [26:23<6:32:18, 12.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17309/306948 [26:23<6:36:11, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17311/306948 [26:24<6:39:03, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17313/306948 [26:24<6:44:47, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17315/306948 [26:24<6:03:16, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17317/306948 [26:24<6:05:03, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17319/306948 [26:24<7:04:29, 11.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17321/306948 [26:25<7:10:37, 11.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17323/306948 [26:25<7:00:27, 11.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17325/306948 [26:25<7:14:37, 11.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17327/306948 [26:25<7:21:06, 10.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17329/306948 [26:25<7:28:44, 10.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17331/306948 [26:25<6:49:19, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17333/306948 [26:26<7:10:51, 11.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17335/306948 [26:26<6:39:12, 12.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17337/306948 [26:26<6:26:27, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17339/306948 [26:26<6:03:37, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17341/306948 [26:26<5:58:38, 13.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17343/306948 [26:26<5:47:18, 13.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17345/306948 [26:26<6:00:36, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17347/306948 [26:27<6:03:04, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17349/306948 [26:27<6:39:40, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17351/306948 [26:27<6:18:45, 12.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17353/306948 [26:27<6:04:27, 13.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17355/306948 [26:27<6:12:31, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17357/306948 [26:27<5:56:43, 13.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17359/306948 [26:27<5:36:12, 14.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17361/306948 [26:28<6:17:23, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17363/306948 [26:28<6:18:25, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17365/306948 [26:28<6:22:01, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17367/306948 [26:28<5:59:49, 13.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17369/306948 [26:28<6:01:00, 13.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17371/306948 [26:28<5:38:45, 14.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17373/306948 [26:29<5:56:05, 13.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17375/306948 [26:29<6:09:44, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17377/306948 [26:29<6:37:15, 12.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17379/306948 [26:29<6:35:13, 12.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17381/306948 [26:29<6:58:28, 11.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17383/306948 [26:29<6:24:24, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17385/306948 [26:30<6:04:59, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17387/306948 [26:30<5:43:38, 14.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17389/306948 [26:30<5:58:04, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17391/306948 [26:30<5:53:56, 13.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17393/306948 [26:30<6:53:08, 11.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17395/306948 [26:30<6:47:16, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17397/306948 [26:31<6:51:30, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17399/306948 [26:31<6:48:08, 11.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17401/306948 [26:31<6:40:40, 12.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17403/306948 [26:31<6:08:00, 13.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17405/306948 [26:31<6:24:33, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17407/306948 [26:31<6:25:42, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17409/306948 [26:32<6:48:22, 11.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17411/306948 [26:32<6:34:46, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17413/306948 [26:32<6:25:16, 12.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17415/306948 [26:32<6:00:56, 13.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17417/306948 [26:32<6:02:30, 13.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17419/306948 [26:32<5:44:31, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17421/306948 [26:32<5:44:31, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17423/306948 [26:32<5:45:04, 13.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17425/306948 [26:33<6:25:25, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17427/306948 [26:33<6:08:17, 13.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17429/306948 [26:33<6:20:13, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17431/306948 [26:33<6:25:25, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17433/306948 [26:33<6:28:40, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17435/306948 [26:33<5:53:08, 13.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17437/306948 [26:34<6:08:50, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17439/306948 [26:34<5:48:30, 13.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17441/306948 [26:34<6:20:12, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17443/306948 [26:34<7:20:55, 10.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17445/306948 [26:34<7:21:10, 10.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17447/306948 [26:34<6:47:39, 11.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17449/306948 [26:35<6:44:32, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17451/306948 [26:35<6:43:42, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17453/306948 [26:35<6:37:01, 12.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17455/306948 [26:35<5:52:07, 13.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17457/306948 [26:35<6:00:29, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17459/306948 [26:35<6:14:46, 12.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17461/306948 [26:36<6:21:22, 12.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17463/306948 [26:36<6:15:13, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17465/306948 [26:36<6:18:13, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17467/306948 [26:36<6:04:11, 13.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17469/306948 [26:36<6:30:38, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17471/306948 [26:36<6:04:52, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17473/306948 [26:36<6:02:16, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17475/306948 [26:37<6:03:26, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17477/306948 [26:37<6:26:21, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17479/306948 [26:37<5:59:30, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17481/306948 [26:37<6:36:26, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17483/306948 [26:37<6:31:08, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17485/306948 [26:37<6:55:30, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17487/306948 [26:38<6:25:42, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17489/306948 [26:38<6:19:02, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17491/306948 [26:38<6:40:52, 12.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17493/306948 [26:38<6:36:46, 12.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17495/306948 [26:38<6:36:17, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17497/306948 [26:39<7:23:31, 10.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17499/306948 [26:39<6:49:23, 11.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17501/306948 [26:39<6:43:40, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17503/306948 [26:39<6:39:28, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17505/306948 [26:39<6:22:18, 12.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17507/306948 [26:39<5:55:27, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17509/306948 [26:39<6:14:33, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17511/306948 [26:40<6:01:49, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17513/306948 [26:40<6:10:43, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17515/306948 [26:40<5:47:32, 13.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17517/306948 [26:40<5:57:12, 13.50it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17519/306948 [26:40<5:51:50, 13.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17521/306948 [26:40<6:25:20, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17523/306948 [26:40<6:02:50, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17525/306948 [26:41<6:07:23, 13.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17527/306948 [26:41<5:55:26, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17529/306948 [26:41<6:03:26, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17531/306948 [26:41<5:57:36, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17533/306948 [26:41<6:19:36, 12.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17535/306948 [26:41<6:07:06, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17537/306948 [26:42<6:28:16, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17539/306948 [26:42<6:22:11, 12.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17541/306948 [26:42<6:35:24, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17543/306948 [26:42<6:09:31, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17545/306948 [26:42<5:59:53, 13.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17547/306948 [26:42<5:56:07, 13.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17549/306948 [26:42<5:56:52, 13.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17551/306948 [26:43<6:07:11, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17553/306948 [26:43<6:05:08, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17555/306948 [26:43<5:59:32, 13.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17557/306948 [26:43<6:29:20, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17559/306948 [26:43<6:04:38, 13.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17561/306948 [26:43<6:19:54, 12.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17563/306948 [26:43<5:54:42, 13.60it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17565/306948 [26:44<6:17:57, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17567/306948 [26:44<6:05:40, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17569/306948 [26:44<6:24:17, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17571/306948 [26:44<6:00:30, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17573/306948 [26:44<6:30:12, 12.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17575/306948 [26:44<6:02:12, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17577/306948 [26:45<6:23:54, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17579/306948 [26:45<6:46:03, 11.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17581/306948 [26:45<6:44:30, 11.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17583/306948 [26:45<6:03:11, 13.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17585/306948 [26:45<6:14:39, 12.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17587/306948 [26:45<6:18:47, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17589/306948 [26:46<6:36:45, 12.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17591/306948 [26:46<6:17:24, 12.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17593/306948 [26:46<6:58:15, 11.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17595/306948 [26:46<7:00:02, 11.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17597/306948 [26:46<7:42:17, 10.43it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17599/306948 [26:46<6:48:59, 11.79it/s]\u001b[A09/17/2021 05:59:53 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615500\n",
            "09/17/2021 05:59:53 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614500] due to args.save_total_limit\n",
            "09/17/2021 05:59:57 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615500\n",
            "\n",
            "Iteration:   6%|▌         | 17601/306948 [26:52<65:54:00,  1.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17603/306948 [26:52<47:44:43,  1.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17605/306948 [26:52<34:59:50,  2.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17607/306948 [26:52<26:23:08,  3.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17609/306948 [26:52<20:31:56,  3.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17611/306948 [26:52<16:25:27,  4.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17613/306948 [26:52<13:37:21,  5.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17615/306948 [26:53<11:27:50,  7.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17617/306948 [26:53<9:47:44,  8.20it/s] \u001b[A\n",
            "Iteration:   6%|▌         | 17619/306948 [26:53<8:29:35,  9.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17621/306948 [26:53<7:51:26, 10.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17623/306948 [26:53<7:03:31, 11.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17625/306948 [26:53<7:31:19, 10.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17627/306948 [26:54<7:14:39, 11.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17629/306948 [26:54<7:13:36, 11.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17631/306948 [26:54<7:01:39, 11.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17633/306948 [26:54<6:41:32, 12.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17635/306948 [26:54<6:25:19, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17637/306948 [26:54<6:26:43, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17639/306948 [26:55<6:14:51, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17641/306948 [26:55<6:28:42, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17643/306948 [26:55<6:07:37, 13.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17645/306948 [26:55<6:52:15, 11.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17647/306948 [26:55<6:19:55, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17649/306948 [26:55<6:30:52, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17651/306948 [26:55<6:13:33, 12.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17653/306948 [26:56<6:07:23, 13.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17655/306948 [26:56<6:42:28, 11.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17657/306948 [26:56<6:35:09, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17659/306948 [26:56<6:34:54, 12.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17661/306948 [26:56<6:54:10, 11.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17663/306948 [26:56<6:14:38, 12.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17665/306948 [26:57<6:53:30, 11.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17667/306948 [26:57<6:42:55, 11.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17669/306948 [26:57<6:48:35, 11.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17671/306948 [26:57<6:28:37, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17673/306948 [26:57<6:24:29, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17675/306948 [26:57<6:27:59, 12.43it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17677/306948 [26:58<6:39:12, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17679/306948 [26:58<6:38:22, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17681/306948 [26:58<6:17:52, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17683/306948 [26:58<6:19:59, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17685/306948 [26:58<6:28:11, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17687/306948 [26:58<6:22:15, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17689/306948 [26:59<6:52:02, 11.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17691/306948 [26:59<6:31:27, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17693/306948 [26:59<6:21:40, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17695/306948 [26:59<6:02:56, 13.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17697/306948 [26:59<6:03:21, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17699/306948 [26:59<6:00:57, 13.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17701/306948 [27:00<6:20:26, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17703/306948 [27:00<5:49:27, 13.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17705/306948 [27:00<6:11:34, 12.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17707/306948 [27:00<6:10:03, 13.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17709/306948 [27:00<6:21:36, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17711/306948 [27:00<6:35:14, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17713/306948 [27:00<6:36:34, 12.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17715/306948 [27:01<6:21:54, 12.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17717/306948 [27:01<6:21:43, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17719/306948 [27:01<7:02:22, 11.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17721/306948 [27:01<7:16:04, 11.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17723/306948 [27:01<6:52:57, 11.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17725/306948 [27:02<6:56:05, 11.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17727/306948 [27:02<6:29:41, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17729/306948 [27:02<6:43:31, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17731/306948 [27:02<6:34:57, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17733/306948 [27:02<7:03:32, 11.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17735/306948 [27:02<6:37:48, 12.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17737/306948 [27:02<6:41:37, 12.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17739/306948 [27:03<6:09:59, 13.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17741/306948 [27:03<6:36:15, 12.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17743/306948 [27:03<5:57:22, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17745/306948 [27:03<6:44:21, 11.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17747/306948 [27:03<6:49:52, 11.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17749/306948 [27:03<6:46:29, 11.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17751/306948 [27:04<6:34:22, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17753/306948 [27:04<6:38:28, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17755/306948 [27:04<6:05:27, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17757/306948 [27:04<5:58:47, 13.43it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17759/306948 [27:04<6:15:50, 12.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17761/306948 [27:04<6:06:08, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17763/306948 [27:05<6:05:09, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17765/306948 [27:05<6:16:01, 12.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17767/306948 [27:05<6:22:17, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17769/306948 [27:05<6:54:35, 11.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17771/306948 [27:05<6:47:47, 11.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17773/306948 [27:05<7:09:18, 11.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17775/306948 [27:06<6:42:24, 11.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17777/306948 [27:06<6:42:40, 11.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17779/306948 [27:06<6:23:56, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17781/306948 [27:06<6:14:02, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17783/306948 [27:06<6:01:19, 13.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17785/306948 [27:06<6:34:18, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17787/306948 [27:07<6:35:32, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17789/306948 [27:07<6:33:37, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17791/306948 [27:07<6:17:25, 12.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17793/306948 [27:07<6:24:00, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17795/306948 [27:07<6:05:13, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17797/306948 [27:07<6:07:03, 13.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17799/306948 [27:07<6:23:33, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17801/306948 [27:08<6:19:39, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17803/306948 [27:08<6:23:42, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17805/306948 [27:08<6:31:07, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17807/306948 [27:08<5:59:05, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17809/306948 [27:08<6:06:10, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17811/306948 [27:08<6:17:11, 12.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17813/306948 [27:09<6:11:58, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17815/306948 [27:09<6:13:37, 12.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17817/306948 [27:09<6:03:29, 13.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17819/306948 [27:09<5:52:37, 13.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17821/306948 [27:09<6:05:50, 13.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17823/306948 [27:09<5:30:09, 14.60it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17825/306948 [27:09<6:33:09, 12.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17827/306948 [27:10<6:00:23, 13.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17829/306948 [27:10<6:10:42, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17831/306948 [27:10<6:20:48, 12.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17833/306948 [27:10<6:31:16, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17835/306948 [27:10<6:09:51, 13.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17837/306948 [27:10<6:28:01, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17839/306948 [27:11<6:30:58, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17841/306948 [27:11<6:43:31, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17843/306948 [27:11<6:35:14, 12.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17845/306948 [27:11<6:54:16, 11.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17847/306948 [27:11<6:26:21, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17849/306948 [27:11<6:26:37, 12.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17851/306948 [27:12<6:08:56, 13.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17853/306948 [27:12<6:01:41, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17855/306948 [27:12<5:41:26, 14.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17857/306948 [27:12<6:13:07, 12.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17859/306948 [27:12<6:05:33, 13.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17861/306948 [27:12<6:18:32, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17863/306948 [27:12<5:57:40, 13.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17865/306948 [27:13<6:37:24, 12.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17867/306948 [27:13<6:25:06, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17869/306948 [27:13<6:31:31, 12.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17871/306948 [27:13<6:23:09, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17873/306948 [27:13<7:23:48, 10.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17875/306948 [27:13<7:08:35, 11.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17877/306948 [27:14<7:11:34, 11.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17879/306948 [27:14<7:08:58, 11.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17881/306948 [27:14<6:57:16, 11.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17883/306948 [27:14<6:27:02, 12.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17885/306948 [27:14<6:15:42, 12.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17887/306948 [27:14<6:06:01, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17889/306948 [27:15<6:26:33, 12.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17891/306948 [27:15<6:03:55, 13.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17893/306948 [27:15<6:20:50, 12.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17895/306948 [27:15<6:19:31, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17897/306948 [27:15<6:06:25, 13.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17899/306948 [27:15<6:20:18, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17901/306948 [27:16<6:28:13, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17903/306948 [27:16<6:05:07, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17905/306948 [27:16<6:23:43, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17907/306948 [27:16<6:05:00, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17909/306948 [27:16<6:12:09, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17911/306948 [27:16<5:58:13, 13.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17913/306948 [27:16<6:34:50, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17915/306948 [27:17<6:34:57, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17917/306948 [27:17<6:42:35, 11.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17919/306948 [27:17<6:38:54, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17921/306948 [27:17<6:32:21, 12.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17923/306948 [27:17<7:02:19, 11.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17925/306948 [27:18<7:19:07, 10.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17927/306948 [27:18<6:48:34, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17929/306948 [27:18<6:44:35, 11.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17931/306948 [27:18<6:13:50, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17933/306948 [27:18<6:22:01, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17935/306948 [27:18<5:59:29, 13.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17937/306948 [27:18<6:52:49, 11.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17939/306948 [27:19<6:21:16, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17941/306948 [27:19<6:30:21, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17943/306948 [27:19<6:16:34, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17945/306948 [27:19<6:21:43, 12.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17947/306948 [27:19<6:16:43, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17949/306948 [27:19<6:10:34, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17951/306948 [27:20<5:52:15, 13.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17953/306948 [27:20<6:22:16, 12.60it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17955/306948 [27:20<5:47:32, 13.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17957/306948 [27:20<5:58:52, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17959/306948 [27:20<5:31:39, 14.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17961/306948 [27:20<6:15:42, 12.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17963/306948 [27:20<6:31:10, 12.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17965/306948 [27:21<6:09:22, 13.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17967/306948 [27:21<6:19:55, 12.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17969/306948 [27:21<6:26:16, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17971/306948 [27:21<6:18:53, 12.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17973/306948 [27:21<6:21:53, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17975/306948 [27:21<5:55:22, 13.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17977/306948 [27:22<6:01:43, 13.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17979/306948 [27:22<6:52:01, 11.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17981/306948 [27:22<6:43:39, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17983/306948 [27:22<6:15:16, 12.83it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17985/306948 [27:22<6:20:25, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17987/306948 [27:22<6:23:56, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17989/306948 [27:22<6:03:16, 13.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17991/306948 [27:23<6:02:10, 13.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17993/306948 [27:23<6:00:28, 13.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17995/306948 [27:23<5:59:54, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17997/306948 [27:23<6:47:30, 11.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 17999/306948 [27:23<6:52:47, 11.67it/s]\u001b[A09/17/2021 06:00:30 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615600\n",
            "09/17/2021 06:00:30 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614600] due to args.save_total_limit\n",
            "09/17/2021 06:00:34 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615600\n",
            "\n",
            "Iteration:   6%|▌         | 18001/306948 [27:29<69:24:08,  1.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18002/306948 [27:29<58:44:16,  1.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18004/306948 [27:29<41:24:56,  1.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18005/306948 [27:29<35:07:50,  2.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18006/306948 [27:29<29:53:29,  2.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18008/306948 [27:29<21:04:08,  3.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18010/306948 [27:30<15:26:18,  5.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18012/306948 [27:30<12:50:12,  6.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18014/306948 [27:30<10:15:39,  7.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18016/306948 [27:30<9:00:07,  8.92it/s] \u001b[A\n",
            "Iteration:   6%|▌         | 18018/306948 [27:30<7:39:59, 10.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18020/306948 [27:30<7:42:09, 10.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18022/306948 [27:30<7:23:58, 10.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18024/306948 [27:31<6:55:13, 11.60it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18026/306948 [27:31<6:32:57, 12.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18028/306948 [27:31<6:41:14, 12.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18030/306948 [27:31<6:14:25, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18032/306948 [27:31<6:19:00, 12.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18034/306948 [27:31<6:05:54, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18036/306948 [27:32<6:32:24, 12.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18038/306948 [27:32<6:14:54, 12.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18040/306948 [27:32<6:06:38, 13.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18042/306948 [27:32<5:39:16, 14.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18044/306948 [27:32<5:45:14, 13.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18046/306948 [27:32<5:27:59, 14.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18048/306948 [27:32<5:58:06, 13.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18050/306948 [27:33<5:59:51, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18052/306948 [27:33<6:00:14, 13.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18054/306948 [27:33<5:51:46, 13.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18056/306948 [27:33<5:58:09, 13.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18058/306948 [27:33<6:06:44, 13.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18060/306948 [27:33<6:19:52, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18062/306948 [27:33<6:01:25, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18064/306948 [27:34<6:18:38, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18066/306948 [27:34<6:15:35, 12.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18068/306948 [27:34<7:03:35, 11.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18070/306948 [27:34<6:58:32, 11.50it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18072/306948 [27:34<7:12:41, 11.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18074/306948 [27:35<7:00:06, 11.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18076/306948 [27:35<7:03:47, 11.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18078/306948 [27:35<6:31:38, 12.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18080/306948 [27:35<6:31:34, 12.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18082/306948 [27:35<6:23:02, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18084/306948 [27:35<5:59:53, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18086/306948 [27:35<5:26:44, 14.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18088/306948 [27:36<5:39:00, 14.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18090/306948 [27:36<6:23:56, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18092/306948 [27:36<6:12:20, 12.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18094/306948 [27:36<5:56:57, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18096/306948 [27:36<6:20:10, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18098/306948 [27:36<6:12:14, 12.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18100/306948 [27:36<6:03:38, 13.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18102/306948 [27:37<6:00:50, 13.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18104/306948 [27:37<5:51:41, 13.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18106/306948 [27:37<6:02:30, 13.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18108/306948 [27:37<6:06:13, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18110/306948 [27:37<5:48:41, 13.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18112/306948 [27:37<6:06:51, 13.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18114/306948 [27:38<6:07:36, 13.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18116/306948 [27:38<6:16:16, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18118/306948 [27:38<5:44:13, 13.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18120/306948 [27:38<5:33:05, 14.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18122/306948 [27:38<5:38:07, 14.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18124/306948 [27:38<5:45:38, 13.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18126/306948 [27:38<6:28:31, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18128/306948 [27:39<6:28:53, 12.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18130/306948 [27:39<6:42:46, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18132/306948 [27:39<7:33:06, 10.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18134/306948 [27:39<7:13:53, 11.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18136/306948 [27:39<6:49:42, 11.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18138/306948 [27:39<6:57:16, 11.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18140/306948 [27:40<6:42:28, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18142/306948 [27:40<6:18:28, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18144/306948 [27:40<7:33:38, 10.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18146/306948 [27:40<6:55:54, 11.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18148/306948 [27:40<6:29:00, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18150/306948 [27:41<6:49:18, 11.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18152/306948 [27:41<6:29:37, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18154/306948 [27:41<6:02:03, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18156/306948 [27:41<6:32:27, 12.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18158/306948 [27:41<6:18:18, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18160/306948 [27:41<6:57:37, 11.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18162/306948 [27:41<6:11:39, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18164/306948 [27:42<5:56:05, 13.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18166/306948 [27:42<5:45:16, 13.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18168/306948 [27:42<6:44:14, 11.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18170/306948 [27:42<6:43:00, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18172/306948 [27:42<6:26:40, 12.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18174/306948 [27:42<6:19:09, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18176/306948 [27:43<6:25:17, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18178/306948 [27:43<6:21:05, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18180/306948 [27:43<6:24:50, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18182/306948 [27:43<6:50:58, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18184/306948 [27:43<6:57:35, 11.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18186/306948 [27:43<6:50:51, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18188/306948 [27:44<6:56:09, 11.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18190/306948 [27:44<6:28:24, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18192/306948 [27:44<6:53:29, 11.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18194/306948 [27:44<6:15:16, 12.82it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18196/306948 [27:44<6:26:52, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18198/306948 [27:44<5:54:36, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18200/306948 [27:44<5:49:04, 13.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18202/306948 [27:45<6:03:17, 13.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18204/306948 [27:45<6:16:19, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18206/306948 [27:45<6:22:50, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18208/306948 [27:45<6:37:27, 12.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18210/306948 [27:45<6:20:51, 12.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18212/306948 [27:45<6:42:11, 11.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18214/306948 [27:46<6:17:36, 12.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18216/306948 [27:46<6:23:31, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18218/306948 [27:46<6:19:40, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18220/306948 [27:46<6:22:23, 12.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18222/306948 [27:46<6:02:40, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18224/306948 [27:46<6:08:38, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18226/306948 [27:47<6:00:56, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18228/306948 [27:47<6:15:29, 12.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18230/306948 [27:47<6:11:30, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18232/306948 [27:47<6:11:32, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18234/306948 [27:47<5:39:21, 14.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18236/306948 [27:47<5:50:50, 13.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18238/306948 [27:47<5:44:11, 13.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18240/306948 [27:48<6:08:28, 13.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18242/306948 [27:48<5:41:30, 14.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18244/306948 [27:48<6:15:54, 12.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18246/306948 [27:48<6:47:52, 11.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18248/306948 [27:48<6:23:48, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18250/306948 [27:48<5:54:45, 13.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18252/306948 [27:49<6:24:55, 12.50it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18254/306948 [27:49<6:38:01, 12.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18256/306948 [27:49<6:14:50, 12.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18258/306948 [27:49<5:51:05, 13.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18260/306948 [27:49<5:49:04, 13.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18262/306948 [27:49<5:26:21, 14.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18264/306948 [27:49<5:31:13, 14.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18266/306948 [27:49<5:29:27, 14.60it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18268/306948 [27:50<5:38:55, 14.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18270/306948 [27:50<5:28:02, 14.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18272/306948 [27:50<6:03:47, 13.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18274/306948 [27:50<5:41:32, 14.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18276/306948 [27:50<6:04:10, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18278/306948 [27:50<6:14:06, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18280/306948 [27:51<6:22:05, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18282/306948 [27:51<6:15:29, 12.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18284/306948 [27:51<6:28:51, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18286/306948 [27:51<5:47:11, 13.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18288/306948 [27:51<6:53:19, 11.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18290/306948 [27:51<6:16:56, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18292/306948 [27:51<5:55:45, 13.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18294/306948 [27:52<6:38:40, 12.07it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18296/306948 [27:52<6:28:40, 12.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18298/306948 [27:52<6:02:31, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18300/306948 [27:52<6:19:20, 12.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18302/306948 [27:52<7:28:04, 10.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18304/306948 [27:53<7:30:59, 10.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18306/306948 [27:53<6:53:25, 11.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18308/306948 [27:53<7:02:06, 11.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18310/306948 [27:53<7:12:48, 11.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18312/306948 [27:53<6:39:50, 12.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18314/306948 [27:53<6:48:51, 11.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18316/306948 [27:54<6:34:26, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18318/306948 [27:54<6:26:41, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18320/306948 [27:54<6:44:13, 11.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18322/306948 [27:54<6:17:17, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18324/306948 [27:54<7:41:38, 10.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18326/306948 [27:54<6:46:06, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18328/306948 [27:55<6:31:16, 12.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18330/306948 [27:55<5:59:34, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18332/306948 [27:55<5:56:51, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18334/306948 [27:55<5:52:06, 13.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18336/306948 [27:55<6:14:38, 12.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18338/306948 [27:55<6:00:49, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18340/306948 [27:55<5:59:30, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18342/306948 [27:56<5:50:27, 13.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18344/306948 [27:56<6:05:18, 13.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18346/306948 [27:56<5:54:30, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18348/306948 [27:56<6:17:22, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18350/306948 [27:56<6:40:04, 12.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18352/306948 [27:56<6:47:55, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18354/306948 [27:57<6:00:42, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18356/306948 [27:57<6:13:29, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18358/306948 [27:57<5:50:58, 13.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18360/306948 [27:57<6:03:57, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18362/306948 [27:57<5:38:20, 14.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18364/306948 [27:57<6:00:49, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18366/306948 [27:57<5:46:17, 13.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18368/306948 [27:58<5:55:55, 13.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18370/306948 [27:58<5:37:57, 14.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18372/306948 [27:58<5:50:14, 13.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18374/306948 [27:58<5:56:42, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18376/306948 [27:58<5:56:48, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18378/306948 [27:58<5:58:15, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18380/306948 [27:58<6:02:42, 13.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18382/306948 [27:59<5:35:56, 14.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18384/306948 [27:59<5:43:15, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18386/306948 [27:59<5:59:06, 13.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18388/306948 [27:59<6:17:31, 12.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18390/306948 [27:59<6:27:16, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18392/306948 [27:59<6:36:33, 12.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18394/306948 [28:00<6:19:37, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18396/306948 [28:00<6:26:05, 12.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18398/306948 [28:00<6:13:41, 12.87it/s]\u001b[A09/17/2021 06:01:06 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615700\n",
            "09/17/2021 06:01:06 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614700] due to args.save_total_limit\n",
            "09/17/2021 06:01:10 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615700\n",
            "\n",
            "Iteration:   6%|▌         | 18400/306948 [28:05<64:37:41,  1.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18402/306948 [28:05<47:03:59,  1.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18404/306948 [28:05<34:45:29,  2.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18406/306948 [28:05<26:10:31,  3.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18408/306948 [28:05<20:05:56,  3.99it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18410/306948 [28:06<15:35:12,  5.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18412/306948 [28:06<13:10:44,  6.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18414/306948 [28:06<10:53:16,  7.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18416/306948 [28:06<9:42:27,  8.26it/s] \u001b[A\n",
            "Iteration:   6%|▌         | 18418/306948 [28:06<8:18:56,  9.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18420/306948 [28:06<7:46:27, 10.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18422/306948 [28:07<7:27:35, 10.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18424/306948 [28:07<7:12:42, 11.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18426/306948 [28:07<7:33:57, 10.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18428/306948 [28:07<7:44:25, 10.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18430/306948 [28:07<7:04:49, 11.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18432/306948 [28:07<7:13:02, 11.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18434/306948 [28:08<6:21:19, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18436/306948 [28:08<6:22:19, 12.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18438/306948 [28:08<6:46:15, 11.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18440/306948 [28:08<6:41:57, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18442/306948 [28:08<6:27:44, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18444/306948 [28:08<6:08:28, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18446/306948 [28:08<5:45:10, 13.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18448/306948 [28:09<6:35:05, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18450/306948 [28:09<6:23:25, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18452/306948 [28:09<6:38:57, 12.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18454/306948 [28:09<6:31:10, 12.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18456/306948 [28:09<6:38:40, 12.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18458/306948 [28:09<6:22:00, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18460/306948 [28:10<6:16:38, 12.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18462/306948 [28:10<5:50:54, 13.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18464/306948 [28:10<5:57:06, 13.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18466/306948 [28:10<5:52:03, 13.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18468/306948 [28:10<6:32:54, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18470/306948 [28:10<6:28:04, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18472/306948 [28:11<7:23:57, 10.83it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18474/306948 [28:11<6:54:04, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18476/306948 [28:11<6:45:50, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18478/306948 [28:11<8:18:02,  9.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18480/306948 [28:11<7:32:04, 10.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18482/306948 [28:12<6:49:14, 11.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18484/306948 [28:12<6:54:10, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18486/306948 [28:12<6:18:33, 12.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18488/306948 [28:12<5:57:39, 13.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18490/306948 [28:12<5:47:43, 13.83it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18492/306948 [28:12<5:47:27, 13.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18494/306948 [28:12<6:02:00, 13.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18496/306948 [28:13<6:12:05, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18498/306948 [28:13<5:43:14, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18500/306948 [28:13<6:11:33, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18502/306948 [28:13<6:10:02, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18504/306948 [28:13<6:44:25, 11.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18506/306948 [28:13<6:49:46, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18508/306948 [28:14<6:42:42, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18511/306948 [28:14<5:59:08, 13.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18513/306948 [28:14<6:34:50, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18515/306948 [28:14<6:22:42, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18517/306948 [28:14<6:43:10, 11.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18519/306948 [28:14<6:37:39, 12.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18521/306948 [28:15<6:18:44, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18523/306948 [28:15<6:12:44, 12.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18525/306948 [28:15<6:11:11, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18527/306948 [28:15<6:18:21, 12.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18529/306948 [28:15<6:23:20, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18531/306948 [28:15<6:11:17, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18533/306948 [28:16<6:12:18, 12.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18535/306948 [28:16<6:29:38, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18537/306948 [28:16<6:24:21, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18539/306948 [28:16<5:54:12, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18541/306948 [28:16<6:05:05, 13.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18543/306948 [28:16<5:50:00, 13.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18545/306948 [28:16<5:57:25, 13.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18547/306948 [28:17<5:51:21, 13.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18549/306948 [28:17<6:20:18, 12.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18551/306948 [28:17<6:07:19, 13.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18553/306948 [28:17<6:03:44, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18555/306948 [28:17<6:37:32, 12.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18557/306948 [28:17<6:25:54, 12.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18559/306948 [28:18<6:09:03, 13.02it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18561/306948 [28:18<6:34:38, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18563/306948 [28:18<6:29:06, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18565/306948 [28:18<6:56:34, 11.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18567/306948 [28:18<6:41:42, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18569/306948 [28:18<6:41:51, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18571/306948 [28:19<6:39:31, 12.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18573/306948 [28:19<6:28:34, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18575/306948 [28:19<6:02:55, 13.24it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18577/306948 [28:19<6:05:43, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18579/306948 [28:19<6:22:18, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18581/306948 [28:19<6:16:48, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18583/306948 [28:19<6:05:45, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18585/306948 [28:20<6:20:47, 12.62it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18587/306948 [28:20<6:36:05, 12.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18589/306948 [28:20<6:29:22, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18591/306948 [28:20<6:09:20, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18593/306948 [28:20<6:06:27, 13.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18595/306948 [28:20<5:54:52, 13.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18597/306948 [28:21<6:07:58, 13.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18599/306948 [28:21<5:45:29, 13.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18601/306948 [28:21<6:21:36, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18603/306948 [28:21<6:47:38, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18605/306948 [28:21<6:40:37, 12.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18607/306948 [28:21<6:11:35, 12.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18609/306948 [28:21<6:05:49, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18611/306948 [28:22<6:00:44, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18613/306948 [28:22<6:32:07, 12.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18615/306948 [28:22<6:21:56, 12.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18617/306948 [28:22<6:32:51, 12.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18619/306948 [28:22<6:34:13, 12.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18621/306948 [28:22<6:10:49, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18623/306948 [28:23<5:51:58, 13.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18625/306948 [28:23<6:24:15, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18627/306948 [28:23<6:07:49, 13.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18629/306948 [28:23<6:37:08, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18631/306948 [28:23<6:16:16, 12.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18633/306948 [28:23<6:12:30, 12.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18635/306948 [28:24<5:53:58, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18637/306948 [28:24<5:56:57, 13.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18639/306948 [28:24<5:51:58, 13.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18641/306948 [28:24<6:26:17, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18643/306948 [28:24<5:56:35, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18645/306948 [28:24<6:01:57, 13.28it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18647/306948 [28:24<5:54:32, 13.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18649/306948 [28:25<6:27:28, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18651/306948 [28:25<6:00:26, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18653/306948 [28:25<6:03:49, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18655/306948 [28:25<6:14:15, 12.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18657/306948 [28:25<6:07:26, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18659/306948 [28:25<5:48:20, 13.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18661/306948 [28:25<5:56:27, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18664/306948 [28:26<5:46:33, 13.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18666/306948 [28:26<5:28:04, 14.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18668/306948 [28:26<5:36:54, 14.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18670/306948 [28:26<5:24:25, 14.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18672/306948 [28:26<5:43:25, 13.99it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18674/306948 [28:26<5:49:07, 13.76it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18676/306948 [28:27<6:19:44, 12.65it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18678/306948 [28:27<5:54:56, 13.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18680/306948 [28:27<6:31:50, 12.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18682/306948 [28:27<6:13:01, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18684/306948 [28:27<6:49:05, 11.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18686/306948 [28:27<6:13:48, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18688/306948 [28:28<6:39:17, 12.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18690/306948 [28:28<6:09:23, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18692/306948 [28:28<6:19:25, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18694/306948 [28:28<6:41:44, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18696/306948 [28:28<6:47:55, 11.78it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18698/306948 [28:28<6:10:23, 12.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18700/306948 [28:28<6:10:49, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18702/306948 [28:29<6:20:19, 12.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18704/306948 [28:29<6:09:46, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18706/306948 [28:29<6:09:24, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18708/306948 [28:29<6:34:12, 12.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18710/306948 [28:29<6:26:52, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18712/306948 [28:29<6:20:55, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18714/306948 [28:30<5:48:13, 13.80it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18716/306948 [28:30<5:42:52, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18718/306948 [28:30<5:25:43, 14.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18720/306948 [28:30<5:44:51, 13.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18722/306948 [28:30<5:59:06, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18724/306948 [28:30<6:21:01, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18726/306948 [28:30<6:07:20, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18728/306948 [28:31<6:18:01, 12.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18730/306948 [28:31<6:07:16, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18732/306948 [28:31<6:17:51, 12.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18734/306948 [28:31<6:53:51, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18736/306948 [28:31<6:59:20, 11.45it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18738/306948 [28:31<6:43:07, 11.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18740/306948 [28:32<6:38:57, 12.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18742/306948 [28:32<6:04:28, 13.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18744/306948 [28:32<6:17:15, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18746/306948 [28:32<5:58:17, 13.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18748/306948 [28:32<5:53:24, 13.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18750/306948 [28:32<5:27:18, 14.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18752/306948 [28:33<6:56:15, 11.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18754/306948 [28:33<6:19:06, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18756/306948 [28:33<6:35:49, 12.13it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18758/306948 [28:33<6:10:58, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18760/306948 [28:33<6:05:34, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18762/306948 [28:33<5:46:50, 13.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18764/306948 [28:33<5:51:09, 13.68it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18766/306948 [28:34<5:53:19, 13.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18768/306948 [28:34<5:50:28, 13.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18770/306948 [28:34<5:58:21, 13.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18772/306948 [28:34<6:07:12, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18774/306948 [28:34<6:03:52, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18776/306948 [28:34<6:28:58, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18778/306948 [28:34<5:57:16, 13.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18780/306948 [28:35<5:57:59, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18782/306948 [28:35<5:58:17, 13.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18784/306948 [28:35<6:13:42, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18786/306948 [28:35<5:55:01, 13.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18788/306948 [28:35<6:08:33, 13.03it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18790/306948 [28:35<5:47:04, 13.84it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18792/306948 [28:36<6:05:03, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18794/306948 [28:36<5:50:53, 13.69it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18796/306948 [28:36<6:23:42, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18798/306948 [28:36<6:30:30, 12.30it/s]\u001b[A09/17/2021 06:01:42 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615800\n",
            "09/17/2021 06:01:42 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614800] due to args.save_total_limit\n",
            "09/17/2021 06:01:46 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615800\n",
            "\n",
            "Iteration:   6%|▌         | 18800/306948 [28:41<64:16:27,  1.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18802/306948 [28:41<47:00:27,  1.70it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18804/306948 [28:41<34:57:23,  2.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18806/306948 [28:41<26:11:29,  3.06it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18808/306948 [28:42<20:16:33,  3.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18810/306948 [28:42<15:40:55,  5.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18812/306948 [28:42<13:00:09,  6.16it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18814/306948 [28:42<10:53:50,  7.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18816/306948 [28:42<9:42:54,  8.24it/s] \u001b[A\n",
            "Iteration:   6%|▌         | 18818/306948 [28:42<8:38:49,  9.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18820/306948 [28:43<8:03:37,  9.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18822/306948 [28:43<7:05:07, 11.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18824/306948 [28:43<7:03:52, 11.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18826/306948 [28:43<7:31:40, 10.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18828/306948 [28:43<7:35:33, 10.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18830/306948 [28:43<6:52:49, 11.63it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18832/306948 [28:44<6:26:00, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18834/306948 [28:44<6:01:18, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18836/306948 [28:44<5:59:56, 13.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18838/306948 [28:44<6:06:07, 13.12it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18840/306948 [28:44<6:27:03, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18842/306948 [28:44<5:59:29, 13.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18844/306948 [28:44<6:36:56, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18846/306948 [28:45<6:34:40, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18848/306948 [28:45<7:58:20, 10.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18850/306948 [28:45<7:09:01, 11.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18852/306948 [28:45<7:02:09, 11.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18854/306948 [28:45<6:39:46, 12.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18856/306948 [28:46<6:21:51, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18858/306948 [28:46<6:09:01, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18860/306948 [28:46<6:01:51, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18862/306948 [28:46<5:45:35, 13.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18864/306948 [28:46<5:50:03, 13.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18866/306948 [28:46<5:37:00, 14.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18868/306948 [28:46<5:55:34, 13.50it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18870/306948 [28:47<5:37:31, 14.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18872/306948 [28:47<5:55:10, 13.52it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18874/306948 [28:47<6:03:28, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18876/306948 [28:47<6:05:13, 13.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18878/306948 [28:47<5:56:02, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18880/306948 [28:47<5:59:34, 13.35it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18882/306948 [28:47<5:49:28, 13.74it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18884/306948 [28:48<5:41:04, 14.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18886/306948 [28:48<5:35:30, 14.31it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18888/306948 [28:48<5:56:21, 13.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18890/306948 [28:48<5:49:12, 13.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18892/306948 [28:48<6:12:55, 12.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18894/306948 [28:48<5:53:23, 13.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18896/306948 [28:48<6:10:19, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18898/306948 [28:49<6:26:50, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18900/306948 [28:49<6:11:21, 12.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18902/306948 [28:49<6:04:16, 13.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18904/306948 [28:49<6:22:12, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18906/306948 [28:49<6:15:55, 12.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18908/306948 [28:49<6:33:58, 12.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18910/306948 [28:50<8:38:10,  9.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18912/306948 [28:50<8:31:40,  9.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18914/306948 [28:50<8:14:35,  9.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18916/306948 [28:50<7:55:41, 10.09it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18918/306948 [28:51<7:20:17, 10.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18920/306948 [28:51<7:19:20, 10.93it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18922/306948 [28:51<7:08:53, 11.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18924/306948 [28:51<6:45:11, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18926/306948 [28:51<6:26:19, 12.43it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18928/306948 [28:51<6:37:44, 12.07it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18930/306948 [28:51<6:11:59, 12.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18932/306948 [28:52<6:20:37, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18934/306948 [28:52<6:21:09, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18936/306948 [28:52<6:21:32, 12.58it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18938/306948 [28:52<6:10:19, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18940/306948 [28:52<6:11:38, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18942/306948 [28:52<6:29:01, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18944/306948 [28:53<6:37:27, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18946/306948 [28:53<6:34:21, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18948/306948 [28:53<6:32:21, 12.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18950/306948 [28:53<6:03:29, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18952/306948 [28:53<5:49:45, 13.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18954/306948 [28:53<6:10:38, 12.95it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18956/306948 [28:54<6:42:05, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18958/306948 [28:54<6:31:44, 12.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18960/306948 [28:54<6:25:10, 12.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18962/306948 [28:54<6:28:58, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18964/306948 [28:54<6:55:59, 11.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18966/306948 [28:54<6:14:43, 12.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18968/306948 [28:55<6:32:57, 12.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18970/306948 [28:55<6:36:42, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18972/306948 [28:55<6:45:01, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18974/306948 [28:55<7:00:44, 11.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18976/306948 [28:55<7:21:27, 10.87it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18978/306948 [28:55<6:49:13, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18980/306948 [28:56<7:49:49, 10.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18982/306948 [28:56<7:10:27, 11.15it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18984/306948 [28:56<7:06:18, 11.26it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18986/306948 [28:56<6:38:43, 12.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18988/306948 [28:56<6:22:38, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18990/306948 [28:56<6:01:03, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18992/306948 [28:57<6:58:39, 11.46it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18994/306948 [28:57<6:27:19, 12.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18996/306948 [28:57<6:22:19, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 18998/306948 [28:57<6:03:43, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19000/306948 [28:57<6:02:08, 13.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19002/306948 [28:57<5:53:07, 13.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19004/306948 [28:57<6:23:31, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19006/306948 [28:58<6:01:35, 13.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19008/306948 [28:58<6:10:52, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19010/306948 [28:58<6:07:51, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19012/306948 [28:58<6:36:20, 12.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19014/306948 [28:58<7:18:46, 10.94it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19016/306948 [28:59<7:01:26, 11.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19018/306948 [28:59<6:32:40, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19020/306948 [28:59<6:22:24, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19022/306948 [28:59<6:17:34, 12.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19024/306948 [28:59<6:46:23, 11.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19026/306948 [28:59<6:23:36, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19028/306948 [28:59<6:48:20, 11.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19030/306948 [29:00<6:22:47, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19032/306948 [29:00<5:57:00, 13.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19034/306948 [29:00<6:00:02, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19036/306948 [29:00<6:14:44, 12.81it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19038/306948 [29:00<6:03:42, 13.19it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19040/306948 [29:00<6:27:32, 12.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19042/306948 [29:01<6:11:59, 12.90it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19044/306948 [29:01<7:06:23, 11.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19046/306948 [29:01<6:40:23, 11.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19048/306948 [29:01<6:33:55, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19050/306948 [29:01<6:11:48, 12.91it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19052/306948 [29:01<6:37:04, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19054/306948 [29:02<6:31:11, 12.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19056/306948 [29:02<6:28:44, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19058/306948 [29:02<6:03:37, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19060/306948 [29:02<6:41:03, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19062/306948 [29:02<6:30:03, 12.30it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19064/306948 [29:02<6:27:51, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19066/306948 [29:02<6:24:34, 12.48it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19068/306948 [29:03<6:24:52, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19070/306948 [29:03<5:59:52, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19072/306948 [29:03<5:59:55, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19074/306948 [29:03<6:03:18, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19076/306948 [29:03<6:02:46, 13.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19078/306948 [29:03<5:33:03, 14.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19080/306948 [29:04<6:12:11, 12.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19082/306948 [29:04<6:21:37, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19084/306948 [29:04<6:46:46, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19086/306948 [29:04<6:29:34, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19088/306948 [29:04<6:16:11, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19090/306948 [29:04<5:40:46, 14.08it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19092/306948 [29:05<6:32:53, 12.21it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19094/306948 [29:05<6:09:34, 12.98it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19096/306948 [29:05<6:58:09, 11.47it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19098/306948 [29:05<6:25:42, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19100/306948 [29:05<6:12:29, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19102/306948 [29:05<5:39:55, 14.11it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19104/306948 [29:05<5:57:48, 13.41it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19106/306948 [29:06<5:36:35, 14.25it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19108/306948 [29:06<6:15:04, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19110/306948 [29:06<5:58:37, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19112/306948 [29:06<6:13:01, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19114/306948 [29:06<5:43:26, 13.97it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19116/306948 [29:06<5:58:32, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19118/306948 [29:07<6:26:43, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19120/306948 [29:07<6:18:43, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19122/306948 [29:07<6:28:50, 12.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19124/306948 [29:07<6:48:51, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19126/306948 [29:07<6:32:23, 12.23it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19128/306948 [29:07<6:27:53, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19130/306948 [29:08<6:54:38, 11.57it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19132/306948 [29:08<6:44:42, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19134/306948 [29:08<6:44:50, 11.85it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19136/306948 [29:08<6:49:14, 11.72it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19138/306948 [29:08<6:22:43, 12.53it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19140/306948 [29:08<6:07:46, 13.04it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19142/306948 [29:08<5:49:48, 13.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19144/306948 [29:09<6:07:40, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19146/306948 [29:09<5:58:19, 13.39it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19148/306948 [29:09<5:54:20, 13.54it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19151/306948 [29:09<5:27:44, 14.64it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19153/306948 [29:09<5:36:04, 14.27it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19155/306948 [29:09<5:52:21, 13.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19157/306948 [29:10<6:20:28, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19159/306948 [29:10<7:59:57,  9.99it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19161/306948 [29:10<7:20:35, 10.89it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19163/306948 [29:10<6:49:31, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19165/306948 [29:10<6:53:45, 11.59it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19167/306948 [29:10<6:16:52, 12.73it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19169/306948 [29:11<6:03:55, 13.18it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19171/306948 [29:11<5:58:39, 13.37it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19173/306948 [29:11<5:57:28, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19175/306948 [29:11<6:06:59, 13.07it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19177/306948 [29:11<7:02:07, 11.36it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19179/306948 [29:11<6:47:21, 11.77it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19181/306948 [29:12<7:02:46, 11.34it/s]\u001b[A\n",
            "Iteration:   6%|▌         | 19183/306948 [29:12<6:48:25, 11.74it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19185/306948 [29:12<6:46:25, 11.80it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19187/306948 [29:12<6:39:15, 12.01it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19189/306948 [29:12<6:53:02, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19191/306948 [29:12<6:25:17, 12.45it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19193/306948 [29:13<6:17:17, 12.71it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19195/306948 [29:13<5:43:08, 13.98it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19197/306948 [29:13<5:53:07, 13.58it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19199/306948 [29:13<5:49:48, 13.71it/s]\u001b[A09/17/2021 06:02:19 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-615900\n",
            "09/17/2021 06:02:19 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-614900] due to args.save_total_limit\n",
            "09/17/2021 06:02:23 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-615900\n",
            "\n",
            "Iteration:   6%|▋         | 19201/306948 [29:18<66:10:58,  1.21it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19203/306948 [29:18<48:03:25,  1.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19205/306948 [29:18<35:45:02,  2.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19207/306948 [29:19<27:10:31,  2.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19209/306948 [29:19<20:41:58,  3.86it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19211/306948 [29:19<17:21:05,  4.61it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19213/306948 [29:19<14:13:59,  5.62it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19215/306948 [29:19<11:36:40,  6.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19217/306948 [29:20<10:16:03,  7.78it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19219/306948 [29:20<9:00:26,  8.87it/s] \u001b[A\n",
            "Iteration:   6%|▋         | 19221/306948 [29:20<8:19:30,  9.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19223/306948 [29:20<7:39:21, 10.44it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19225/306948 [29:20<7:38:45, 10.45it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19227/306948 [29:20<6:49:44, 11.70it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19229/306948 [29:20<6:48:54, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19231/306948 [29:21<6:19:25, 12.64it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19233/306948 [29:21<6:21:38, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19235/306948 [29:21<6:15:40, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19237/306948 [29:21<6:51:03, 11.67it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19239/306948 [29:21<6:38:00, 12.05it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19241/306948 [29:21<6:32:24, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19243/306948 [29:22<6:09:06, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19245/306948 [29:22<6:49:03, 11.72it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19247/306948 [29:22<6:23:04, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19249/306948 [29:22<6:26:17, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19251/306948 [29:22<6:09:08, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19253/306948 [29:22<6:05:35, 13.12it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19255/306948 [29:23<6:15:49, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19257/306948 [29:23<6:05:52, 13.11it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19259/306948 [29:23<5:37:29, 14.21it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19261/306948 [29:23<5:35:36, 14.29it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19263/306948 [29:23<5:17:49, 15.09it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19265/306948 [29:23<5:42:33, 14.00it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19267/306948 [29:23<5:48:57, 13.74it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19269/306948 [29:24<6:20:53, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19271/306948 [29:24<5:54:19, 13.53it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19273/306948 [29:24<5:43:30, 13.96it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19275/306948 [29:24<6:37:23, 12.06it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19277/306948 [29:24<6:26:12, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19279/306948 [29:24<6:30:46, 12.27it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19281/306948 [29:25<6:34:34, 12.15it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19283/306948 [29:25<6:31:49, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19285/306948 [29:25<6:24:00, 12.48it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19287/306948 [29:25<5:51:38, 13.63it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19289/306948 [29:25<6:28:45, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19291/306948 [29:25<6:10:25, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19293/306948 [29:25<6:28:46, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19295/306948 [29:26<6:31:49, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19297/306948 [29:26<7:01:09, 11.38it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19299/306948 [29:26<6:21:59, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19301/306948 [29:26<6:24:30, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19303/306948 [29:26<6:21:33, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19305/306948 [29:26<6:20:18, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19307/306948 [29:27<6:11:50, 12.89it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19309/306948 [29:27<6:47:29, 11.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19311/306948 [29:27<6:28:03, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19313/306948 [29:27<6:25:38, 12.43it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19315/306948 [29:27<6:21:47, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19317/306948 [29:27<6:04:40, 13.15it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19319/306948 [29:28<5:57:45, 13.40it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19321/306948 [29:28<6:37:32, 12.06it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19323/306948 [29:28<5:54:35, 13.52it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19325/306948 [29:28<6:12:19, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19327/306948 [29:28<6:22:37, 12.53it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19329/306948 [29:28<6:25:54, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19331/306948 [29:28<6:01:57, 13.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19333/306948 [29:29<6:02:29, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19335/306948 [29:29<6:00:52, 13.28it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19337/306948 [29:29<6:00:11, 13.31it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19339/306948 [29:29<5:37:06, 14.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19341/306948 [29:29<6:08:35, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19343/306948 [29:29<5:51:57, 13.62it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19345/306948 [29:30<6:11:22, 12.91it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19347/306948 [29:30<5:52:27, 13.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19349/306948 [29:30<5:55:50, 13.47it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19351/306948 [29:30<5:22:51, 14.85it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19353/306948 [29:30<5:55:05, 13.50it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19355/306948 [29:30<5:53:07, 13.57it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19357/306948 [29:30<5:57:54, 13.39it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19359/306948 [29:30<5:32:37, 14.41it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19361/306948 [29:31<6:14:55, 12.78it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19363/306948 [29:31<6:37:11, 12.07it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19365/306948 [29:31<6:41:57, 11.92it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19367/306948 [29:31<6:48:35, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19369/306948 [29:31<6:41:12, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19371/306948 [29:32<6:11:04, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19373/306948 [29:32<6:18:41, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19375/306948 [29:32<5:57:31, 13.41it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19377/306948 [29:32<6:06:27, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19379/306948 [29:32<6:13:06, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19381/306948 [29:32<7:22:48, 10.82it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19383/306948 [29:33<6:48:36, 11.73it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19385/306948 [29:33<6:41:22, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19387/306948 [29:33<6:29:02, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19389/306948 [29:33<6:45:45, 11.81it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19391/306948 [29:33<6:06:19, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19393/306948 [29:33<6:06:48, 13.07it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19395/306948 [29:33<5:55:17, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19397/306948 [29:34<6:20:44, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19399/306948 [29:34<5:55:11, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19401/306948 [29:34<6:33:41, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19403/306948 [29:34<7:01:32, 11.37it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19405/306948 [29:34<7:02:13, 11.35it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19407/306948 [29:34<6:32:17, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19409/306948 [29:35<6:29:31, 12.30it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19411/306948 [29:35<6:04:35, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19413/306948 [29:35<7:20:16, 10.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19415/306948 [29:35<6:54:44, 11.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19417/306948 [29:35<6:41:13, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19419/306948 [29:35<7:14:16, 11.03it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19421/306948 [29:36<6:45:46, 11.81it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19423/306948 [29:36<6:17:20, 12.70it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19425/306948 [29:36<6:32:48, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19427/306948 [29:36<6:43:33, 11.87it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19429/306948 [29:36<6:33:35, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19431/306948 [29:36<6:10:59, 12.92it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19433/306948 [29:37<6:15:09, 12.77it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19435/306948 [29:37<6:08:56, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19437/306948 [29:37<6:16:47, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19439/306948 [29:37<6:15:55, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19441/306948 [29:37<6:42:23, 11.91it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19443/306948 [29:37<6:26:35, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19445/306948 [29:38<6:06:11, 13.09it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19447/306948 [29:38<6:05:15, 13.12it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19449/306948 [29:38<6:12:46, 12.85it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19451/306948 [29:38<6:14:13, 12.80it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19453/306948 [29:38<6:15:55, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19455/306948 [29:38<5:40:14, 14.08it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19457/306948 [29:38<5:57:59, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19459/306948 [29:39<6:20:13, 12.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19461/306948 [29:39<6:23:22, 12.50it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19463/306948 [29:39<6:12:21, 12.87it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19465/306948 [29:39<6:25:47, 12.42it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19467/306948 [29:39<6:06:42, 13.07it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19469/306948 [29:39<6:33:35, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19471/306948 [29:40<6:42:35, 11.90it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19473/306948 [29:40<6:42:07, 11.91it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19475/306948 [29:40<6:56:35, 11.50it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19477/306948 [29:40<7:23:25, 10.81it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19479/306948 [29:40<6:52:27, 11.62it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19481/306948 [29:40<6:37:23, 12.06it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19483/306948 [29:41<6:31:21, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19485/306948 [29:41<6:17:41, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19487/306948 [29:41<6:08:36, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19489/306948 [29:41<6:15:22, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19491/306948 [29:41<6:20:17, 12.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19493/306948 [29:41<6:28:57, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19495/306948 [29:42<5:56:53, 13.42it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19497/306948 [29:42<6:17:43, 12.68it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19499/306948 [29:42<5:55:15, 13.49it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19501/306948 [29:42<6:07:49, 13.02it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19503/306948 [29:42<5:59:46, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19505/306948 [29:42<6:29:14, 12.31it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19507/306948 [29:43<6:49:22, 11.70it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19509/306948 [29:43<6:31:07, 12.25it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19511/306948 [29:43<6:20:12, 12.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19513/306948 [29:43<6:25:53, 12.41it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19515/306948 [29:43<6:01:24, 13.26it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19517/306948 [29:43<6:47:29, 11.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19519/306948 [29:43<6:54:51, 11.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19521/306948 [29:44<7:11:50, 11.09it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19523/306948 [29:44<6:27:03, 12.38it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19525/306948 [29:44<6:52:11, 11.62it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19527/306948 [29:44<6:21:28, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19529/306948 [29:44<7:28:23, 10.68it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19531/306948 [29:44<6:36:36, 12.08it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19533/306948 [29:45<6:18:14, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19535/306948 [29:45<6:34:16, 12.15it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19537/306948 [29:45<6:27:38, 12.36it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19539/306948 [29:45<6:21:28, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19541/306948 [29:45<6:02:51, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19543/306948 [29:45<6:03:44, 13.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19545/306948 [29:46<6:12:22, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19547/306948 [29:46<6:33:29, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19549/306948 [29:46<6:41:31, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19551/306948 [29:46<6:20:37, 12.58it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19553/306948 [29:46<6:19:42, 12.61it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19555/306948 [29:46<5:48:01, 13.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19557/306948 [29:46<5:47:05, 13.80it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19559/306948 [29:47<6:18:14, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19561/306948 [29:47<6:31:50, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19563/306948 [29:47<6:40:36, 11.96it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19565/306948 [29:47<6:42:36, 11.90it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19567/306948 [29:47<6:27:41, 12.35it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19569/306948 [29:47<6:17:42, 12.68it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19571/306948 [29:48<6:05:42, 13.10it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19573/306948 [29:48<6:04:23, 13.14it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19575/306948 [29:48<6:08:14, 13.01it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19577/306948 [29:48<5:53:22, 13.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19579/306948 [29:48<6:43:19, 11.87it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19581/306948 [29:48<6:35:42, 12.10it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19583/306948 [29:49<5:59:30, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19585/306948 [29:49<6:27:17, 12.37it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19587/306948 [29:49<5:53:26, 13.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19589/306948 [29:49<6:17:27, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19591/306948 [29:49<5:54:00, 13.53it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19593/306948 [29:49<5:55:32, 13.47it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19595/306948 [29:49<5:39:22, 14.11it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19597/306948 [29:50<6:31:25, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19599/306948 [29:50<6:17:27, 12.69it/s]\u001b[A09/17/2021 06:02:56 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616000\n",
            "09/17/2021 06:02:56 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615000] due to args.save_total_limit\n",
            "09/17/2021 06:03:00 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616000\n",
            "\n",
            "Iteration:   6%|▋         | 19601/306948 [29:55<66:18:12,  1.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19603/306948 [29:55<48:49:59,  1.63it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19604/306948 [29:55<42:19:50,  1.89it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19606/306948 [29:55<29:56:15,  2.67it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19608/306948 [29:56<22:42:51,  3.51it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19610/306948 [29:56<17:07:50,  4.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19612/306948 [29:56<14:20:18,  5.57it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19614/306948 [29:56<11:35:41,  6.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19616/306948 [29:56<9:58:42,  8.00it/s] \u001b[A\n",
            "Iteration:   6%|▋         | 19618/306948 [29:56<8:40:16,  9.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19620/306948 [29:57<7:56:32, 10.05it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19622/306948 [29:57<7:27:53, 10.69it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19624/306948 [29:57<7:46:43, 10.26it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19626/306948 [29:57<6:48:46, 11.71it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19628/306948 [29:57<7:20:28, 10.87it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19630/306948 [29:57<7:07:02, 11.21it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19632/306948 [29:58<6:54:33, 11.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19634/306948 [29:58<6:17:52, 12.67it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19636/306948 [29:58<6:02:47, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19638/306948 [29:58<5:41:50, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19640/306948 [29:58<6:00:19, 13.29it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19642/306948 [29:58<5:25:41, 14.70it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19644/306948 [29:58<5:39:31, 14.10it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19646/306948 [29:59<6:01:18, 13.25it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19648/306948 [29:59<6:15:02, 12.77it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19650/306948 [29:59<6:07:10, 13.04it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19652/306948 [29:59<6:41:07, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19654/306948 [29:59<6:08:22, 13.00it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19656/306948 [29:59<6:22:44, 12.51it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19658/306948 [30:00<6:18:18, 12.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19660/306948 [30:00<6:03:38, 13.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19662/306948 [30:00<5:48:36, 13.74it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19664/306948 [30:00<6:08:57, 12.98it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19666/306948 [30:00<6:41:01, 11.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19668/306948 [30:00<6:37:14, 12.05it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19670/306948 [30:00<6:11:44, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19672/306948 [30:01<6:10:25, 12.93it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19674/306948 [30:01<5:45:06, 13.87it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19676/306948 [30:01<6:02:19, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19678/306948 [30:01<5:40:48, 14.05it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19680/306948 [30:01<5:45:03, 13.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19682/306948 [30:01<6:47:17, 11.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19684/306948 [30:02<6:22:33, 12.52it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19686/306948 [30:02<5:48:06, 13.75it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19688/306948 [30:02<6:11:40, 12.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19690/306948 [30:02<6:28:53, 12.31it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19692/306948 [30:02<6:17:33, 12.68it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19694/306948 [30:02<6:22:56, 12.50it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19696/306948 [30:03<6:33:28, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19698/306948 [30:03<5:55:11, 13.48it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19700/306948 [30:03<6:00:02, 13.30it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19702/306948 [30:03<5:39:52, 14.09it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19704/306948 [30:03<5:41:48, 14.01it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19706/306948 [30:03<5:38:52, 14.13it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19708/306948 [30:03<6:24:29, 12.45it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19710/306948 [30:04<6:03:17, 13.18it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19712/306948 [30:04<6:02:00, 13.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19714/306948 [30:04<6:21:34, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19716/306948 [30:04<6:18:36, 12.64it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19718/306948 [30:04<5:51:58, 13.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19720/306948 [30:04<6:20:13, 12.59it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19722/306948 [30:04<6:10:56, 12.91it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19724/306948 [30:05<5:53:37, 13.54it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19726/306948 [30:05<5:41:14, 14.03it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19728/306948 [30:05<6:08:24, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19730/306948 [30:05<5:58:31, 13.35it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19732/306948 [30:05<7:20:30, 10.87it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19734/306948 [30:05<7:17:53, 10.93it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19736/306948 [30:06<7:01:18, 11.36it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19738/306948 [30:06<7:10:05, 11.13it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19740/306948 [30:06<7:16:56, 10.96it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19742/306948 [30:06<6:47:25, 11.75it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19744/306948 [30:06<7:29:04, 10.66it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19746/306948 [30:07<7:22:38, 10.81it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19748/306948 [30:07<7:21:17, 10.85it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19750/306948 [30:07<6:52:25, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19752/306948 [30:07<6:33:09, 12.17it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19754/306948 [30:07<6:09:48, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19756/306948 [30:07<6:48:17, 11.72it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19758/306948 [30:08<7:14:36, 11.01it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19760/306948 [30:08<6:46:28, 11.78it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19762/306948 [30:08<6:21:14, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19764/306948 [30:08<6:21:45, 12.54it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19766/306948 [30:08<5:55:37, 13.46it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19768/306948 [30:08<6:16:09, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19770/306948 [30:08<6:14:22, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19772/306948 [30:09<6:16:58, 12.70it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19774/306948 [30:09<6:02:36, 13.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19776/306948 [30:09<6:03:38, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19778/306948 [30:09<5:58:39, 13.34it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19780/306948 [30:09<6:32:10, 12.20it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19782/306948 [30:09<5:58:30, 13.35it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19784/306948 [30:10<5:59:14, 13.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19786/306948 [30:10<5:47:33, 13.77it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19788/306948 [30:10<5:51:22, 13.62it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19790/306948 [30:10<5:41:59, 13.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19792/306948 [30:10<6:14:59, 12.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19794/306948 [30:10<6:12:07, 12.86it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19796/306948 [30:10<6:28:14, 12.33it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19798/306948 [30:11<6:06:40, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19800/306948 [30:11<6:21:15, 12.55it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19802/306948 [30:11<6:09:19, 12.96it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19804/306948 [30:11<6:26:05, 12.40it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19806/306948 [30:11<5:58:45, 13.34it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19808/306948 [30:11<5:48:59, 13.71it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19810/306948 [30:11<5:33:19, 14.36it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19812/306948 [30:12<5:48:37, 13.73it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19814/306948 [30:12<5:50:05, 13.67it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19816/306948 [30:12<6:02:13, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19818/306948 [30:12<5:55:20, 13.47it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19820/306948 [30:12<6:27:11, 12.36it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19822/306948 [30:12<5:50:59, 13.63it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19824/306948 [30:13<6:12:39, 12.84it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19826/306948 [30:13<5:52:21, 13.58it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19828/306948 [30:13<6:05:55, 13.08it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19830/306948 [30:13<5:57:31, 13.38it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19832/306948 [30:13<5:59:03, 13.33it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19834/306948 [30:13<5:39:08, 14.11it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19836/306948 [30:13<6:02:22, 13.21it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19838/306948 [30:14<5:59:50, 13.30it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19840/306948 [30:14<6:24:33, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19842/306948 [30:14<6:11:19, 12.89it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19844/306948 [30:14<6:15:14, 12.75it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19846/306948 [30:14<5:47:23, 13.77it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19848/306948 [30:14<7:06:10, 11.23it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19850/306948 [30:15<6:41:10, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19852/306948 [30:15<6:39:42, 11.97it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19854/306948 [30:15<6:16:18, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19856/306948 [30:15<6:39:05, 11.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19858/306948 [30:15<6:24:07, 12.46it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19860/306948 [30:15<6:55:04, 11.53it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19862/306948 [30:16<6:31:25, 12.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19864/306948 [30:16<6:40:26, 11.95it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19866/306948 [30:16<7:10:59, 11.10it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19868/306948 [30:16<7:04:18, 11.28it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19870/306948 [30:16<6:23:42, 12.47it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19872/306948 [30:17<7:03:23, 11.30it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19874/306948 [30:17<6:28:12, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19876/306948 [30:17<6:29:00, 12.30it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19878/306948 [30:17<6:09:41, 12.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19880/306948 [30:17<6:51:58, 11.61it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19882/306948 [30:17<6:34:45, 12.12it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19884/306948 [30:17<6:20:56, 12.56it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19886/306948 [30:18<6:01:05, 13.25it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19888/306948 [30:18<5:51:53, 13.60it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19890/306948 [30:18<5:47:45, 13.76it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19892/306948 [30:18<5:40:22, 14.06it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19894/306948 [30:18<5:43:06, 13.94it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19896/306948 [30:18<6:41:02, 11.93it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19898/306948 [30:18<6:08:12, 12.99it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19900/306948 [30:19<7:06:26, 11.22it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19902/306948 [30:19<6:26:34, 12.38it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19904/306948 [30:19<6:41:13, 11.92it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19906/306948 [30:19<6:30:53, 12.24it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19908/306948 [30:19<6:28:11, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19910/306948 [30:20<7:09:18, 11.14it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19912/306948 [30:20<7:00:01, 11.39it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19914/306948 [30:20<6:15:59, 12.72it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19916/306948 [30:20<6:41:50, 11.90it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19918/306948 [30:20<6:22:58, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19920/306948 [30:20<6:34:27, 12.13it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19922/306948 [30:20<6:06:38, 13.05it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19924/306948 [30:21<6:17:05, 12.69it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19926/306948 [30:21<6:03:31, 13.16it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19928/306948 [30:21<6:12:48, 12.83it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19930/306948 [30:21<6:22:52, 12.49it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19932/306948 [30:21<6:32:44, 12.18it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19934/306948 [30:21<6:14:02, 12.79it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19936/306948 [30:22<6:28:14, 12.32it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19938/306948 [30:22<6:20:30, 12.57it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19940/306948 [30:22<6:24:31, 12.44it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19942/306948 [30:22<6:45:34, 11.79it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19944/306948 [30:22<6:42:41, 11.88it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19946/306948 [30:22<7:04:55, 11.26it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19948/306948 [30:23<7:11:40, 11.08it/s]\u001b[A\n",
            "Iteration:   6%|▋         | 19950/306948 [30:23<6:34:05, 12.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19952/306948 [30:23<7:22:58, 10.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19954/306948 [30:23<7:17:55, 10.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19956/306948 [30:23<6:51:12, 11.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19958/306948 [30:23<6:27:11, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19960/306948 [30:24<6:25:35, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19962/306948 [30:24<6:15:34, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19964/306948 [30:24<6:28:51, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19966/306948 [30:24<6:03:59, 13.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19968/306948 [30:24<6:21:26, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19970/306948 [30:24<6:00:24, 13.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19972/306948 [30:25<6:21:37, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19974/306948 [30:25<6:15:13, 12.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19976/306948 [30:25<6:28:57, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19978/306948 [30:25<6:17:53, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19980/306948 [30:25<6:42:37, 11.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19982/306948 [30:25<6:24:52, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19984/306948 [30:26<6:11:24, 12.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19986/306948 [30:26<6:02:00, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19988/306948 [30:26<6:24:17, 12.45it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19990/306948 [30:26<6:02:47, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19992/306948 [30:26<6:59:11, 11.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19994/306948 [30:26<7:05:52, 11.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19996/306948 [30:27<7:03:00, 11.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 19998/306948 [30:27<6:25:07, 12.42it/s]\u001b[A09/17/2021 06:03:33 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616100\n",
            "09/17/2021 06:03:33 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615100] due to args.save_total_limit\n",
            "09/17/2021 06:03:37 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616100\n",
            "\n",
            "Iteration:   7%|▋         | 20000/306948 [30:32<65:59:12,  1.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20002/306948 [30:32<47:50:06,  1.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20004/306948 [30:32<36:17:15,  2.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20006/306948 [30:32<27:47:28,  2.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20008/306948 [30:33<21:24:15,  3.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20010/306948 [30:33<16:37:11,  4.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20012/306948 [30:33<13:57:10,  5.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20014/306948 [30:33<12:08:43,  6.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20016/306948 [30:33<10:17:55,  7.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20018/306948 [30:33<8:45:27,  9.10it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 20020/306948 [30:34<7:50:37, 10.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20022/306948 [30:34<6:54:51, 11.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20024/306948 [30:34<6:32:30, 12.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20026/306948 [30:34<6:42:09, 11.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20028/306948 [30:34<6:25:25, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20030/306948 [30:34<6:06:41, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20032/306948 [30:34<6:18:33, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20034/306948 [30:35<6:16:22, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20036/306948 [30:35<6:08:12, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20038/306948 [30:35<6:09:59, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20040/306948 [30:35<7:04:38, 11.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20042/306948 [30:35<6:52:47, 11.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20044/306948 [30:35<6:30:54, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20046/306948 [30:36<6:04:49, 13.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20048/306948 [30:36<6:08:13, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20050/306948 [30:36<6:01:22, 13.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20052/306948 [30:36<6:20:26, 12.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20054/306948 [30:36<5:59:35, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20056/306948 [30:36<6:04:11, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20058/306948 [30:36<5:46:22, 13.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20060/306948 [30:37<6:02:28, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20062/306948 [30:37<5:45:26, 13.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20064/306948 [30:37<6:34:55, 12.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20066/306948 [30:37<6:15:16, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20068/306948 [30:37<6:13:53, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20070/306948 [30:37<5:51:26, 13.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20072/306948 [30:38<6:26:01, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20074/306948 [30:38<6:15:40, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20076/306948 [30:38<6:38:50, 11.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20078/306948 [30:38<6:01:59, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20080/306948 [30:38<6:30:48, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20082/306948 [30:38<6:22:24, 12.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20084/306948 [30:38<6:10:03, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20086/306948 [30:39<6:28:55, 12.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20088/306948 [30:39<6:25:31, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20090/306948 [30:39<6:15:17, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20092/306948 [30:39<6:10:36, 12.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20094/306948 [30:39<6:03:28, 13.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20096/306948 [30:39<6:45:55, 11.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20098/306948 [30:40<6:34:35, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20100/306948 [30:40<6:25:01, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20102/306948 [30:40<6:11:41, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20104/306948 [30:40<6:24:26, 12.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20106/306948 [30:40<5:52:36, 13.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20108/306948 [30:40<6:03:12, 13.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20110/306948 [30:41<6:10:51, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20112/306948 [30:41<6:23:45, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20114/306948 [30:41<6:37:03, 12.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20116/306948 [30:41<6:54:15, 11.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20118/306948 [30:41<6:32:04, 12.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20120/306948 [30:41<7:20:06, 10.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20122/306948 [30:42<6:41:38, 11.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20124/306948 [30:42<7:02:42, 11.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20126/306948 [30:42<7:02:24, 11.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20128/306948 [30:42<7:38:16, 10.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20130/306948 [30:42<7:09:06, 11.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20132/306948 [30:43<7:08:20, 11.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20134/306948 [30:43<6:50:21, 11.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20136/306948 [30:43<6:33:55, 12.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20138/306948 [30:43<6:10:56, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20140/306948 [30:43<6:15:22, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20142/306948 [30:43<6:20:32, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20144/306948 [30:43<6:23:28, 12.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20146/306948 [30:44<6:35:03, 12.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20148/306948 [30:44<6:36:44, 12.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20150/306948 [30:44<6:18:34, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20152/306948 [30:44<7:44:41, 10.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20154/306948 [30:44<6:55:59, 11.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20156/306948 [30:45<6:59:31, 11.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20158/306948 [30:45<6:49:07, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20160/306948 [30:45<6:40:16, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20162/306948 [30:45<6:18:37, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20164/306948 [30:45<6:09:50, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20166/306948 [30:45<5:53:10, 13.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20168/306948 [30:45<6:01:33, 13.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20170/306948 [30:46<6:07:25, 13.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20172/306948 [30:46<6:15:43, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20174/306948 [30:46<5:58:28, 13.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20176/306948 [30:46<6:10:17, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20178/306948 [30:46<5:56:56, 13.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20180/306948 [30:46<6:19:23, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20182/306948 [30:46<5:56:03, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20184/306948 [30:47<5:53:33, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20186/306948 [30:47<6:25:08, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20188/306948 [30:47<6:44:15, 11.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20190/306948 [30:47<6:24:33, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20192/306948 [30:47<6:21:55, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20194/306948 [30:47<6:11:04, 12.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20196/306948 [30:48<6:21:09, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20198/306948 [30:48<5:52:56, 13.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20200/306948 [30:48<6:22:08, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20202/306948 [30:48<6:57:52, 11.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20204/306948 [30:48<6:27:25, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20206/306948 [30:48<6:40:06, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20208/306948 [30:49<6:47:21, 11.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20210/306948 [30:49<6:08:01, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20212/306948 [30:49<6:12:26, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20214/306948 [30:49<6:07:25, 13.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20216/306948 [30:49<5:55:50, 13.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20218/306948 [30:49<5:50:09, 13.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20220/306948 [30:50<6:20:17, 12.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20222/306948 [30:50<6:11:09, 12.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20224/306948 [30:50<6:38:27, 11.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20226/306948 [30:50<6:44:50, 11.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20228/306948 [30:50<6:49:12, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20230/306948 [30:50<6:35:01, 12.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20232/306948 [30:51<6:35:15, 12.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20234/306948 [30:51<6:23:33, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20236/306948 [30:51<7:04:54, 11.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20238/306948 [30:51<6:47:55, 11.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20240/306948 [30:51<7:03:08, 11.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20242/306948 [30:51<6:26:22, 12.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20244/306948 [30:52<6:45:49, 11.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20246/306948 [30:52<6:33:50, 12.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20248/306948 [30:52<6:21:26, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20250/306948 [30:52<6:21:49, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20252/306948 [30:52<6:42:19, 11.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20254/306948 [30:52<6:14:54, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20256/306948 [30:52<5:58:14, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20258/306948 [30:53<6:01:15, 13.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20260/306948 [30:53<5:58:10, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20262/306948 [30:53<6:33:52, 12.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20264/306948 [30:53<7:21:42, 10.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20266/306948 [30:53<7:08:08, 11.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20268/306948 [30:54<6:43:51, 11.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20270/306948 [30:54<6:26:39, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20272/306948 [30:54<6:37:57, 12.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20274/306948 [30:54<5:58:03, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20276/306948 [30:54<6:16:21, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20278/306948 [30:54<5:49:51, 13.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20280/306948 [30:54<6:01:52, 13.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20282/306948 [30:55<6:01:23, 13.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20284/306948 [30:55<6:17:28, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20286/306948 [30:55<5:56:30, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20288/306948 [30:55<5:56:05, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20290/306948 [30:55<6:21:19, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20292/306948 [30:55<6:32:31, 12.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20294/306948 [30:56<6:17:29, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20296/306948 [30:56<6:25:49, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20298/306948 [30:56<6:14:30, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20300/306948 [30:56<6:27:56, 12.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20302/306948 [30:56<5:55:58, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20304/306948 [30:56<6:03:10, 13.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20306/306948 [30:56<5:52:32, 13.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20308/306948 [30:57<5:56:57, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20310/306948 [30:57<6:02:33, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20312/306948 [30:57<6:24:40, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20314/306948 [30:57<6:26:40, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20316/306948 [30:57<6:21:47, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20318/306948 [30:57<6:12:36, 12.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20320/306948 [30:58<5:58:37, 13.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20322/306948 [30:58<6:00:36, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20324/306948 [30:58<6:15:37, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20326/306948 [30:58<5:52:41, 13.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20328/306948 [30:58<6:04:17, 13.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20330/306948 [30:58<5:32:27, 14.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20332/306948 [30:58<5:39:44, 14.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20334/306948 [30:59<5:39:24, 14.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20336/306948 [30:59<5:54:31, 13.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20338/306948 [30:59<5:44:40, 13.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20340/306948 [30:59<6:13:08, 12.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20342/306948 [30:59<6:46:23, 11.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20344/306948 [30:59<6:27:08, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20346/306948 [31:00<6:43:48, 11.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20348/306948 [31:00<6:25:45, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20350/306948 [31:00<6:02:30, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20352/306948 [31:00<6:09:00, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20354/306948 [31:00<6:06:29, 13.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20356/306948 [31:00<7:10:32, 11.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20358/306948 [31:00<6:23:41, 12.45it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20360/306948 [31:01<6:47:28, 11.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20362/306948 [31:01<6:07:25, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20364/306948 [31:01<7:19:20, 10.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20366/306948 [31:01<6:46:22, 11.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20368/306948 [31:01<6:21:28, 12.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20370/306948 [31:01<6:06:24, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20372/306948 [31:02<5:57:23, 13.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20374/306948 [31:02<5:39:35, 14.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20376/306948 [31:02<7:13:47, 11.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20378/306948 [31:02<7:00:09, 11.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20380/306948 [31:02<7:20:19, 10.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20382/306948 [31:03<6:57:45, 11.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20384/306948 [31:03<6:50:15, 11.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20386/306948 [31:03<6:10:53, 12.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20388/306948 [31:03<6:16:47, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20390/306948 [31:03<6:05:59, 13.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20392/306948 [31:03<6:14:52, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20394/306948 [31:03<6:20:28, 12.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20396/306948 [31:04<6:26:16, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20398/306948 [31:04<5:54:23, 13.48it/s]\u001b[A09/17/2021 06:04:10 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616200\n",
            "09/17/2021 06:04:10 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615200] due to args.save_total_limit\n",
            "09/17/2021 06:04:14 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616200\n",
            "\n",
            "Iteration:   7%|▋         | 20400/306948 [31:09<63:47:11,  1.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20402/306948 [31:09<46:28:50,  1.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20404/306948 [31:09<34:31:23,  2.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20406/306948 [31:09<25:35:35,  3.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20408/306948 [31:09<19:38:42,  4.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20410/306948 [31:09<15:27:00,  5.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20412/306948 [31:10<12:55:23,  6.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20414/306948 [31:10<10:48:55,  7.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20416/306948 [31:10<9:23:31,  8.47it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 20418/306948 [31:10<8:06:25,  9.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20420/306948 [31:10<7:35:02, 10.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20422/306948 [31:10<6:58:09, 11.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20424/306948 [31:11<6:53:47, 11.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20426/306948 [31:11<6:31:38, 12.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20428/306948 [31:11<6:43:55, 11.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20430/306948 [31:11<6:17:51, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20432/306948 [31:11<6:39:24, 11.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20434/306948 [31:11<6:25:08, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20436/306948 [31:11<6:17:23, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20438/306948 [31:12<6:37:11, 12.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20440/306948 [31:12<6:16:44, 12.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20442/306948 [31:12<6:07:16, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20444/306948 [31:12<6:14:42, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20446/306948 [31:12<5:51:57, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20448/306948 [31:12<6:15:53, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20450/306948 [31:13<6:12:45, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20452/306948 [31:13<6:15:04, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20454/306948 [31:13<6:10:15, 12.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20456/306948 [31:13<6:41:22, 11.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20458/306948 [31:13<6:30:00, 12.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20460/306948 [31:13<6:09:29, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20462/306948 [31:13<5:46:34, 13.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20464/306948 [31:14<6:10:18, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20466/306948 [31:14<5:58:09, 13.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20468/306948 [31:14<5:37:22, 14.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20470/306948 [31:14<5:36:32, 14.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20472/306948 [31:14<5:52:43, 13.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20474/306948 [31:14<5:54:22, 13.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20476/306948 [31:15<6:15:34, 12.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20478/306948 [31:15<5:56:58, 13.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20480/306948 [31:15<6:36:20, 12.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20482/306948 [31:15<6:17:44, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20484/306948 [31:15<6:26:25, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20486/306948 [31:15<6:07:46, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20488/306948 [31:15<6:10:22, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20490/306948 [31:16<5:47:33, 13.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20492/306948 [31:16<6:48:06, 11.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20494/306948 [31:16<6:19:18, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20496/306948 [31:16<6:08:50, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20498/306948 [31:16<5:48:37, 13.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20500/306948 [31:16<6:18:48, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20502/306948 [31:17<6:03:38, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20504/306948 [31:17<6:49:00, 11.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20506/306948 [31:17<6:30:15, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20508/306948 [31:17<6:31:22, 12.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20510/306948 [31:17<6:17:17, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20512/306948 [31:17<6:11:49, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20514/306948 [31:18<5:56:13, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20516/306948 [31:18<6:26:47, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20518/306948 [31:18<5:48:10, 13.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20520/306948 [31:18<6:19:23, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20522/306948 [31:18<6:18:48, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20524/306948 [31:18<6:27:22, 12.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20526/306948 [31:19<6:59:11, 11.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20528/306948 [31:19<6:35:04, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20530/306948 [31:19<6:49:22, 11.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20532/306948 [31:19<6:55:58, 11.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20534/306948 [31:19<6:24:31, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20536/306948 [31:19<6:53:40, 11.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20538/306948 [31:20<6:33:57, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20540/306948 [31:20<6:34:27, 12.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20542/306948 [31:20<6:49:34, 11.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20544/306948 [31:20<6:46:16, 11.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20546/306948 [31:20<6:28:30, 12.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20548/306948 [31:20<6:27:30, 12.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20550/306948 [31:20<6:15:54, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20552/306948 [31:21<6:37:37, 12.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20554/306948 [31:21<6:09:44, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20556/306948 [31:21<6:25:28, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20558/306948 [31:21<6:02:36, 13.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20560/306948 [31:21<6:18:54, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20562/306948 [31:21<5:52:07, 13.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20564/306948 [31:22<5:51:02, 13.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20566/306948 [31:22<6:11:34, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20568/306948 [31:22<6:34:53, 12.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20570/306948 [31:22<6:06:36, 13.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20572/306948 [31:22<6:29:55, 12.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20574/306948 [31:22<6:30:47, 12.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20576/306948 [31:23<6:13:10, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20578/306948 [31:23<5:52:08, 13.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20580/306948 [31:23<6:01:57, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20582/306948 [31:23<6:21:16, 12.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20584/306948 [31:23<6:44:02, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20586/306948 [31:23<6:29:08, 12.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20588/306948 [31:23<6:25:52, 12.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20590/306948 [31:24<5:58:27, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20592/306948 [31:24<6:08:32, 12.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20594/306948 [31:24<5:36:36, 14.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20596/306948 [31:24<5:56:56, 13.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20598/306948 [31:24<5:34:28, 14.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20600/306948 [31:24<5:50:16, 13.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20602/306948 [31:24<5:46:12, 13.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20604/306948 [31:25<6:33:56, 12.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20606/306948 [31:25<6:22:16, 12.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20608/306948 [31:25<6:25:05, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20610/306948 [31:25<6:06:59, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20612/306948 [31:25<6:24:32, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20614/306948 [31:25<6:14:13, 12.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20616/306948 [31:26<6:16:00, 12.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20618/306948 [31:26<6:20:27, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20620/306948 [31:26<7:46:31, 10.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20622/306948 [31:26<7:02:19, 11.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20624/306948 [31:26<7:03:13, 11.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20626/306948 [31:27<6:24:54, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20628/306948 [31:27<6:11:04, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20630/306948 [31:27<6:06:16, 13.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20632/306948 [31:27<6:22:23, 12.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20634/306948 [31:27<6:28:50, 12.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20636/306948 [31:27<6:27:19, 12.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20638/306948 [31:27<5:55:15, 13.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20640/306948 [31:28<5:55:57, 13.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20642/306948 [31:28<5:56:10, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20644/306948 [31:28<6:18:04, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20646/306948 [31:28<6:14:33, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20648/306948 [31:28<6:36:18, 12.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20650/306948 [31:28<6:49:50, 11.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20652/306948 [31:29<6:37:35, 12.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20654/306948 [31:29<6:01:31, 13.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20656/306948 [31:29<6:13:34, 12.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20658/306948 [31:29<5:46:47, 13.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20660/306948 [31:29<5:52:57, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20662/306948 [31:29<6:32:25, 12.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20664/306948 [31:29<6:23:43, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20666/306948 [31:30<6:13:22, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20668/306948 [31:30<6:15:03, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20670/306948 [31:30<6:24:52, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20672/306948 [31:30<6:26:56, 12.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20674/306948 [31:30<6:19:28, 12.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20676/306948 [31:31<7:53:23, 10.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20678/306948 [31:31<7:17:04, 10.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20680/306948 [31:31<6:53:54, 11.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20682/306948 [31:31<6:29:39, 12.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20684/306948 [31:31<6:57:03, 11.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20686/306948 [31:31<6:26:04, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20688/306948 [31:32<6:30:15, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20690/306948 [31:32<6:06:51, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20692/306948 [31:32<6:32:43, 12.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20694/306948 [31:32<6:08:45, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20696/306948 [31:32<6:01:42, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20698/306948 [31:32<5:58:25, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20700/306948 [31:32<6:08:56, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20702/306948 [31:33<5:52:46, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20704/306948 [31:33<5:49:15, 13.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20706/306948 [31:33<5:55:47, 13.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20708/306948 [31:33<6:13:22, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20710/306948 [31:33<5:51:47, 13.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20712/306948 [31:33<5:43:43, 13.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20714/306948 [31:33<5:38:56, 14.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20716/306948 [31:34<6:15:35, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20718/306948 [31:34<6:02:26, 13.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20720/306948 [31:34<6:11:36, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20722/306948 [31:34<6:01:36, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20724/306948 [31:34<6:19:48, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20726/306948 [31:34<6:12:34, 12.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20728/306948 [31:35<6:28:17, 12.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20730/306948 [31:35<6:09:29, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20732/306948 [31:35<6:02:38, 13.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20734/306948 [31:35<5:41:10, 13.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20736/306948 [31:35<6:08:22, 12.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20738/306948 [31:35<5:59:44, 13.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20740/306948 [31:35<6:12:22, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20742/306948 [31:36<6:08:54, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20744/306948 [31:36<6:06:04, 13.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20746/306948 [31:36<6:30:06, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20748/306948 [31:36<7:04:22, 11.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20750/306948 [31:36<6:33:26, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20752/306948 [31:37<7:01:13, 11.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20754/306948 [31:37<6:08:39, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20756/306948 [31:37<6:31:04, 12.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20758/306948 [31:37<5:52:52, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20760/306948 [31:37<6:03:27, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20762/306948 [31:37<5:51:59, 13.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20764/306948 [31:37<6:47:01, 11.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20766/306948 [31:38<6:31:30, 12.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20768/306948 [31:38<6:48:28, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20770/306948 [31:38<6:50:05, 11.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20772/306948 [31:38<6:43:03, 11.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20774/306948 [31:38<6:10:30, 12.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20776/306948 [31:38<6:43:21, 11.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20778/306948 [31:39<6:09:21, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20780/306948 [31:39<6:22:44, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20782/306948 [31:39<6:43:56, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20784/306948 [31:39<6:50:29, 11.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20786/306948 [31:39<6:36:01, 12.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20788/306948 [31:39<6:36:33, 12.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20790/306948 [31:40<6:39:31, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20792/306948 [31:40<8:30:43,  9.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20794/306948 [31:40<7:55:20, 10.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20796/306948 [31:40<7:22:12, 10.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20798/306948 [31:40<7:23:30, 10.75it/s]\u001b[A09/17/2021 06:04:47 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616300\n",
            "09/17/2021 06:04:47 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615300] due to args.save_total_limit\n",
            "09/17/2021 06:04:51 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616300\n",
            "\n",
            "Iteration:   7%|▋         | 20800/306948 [31:46<67:11:55,  1.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20802/306948 [31:46<48:33:01,  1.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20804/306948 [31:46<36:53:07,  2.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20806/306948 [31:46<27:26:05,  2.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20808/306948 [31:46<20:51:52,  3.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20810/306948 [31:46<16:30:53,  4.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20812/306948 [31:47<13:30:34,  5.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20814/306948 [31:47<11:21:24,  7.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20816/306948 [31:47<10:15:40,  7.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20818/306948 [31:47<8:38:56,  9.19it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 20820/306948 [31:47<8:24:40,  9.45it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20822/306948 [31:47<7:32:13, 10.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20824/306948 [31:48<7:46:28, 10.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20826/306948 [31:48<7:07:03, 11.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20828/306948 [31:48<6:43:49, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20830/306948 [31:48<6:47:34, 11.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20832/306948 [31:48<6:24:35, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20834/306948 [31:48<6:35:34, 12.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20836/306948 [31:49<6:21:10, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20838/306948 [31:49<6:23:33, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20840/306948 [31:49<6:17:41, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20842/306948 [31:49<6:22:56, 12.45it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20844/306948 [31:49<6:41:59, 11.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20846/306948 [31:49<6:56:51, 11.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20848/306948 [31:50<7:26:24, 10.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20850/306948 [31:50<6:41:14, 11.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20852/306948 [31:50<6:38:58, 11.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20854/306948 [31:50<6:36:13, 12.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20856/306948 [31:50<7:02:10, 11.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20858/306948 [31:50<6:51:40, 11.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20860/306948 [31:51<6:35:21, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20862/306948 [31:51<6:29:21, 12.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20864/306948 [31:51<6:17:56, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20866/306948 [31:51<6:36:17, 12.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20868/306948 [31:51<6:41:11, 11.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20870/306948 [31:51<6:23:23, 12.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20872/306948 [31:52<6:13:10, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20874/306948 [31:52<6:07:32, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20876/306948 [31:52<6:13:39, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20878/306948 [31:52<6:03:23, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20880/306948 [31:52<6:12:42, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20882/306948 [31:52<6:10:48, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20884/306948 [31:52<6:35:09, 12.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20886/306948 [31:53<6:24:42, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20888/306948 [31:53<6:11:19, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20890/306948 [31:53<6:27:34, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20892/306948 [31:53<6:20:25, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20894/306948 [31:53<6:14:33, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20896/306948 [31:53<6:26:09, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20898/306948 [31:54<6:17:45, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20900/306948 [31:54<6:13:02, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20902/306948 [31:54<5:53:21, 13.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20904/306948 [31:54<5:57:19, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20906/306948 [31:54<5:39:52, 14.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20908/306948 [31:54<5:58:25, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20910/306948 [31:54<5:57:02, 13.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20912/306948 [31:55<5:57:05, 13.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20914/306948 [31:55<6:19:06, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20916/306948 [31:55<6:38:44, 11.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20918/306948 [31:55<6:25:09, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20920/306948 [31:55<8:13:36,  9.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20922/306948 [31:56<7:29:52, 10.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20924/306948 [31:56<6:50:51, 11.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20926/306948 [31:56<6:31:13, 12.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20928/306948 [31:56<6:10:44, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20930/306948 [31:56<5:52:36, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20932/306948 [31:56<6:29:50, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20934/306948 [31:56<6:24:11, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20936/306948 [31:57<6:23:59, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20938/306948 [31:57<5:52:56, 13.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20940/306948 [31:57<5:47:51, 13.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20942/306948 [31:57<5:31:29, 14.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20944/306948 [31:57<5:50:55, 13.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20946/306948 [31:57<5:27:48, 14.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20948/306948 [31:57<5:47:21, 13.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20950/306948 [31:58<6:01:59, 13.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20952/306948 [31:58<6:21:31, 12.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20954/306948 [31:58<5:57:25, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20956/306948 [31:58<6:10:31, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20958/306948 [31:58<6:11:39, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20960/306948 [31:58<6:18:57, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20962/306948 [31:59<6:33:44, 12.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20964/306948 [31:59<6:28:33, 12.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20966/306948 [31:59<6:10:03, 12.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20968/306948 [31:59<6:20:29, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20970/306948 [31:59<5:57:12, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20972/306948 [31:59<5:59:49, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20974/306948 [32:00<5:58:14, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20976/306948 [32:00<5:55:27, 13.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20978/306948 [32:00<5:53:16, 13.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20980/306948 [32:00<6:16:13, 12.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20982/306948 [32:00<6:42:17, 11.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20984/306948 [32:00<6:57:14, 11.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20986/306948 [32:00<6:23:03, 12.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20988/306948 [32:01<6:24:11, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20990/306948 [32:01<6:05:39, 13.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20992/306948 [32:01<6:03:01, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20994/306948 [32:01<5:52:36, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20996/306948 [32:01<5:54:30, 13.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 20998/306948 [32:01<6:15:48, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21000/306948 [32:02<6:15:55, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21002/306948 [32:02<5:50:15, 13.61it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21004/306948 [32:02<6:01:16, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21006/306948 [32:02<5:47:44, 13.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21008/306948 [32:02<6:36:51, 12.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21010/306948 [32:02<6:13:02, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21012/306948 [32:03<6:20:39, 12.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21014/306948 [32:03<6:59:04, 11.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21016/306948 [32:03<6:37:01, 12.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21018/306948 [32:03<5:59:59, 13.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21020/306948 [32:03<6:13:20, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21022/306948 [32:03<6:18:47, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21024/306948 [32:03<6:30:01, 12.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21026/306948 [32:04<6:03:06, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21028/306948 [32:04<6:38:35, 11.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21030/306948 [32:04<8:13:22,  9.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21032/306948 [32:04<7:16:41, 10.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21034/306948 [32:04<7:12:34, 11.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21036/306948 [32:05<7:18:43, 10.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21038/306948 [32:05<6:50:53, 11.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21040/306948 [32:05<6:34:51, 12.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21042/306948 [32:05<6:14:24, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21044/306948 [32:05<6:08:46, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21046/306948 [32:05<6:25:47, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21048/306948 [32:06<6:30:42, 12.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21050/306948 [32:06<6:16:41, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21052/306948 [32:06<6:20:06, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21054/306948 [32:06<5:58:32, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21056/306948 [32:06<6:25:55, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21058/306948 [32:06<6:18:14, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21060/306948 [32:06<5:58:06, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21062/306948 [32:07<5:56:59, 13.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21064/306948 [32:07<6:10:45, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21066/306948 [32:07<6:16:47, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21068/306948 [32:07<6:28:36, 12.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21070/306948 [32:07<5:56:21, 13.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21072/306948 [32:07<6:12:43, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21074/306948 [32:08<6:09:32, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21076/306948 [32:08<6:15:05, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21078/306948 [32:08<5:56:45, 13.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21080/306948 [32:08<8:18:04,  9.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21082/306948 [32:08<7:19:51, 10.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21084/306948 [32:08<7:03:23, 11.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21086/306948 [32:09<6:32:19, 12.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21088/306948 [32:09<6:25:22, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21090/306948 [32:09<6:09:05, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21092/306948 [32:09<6:57:25, 11.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21094/306948 [32:09<6:41:30, 11.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21096/306948 [32:09<6:55:53, 11.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21098/306948 [32:10<6:21:26, 12.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21100/306948 [32:10<6:52:22, 11.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21102/306948 [32:10<6:19:14, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21104/306948 [32:10<6:14:59, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21106/306948 [32:10<5:55:35, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21108/306948 [32:10<6:02:26, 13.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21110/306948 [32:11<6:39:41, 11.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21112/306948 [32:11<6:36:07, 12.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21114/306948 [32:11<6:28:22, 12.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21116/306948 [32:11<6:11:29, 12.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21118/306948 [32:11<7:23:42, 10.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21120/306948 [32:11<7:14:28, 10.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21122/306948 [32:12<6:47:37, 11.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21124/306948 [32:12<7:11:07, 11.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21126/306948 [32:12<6:38:07, 11.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21128/306948 [32:12<6:45:54, 11.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21130/306948 [32:12<6:12:16, 12.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21132/306948 [32:12<6:21:33, 12.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21134/306948 [32:13<5:46:30, 13.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21136/306948 [32:13<6:08:23, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21138/306948 [32:13<5:59:31, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21140/306948 [32:13<5:59:33, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21142/306948 [32:13<5:35:44, 14.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21144/306948 [32:13<5:26:57, 14.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21146/306948 [32:13<5:33:10, 14.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21148/306948 [32:14<6:18:08, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21150/306948 [32:14<5:44:33, 13.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21152/306948 [32:14<6:16:44, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21154/306948 [32:14<5:45:39, 13.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21156/306948 [32:14<6:17:52, 12.61it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21158/306948 [32:14<5:58:02, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21160/306948 [32:14<6:11:43, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21162/306948 [32:15<5:59:20, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21164/306948 [32:15<6:27:22, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21166/306948 [32:15<6:06:55, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21168/306948 [32:15<6:04:57, 13.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21170/306948 [32:15<6:04:43, 13.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21172/306948 [32:15<6:32:44, 12.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21174/306948 [32:16<6:06:34, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21176/306948 [32:16<6:34:59, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21178/306948 [32:16<6:50:35, 11.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21180/306948 [32:16<7:00:36, 11.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21182/306948 [32:16<7:06:17, 11.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21184/306948 [32:17<7:24:38, 10.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21186/306948 [32:17<6:55:06, 11.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21188/306948 [32:17<6:53:07, 11.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21190/306948 [32:17<6:42:38, 11.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21192/306948 [32:17<6:39:58, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21194/306948 [32:17<6:16:45, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21196/306948 [32:17<6:27:33, 12.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21198/306948 [32:18<6:19:46, 12.54it/s]\u001b[A09/17/2021 06:05:24 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616400\n",
            "09/17/2021 06:05:24 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615400] due to args.save_total_limit\n",
            "09/17/2021 06:05:28 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616400\n",
            "\n",
            "Iteration:   7%|▋         | 21200/306948 [32:23<66:36:35,  1.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21202/306948 [32:23<48:16:49,  1.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21204/306948 [32:23<35:43:19,  2.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21206/306948 [32:23<26:31:04,  2.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21208/306948 [32:23<20:23:44,  3.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21210/306948 [32:24<15:55:27,  4.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21212/306948 [32:24<12:50:55,  6.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21214/306948 [32:24<10:43:30,  7.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21216/306948 [32:24<9:47:40,  8.10it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 21218/306948 [32:24<8:22:51,  9.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21220/306948 [32:24<8:47:47,  9.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21222/306948 [32:25<7:50:56, 10.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21224/306948 [32:25<7:13:38, 10.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21226/306948 [32:25<7:00:55, 11.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21228/306948 [32:25<6:38:45, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21230/306948 [32:25<6:08:40, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21232/306948 [32:25<6:06:56, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21234/306948 [32:25<5:53:24, 13.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21236/306948 [32:26<6:10:30, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21238/306948 [32:26<6:18:06, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21240/306948 [32:26<6:39:53, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21242/306948 [32:26<6:18:20, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21244/306948 [32:26<7:04:57, 11.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21246/306948 [32:26<6:48:55, 11.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21248/306948 [32:27<6:39:14, 11.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21250/306948 [32:27<6:03:53, 13.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21252/306948 [32:27<5:55:55, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21254/306948 [32:27<5:59:03, 13.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21256/306948 [32:27<6:27:41, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21258/306948 [32:27<6:08:15, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21260/306948 [32:28<6:48:43, 11.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21262/306948 [32:28<6:21:33, 12.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21264/306948 [32:28<6:17:41, 12.61it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21266/306948 [32:28<6:32:47, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21268/306948 [32:28<6:22:49, 12.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21270/306948 [32:28<6:14:25, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21272/306948 [32:29<6:07:42, 12.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21274/306948 [32:29<5:57:29, 13.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21276/306948 [32:29<5:50:25, 13.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21278/306948 [32:29<5:51:40, 13.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21280/306948 [32:29<5:50:46, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21282/306948 [32:29<5:58:15, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21284/306948 [32:29<6:17:21, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21286/306948 [32:30<5:45:35, 13.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21288/306948 [32:30<5:54:48, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21290/306948 [32:30<5:38:24, 14.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21292/306948 [32:30<6:01:01, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21294/306948 [32:30<6:24:08, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21296/306948 [32:30<6:27:34, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21298/306948 [32:30<6:01:32, 13.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21300/306948 [32:31<6:12:58, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21302/306948 [32:31<6:30:49, 12.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21304/306948 [32:31<6:27:35, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21306/306948 [32:31<5:56:50, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21308/306948 [32:31<6:08:51, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21310/306948 [32:31<5:39:14, 14.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21312/306948 [32:32<5:55:50, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21314/306948 [32:32<5:42:28, 13.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21316/306948 [32:32<5:43:58, 13.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21318/306948 [32:32<5:27:59, 14.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21320/306948 [32:32<6:04:14, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21322/306948 [32:32<5:59:39, 13.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21324/306948 [32:32<6:24:37, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21326/306948 [32:33<5:49:17, 13.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21328/306948 [32:33<6:11:03, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21330/306948 [32:33<6:20:44, 12.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21332/306948 [32:33<6:32:53, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21334/306948 [32:33<6:07:44, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21336/306948 [32:33<6:26:29, 12.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21338/306948 [32:34<6:40:33, 11.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21340/306948 [32:34<6:56:48, 11.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21342/306948 [32:34<6:35:31, 12.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21344/306948 [32:34<6:18:25, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21346/306948 [32:34<5:55:55, 13.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21348/306948 [32:34<6:09:27, 12.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21350/306948 [32:35<5:52:57, 13.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21352/306948 [32:35<5:59:00, 13.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21354/306948 [32:35<6:00:44, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21356/306948 [32:35<6:29:36, 12.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21358/306948 [32:35<6:23:46, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21360/306948 [32:35<6:42:04, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21362/306948 [32:35<6:16:35, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21364/306948 [32:36<6:25:57, 12.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21366/306948 [32:36<6:05:27, 13.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21368/306948 [32:36<6:08:35, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21370/306948 [32:36<5:56:55, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21372/306948 [32:36<6:32:04, 12.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21374/306948 [32:36<6:42:44, 11.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21376/306948 [32:37<6:23:11, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21378/306948 [32:37<6:06:54, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21380/306948 [32:37<6:22:02, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21382/306948 [32:37<6:10:14, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21384/306948 [32:37<6:11:25, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21386/306948 [32:37<6:19:11, 12.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21388/306948 [32:38<6:33:53, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21390/306948 [32:38<6:19:12, 12.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21392/306948 [32:38<6:16:51, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21394/306948 [32:38<6:47:34, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21396/306948 [32:38<6:46:02, 11.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21398/306948 [32:38<6:09:00, 12.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21400/306948 [32:39<6:16:53, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21402/306948 [32:39<6:22:54, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21404/306948 [32:39<6:16:15, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21406/306948 [32:39<6:42:04, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21408/306948 [32:39<6:36:23, 12.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21410/306948 [32:39<6:10:33, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21412/306948 [32:40<6:24:09, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21414/306948 [32:40<5:54:19, 13.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21416/306948 [32:40<5:58:21, 13.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21418/306948 [32:40<5:55:22, 13.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21420/306948 [32:40<5:51:47, 13.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21422/306948 [32:40<6:06:49, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21424/306948 [32:40<6:59:17, 11.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21426/306948 [32:41<6:34:33, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21428/306948 [32:41<6:27:11, 12.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21430/306948 [32:41<6:54:13, 11.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21432/306948 [32:41<6:37:28, 11.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21434/306948 [32:41<6:19:47, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21436/306948 [32:41<6:48:07, 11.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21438/306948 [32:42<6:13:30, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21440/306948 [32:42<6:15:43, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21442/306948 [32:42<5:56:14, 13.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21444/306948 [32:42<6:05:54, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21446/306948 [32:42<6:23:42, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21448/306948 [32:42<6:43:37, 11.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21450/306948 [32:43<6:26:52, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21452/306948 [32:43<7:00:59, 11.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21454/306948 [32:43<6:31:40, 12.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21456/306948 [32:43<7:14:50, 10.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21458/306948 [32:43<7:20:27, 10.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21460/306948 [32:43<6:58:56, 11.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21462/306948 [32:44<6:19:49, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21464/306948 [32:44<6:12:50, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21466/306948 [32:44<6:10:18, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21468/306948 [32:44<6:24:51, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21470/306948 [32:44<6:11:04, 12.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21472/306948 [32:44<6:26:39, 12.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21474/306948 [32:45<6:16:42, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21476/306948 [32:45<6:03:11, 13.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21478/306948 [32:45<5:59:20, 13.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21480/306948 [32:45<6:08:34, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21482/306948 [32:45<5:51:38, 13.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21484/306948 [32:45<6:23:59, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21486/306948 [32:45<6:10:06, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21488/306948 [32:46<5:58:43, 13.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21490/306948 [32:46<5:32:17, 14.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21492/306948 [32:46<5:41:47, 13.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21494/306948 [32:46<5:40:56, 13.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21496/306948 [32:46<6:15:53, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21498/306948 [32:46<6:10:00, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21500/306948 [32:47<6:13:26, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21502/306948 [32:47<5:57:12, 13.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21504/306948 [32:47<5:53:28, 13.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21506/306948 [32:47<5:43:26, 13.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21508/306948 [32:47<6:13:20, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21510/306948 [32:47<5:41:06, 13.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21512/306948 [32:47<6:14:59, 12.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21514/306948 [32:48<5:44:27, 13.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21516/306948 [32:48<6:01:06, 13.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21518/306948 [32:48<5:45:56, 13.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21520/306948 [32:48<5:54:18, 13.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21522/306948 [32:48<6:32:35, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21524/306948 [32:48<6:29:07, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21526/306948 [32:48<6:06:22, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21528/306948 [32:49<6:31:59, 12.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21530/306948 [32:49<6:03:57, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21532/306948 [32:49<6:16:32, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21534/306948 [32:49<6:08:39, 12.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21536/306948 [32:49<6:26:59, 12.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21538/306948 [32:49<6:03:53, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21540/306948 [32:50<6:06:02, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21542/306948 [32:50<5:55:07, 13.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21544/306948 [32:50<6:23:29, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21546/306948 [32:50<6:07:28, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21548/306948 [32:50<6:29:57, 12.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21550/306948 [32:50<6:03:36, 13.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21552/306948 [32:51<6:21:38, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21554/306948 [32:51<6:10:44, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21556/306948 [32:51<6:09:55, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21558/306948 [32:51<6:17:01, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21560/306948 [32:51<6:14:52, 12.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21562/306948 [32:51<6:04:52, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21564/306948 [32:51<6:02:25, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21566/306948 [32:52<5:56:38, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21568/306948 [32:52<6:31:47, 12.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21570/306948 [32:52<6:20:44, 12.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21572/306948 [32:52<6:41:29, 11.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21574/306948 [32:52<6:09:27, 12.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21576/306948 [32:52<6:47:24, 11.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21578/306948 [32:53<6:59:27, 11.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21580/306948 [32:53<6:33:36, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21582/306948 [32:53<6:27:22, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21584/306948 [32:53<6:18:46, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21586/306948 [32:53<5:41:41, 13.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21588/306948 [32:53<5:46:21, 13.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21590/306948 [32:54<5:54:24, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21592/306948 [32:54<6:00:13, 13.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21595/306948 [32:54<5:26:40, 14.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21597/306948 [32:54<6:10:26, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21599/306948 [32:54<6:23:23, 12.40it/s]\u001b[A09/17/2021 06:06:01 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616500\n",
            "09/17/2021 06:06:01 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615500] due to args.save_total_limit\n",
            "09/17/2021 06:06:05 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616500\n",
            "\n",
            "Iteration:   7%|▋         | 21601/306948 [32:59<63:45:45,  1.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21603/306948 [33:00<47:18:05,  1.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21604/306948 [33:00<41:21:01,  1.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21606/306948 [33:00<30:13:17,  2.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21608/306948 [33:00<22:40:13,  3.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21610/306948 [33:00<17:37:49,  4.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21612/306948 [33:01<14:55:13,  5.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21614/306948 [33:01<11:43:03,  6.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21616/306948 [33:01<9:52:15,  8.03it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 21618/306948 [33:01<8:30:12,  9.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21620/306948 [33:01<7:33:31, 10.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21622/306948 [33:01<6:46:39, 11.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21624/306948 [33:01<6:22:48, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21626/306948 [33:01<5:53:58, 13.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21628/306948 [33:02<6:17:35, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21630/306948 [33:02<6:08:02, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21632/306948 [33:02<5:58:24, 13.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21634/306948 [33:02<5:33:08, 14.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21636/306948 [33:02<5:31:59, 14.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21638/306948 [33:02<5:18:17, 14.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21640/306948 [33:02<5:44:56, 13.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21642/306948 [33:03<5:29:17, 14.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21644/306948 [33:03<5:47:36, 13.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21646/306948 [33:03<5:36:25, 14.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21648/306948 [33:03<6:09:01, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21650/306948 [33:03<6:08:24, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21652/306948 [33:03<5:42:23, 13.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21654/306948 [33:03<5:33:08, 14.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21656/306948 [33:04<5:31:46, 14.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21658/306948 [33:04<5:28:08, 14.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21660/306948 [33:04<5:35:37, 14.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21662/306948 [33:04<5:38:08, 14.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21664/306948 [33:04<5:37:19, 14.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21666/306948 [33:04<6:10:43, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21668/306948 [33:05<6:38:06, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21670/306948 [33:05<6:10:07, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21672/306948 [33:05<6:30:01, 12.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21674/306948 [33:05<6:10:14, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21676/306948 [33:05<6:19:06, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21678/306948 [33:05<7:41:57, 10.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21680/306948 [33:06<7:11:37, 11.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21682/306948 [33:06<6:34:04, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21684/306948 [33:06<6:40:38, 11.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21686/306948 [33:06<6:17:51, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21688/306948 [33:06<6:36:03, 12.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21690/306948 [33:06<6:19:31, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21692/306948 [33:07<6:24:35, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21694/306948 [33:07<6:14:57, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21696/306948 [33:07<6:38:38, 11.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21698/306948 [33:07<6:28:44, 12.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21700/306948 [33:07<6:24:47, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21702/306948 [33:07<5:53:34, 13.45it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21704/306948 [33:07<5:57:21, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21706/306948 [33:08<6:40:54, 11.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21708/306948 [33:08<6:18:51, 12.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21710/306948 [33:08<5:48:33, 13.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21712/306948 [33:08<5:53:35, 13.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21714/306948 [33:08<5:29:22, 14.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21716/306948 [33:08<5:27:01, 14.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21718/306948 [33:08<5:32:47, 14.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21720/306948 [33:09<5:53:40, 13.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21722/306948 [33:09<6:17:06, 12.61it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21724/306948 [33:09<7:06:24, 11.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21726/306948 [33:09<6:42:35, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21728/306948 [33:09<6:23:20, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21730/306948 [33:09<5:53:02, 13.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21732/306948 [33:10<6:42:52, 11.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21734/306948 [33:10<6:30:32, 12.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21736/306948 [33:10<6:27:54, 12.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21738/306948 [33:10<6:12:19, 12.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21740/306948 [33:10<6:11:32, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21742/306948 [33:10<5:45:59, 13.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21744/306948 [33:11<6:25:16, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21746/306948 [33:11<5:47:37, 13.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21748/306948 [33:11<6:00:42, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21750/306948 [33:11<5:48:09, 13.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21752/306948 [33:11<5:51:18, 13.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21754/306948 [33:11<6:12:54, 12.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21756/306948 [33:12<7:00:08, 11.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21758/306948 [33:12<6:46:56, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21760/306948 [33:12<6:40:48, 11.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21762/306948 [33:12<6:11:36, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21764/306948 [33:12<6:13:21, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21766/306948 [33:12<5:49:55, 13.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21768/306948 [33:12<5:57:16, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21770/306948 [33:13<6:00:18, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21772/306948 [33:13<6:35:49, 12.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21774/306948 [33:13<6:30:19, 12.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21776/306948 [33:13<6:28:16, 12.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21778/306948 [33:13<6:19:11, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21780/306948 [33:13<6:42:35, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21782/306948 [33:14<6:25:05, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21784/306948 [33:14<6:36:01, 12.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21786/306948 [33:14<6:45:16, 11.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21788/306948 [33:14<7:00:54, 11.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21790/306948 [33:14<6:35:27, 12.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21792/306948 [33:15<7:08:35, 11.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21794/306948 [33:15<6:33:55, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21796/306948 [33:15<6:41:55, 11.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21798/306948 [33:15<6:30:27, 12.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21800/306948 [33:15<6:33:07, 12.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21802/306948 [33:15<6:19:39, 12.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21804/306948 [33:16<6:56:29, 11.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21806/306948 [33:16<7:53:27, 10.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21808/306948 [33:16<7:20:24, 10.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21810/306948 [33:16<7:42:09, 10.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21812/306948 [33:16<7:40:10, 10.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21814/306948 [33:16<7:00:00, 11.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21816/306948 [33:17<6:42:50, 11.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21818/306948 [33:17<6:41:35, 11.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21820/306948 [33:17<6:50:30, 11.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21822/306948 [33:17<6:33:54, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21824/306948 [33:17<7:07:24, 11.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21826/306948 [33:17<6:36:20, 11.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21828/306948 [33:18<6:41:13, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21830/306948 [33:18<6:57:32, 11.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21832/306948 [33:18<6:31:35, 12.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21834/306948 [33:18<6:03:32, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21836/306948 [33:18<5:45:14, 13.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21838/306948 [33:18<6:08:03, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21840/306948 [33:19<6:08:41, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21842/306948 [33:19<5:49:39, 13.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21844/306948 [33:19<6:07:14, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21846/306948 [33:19<5:50:34, 13.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21848/306948 [33:19<6:27:52, 12.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21850/306948 [33:19<6:39:04, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21852/306948 [33:20<6:22:25, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21854/306948 [33:20<5:51:49, 13.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21856/306948 [33:20<5:59:40, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21858/306948 [33:20<6:26:53, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21860/306948 [33:20<6:24:08, 12.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21862/306948 [33:20<6:07:48, 12.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21864/306948 [33:20<6:21:21, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21866/306948 [33:21<5:57:05, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21868/306948 [33:21<6:03:15, 13.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21870/306948 [33:21<6:04:44, 13.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21872/306948 [33:21<5:59:10, 13.23it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21874/306948 [33:21<5:57:04, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21876/306948 [33:21<6:17:01, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21878/306948 [33:21<5:57:15, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21880/306948 [33:22<5:53:17, 13.45it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21882/306948 [33:22<5:38:57, 14.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21884/306948 [33:22<6:23:25, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21886/306948 [33:22<7:21:12, 10.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21888/306948 [33:22<7:25:50, 10.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21890/306948 [33:23<6:42:29, 11.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21892/306948 [33:23<6:36:02, 12.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21894/306948 [33:23<6:06:23, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21896/306948 [33:23<6:04:12, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21898/306948 [33:23<5:39:51, 13.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21900/306948 [33:23<5:52:56, 13.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21902/306948 [33:23<5:34:43, 14.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21904/306948 [33:24<6:01:19, 13.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21906/306948 [33:24<5:26:11, 14.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21908/306948 [33:24<5:38:29, 14.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21910/306948 [33:24<6:07:54, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21912/306948 [33:24<6:29:13, 12.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21914/306948 [33:24<6:00:09, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21916/306948 [33:24<6:22:46, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21918/306948 [33:25<5:51:21, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21920/306948 [33:25<6:13:19, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21922/306948 [33:25<6:09:22, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21924/306948 [33:25<6:55:11, 11.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21926/306948 [33:25<6:38:48, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21928/306948 [33:25<6:39:08, 11.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21930/306948 [33:26<6:06:29, 12.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21932/306948 [33:26<6:33:44, 12.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21934/306948 [33:26<6:24:35, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21936/306948 [33:26<6:09:40, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21938/306948 [33:26<6:05:52, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21940/306948 [33:26<6:16:04, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21942/306948 [33:27<6:23:22, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21944/306948 [33:27<7:10:19, 11.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21946/306948 [33:27<7:01:58, 11.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21948/306948 [33:27<7:25:40, 10.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21950/306948 [33:27<6:55:12, 11.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21952/306948 [33:28<7:20:50, 10.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21954/306948 [33:28<6:44:19, 11.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21956/306948 [33:28<6:24:51, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21958/306948 [33:28<5:57:29, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21960/306948 [33:28<6:16:01, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21962/306948 [33:28<6:19:34, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21964/306948 [33:28<6:40:18, 11.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21966/306948 [33:29<6:24:44, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21968/306948 [33:29<6:15:27, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21970/306948 [33:29<5:47:53, 13.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21972/306948 [33:29<6:38:48, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21974/306948 [33:29<6:16:15, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21976/306948 [33:29<5:53:58, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21978/306948 [33:30<6:01:43, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21980/306948 [33:30<6:11:24, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21982/306948 [33:30<6:18:42, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21984/306948 [33:30<6:33:33, 12.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21986/306948 [33:30<5:53:43, 13.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21988/306948 [33:30<6:40:16, 11.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21990/306948 [33:31<7:35:30, 10.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21992/306948 [33:31<7:04:00, 11.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21994/306948 [33:31<6:24:25, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21996/306948 [33:31<6:38:59, 11.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 21998/306948 [33:31<6:36:24, 11.98it/s]\u001b[A09/17/2021 06:06:38 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616600\n",
            "09/17/2021 06:06:38 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615600] due to args.save_total_limit\n",
            "09/17/2021 06:06:42 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616600\n",
            "\n",
            "Iteration:   7%|▋         | 22000/306948 [33:36<66:31:05,  1.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22002/306948 [33:37<48:12:34,  1.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22004/306948 [33:37<35:54:34,  2.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22006/306948 [33:37<26:41:56,  2.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22008/306948 [33:37<20:37:45,  3.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22010/306948 [33:37<15:58:09,  4.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22012/306948 [33:37<12:55:06,  6.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22014/306948 [33:37<10:50:39,  7.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22016/306948 [33:38<9:47:14,  8.09it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 22018/306948 [33:38<8:13:07,  9.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22020/306948 [33:38<7:25:42, 10.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22022/306948 [33:38<6:58:40, 11.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22024/306948 [33:38<6:57:17, 11.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22026/306948 [33:38<6:22:23, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22028/306948 [33:39<6:26:07, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22030/306948 [33:39<5:58:43, 13.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22032/306948 [33:39<6:25:14, 12.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22034/306948 [33:39<6:47:14, 11.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22036/306948 [33:39<7:00:42, 11.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22038/306948 [33:39<6:22:42, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22040/306948 [33:40<6:22:12, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22042/306948 [33:40<6:19:38, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22044/306948 [33:40<6:16:09, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22046/306948 [33:40<6:12:37, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22048/306948 [33:40<6:05:33, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22050/306948 [33:40<5:48:19, 13.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22052/306948 [33:40<6:14:17, 12.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22054/306948 [33:41<6:19:51, 12.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22056/306948 [33:41<6:16:08, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22058/306948 [33:41<5:51:44, 13.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22060/306948 [33:41<5:45:08, 13.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22062/306948 [33:41<5:52:21, 13.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22064/306948 [33:41<5:53:14, 13.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22066/306948 [33:41<5:41:53, 13.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22068/306948 [33:42<6:20:54, 12.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22070/306948 [33:42<6:17:37, 12.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22072/306948 [33:42<6:33:02, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22074/306948 [33:42<6:10:39, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22076/306948 [33:42<6:05:04, 13.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22078/306948 [33:42<6:13:41, 12.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22080/306948 [33:43<6:00:38, 13.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22082/306948 [33:43<6:17:52, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22084/306948 [33:43<6:21:35, 12.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22086/306948 [33:43<6:15:37, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22088/306948 [33:43<6:30:32, 12.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22090/306948 [33:43<6:08:12, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22092/306948 [33:44<6:21:35, 12.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22094/306948 [33:44<6:22:33, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22096/306948 [33:44<6:16:35, 12.61it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22098/306948 [33:44<6:19:06, 12.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22100/306948 [33:44<6:17:04, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22102/306948 [33:44<5:59:56, 13.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22104/306948 [33:44<5:57:12, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22106/306948 [33:45<6:09:28, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22108/306948 [33:45<6:10:33, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22110/306948 [33:45<6:19:00, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22112/306948 [33:45<6:10:04, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22114/306948 [33:45<6:16:47, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22116/306948 [33:45<6:07:04, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22118/306948 [33:46<6:01:28, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22120/306948 [33:46<6:10:12, 12.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22122/306948 [33:46<5:52:45, 13.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22124/306948 [33:46<5:57:09, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22126/306948 [33:46<5:54:09, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22128/306948 [33:46<6:26:41, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22130/306948 [33:47<6:11:58, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22132/306948 [33:47<6:42:45, 11.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22134/306948 [33:47<6:48:36, 11.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22136/306948 [33:47<6:35:01, 12.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22138/306948 [33:47<6:35:07, 12.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22140/306948 [33:47<6:49:30, 11.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22142/306948 [33:48<6:41:00, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22144/306948 [33:48<6:58:46, 11.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22146/306948 [33:48<6:25:47, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22148/306948 [33:48<6:25:12, 12.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22150/306948 [33:48<5:49:45, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22152/306948 [33:48<5:39:20, 13.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22154/306948 [33:48<5:25:09, 14.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22156/306948 [33:49<5:36:47, 14.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22158/306948 [33:49<5:38:02, 14.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22160/306948 [33:49<5:38:11, 14.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22162/306948 [33:49<5:34:43, 14.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22164/306948 [33:49<5:42:59, 13.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22166/306948 [33:49<5:44:22, 13.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22168/306948 [33:49<5:41:55, 13.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22170/306948 [33:50<5:32:23, 14.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22172/306948 [33:50<6:12:22, 12.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22174/306948 [33:50<6:12:36, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22176/306948 [33:50<6:07:09, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22178/306948 [33:50<6:02:46, 13.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22180/306948 [33:50<6:36:23, 11.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22182/306948 [33:51<6:26:54, 12.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22184/306948 [33:51<6:32:32, 12.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22186/306948 [33:51<7:07:41, 11.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22188/306948 [33:51<6:43:26, 11.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22190/306948 [33:51<6:07:31, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22192/306948 [33:51<6:58:25, 11.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22194/306948 [33:52<6:46:00, 11.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22196/306948 [33:52<6:41:48, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22198/306948 [33:52<6:30:06, 12.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22200/306948 [33:52<6:29:51, 12.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22202/306948 [33:52<5:55:47, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22204/306948 [33:52<6:12:21, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22206/306948 [33:53<6:09:14, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22208/306948 [33:53<6:21:47, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22210/306948 [33:53<6:05:13, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22212/306948 [33:53<6:32:29, 12.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22214/306948 [33:53<6:27:02, 12.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22216/306948 [33:53<6:34:30, 12.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22218/306948 [33:54<6:16:33, 12.60it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22220/306948 [33:54<6:52:53, 11.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22222/306948 [33:54<6:14:04, 12.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22224/306948 [33:54<6:05:26, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22226/306948 [33:54<6:01:59, 13.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22228/306948 [33:54<6:07:57, 12.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22230/306948 [33:54<5:43:27, 13.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22232/306948 [33:55<5:40:46, 13.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22234/306948 [33:55<5:54:36, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22236/306948 [33:55<5:51:33, 13.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22238/306948 [33:55<6:27:20, 12.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22240/306948 [33:55<6:20:26, 12.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22242/306948 [33:55<6:15:57, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22244/306948 [33:56<6:17:44, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22246/306948 [33:56<5:58:28, 13.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22248/306948 [33:56<6:13:43, 12.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22250/306948 [33:56<6:05:52, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22252/306948 [33:56<6:40:37, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22254/306948 [33:56<6:13:26, 12.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22256/306948 [33:57<7:20:22, 10.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22258/306948 [33:57<6:47:00, 11.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22260/306948 [33:57<6:32:15, 12.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22262/306948 [33:57<6:01:34, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22264/306948 [33:57<6:36:23, 11.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22266/306948 [33:57<6:22:37, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22268/306948 [33:57<6:15:13, 12.64it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22270/306948 [33:58<5:41:33, 13.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22272/306948 [33:58<5:40:44, 13.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22274/306948 [33:58<6:24:09, 12.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22276/306948 [33:58<6:30:29, 12.15it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22278/306948 [33:58<5:56:57, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22280/306948 [33:58<6:07:24, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22282/306948 [33:59<6:13:01, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22284/306948 [33:59<5:59:27, 13.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22286/306948 [33:59<5:39:20, 13.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22288/306948 [33:59<5:38:56, 14.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22290/306948 [33:59<5:26:40, 14.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22292/306948 [33:59<5:32:46, 14.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22294/306948 [33:59<5:51:15, 13.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22296/306948 [34:00<6:01:22, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22298/306948 [34:00<5:38:50, 14.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22300/306948 [34:00<6:10:05, 12.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22302/306948 [34:00<6:05:32, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22304/306948 [34:00<6:05:30, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22306/306948 [34:00<5:49:13, 13.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22308/306948 [34:00<6:02:11, 13.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22310/306948 [34:01<5:52:05, 13.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22312/306948 [34:01<6:12:44, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22314/306948 [34:01<6:10:19, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22316/306948 [34:01<6:09:28, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22318/306948 [34:01<5:55:28, 13.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22320/306948 [34:01<6:24:30, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22322/306948 [34:02<5:51:36, 13.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22324/306948 [34:02<5:55:36, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22326/306948 [34:02<6:02:42, 13.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22328/306948 [34:02<6:19:39, 12.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22330/306948 [34:02<6:10:35, 12.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22332/306948 [34:02<6:14:59, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22334/306948 [34:03<6:24:26, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22336/306948 [34:03<6:17:03, 12.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22338/306948 [34:03<6:13:18, 12.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22340/306948 [34:03<6:00:51, 13.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22342/306948 [34:03<5:57:29, 13.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22344/306948 [34:03<6:06:01, 12.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22346/306948 [34:03<6:16:50, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22348/306948 [34:04<6:23:07, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22350/306948 [34:04<5:54:47, 13.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22352/306948 [34:04<6:08:39, 12.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22354/306948 [34:04<6:09:02, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22356/306948 [34:04<6:17:26, 12.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22358/306948 [34:04<6:23:10, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22360/306948 [34:05<6:32:02, 12.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22362/306948 [34:05<6:10:44, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22364/306948 [34:05<6:16:44, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22366/306948 [34:05<6:02:54, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22368/306948 [34:05<6:11:42, 12.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22370/306948 [34:05<5:52:55, 13.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22372/306948 [34:05<5:58:58, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22374/306948 [34:06<6:12:52, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22376/306948 [34:06<6:51:15, 11.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22378/306948 [34:06<6:24:32, 12.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22380/306948 [34:06<6:12:31, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22382/306948 [34:06<5:51:04, 13.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22384/306948 [34:06<5:51:14, 13.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22386/306948 [34:07<5:49:59, 13.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22388/306948 [34:07<5:55:26, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22390/306948 [34:07<5:41:20, 13.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22392/306948 [34:07<6:37:59, 11.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22394/306948 [34:07<6:27:58, 12.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22396/306948 [34:07<7:11:13, 11.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22398/306948 [34:08<6:28:24, 12.21it/s]\u001b[A09/17/2021 06:07:14 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616700\n",
            "09/17/2021 06:07:14 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615700] due to args.save_total_limit\n",
            "09/17/2021 06:07:18 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616700\n",
            "\n",
            "Iteration:   7%|▋         | 22400/306948 [34:13<64:44:24,  1.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22402/306948 [34:13<47:10:50,  1.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22404/306948 [34:13<35:20:05,  2.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22406/306948 [34:13<26:17:13,  3.01it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22408/306948 [34:13<20:20:53,  3.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22410/306948 [34:13<15:40:00,  5.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22412/306948 [34:14<13:05:59,  6.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22414/306948 [34:14<10:40:50,  7.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22416/306948 [34:14<9:30:07,  8.32it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 22418/306948 [34:14<8:18:06,  9.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22420/306948 [34:14<8:10:02,  9.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22422/306948 [34:14<7:49:26, 10.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22424/306948 [34:15<8:10:13,  9.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22426/306948 [34:15<7:17:51, 10.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22428/306948 [34:15<7:16:19, 10.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22430/306948 [34:15<7:17:53, 10.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22432/306948 [34:15<7:18:52, 10.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22434/306948 [34:15<7:01:25, 11.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22436/306948 [34:16<6:41:37, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22438/306948 [34:16<6:36:33, 11.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22440/306948 [34:16<6:33:24, 12.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22442/306948 [34:16<5:54:18, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22444/306948 [34:16<6:10:51, 12.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22446/306948 [34:16<5:55:05, 13.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22448/306948 [34:17<6:14:35, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22450/306948 [34:17<5:59:39, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22452/306948 [34:17<6:12:55, 12.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22454/306948 [34:17<6:27:29, 12.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22456/306948 [34:17<6:25:35, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22458/306948 [34:17<5:53:15, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22460/306948 [34:17<6:40:35, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22462/306948 [34:18<6:16:37, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22464/306948 [34:18<6:54:48, 11.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22466/306948 [34:18<6:29:19, 12.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22468/306948 [34:18<6:36:20, 11.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22470/306948 [34:18<6:36:09, 11.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22472/306948 [34:18<6:28:22, 12.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22474/306948 [34:19<6:32:50, 12.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22476/306948 [34:19<6:40:58, 11.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22478/306948 [34:19<6:11:06, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22480/306948 [34:19<6:56:51, 11.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22482/306948 [34:19<6:32:54, 12.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22484/306948 [34:20<6:51:27, 11.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22486/306948 [34:20<6:31:37, 12.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22488/306948 [34:20<6:18:02, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22490/306948 [34:20<6:08:58, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22492/306948 [34:20<6:57:18, 11.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22494/306948 [34:20<6:32:21, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22496/306948 [34:20<6:31:17, 12.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22498/306948 [34:21<6:21:59, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22500/306948 [34:21<5:55:30, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22502/306948 [34:21<5:56:38, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22504/306948 [34:21<5:56:09, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22506/306948 [34:21<5:50:45, 13.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22508/306948 [34:21<5:58:32, 13.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22510/306948 [34:22<6:00:42, 13.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22512/306948 [34:22<5:58:44, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22514/306948 [34:22<5:37:59, 14.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22516/306948 [34:22<5:44:36, 13.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22518/306948 [34:22<5:58:36, 13.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22520/306948 [34:22<6:11:48, 12.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22522/306948 [34:22<6:17:55, 12.54it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22524/306948 [34:23<6:32:20, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22526/306948 [34:23<6:03:32, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22528/306948 [34:23<6:31:35, 12.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22530/306948 [34:23<6:57:48, 11.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22532/306948 [34:23<6:56:46, 11.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22534/306948 [34:23<6:32:25, 12.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22536/306948 [34:24<6:44:19, 11.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22538/306948 [34:24<6:11:08, 12.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22540/306948 [34:24<6:01:14, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22542/306948 [34:24<5:58:51, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22544/306948 [34:24<6:23:24, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22546/306948 [34:24<6:03:05, 13.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22548/306948 [34:25<6:04:45, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22550/306948 [34:25<6:13:40, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22552/306948 [34:25<6:02:24, 13.08it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22554/306948 [34:25<5:51:08, 13.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22556/306948 [34:25<6:38:44, 11.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22558/306948 [34:25<6:18:20, 12.53it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22560/306948 [34:25<6:26:17, 12.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22562/306948 [34:26<6:14:27, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22564/306948 [34:26<6:08:14, 12.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22566/306948 [34:26<5:46:40, 13.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22568/306948 [34:26<5:50:43, 13.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22570/306948 [34:26<5:40:59, 13.90it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22572/306948 [34:26<5:36:15, 14.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22574/306948 [34:26<5:30:59, 14.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22576/306948 [34:27<5:37:45, 14.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22578/306948 [34:27<5:54:50, 13.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22580/306948 [34:27<6:12:13, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22582/306948 [34:27<5:51:38, 13.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22584/306948 [34:27<5:55:34, 13.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22586/306948 [34:27<6:00:57, 13.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22588/306948 [34:28<5:51:02, 13.50it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22590/306948 [34:28<5:21:09, 14.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22592/306948 [34:28<5:37:08, 14.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22594/306948 [34:28<5:35:59, 14.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22596/306948 [34:28<6:08:52, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22598/306948 [34:28<5:49:07, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22600/306948 [34:28<6:04:42, 12.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22602/306948 [34:29<5:58:30, 13.22it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22604/306948 [34:29<6:17:31, 12.55it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22606/306948 [34:29<6:12:03, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22608/306948 [34:29<6:10:05, 12.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22610/306948 [34:29<6:45:33, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22612/306948 [34:29<6:48:54, 11.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22614/306948 [34:30<6:07:42, 12.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22616/306948 [34:30<6:12:22, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22618/306948 [34:30<5:49:16, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22620/306948 [34:30<5:44:39, 13.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22622/306948 [34:30<6:45:28, 11.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22624/306948 [34:30<6:40:32, 11.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22626/306948 [34:31<6:10:48, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22628/306948 [34:31<5:45:01, 13.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22630/306948 [34:31<5:41:13, 13.89it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22632/306948 [34:31<5:54:50, 13.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22634/306948 [34:31<6:23:17, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22636/306948 [34:31<6:36:50, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22638/306948 [34:31<6:08:11, 12.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22640/306948 [34:32<6:12:29, 12.72it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22642/306948 [34:32<6:05:08, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22644/306948 [34:32<5:59:45, 13.17it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22646/306948 [34:32<5:49:18, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22648/306948 [34:32<6:14:35, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22650/306948 [34:32<6:05:39, 12.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22652/306948 [34:33<6:08:19, 12.86it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22654/306948 [34:33<6:04:58, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22656/306948 [34:33<5:53:03, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22658/306948 [34:33<5:40:36, 13.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22660/306948 [34:33<6:09:04, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22662/306948 [34:33<6:14:15, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22664/306948 [34:33<5:55:42, 13.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22666/306948 [34:34<5:48:59, 13.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22668/306948 [34:34<6:07:05, 12.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22670/306948 [34:34<6:14:36, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22672/306948 [34:34<6:20:11, 12.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22674/306948 [34:34<6:06:29, 12.93it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22676/306948 [34:34<6:30:30, 12.13it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22678/306948 [34:35<6:22:12, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22680/306948 [34:35<6:49:16, 11.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22682/306948 [34:35<6:14:13, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22684/306948 [34:35<6:57:29, 11.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22686/306948 [34:35<6:18:41, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22688/306948 [34:35<6:22:34, 12.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22690/306948 [34:35<5:57:13, 13.26it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22692/306948 [34:36<6:25:52, 12.28it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22694/306948 [34:36<6:05:02, 12.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22696/306948 [34:36<6:13:33, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22698/306948 [34:36<6:25:57, 12.27it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22700/306948 [34:36<6:30:05, 12.14it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22702/306948 [34:36<6:13:35, 12.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22704/306948 [34:37<6:36:24, 11.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22706/306948 [34:37<6:22:01, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22708/306948 [34:37<6:14:27, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22710/306948 [34:37<6:27:07, 12.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22712/306948 [34:37<6:56:09, 11.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22714/306948 [34:38<7:03:28, 11.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22716/306948 [34:38<6:45:56, 11.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22718/306948 [34:38<6:37:44, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22720/306948 [34:38<6:24:57, 12.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22722/306948 [34:38<5:48:44, 13.58it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22724/306948 [34:38<6:13:53, 12.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22726/306948 [34:38<5:46:57, 13.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22728/306948 [34:39<6:57:12, 11.35it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22730/306948 [34:39<7:08:00, 11.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22732/306948 [34:39<7:16:29, 10.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22734/306948 [34:39<6:19:51, 12.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22736/306948 [34:39<6:21:43, 12.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22738/306948 [34:39<5:57:28, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22740/306948 [34:40<6:09:12, 12.83it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22742/306948 [34:40<6:22:56, 12.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22744/306948 [34:40<6:04:21, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22746/306948 [34:40<6:12:10, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22748/306948 [34:40<6:05:09, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22750/306948 [34:40<5:53:36, 13.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22752/306948 [34:41<6:05:12, 12.97it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22754/306948 [34:41<5:56:25, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22756/306948 [34:41<6:01:23, 13.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22758/306948 [34:41<5:39:44, 13.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22760/306948 [34:41<6:03:36, 13.03it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22762/306948 [34:41<5:59:26, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22764/306948 [34:41<5:55:32, 13.32it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22766/306948 [34:42<5:41:25, 13.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22768/306948 [34:42<6:02:20, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22770/306948 [34:42<5:49:04, 13.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22772/306948 [34:42<5:57:39, 13.24it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22774/306948 [34:42<5:51:54, 13.46it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22776/306948 [34:42<6:39:08, 11.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22778/306948 [34:43<6:19:30, 12.48it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22780/306948 [34:43<6:35:08, 11.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22782/306948 [34:43<6:37:22, 11.92it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22784/306948 [34:43<7:12:42, 10.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22786/306948 [34:43<6:44:52, 11.70it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22788/306948 [34:43<6:56:29, 11.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22790/306948 [34:44<6:45:31, 11.68it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22792/306948 [34:44<6:49:33, 11.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22794/306948 [34:44<6:31:12, 12.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22796/306948 [34:44<6:21:19, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22798/306948 [34:44<5:48:46, 13.58it/s]\u001b[A09/17/2021 06:07:51 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616800\n",
            "09/17/2021 06:07:51 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615800] due to args.save_total_limit\n",
            "09/17/2021 06:07:55 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616800\n",
            "\n",
            "Iteration:   7%|▋         | 22800/306948 [34:49<65:44:15,  1.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22802/306948 [34:50<47:36:51,  1.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22804/306948 [34:50<35:53:56,  2.20it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22806/306948 [34:50<26:21:41,  2.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22808/306948 [34:50<20:28:35,  3.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22810/306948 [34:50<16:39:36,  4.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22812/306948 [34:50<14:07:30,  5.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22814/306948 [34:51<11:16:18,  7.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22816/306948 [34:51<9:50:40,  8.02it/s] \u001b[A\n",
            "Iteration:   7%|▋         | 22818/306948 [34:51<8:46:42,  8.99it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22820/306948 [34:51<8:17:12,  9.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22822/306948 [34:51<7:23:38, 10.67it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22824/306948 [34:51<7:44:46, 10.19it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22826/306948 [34:51<6:53:59, 11.44it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22828/306948 [34:52<6:47:57, 11.61it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22830/306948 [34:52<6:33:57, 12.02it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22832/306948 [34:52<6:42:14, 11.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22834/306948 [34:52<6:40:05, 11.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22836/306948 [34:52<6:41:01, 11.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22838/306948 [34:52<6:16:01, 12.59it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22840/306948 [34:53<6:24:00, 12.33it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22842/306948 [34:53<6:21:54, 12.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22844/306948 [34:53<6:21:08, 12.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22846/306948 [34:53<5:42:56, 13.81it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22848/306948 [34:53<5:53:29, 13.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22850/306948 [34:53<6:01:24, 13.10it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22852/306948 [34:54<5:57:29, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22854/306948 [34:54<6:08:46, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22856/306948 [34:54<6:10:40, 12.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22858/306948 [34:54<6:42:34, 11.76it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22860/306948 [34:54<6:16:53, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22862/306948 [34:54<5:56:06, 13.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22864/306948 [34:55<6:49:38, 11.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22866/306948 [34:55<6:42:01, 11.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22868/306948 [34:55<6:45:03, 11.69it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22870/306948 [34:55<6:08:43, 12.84it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22872/306948 [34:55<6:37:22, 11.91it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22874/306948 [34:55<6:11:28, 12.75it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22876/306948 [34:56<6:23:33, 12.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22878/306948 [34:56<6:01:39, 13.09it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22880/306948 [34:56<5:53:26, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22882/306948 [34:56<5:34:24, 14.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22884/306948 [34:56<5:28:07, 14.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22886/306948 [34:56<6:08:32, 12.85it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22888/306948 [34:56<6:14:13, 12.65it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22890/306948 [34:57<6:04:08, 13.00it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22892/306948 [34:57<6:29:28, 12.16it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22894/306948 [34:57<5:54:49, 13.34it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22896/306948 [34:57<5:53:55, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22898/306948 [34:57<6:02:18, 13.07it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22900/306948 [34:57<6:11:40, 12.74it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22902/306948 [34:57<5:56:16, 13.29it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22904/306948 [34:58<6:32:55, 12.05it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22906/306948 [34:58<6:18:11, 12.52it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22908/306948 [34:58<6:16:56, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22910/306948 [34:58<5:53:43, 13.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22912/306948 [34:58<6:22:37, 12.37it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22914/306948 [34:58<5:59:02, 13.18it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22916/306948 [34:59<6:16:35, 12.57it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22918/306948 [34:59<6:01:01, 13.11it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22920/306948 [34:59<6:10:20, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22922/306948 [34:59<6:56:01, 11.38it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22924/306948 [34:59<7:19:21, 10.77it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22926/306948 [34:59<6:36:25, 11.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22928/306948 [35:00<6:22:55, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22930/306948 [35:00<6:16:46, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22932/306948 [35:00<6:24:50, 12.30it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22934/306948 [35:00<6:20:49, 12.43it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22936/306948 [35:00<6:13:47, 12.66it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22938/306948 [35:00<6:22:02, 12.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22940/306948 [35:01<6:05:10, 12.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22942/306948 [35:01<5:45:11, 13.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22944/306948 [35:01<5:38:34, 13.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22946/306948 [35:01<5:45:12, 13.71it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22948/306948 [35:01<5:58:18, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22950/306948 [35:01<6:09:40, 12.80it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22952/306948 [35:01<6:18:57, 12.49it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22954/306948 [35:02<6:05:05, 12.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22956/306948 [35:02<6:10:24, 12.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22958/306948 [35:02<6:02:28, 13.06it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22960/306948 [35:02<6:16:43, 12.56it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22962/306948 [35:02<5:57:11, 13.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22964/306948 [35:02<6:14:52, 12.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22966/306948 [35:03<6:05:51, 12.94it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22968/306948 [35:03<6:05:29, 12.95it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22970/306948 [35:03<5:52:36, 13.42it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22972/306948 [35:03<5:50:25, 13.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22974/306948 [35:03<6:38:47, 11.87it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22976/306948 [35:03<6:23:00, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22978/306948 [35:03<6:19:36, 12.47it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22980/306948 [35:04<6:14:59, 12.62it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22982/306948 [35:04<5:39:03, 13.96it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22984/306948 [35:04<5:55:27, 13.31it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22986/306948 [35:04<6:47:01, 11.63it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22988/306948 [35:04<6:26:23, 12.25it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22990/306948 [35:04<6:02:48, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22992/306948 [35:05<6:22:52, 12.36it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22994/306948 [35:05<6:09:07, 12.82it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22996/306948 [35:05<6:35:06, 11.98it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 22998/306948 [35:05<7:14:54, 10.88it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23000/306948 [35:05<7:18:28, 10.79it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23002/306948 [35:05<6:41:35, 11.78it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23004/306948 [35:06<6:11:49, 12.73it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23006/306948 [35:06<5:58:17, 13.21it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23008/306948 [35:06<6:18:14, 12.51it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23010/306948 [35:06<5:53:15, 13.40it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23012/306948 [35:06<6:00:46, 13.12it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23014/306948 [35:06<5:53:30, 13.39it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23016/306948 [35:06<6:02:48, 13.04it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23018/306948 [35:07<5:52:53, 13.41it/s]\u001b[A\n",
            "Iteration:   7%|▋         | 23020/306948 [35:07<6:02:28, 13.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23022/306948 [35:07<7:51:52, 10.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23024/306948 [35:07<7:24:21, 10.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23026/306948 [35:07<7:08:18, 11.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23028/306948 [35:08<6:57:14, 11.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23030/306948 [35:08<7:00:11, 11.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23032/306948 [35:08<6:51:18, 11.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23034/306948 [35:08<6:04:59, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23036/306948 [35:08<6:09:23, 12.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23038/306948 [35:08<6:02:06, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23040/306948 [35:09<6:44:03, 11.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23042/306948 [35:09<6:22:17, 12.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23044/306948 [35:09<6:06:51, 12.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23046/306948 [35:09<5:29:37, 14.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23048/306948 [35:09<5:30:00, 14.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23050/306948 [35:09<5:41:41, 13.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23052/306948 [35:09<5:54:39, 13.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23054/306948 [35:10<5:54:30, 13.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23056/306948 [35:10<6:34:59, 11.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23058/306948 [35:10<6:10:58, 12.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23060/306948 [35:10<6:32:25, 12.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23062/306948 [35:10<6:07:38, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23064/306948 [35:10<6:05:20, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23066/306948 [35:11<5:52:31, 13.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23068/306948 [35:11<6:23:36, 12.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23070/306948 [35:11<6:21:01, 12.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23072/306948 [35:11<6:01:05, 13.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23074/306948 [35:11<5:53:26, 13.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23076/306948 [35:11<6:23:01, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23078/306948 [35:11<6:23:57, 12.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23080/306948 [35:12<7:09:05, 11.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23082/306948 [35:12<6:51:05, 11.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23084/306948 [35:12<6:48:27, 11.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23086/306948 [35:12<6:27:27, 12.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23088/306948 [35:12<6:39:16, 11.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23090/306948 [35:12<6:11:58, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23092/306948 [35:13<6:30:07, 12.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23094/306948 [35:13<6:33:28, 12.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23096/306948 [35:13<7:08:52, 11.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23098/306948 [35:13<7:14:44, 10.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23100/306948 [35:13<6:55:02, 11.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23102/306948 [35:14<6:34:02, 12.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23104/306948 [35:14<6:39:30, 11.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23106/306948 [35:14<6:00:46, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23108/306948 [35:14<6:23:00, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23110/306948 [35:14<7:11:29, 10.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23112/306948 [35:14<7:17:53, 10.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23114/306948 [35:15<6:50:37, 11.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23116/306948 [35:15<6:33:15, 12.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23118/306948 [35:15<5:55:20, 13.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23120/306948 [35:15<6:02:13, 13.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23122/306948 [35:15<6:04:15, 12.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23124/306948 [35:15<6:10:04, 12.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23126/306948 [35:15<5:59:31, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23128/306948 [35:16<5:49:03, 13.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23130/306948 [35:16<5:45:11, 13.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23132/306948 [35:16<5:59:42, 13.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23134/306948 [35:16<5:49:55, 13.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23136/306948 [35:16<6:07:38, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23138/306948 [35:16<6:06:40, 12.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23140/306948 [35:17<6:37:39, 11.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23142/306948 [35:17<6:32:22, 12.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23144/306948 [35:17<6:26:48, 12.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23146/306948 [35:17<6:20:45, 12.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23148/306948 [35:17<6:30:11, 12.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23150/306948 [35:17<6:37:22, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23152/306948 [35:18<6:24:21, 12.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23154/306948 [35:18<6:07:09, 12.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23156/306948 [35:18<5:59:50, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23158/306948 [35:18<5:52:00, 13.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23160/306948 [35:18<5:56:20, 13.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23162/306948 [35:18<5:28:07, 14.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23164/306948 [35:18<5:33:33, 14.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23166/306948 [35:19<5:20:20, 14.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23168/306948 [35:19<5:52:36, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23170/306948 [35:19<5:44:08, 13.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23172/306948 [35:19<5:56:24, 13.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23174/306948 [35:19<5:47:36, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23176/306948 [35:19<5:58:22, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23178/306948 [35:19<5:27:16, 14.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23180/306948 [35:20<5:30:08, 14.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23182/306948 [35:20<5:29:54, 14.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23184/306948 [35:20<5:47:39, 13.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23186/306948 [35:20<5:27:40, 14.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23188/306948 [35:20<5:44:05, 13.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23190/306948 [35:20<6:31:43, 12.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23192/306948 [35:20<6:26:23, 12.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23194/306948 [35:21<6:24:22, 12.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23196/306948 [35:21<6:40:55, 11.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23198/306948 [35:21<6:16:31, 12.56it/s]\u001b[A09/17/2021 06:08:27 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-616900\n",
            "09/17/2021 06:08:27 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-615900] due to args.save_total_limit\n",
            "09/17/2021 06:08:31 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-616900\n",
            "\n",
            "Iteration:   8%|▊         | 23200/306948 [35:26<64:43:59,  1.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23202/306948 [35:26<46:45:23,  1.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23204/306948 [35:26<35:16:00,  2.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23206/306948 [35:27<25:55:51,  3.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23208/306948 [35:27<19:41:11,  4.00it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23210/306948 [35:27<15:43:14,  5.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23212/306948 [35:27<12:50:00,  6.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23214/306948 [35:27<11:13:37,  7.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23216/306948 [35:27<9:45:15,  8.08it/s] \u001b[A\n",
            "Iteration:   8%|▊         | 23218/306948 [35:27<8:07:54,  9.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23220/306948 [35:28<7:59:21,  9.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23222/306948 [35:28<8:05:05,  9.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23224/306948 [35:28<7:22:58, 10.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23226/306948 [35:28<6:43:05, 11.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23228/306948 [35:28<6:41:38, 11.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23230/306948 [35:28<6:50:23, 11.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23232/306948 [35:29<6:53:27, 11.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23234/306948 [35:29<6:18:43, 12.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23236/306948 [35:29<6:44:25, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23238/306948 [35:29<6:41:22, 11.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23240/306948 [35:29<6:23:00, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23242/306948 [35:29<5:52:52, 13.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23244/306948 [35:30<5:44:06, 13.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23246/306948 [35:30<5:47:42, 13.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23248/306948 [35:30<5:52:28, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23250/306948 [35:30<5:52:31, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23252/306948 [35:30<6:14:22, 12.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23254/306948 [35:30<5:47:31, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23256/306948 [35:30<5:32:35, 14.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23258/306948 [35:31<5:21:19, 14.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23260/306948 [35:31<5:40:58, 13.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23262/306948 [35:31<5:13:13, 15.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23264/306948 [35:31<5:36:47, 14.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23266/306948 [35:31<5:14:41, 15.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23268/306948 [35:31<5:36:38, 14.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23270/306948 [35:31<5:29:40, 14.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23272/306948 [35:32<5:47:29, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23274/306948 [35:32<5:49:53, 13.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23276/306948 [35:32<6:08:28, 12.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23278/306948 [35:32<5:59:20, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23280/306948 [35:32<5:49:14, 13.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23282/306948 [35:32<5:59:55, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23284/306948 [35:32<6:15:44, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23286/306948 [35:33<6:09:44, 12.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23288/306948 [35:33<6:22:01, 12.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23290/306948 [35:33<5:57:35, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23292/306948 [35:33<5:49:52, 13.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23294/306948 [35:33<6:07:19, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23296/306948 [35:34<7:15:42, 10.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23298/306948 [35:34<7:02:18, 11.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23300/306948 [35:34<7:11:37, 10.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23303/306948 [35:34<5:59:08, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23305/306948 [35:34<6:19:29, 12.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23307/306948 [35:34<6:06:04, 12.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23309/306948 [35:35<6:45:02, 11.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23311/306948 [35:35<7:11:08, 10.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23313/306948 [35:35<6:49:26, 11.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23315/306948 [35:35<6:26:56, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23317/306948 [35:35<6:16:57, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23319/306948 [35:35<6:22:53, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23321/306948 [35:36<6:08:50, 12.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23323/306948 [35:36<5:47:19, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23325/306948 [35:36<6:31:11, 12.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23327/306948 [35:36<6:32:28, 12.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23329/306948 [35:36<6:38:31, 11.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23331/306948 [35:36<6:21:21, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23333/306948 [35:36<6:11:33, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23335/306948 [35:37<5:58:29, 13.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23337/306948 [35:37<6:16:15, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23339/306948 [35:37<5:50:54, 13.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23341/306948 [35:37<6:12:40, 12.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23343/306948 [35:37<6:48:42, 11.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23345/306948 [35:37<6:43:32, 11.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23347/306948 [35:38<6:31:17, 12.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23349/306948 [35:38<6:33:41, 12.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23351/306948 [35:38<6:23:34, 12.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23353/306948 [35:38<6:16:37, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23355/306948 [35:38<6:05:23, 12.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23357/306948 [35:38<6:04:54, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23359/306948 [35:39<5:46:59, 13.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23361/306948 [35:39<5:44:38, 13.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23363/306948 [35:39<5:58:37, 13.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23365/306948 [35:39<6:41:58, 11.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23367/306948 [35:39<6:27:31, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23369/306948 [35:39<6:44:31, 11.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23371/306948 [35:40<6:07:29, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23373/306948 [35:40<6:01:25, 13.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23375/306948 [35:40<5:41:11, 13.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23377/306948 [35:40<5:54:34, 13.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23379/306948 [35:40<6:39:44, 11.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23381/306948 [35:40<6:15:01, 12.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23383/306948 [35:40<6:34:33, 11.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23385/306948 [35:41<6:44:16, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23387/306948 [35:41<6:30:54, 12.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23389/306948 [35:41<6:21:39, 12.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23391/306948 [35:41<6:21:05, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23393/306948 [35:41<6:15:19, 12.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23395/306948 [35:41<6:02:31, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23397/306948 [35:42<5:55:15, 13.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23399/306948 [35:42<6:04:37, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23401/306948 [35:42<6:22:58, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23403/306948 [35:42<5:51:32, 13.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23405/306948 [35:42<5:44:10, 13.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23407/306948 [35:42<5:50:30, 13.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23409/306948 [35:42<6:11:24, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23411/306948 [35:43<6:18:54, 12.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23413/306948 [35:43<6:19:22, 12.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23415/306948 [35:43<6:09:19, 12.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23417/306948 [35:43<6:29:01, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23419/306948 [35:43<5:58:47, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23421/306948 [35:43<5:51:52, 13.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23423/306948 [35:44<5:45:55, 13.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23425/306948 [35:44<6:19:33, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23427/306948 [35:44<5:54:19, 13.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23429/306948 [35:44<6:21:27, 12.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23431/306948 [35:44<6:02:35, 13.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23433/306948 [35:44<5:51:12, 13.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23435/306948 [35:44<5:46:38, 13.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23437/306948 [35:45<6:13:35, 12.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23439/306948 [35:45<6:05:56, 12.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23441/306948 [35:45<6:06:20, 12.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23443/306948 [35:45<5:45:43, 13.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23445/306948 [35:45<6:17:41, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23447/306948 [35:45<5:46:39, 13.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23449/306948 [35:46<5:58:37, 13.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23451/306948 [35:46<6:10:16, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23453/306948 [35:46<6:09:03, 12.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23455/306948 [35:46<5:56:26, 13.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23457/306948 [35:46<5:52:15, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23459/306948 [35:46<6:17:04, 12.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23461/306948 [35:47<6:13:11, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23463/306948 [35:47<6:04:01, 12.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23465/306948 [35:47<6:15:48, 12.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23467/306948 [35:47<6:07:33, 12.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23469/306948 [35:47<5:59:44, 13.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23471/306948 [35:47<5:47:41, 13.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23473/306948 [35:48<7:17:33, 10.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23475/306948 [35:48<6:56:14, 11.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23477/306948 [35:48<6:49:22, 11.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23479/306948 [35:48<6:21:32, 12.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23481/306948 [35:48<6:27:10, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23483/306948 [35:48<6:26:35, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23485/306948 [35:48<6:14:56, 12.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23487/306948 [35:49<6:15:34, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23489/306948 [35:49<6:10:26, 12.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23491/306948 [35:49<6:18:45, 12.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23493/306948 [35:49<6:49:18, 11.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23495/306948 [35:49<7:37:52, 10.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23497/306948 [35:50<7:07:39, 11.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23499/306948 [35:50<6:27:18, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23501/306948 [35:50<6:12:29, 12.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23503/306948 [35:50<6:00:45, 13.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23505/306948 [35:50<5:55:12, 13.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23507/306948 [35:50<5:29:13, 14.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23509/306948 [35:50<6:16:05, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23511/306948 [35:51<6:26:41, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23513/306948 [35:51<6:15:39, 12.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23515/306948 [35:51<5:46:04, 13.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23517/306948 [35:51<5:57:22, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23519/306948 [35:51<6:25:52, 12.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23521/306948 [35:51<7:03:54, 11.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23523/306948 [35:52<6:42:24, 11.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23525/306948 [35:52<6:23:37, 12.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23527/306948 [35:52<5:57:56, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23529/306948 [35:52<6:13:11, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23531/306948 [35:52<6:02:42, 13.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23533/306948 [35:52<6:14:06, 12.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23535/306948 [35:52<6:08:53, 12.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23537/306948 [35:53<6:07:42, 12.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23539/306948 [35:53<5:46:03, 13.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23541/306948 [35:53<6:49:04, 11.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23543/306948 [35:53<6:44:00, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23545/306948 [35:53<6:53:08, 11.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23547/306948 [35:53<6:20:48, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23549/306948 [35:54<6:20:28, 12.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23551/306948 [35:54<5:58:14, 13.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23553/306948 [35:54<6:16:16, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23555/306948 [35:54<5:58:20, 13.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23557/306948 [35:54<6:09:59, 12.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23559/306948 [35:54<6:01:38, 13.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23561/306948 [35:55<6:46:17, 11.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23563/306948 [35:55<6:41:35, 11.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23565/306948 [35:55<6:28:48, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23567/306948 [35:55<5:52:12, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23569/306948 [35:55<6:41:25, 11.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23571/306948 [35:55<6:22:46, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23573/306948 [35:56<6:16:31, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23575/306948 [35:56<6:17:02, 12.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23577/306948 [35:56<6:38:21, 11.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23579/306948 [35:56<6:09:13, 12.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23581/306948 [35:56<6:33:37, 12.00it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23583/306948 [35:56<6:19:25, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23585/306948 [35:57<6:26:11, 12.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23587/306948 [35:57<6:08:29, 12.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23589/306948 [35:57<6:25:36, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23591/306948 [35:57<5:45:52, 13.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23593/306948 [35:57<5:53:36, 13.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23595/306948 [35:57<6:31:52, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23597/306948 [35:58<6:48:52, 11.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23599/306948 [35:58<6:13:30, 12.64it/s]\u001b[A09/17/2021 06:09:04 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-617000\n",
            "09/17/2021 06:09:04 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-616000] due to args.save_total_limit\n",
            "09/17/2021 06:09:08 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-617000\n",
            "\n",
            "Iteration:   8%|▊         | 23601/306948 [36:03<65:03:34,  1.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23603/306948 [36:03<47:35:27,  1.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23605/306948 [36:03<35:07:55,  2.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23607/306948 [36:03<26:59:58,  2.92it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23609/306948 [36:03<20:58:26,  3.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23611/306948 [36:04<16:03:41,  4.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23613/306948 [36:04<13:18:38,  5.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23615/306948 [36:04<11:04:36,  7.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23617/306948 [36:04<9:36:53,  8.19it/s] \u001b[A\n",
            "Iteration:   8%|▊         | 23619/306948 [36:04<8:13:08,  9.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23621/306948 [36:04<7:50:56, 10.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23623/306948 [36:05<7:02:26, 11.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23625/306948 [36:05<6:43:48, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23627/306948 [36:05<6:11:22, 12.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23629/306948 [36:05<6:37:47, 11.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23631/306948 [36:05<6:52:06, 11.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23633/306948 [36:05<6:26:16, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23635/306948 [36:05<6:19:43, 12.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23637/306948 [36:06<6:34:20, 11.97it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23639/306948 [36:06<6:07:16, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23641/306948 [36:06<6:14:13, 12.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23643/306948 [36:06<5:58:46, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23645/306948 [36:06<6:44:15, 11.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23647/306948 [36:06<6:28:17, 12.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23649/306948 [36:07<6:18:45, 12.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23651/306948 [36:07<6:59:04, 11.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23653/306948 [36:07<6:43:50, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23655/306948 [36:07<6:03:28, 12.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23657/306948 [36:07<6:16:21, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23659/306948 [36:07<5:58:58, 13.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23661/306948 [36:08<6:17:50, 12.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23663/306948 [36:08<6:10:08, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23665/306948 [36:08<6:19:11, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23667/306948 [36:08<6:16:45, 12.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23669/306948 [36:08<6:29:54, 12.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23671/306948 [36:08<6:46:50, 11.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23673/306948 [36:09<7:21:07, 10.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23675/306948 [36:09<7:46:00, 10.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23677/306948 [36:09<7:55:23,  9.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23679/306948 [36:09<7:57:40,  9.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23681/306948 [36:09<7:31:09, 10.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23683/306948 [36:10<6:44:47, 11.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23685/306948 [36:10<6:48:05, 11.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23687/306948 [36:10<6:32:54, 12.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23689/306948 [36:10<6:34:31, 11.97it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23691/306948 [36:10<6:08:58, 12.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23693/306948 [36:10<6:13:11, 12.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23695/306948 [36:10<5:56:45, 13.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23697/306948 [36:11<7:48:10, 10.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23699/306948 [36:11<7:09:58, 10.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23701/306948 [36:11<7:11:57, 10.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23703/306948 [36:11<6:52:31, 11.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23705/306948 [36:11<6:36:39, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23707/306948 [36:12<6:12:50, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23709/306948 [36:12<6:25:06, 12.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23711/306948 [36:12<6:01:31, 13.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23713/306948 [36:12<6:15:17, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23715/306948 [36:12<5:59:33, 13.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23717/306948 [36:12<6:07:03, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23719/306948 [36:13<6:20:49, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23721/306948 [36:13<6:22:16, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23723/306948 [36:13<6:09:57, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23725/306948 [36:13<6:06:47, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23727/306948 [36:13<5:53:22, 13.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23729/306948 [36:13<6:27:04, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23731/306948 [36:13<6:36:15, 11.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23733/306948 [36:14<6:57:40, 11.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23735/306948 [36:14<7:12:38, 10.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23737/306948 [36:14<6:46:12, 11.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23739/306948 [36:14<6:27:22, 12.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23741/306948 [36:14<6:42:46, 11.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23743/306948 [36:15<6:30:15, 12.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23745/306948 [36:15<6:07:36, 12.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23747/306948 [36:15<6:49:54, 11.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23749/306948 [36:15<6:41:25, 11.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23751/306948 [36:15<6:23:13, 12.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23753/306948 [36:15<6:38:55, 11.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23755/306948 [36:15<6:12:31, 12.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23757/306948 [36:16<5:51:32, 13.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23759/306948 [36:16<6:09:19, 12.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23761/306948 [36:16<6:27:17, 12.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23763/306948 [36:16<6:06:50, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23765/306948 [36:16<6:04:06, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23767/306948 [36:16<6:36:35, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23769/306948 [36:17<6:44:41, 11.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23771/306948 [36:17<6:17:21, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23773/306948 [36:17<6:08:45, 12.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23775/306948 [36:17<5:43:47, 13.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23777/306948 [36:17<6:00:59, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23779/306948 [36:17<6:25:56, 12.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23781/306948 [36:18<6:26:31, 12.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23783/306948 [36:18<6:18:57, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23785/306948 [36:18<7:06:16, 11.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23787/306948 [36:18<6:36:39, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23789/306948 [36:18<7:18:22, 10.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23791/306948 [36:18<6:27:33, 12.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23793/306948 [36:19<6:54:59, 11.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23795/306948 [36:19<6:22:32, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23797/306948 [36:19<6:32:41, 12.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23799/306948 [36:19<6:29:14, 12.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23801/306948 [36:19<6:34:40, 11.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23803/306948 [36:19<6:11:58, 12.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23805/306948 [36:20<6:22:47, 12.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23807/306948 [36:20<6:26:27, 12.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23809/306948 [36:20<6:31:33, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23811/306948 [36:20<6:16:51, 12.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23813/306948 [36:20<6:12:56, 12.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23815/306948 [36:20<5:42:33, 13.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23817/306948 [36:21<6:16:14, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23819/306948 [36:21<6:00:27, 13.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23821/306948 [36:21<5:56:51, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23823/306948 [36:21<5:34:45, 14.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23825/306948 [36:21<5:55:08, 13.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23827/306948 [36:21<6:14:59, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23829/306948 [36:21<6:17:15, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23831/306948 [36:22<6:43:54, 11.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23833/306948 [36:22<7:11:10, 10.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23835/306948 [36:22<6:47:10, 11.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23837/306948 [36:22<7:08:40, 11.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23839/306948 [36:22<6:18:20, 12.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23841/306948 [36:22<6:26:07, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23843/306948 [36:23<6:44:44, 11.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23845/306948 [36:23<7:04:40, 11.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23847/306948 [36:23<6:26:38, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23849/306948 [36:23<6:11:38, 12.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23851/306948 [36:23<5:54:55, 13.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23853/306948 [36:23<6:00:48, 13.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23855/306948 [36:24<6:05:25, 12.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23857/306948 [36:24<6:20:53, 12.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23859/306948 [36:24<6:28:54, 12.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23861/306948 [36:24<6:44:40, 11.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23863/306948 [36:24<6:25:06, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23865/306948 [36:24<6:25:51, 12.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23867/306948 [36:25<7:51:03, 10.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23869/306948 [36:25<7:57:08,  9.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23871/306948 [36:25<7:00:14, 11.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23873/306948 [36:25<6:37:20, 11.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23875/306948 [36:25<6:29:11, 12.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23877/306948 [36:26<6:28:47, 12.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23879/306948 [36:26<6:09:37, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23881/306948 [36:26<6:12:36, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23883/306948 [36:26<6:19:21, 12.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23885/306948 [36:26<6:18:48, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23887/306948 [36:26<5:43:55, 13.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23889/306948 [36:26<6:43:02, 11.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23891/306948 [36:27<6:25:31, 12.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23893/306948 [36:27<6:19:26, 12.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23895/306948 [36:27<6:04:13, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23897/306948 [36:27<5:59:54, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23899/306948 [36:27<6:08:43, 12.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23901/306948 [36:27<6:06:39, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23903/306948 [36:28<5:56:54, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23905/306948 [36:28<6:09:47, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23907/306948 [36:28<6:03:47, 12.97it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23909/306948 [36:28<6:20:17, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23911/306948 [36:28<6:04:23, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23913/306948 [36:28<6:24:30, 12.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23915/306948 [36:29<6:29:18, 12.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23917/306948 [36:29<6:29:10, 12.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23919/306948 [36:29<6:11:21, 12.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23921/306948 [36:29<6:37:21, 11.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23923/306948 [36:29<7:59:51,  9.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23925/306948 [36:29<7:16:04, 10.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23927/306948 [36:30<6:28:04, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23929/306948 [36:30<7:03:37, 11.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23931/306948 [36:30<6:17:11, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23933/306948 [36:30<6:24:09, 12.28it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23935/306948 [36:30<6:03:54, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23937/306948 [36:30<6:12:11, 12.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23939/306948 [36:30<5:42:54, 13.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23941/306948 [36:31<6:09:12, 12.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23943/306948 [36:31<5:50:15, 13.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23945/306948 [36:31<6:02:07, 13.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23947/306948 [36:31<5:51:32, 13.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23949/306948 [36:31<6:46:42, 11.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23951/306948 [36:31<6:21:18, 12.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23953/306948 [36:32<6:17:23, 12.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23955/306948 [36:32<5:43:42, 13.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23957/306948 [36:32<6:01:45, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23959/306948 [36:32<6:24:58, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23961/306948 [36:32<7:02:19, 11.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23963/306948 [36:32<6:26:05, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23965/306948 [36:33<6:20:35, 12.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23967/306948 [36:33<6:06:51, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23969/306948 [36:33<6:16:05, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23971/306948 [36:33<5:58:01, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23973/306948 [36:33<6:16:55, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23975/306948 [36:33<6:01:19, 13.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23977/306948 [36:33<5:59:58, 13.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23979/306948 [36:34<5:44:44, 13.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23981/306948 [36:34<5:40:19, 13.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23983/306948 [36:34<5:30:52, 14.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23985/306948 [36:34<5:56:56, 13.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23987/306948 [36:34<5:40:53, 13.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23989/306948 [36:34<5:48:15, 13.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23991/306948 [36:34<5:24:47, 14.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23993/306948 [36:35<6:11:46, 12.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23995/306948 [36:35<5:58:13, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23997/306948 [36:35<6:23:37, 12.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 23999/306948 [36:35<7:45:36, 10.13it/s]\u001b[A09/17/2021 06:09:42 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-617100\n",
            "09/17/2021 06:09:42 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-616100] due to args.save_total_limit\n",
            "09/17/2021 06:09:46 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-617100\n",
            "\n",
            "Iteration:   8%|▊         | 24001/306948 [36:40<64:53:46,  1.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24003/306948 [36:41<47:36:41,  1.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24005/306948 [36:41<35:14:26,  2.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24006/306948 [36:41<30:46:14,  2.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24008/306948 [36:41<22:19:11,  3.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24010/306948 [36:41<16:50:07,  4.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24012/306948 [36:41<13:33:00,  5.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24014/306948 [36:41<11:12:55,  7.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24016/306948 [36:42<10:03:54,  7.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24018/306948 [36:42<8:54:49,  8.82it/s] \u001b[A\n",
            "Iteration:   8%|▊         | 24020/306948 [36:42<8:08:40,  9.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24022/306948 [36:42<7:41:32, 10.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24024/306948 [36:42<7:29:28, 10.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24026/306948 [36:42<7:30:05, 10.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24028/306948 [36:43<6:53:42, 11.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24030/306948 [36:43<6:27:51, 12.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24032/306948 [36:43<6:15:12, 12.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24034/306948 [36:43<5:57:14, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24036/306948 [36:43<6:39:44, 11.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24038/306948 [36:43<6:16:20, 12.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24040/306948 [36:44<7:08:06, 11.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24042/306948 [36:44<6:45:54, 11.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24044/306948 [36:44<6:40:49, 11.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24046/306948 [36:44<6:33:20, 11.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24048/306948 [36:44<6:28:05, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24050/306948 [36:44<6:24:59, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24052/306948 [36:45<6:37:12, 11.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24054/306948 [36:45<6:13:25, 12.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24056/306948 [36:45<6:15:24, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24058/306948 [36:45<6:53:09, 11.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24060/306948 [36:45<6:47:26, 11.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24062/306948 [36:45<6:00:46, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24064/306948 [36:45<5:54:53, 13.28it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24066/306948 [36:46<5:50:37, 13.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24068/306948 [36:46<6:05:59, 12.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24070/306948 [36:46<5:50:34, 13.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24072/306948 [36:46<5:58:54, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24074/306948 [36:46<5:59:39, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24076/306948 [36:46<6:12:02, 12.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24078/306948 [36:47<5:54:39, 13.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24080/306948 [36:47<6:14:28, 12.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24082/306948 [36:47<6:09:13, 12.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24084/306948 [36:47<6:18:42, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24086/306948 [36:47<6:06:09, 12.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24088/306948 [36:47<6:20:14, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24090/306948 [36:48<6:01:37, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24092/306948 [36:48<6:03:03, 12.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24094/306948 [36:48<6:01:24, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24096/306948 [36:48<6:41:21, 11.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24098/306948 [36:48<6:58:59, 11.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24100/306948 [36:48<7:08:28, 11.00it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24103/306948 [36:49<5:49:52, 13.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24105/306948 [36:49<6:00:46, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24107/306948 [36:49<6:31:03, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24109/306948 [36:49<6:35:48, 11.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24111/306948 [36:49<6:07:19, 12.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24113/306948 [36:49<5:53:09, 13.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24115/306948 [36:49<5:41:51, 13.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24117/306948 [36:50<5:37:30, 13.97it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24119/306948 [36:50<5:30:56, 14.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24121/306948 [36:50<6:13:33, 12.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24123/306948 [36:50<5:55:41, 13.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24125/306948 [36:50<6:02:25, 13.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24127/306948 [36:50<5:44:36, 13.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24129/306948 [36:51<6:07:13, 12.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24131/306948 [36:51<6:29:34, 12.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24133/306948 [36:51<6:36:08, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24135/306948 [36:51<5:56:59, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24137/306948 [36:51<6:06:06, 12.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24139/306948 [36:51<6:03:35, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24141/306948 [36:51<5:51:33, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24143/306948 [36:52<6:12:49, 12.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24145/306948 [36:52<6:24:09, 12.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24147/306948 [36:52<6:21:35, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24149/306948 [36:52<6:38:04, 11.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24151/306948 [36:52<6:08:02, 12.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24153/306948 [36:52<6:01:51, 13.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24155/306948 [36:53<5:39:32, 13.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24157/306948 [36:53<6:00:47, 13.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24159/306948 [36:53<5:38:21, 13.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24161/306948 [36:53<6:19:24, 12.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24163/306948 [36:53<5:57:54, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24165/306948 [36:53<6:10:35, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24167/306948 [36:54<5:59:21, 13.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24169/306948 [36:54<5:49:22, 13.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24171/306948 [36:54<5:49:43, 13.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24173/306948 [36:54<6:04:35, 12.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24175/306948 [36:54<5:47:03, 13.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24177/306948 [36:54<5:54:04, 13.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24179/306948 [36:54<5:53:56, 13.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24181/306948 [36:55<6:34:15, 11.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24183/306948 [36:55<6:21:47, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24185/306948 [36:55<6:19:24, 12.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24187/306948 [36:55<5:54:51, 13.28it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24189/306948 [36:55<6:13:27, 12.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24191/306948 [36:55<7:04:44, 11.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24193/306948 [36:56<6:36:42, 11.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24195/306948 [36:56<6:37:37, 11.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24197/306948 [36:56<7:46:53, 10.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24199/306948 [36:56<7:12:00, 10.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24201/306948 [36:56<6:57:11, 11.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24203/306948 [36:56<6:19:34, 12.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24205/306948 [36:57<6:05:04, 12.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24207/306948 [36:57<5:59:01, 13.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24209/306948 [36:57<6:02:00, 13.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24211/306948 [36:57<5:43:04, 13.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24213/306948 [36:57<5:52:02, 13.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24215/306948 [36:57<6:21:35, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24217/306948 [36:58<6:15:49, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24219/306948 [36:58<5:54:52, 13.28it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24221/306948 [36:58<5:48:13, 13.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24223/306948 [36:58<5:34:15, 14.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24225/306948 [36:58<6:01:07, 13.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24227/306948 [36:58<5:31:47, 14.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24229/306948 [36:58<6:03:07, 12.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24231/306948 [36:59<5:37:39, 13.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24233/306948 [36:59<5:49:31, 13.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24235/306948 [36:59<5:41:32, 13.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24237/306948 [36:59<5:48:42, 13.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24239/306948 [36:59<5:34:28, 14.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24241/306948 [36:59<5:38:32, 13.92it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24243/306948 [36:59<5:27:59, 14.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24245/306948 [37:00<5:54:46, 13.28it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24247/306948 [37:00<6:21:27, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24249/306948 [37:00<6:39:21, 11.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24251/306948 [37:00<6:52:34, 11.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24253/306948 [37:00<6:34:31, 11.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24255/306948 [37:00<6:07:07, 12.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24257/306948 [37:01<6:48:50, 11.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24259/306948 [37:01<6:23:15, 12.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24261/306948 [37:01<6:21:40, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24263/306948 [37:01<5:55:31, 13.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24265/306948 [37:01<5:55:10, 13.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24267/306948 [37:01<5:41:35, 13.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24269/306948 [37:02<6:02:05, 13.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24271/306948 [37:02<5:44:35, 13.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24273/306948 [37:02<5:44:48, 13.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24275/306948 [37:02<5:36:14, 14.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24277/306948 [37:02<5:42:25, 13.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24279/306948 [37:02<5:15:57, 14.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24281/306948 [37:02<5:40:32, 13.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24283/306948 [37:02<5:24:57, 14.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24285/306948 [37:03<5:43:48, 13.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24287/306948 [37:03<6:16:10, 12.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24289/306948 [37:03<6:24:30, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24291/306948 [37:03<6:00:20, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24293/306948 [37:03<6:25:59, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24295/306948 [37:03<6:10:41, 12.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24297/306948 [37:04<6:10:55, 12.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24299/306948 [37:04<6:03:34, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24301/306948 [37:04<6:03:38, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24303/306948 [37:04<5:44:47, 13.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24305/306948 [37:04<6:10:56, 12.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24307/306948 [37:04<5:47:02, 13.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24309/306948 [37:05<6:04:05, 12.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24311/306948 [37:05<5:59:58, 13.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24313/306948 [37:05<6:16:18, 12.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24315/306948 [37:05<5:53:43, 13.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24317/306948 [37:05<5:56:22, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24319/306948 [37:05<5:54:58, 13.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24321/306948 [37:05<6:02:11, 13.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24323/306948 [37:06<5:43:46, 13.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24325/306948 [37:06<6:00:25, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24327/306948 [37:06<6:31:28, 12.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24329/306948 [37:06<6:45:55, 11.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24331/306948 [37:06<6:41:48, 11.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24333/306948 [37:06<6:22:52, 12.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24335/306948 [37:07<6:13:10, 12.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24337/306948 [37:07<6:04:14, 12.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24339/306948 [37:07<6:05:48, 12.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24341/306948 [37:07<6:07:12, 12.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24343/306948 [37:07<6:29:59, 12.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24345/306948 [37:07<6:46:57, 11.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24347/306948 [37:08<6:18:27, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24349/306948 [37:08<6:20:40, 12.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24351/306948 [37:08<6:04:43, 12.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24353/306948 [37:08<7:21:09, 10.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24356/306948 [37:08<6:14:59, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24358/306948 [37:08<6:00:47, 13.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24360/306948 [37:09<6:09:59, 12.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24362/306948 [37:09<6:25:00, 12.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24364/306948 [37:09<6:21:31, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24366/306948 [37:09<6:11:58, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24368/306948 [37:09<6:06:29, 12.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24370/306948 [37:09<6:00:28, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24372/306948 [37:10<6:19:41, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24374/306948 [37:10<5:57:39, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24376/306948 [37:10<6:09:25, 12.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24378/306948 [37:10<5:34:27, 14.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24380/306948 [37:10<5:45:55, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24382/306948 [37:10<6:01:12, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24384/306948 [37:10<6:03:42, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24386/306948 [37:11<6:04:25, 12.92it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24388/306948 [37:11<6:26:12, 12.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24390/306948 [37:11<6:11:24, 12.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24392/306948 [37:11<6:11:12, 12.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24394/306948 [37:11<5:38:00, 13.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24396/306948 [37:11<5:40:27, 13.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24398/306948 [37:11<5:39:25, 13.87it/s]\u001b[A09/17/2021 06:10:18 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-617200\n",
            "09/17/2021 06:10:18 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-616200] due to args.save_total_limit\n",
            "09/17/2021 06:10:21 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-617200\n",
            "\n",
            "Iteration:   8%|▊         | 24400/306948 [37:16<59:16:22,  1.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24402/306948 [37:16<42:59:04,  1.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24404/306948 [37:16<31:54:29,  2.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24406/306948 [37:17<23:58:57,  3.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24408/306948 [37:17<19:08:54,  4.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24410/306948 [37:17<15:14:25,  5.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24412/306948 [37:17<13:28:27,  5.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24414/306948 [37:17<10:55:32,  7.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24416/306948 [37:17<9:11:13,  8.54it/s] \u001b[A\n",
            "Iteration:   8%|▊         | 24418/306948 [37:18<7:51:55,  9.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24420/306948 [37:18<9:18:32,  8.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24422/306948 [37:18<8:18:30,  9.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24424/306948 [37:18<7:40:16, 10.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24426/306948 [37:18<6:53:47, 11.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24428/306948 [37:19<7:01:16, 11.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24430/306948 [37:19<6:24:56, 12.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24432/306948 [37:19<6:30:51, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24434/306948 [37:19<6:16:53, 12.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24436/306948 [37:19<6:27:29, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24438/306948 [37:19<6:11:57, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24440/306948 [37:19<6:02:24, 12.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24442/306948 [37:20<5:53:02, 13.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24444/306948 [37:20<5:44:19, 13.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24446/306948 [37:20<5:42:06, 13.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24448/306948 [37:20<6:26:39, 12.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24450/306948 [37:20<5:57:23, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24452/306948 [37:20<6:05:13, 12.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24454/306948 [37:21<6:20:43, 12.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24456/306948 [37:21<6:33:55, 11.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24458/306948 [37:21<6:05:19, 12.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24460/306948 [37:21<8:04:00,  9.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24462/306948 [37:21<8:05:41,  9.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24464/306948 [37:22<7:59:03,  9.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24466/306948 [37:22<7:24:21, 10.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24468/306948 [37:22<6:58:34, 11.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24470/306948 [37:22<6:52:05, 11.42it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24472/306948 [37:22<6:40:54, 11.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24474/306948 [37:22<6:14:16, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24476/306948 [37:23<6:20:34, 12.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24478/306948 [37:23<5:50:57, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24480/306948 [37:23<5:48:01, 13.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24482/306948 [37:23<5:30:09, 14.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24484/306948 [37:23<5:37:03, 13.97it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24486/306948 [37:23<5:23:44, 14.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24488/306948 [37:23<6:37:39, 11.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24490/306948 [37:24<6:13:34, 12.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24492/306948 [37:24<5:52:26, 13.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24494/306948 [37:24<6:03:06, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24496/306948 [37:24<5:58:22, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24498/306948 [37:24<5:34:42, 14.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24500/306948 [37:24<5:37:08, 13.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24502/306948 [37:24<5:51:33, 13.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24504/306948 [37:25<5:46:19, 13.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24506/306948 [37:25<6:00:31, 13.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24508/306948 [37:25<6:14:21, 12.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24510/306948 [37:25<5:59:53, 13.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24512/306948 [37:25<6:02:31, 12.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24514/306948 [37:25<5:38:45, 13.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24516/306948 [37:26<6:11:37, 12.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24518/306948 [37:26<5:53:41, 13.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24520/306948 [37:26<5:53:56, 13.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24522/306948 [37:26<5:42:35, 13.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24524/306948 [37:26<5:54:49, 13.27it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24526/306948 [37:26<6:07:27, 12.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24528/306948 [37:26<6:19:25, 12.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24530/306948 [37:27<6:08:40, 12.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24532/306948 [37:27<6:27:52, 12.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24534/306948 [37:27<6:16:41, 12.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24536/306948 [37:27<6:25:34, 12.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24538/306948 [37:27<6:03:15, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24540/306948 [37:27<6:25:54, 12.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24542/306948 [37:28<6:14:51, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24544/306948 [37:28<7:23:31, 10.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24546/306948 [37:28<6:21:39, 12.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24548/306948 [37:28<6:24:20, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24550/306948 [37:28<5:57:44, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24552/306948 [37:28<6:06:53, 12.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24554/306948 [37:29<6:04:20, 12.92it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24556/306948 [37:29<6:30:43, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24558/306948 [37:29<6:20:12, 12.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24560/306948 [37:29<6:10:01, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24562/306948 [37:29<5:43:39, 13.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24564/306948 [37:29<5:58:52, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24566/306948 [37:29<5:48:27, 13.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24568/306948 [37:30<5:46:58, 13.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24570/306948 [37:30<5:50:27, 13.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24572/306948 [37:30<5:57:33, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24574/306948 [37:30<5:58:10, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24576/306948 [37:30<5:46:22, 13.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24578/306948 [37:30<5:57:18, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24580/306948 [37:31<6:30:28, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24582/306948 [37:31<6:12:16, 12.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24584/306948 [37:31<6:04:23, 12.92it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24586/306948 [37:31<6:03:54, 12.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24588/306948 [37:31<5:53:00, 13.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24590/306948 [37:31<5:44:37, 13.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24592/306948 [37:31<5:51:37, 13.38it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24594/306948 [37:32<5:39:06, 13.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24596/306948 [37:32<5:50:48, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24598/306948 [37:32<6:02:26, 12.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24600/306948 [37:32<5:57:32, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24602/306948 [37:32<5:46:44, 13.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24604/306948 [37:32<5:51:14, 13.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24606/306948 [37:32<6:03:00, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24608/306948 [37:33<6:08:07, 12.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24610/306948 [37:33<5:45:43, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24612/306948 [37:33<6:03:16, 12.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24614/306948 [37:33<5:56:41, 13.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24616/306948 [37:33<6:24:21, 12.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24618/306948 [37:33<6:06:28, 12.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24620/306948 [37:34<7:01:29, 11.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24622/306948 [37:34<6:21:55, 12.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24624/306948 [37:34<6:27:36, 12.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24626/306948 [37:34<6:00:54, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24628/306948 [37:34<5:43:32, 13.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24630/306948 [37:34<5:40:49, 13.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24632/306948 [37:34<5:30:36, 14.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24634/306948 [37:35<5:40:36, 13.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24636/306948 [37:35<6:03:38, 12.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24638/306948 [37:35<5:39:03, 13.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24640/306948 [37:35<6:24:59, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24642/306948 [37:35<5:55:18, 13.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24644/306948 [37:35<5:45:57, 13.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24646/306948 [37:36<5:44:51, 13.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24648/306948 [37:36<6:26:52, 12.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24650/306948 [37:36<6:36:37, 11.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24652/306948 [37:36<6:43:33, 11.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24654/306948 [37:36<6:17:09, 12.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24656/306948 [37:36<6:23:44, 12.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24658/306948 [37:37<6:11:21, 12.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24660/306948 [37:37<6:14:53, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24662/306948 [37:37<5:57:56, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24664/306948 [37:37<6:42:34, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24666/306948 [37:37<6:27:53, 12.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24668/306948 [37:37<6:29:52, 12.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24670/306948 [37:38<6:19:18, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24672/306948 [37:38<6:29:06, 12.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24674/306948 [37:38<6:15:55, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24676/306948 [37:38<6:38:46, 11.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24678/306948 [37:38<6:05:51, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24680/306948 [37:38<6:20:47, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24682/306948 [37:39<6:30:30, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24684/306948 [37:39<6:20:51, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24686/306948 [37:39<5:57:17, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24688/306948 [37:39<5:58:49, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24690/306948 [37:39<5:43:32, 13.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24692/306948 [37:39<5:46:50, 13.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24694/306948 [37:39<6:14:57, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24696/306948 [37:40<6:16:15, 12.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24698/306948 [37:40<6:06:01, 12.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24700/306948 [37:40<6:14:50, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24702/306948 [37:40<6:02:56, 12.96it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24704/306948 [37:40<6:18:24, 12.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24706/306948 [37:40<6:13:47, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24708/306948 [37:41<6:13:08, 12.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24710/306948 [37:41<6:17:09, 12.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24712/306948 [37:41<6:09:46, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24714/306948 [37:41<5:57:32, 13.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24716/306948 [37:41<6:40:35, 11.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24718/306948 [37:41<5:55:46, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24720/306948 [37:41<5:52:22, 13.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24722/306948 [37:42<6:05:50, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24724/306948 [37:42<6:15:47, 12.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24726/306948 [37:42<6:13:39, 12.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24728/306948 [37:42<6:05:45, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24730/306948 [37:42<6:08:31, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24732/306948 [37:42<6:38:11, 11.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24734/306948 [37:43<6:51:04, 11.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24736/306948 [37:43<7:02:03, 11.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24738/306948 [37:43<6:38:25, 11.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24740/306948 [37:43<6:41:05, 11.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24742/306948 [37:43<6:06:24, 12.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24744/306948 [37:43<6:14:54, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24746/306948 [37:44<5:52:02, 13.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24748/306948 [37:44<5:55:20, 13.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24750/306948 [37:44<5:40:00, 13.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24752/306948 [37:44<6:10:41, 12.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24754/306948 [37:44<6:07:35, 12.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24756/306948 [37:44<6:26:54, 12.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24758/306948 [37:45<6:23:51, 12.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24760/306948 [37:45<6:36:01, 11.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24762/306948 [37:45<6:30:18, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24764/306948 [37:45<7:11:18, 10.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24766/306948 [37:45<6:47:41, 11.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24768/306948 [37:45<6:38:24, 11.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24770/306948 [37:46<6:27:51, 12.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24772/306948 [37:46<6:52:11, 11.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24774/306948 [37:46<6:26:40, 12.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24776/306948 [37:46<6:43:07, 11.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24778/306948 [37:46<6:21:35, 12.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24780/306948 [37:46<6:36:57, 11.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24782/306948 [37:47<7:22:28, 10.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24784/306948 [37:47<7:17:39, 10.75it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24786/306948 [37:47<6:32:41, 11.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24788/306948 [37:47<6:18:18, 12.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24790/306948 [37:47<6:15:56, 12.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24792/306948 [37:47<6:26:56, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24794/306948 [37:48<6:06:34, 12.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24796/306948 [37:48<6:19:21, 12.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24798/306948 [37:48<6:03:45, 12.93it/s]\u001b[A09/17/2021 06:10:54 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-617300\n",
            "09/17/2021 06:10:54 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-616300] due to args.save_total_limit\n",
            "09/17/2021 06:10:58 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-617300\n",
            "\n",
            "Iteration:   8%|▊         | 24800/306948 [37:53<64:34:10,  1.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24802/306948 [37:53<46:48:57,  1.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24804/306948 [37:53<34:56:45,  2.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24806/306948 [37:54<26:41:47,  2.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24808/306948 [37:54<20:28:50,  3.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24810/306948 [37:54<16:09:50,  4.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24812/306948 [37:54<13:47:21,  5.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24814/306948 [37:54<11:01:23,  7.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24816/306948 [37:54<9:58:59,  7.85it/s] \u001b[A\n",
            "Iteration:   8%|▊         | 24818/306948 [37:55<8:49:20,  8.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24820/306948 [37:55<8:24:00,  9.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24822/306948 [37:55<7:46:34, 10.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24824/306948 [37:55<7:51:18,  9.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24826/306948 [37:55<6:49:52, 11.47it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24828/306948 [37:55<6:27:04, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24830/306948 [37:56<6:46:33, 11.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24832/306948 [37:56<6:30:50, 12.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24834/306948 [37:56<6:24:43, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24836/306948 [37:56<6:45:26, 11.60it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24838/306948 [37:56<6:58:41, 11.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24840/306948 [37:56<6:33:50, 11.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24842/306948 [37:56<6:15:29, 12.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24844/306948 [37:57<6:48:06, 11.52it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24846/306948 [37:57<6:41:39, 11.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24848/306948 [37:57<6:46:53, 11.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24850/306948 [37:57<6:12:14, 12.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24852/306948 [37:57<6:06:55, 12.81it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24854/306948 [37:57<6:26:12, 12.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24856/306948 [37:58<6:26:00, 12.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24858/306948 [37:58<5:43:15, 13.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24860/306948 [37:58<6:10:30, 12.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24862/306948 [37:58<5:40:51, 13.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24864/306948 [37:58<5:56:10, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24866/306948 [37:58<6:02:02, 12.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24868/306948 [37:59<6:01:28, 13.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24870/306948 [37:59<5:31:39, 14.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24872/306948 [37:59<5:43:52, 13.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24874/306948 [37:59<5:19:45, 14.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24876/306948 [37:59<5:46:47, 13.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24878/306948 [37:59<5:46:51, 13.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24880/306948 [37:59<5:51:35, 13.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24882/306948 [38:00<5:48:41, 13.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24884/306948 [38:00<5:45:29, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24886/306948 [38:00<5:19:03, 14.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24888/306948 [38:00<5:49:52, 13.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24890/306948 [38:00<5:49:37, 13.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24892/306948 [38:00<5:37:57, 13.91it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24894/306948 [38:00<6:16:34, 12.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24896/306948 [38:01<6:54:58, 11.33it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24898/306948 [38:01<6:35:22, 11.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24900/306948 [38:01<6:34:56, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24902/306948 [38:01<6:49:02, 11.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24904/306948 [38:01<7:08:13, 10.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24906/306948 [38:02<7:12:58, 10.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24908/306948 [38:02<7:02:56, 11.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24910/306948 [38:02<6:26:01, 12.18it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24912/306948 [38:02<6:32:18, 11.98it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24914/306948 [38:02<7:22:06, 10.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24916/306948 [38:02<7:12:50, 10.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24918/306948 [38:03<6:34:51, 11.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24920/306948 [38:03<6:48:49, 11.50it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24922/306948 [38:03<6:33:15, 11.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24924/306948 [38:03<6:18:05, 12.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24926/306948 [38:03<5:52:12, 13.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24928/306948 [38:03<6:17:53, 12.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24930/306948 [38:04<5:56:03, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24932/306948 [38:04<6:07:51, 12.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24934/306948 [38:04<5:47:25, 13.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24936/306948 [38:04<6:08:26, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24938/306948 [38:04<6:00:07, 13.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24940/306948 [38:04<5:50:24, 13.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24942/306948 [38:04<6:05:53, 12.85it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24944/306948 [38:05<6:37:12, 11.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24946/306948 [38:05<6:13:28, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24948/306948 [38:05<6:10:10, 12.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24950/306948 [38:05<5:56:44, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24952/306948 [38:05<5:58:32, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24954/306948 [38:05<6:07:41, 12.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24956/306948 [38:06<6:00:26, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24958/306948 [38:06<5:30:12, 14.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24960/306948 [38:06<5:33:30, 14.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24962/306948 [38:06<5:22:12, 14.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24964/306948 [38:06<5:36:54, 13.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24966/306948 [38:06<6:01:17, 13.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24968/306948 [38:06<5:45:44, 13.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24970/306948 [38:06<5:26:16, 14.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24972/306948 [38:07<5:57:34, 13.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24974/306948 [38:07<5:56:14, 13.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24976/306948 [38:07<6:11:09, 12.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24978/306948 [38:07<6:14:47, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24980/306948 [38:07<6:12:08, 12.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24982/306948 [38:07<5:46:02, 13.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24984/306948 [38:08<5:34:25, 14.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24986/306948 [38:08<5:44:35, 13.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24988/306948 [38:08<5:55:43, 13.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24990/306948 [38:08<5:39:48, 13.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24992/306948 [38:08<5:55:09, 13.23it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24994/306948 [38:08<6:24:35, 12.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24996/306948 [38:09<6:19:59, 12.37it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 24998/306948 [38:09<5:55:37, 13.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25000/306948 [38:09<5:56:45, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25002/306948 [38:09<5:55:42, 13.21it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25004/306948 [38:09<6:26:50, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25006/306948 [38:09<6:21:20, 12.32it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25008/306948 [38:09<6:16:06, 12.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25010/306948 [38:10<5:44:34, 13.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25012/306948 [38:10<6:05:49, 12.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25014/306948 [38:10<6:26:54, 12.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25016/306948 [38:10<6:50:42, 11.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25018/306948 [38:10<6:14:35, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25020/306948 [38:10<6:31:25, 12.00it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25022/306948 [38:11<5:55:32, 13.22it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25024/306948 [38:11<6:08:57, 12.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25026/306948 [38:11<6:07:16, 12.79it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25028/306948 [38:11<6:13:14, 12.59it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25030/306948 [38:11<5:44:49, 13.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25032/306948 [38:11<5:52:55, 13.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25034/306948 [38:11<5:30:48, 14.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25036/306948 [38:12<5:44:37, 13.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25038/306948 [38:12<5:59:57, 13.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25040/306948 [38:12<6:17:18, 12.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25042/306948 [38:12<5:54:43, 13.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25044/306948 [38:12<6:08:13, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25046/306948 [38:12<6:14:22, 12.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25048/306948 [38:13<6:50:58, 11.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25050/306948 [38:13<6:09:01, 12.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25052/306948 [38:13<6:42:48, 11.66it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25054/306948 [38:13<6:11:42, 12.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25056/306948 [38:13<6:11:26, 12.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25058/306948 [38:13<6:33:57, 11.93it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25060/306948 [38:14<6:30:01, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25062/306948 [38:14<6:49:14, 11.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25064/306948 [38:14<6:41:23, 11.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25066/306948 [38:14<6:44:45, 11.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25068/306948 [38:14<6:20:39, 12.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25070/306948 [38:14<5:51:55, 13.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25072/306948 [38:15<5:49:16, 13.45it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25074/306948 [38:15<6:30:33, 12.03it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25076/306948 [38:15<6:30:00, 12.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25078/306948 [38:15<6:42:03, 11.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25080/306948 [38:15<6:57:45, 11.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25082/306948 [38:15<6:31:54, 11.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25084/306948 [38:16<6:48:45, 11.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25086/306948 [38:16<6:06:23, 12.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25088/306948 [38:16<6:04:18, 12.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25090/306948 [38:16<5:53:27, 13.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25092/306948 [38:16<6:27:09, 12.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25094/306948 [38:16<6:41:53, 11.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25096/306948 [38:17<6:51:53, 11.40it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25098/306948 [38:17<6:17:51, 12.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25100/306948 [38:17<6:11:31, 12.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25102/306948 [38:17<5:49:52, 13.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25104/306948 [38:17<5:55:45, 13.20it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25106/306948 [38:17<5:38:50, 13.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25108/306948 [38:17<6:01:30, 12.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25110/306948 [38:18<5:43:33, 13.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25112/306948 [38:18<6:09:20, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25114/306948 [38:18<6:01:02, 13.01it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25116/306948 [38:18<5:58:23, 13.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25118/306948 [38:18<5:31:39, 14.16it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25120/306948 [38:18<5:59:02, 13.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25122/306948 [38:18<5:42:15, 13.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25124/306948 [38:19<6:10:34, 12.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25126/306948 [38:19<5:54:10, 13.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25128/306948 [38:19<6:21:25, 12.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25130/306948 [38:19<5:58:04, 13.12it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25132/306948 [38:19<5:56:30, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25134/306948 [38:19<5:48:32, 13.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25136/306948 [38:20<5:52:53, 13.31it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25138/306948 [38:20<5:31:25, 14.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25140/306948 [38:20<5:35:43, 13.99it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25142/306948 [38:20<5:32:04, 14.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25144/306948 [38:20<5:46:54, 13.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25146/306948 [38:20<5:45:53, 13.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25148/306948 [38:20<6:13:24, 12.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25150/306948 [38:21<5:54:18, 13.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25152/306948 [38:21<5:59:16, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25154/306948 [38:21<6:05:07, 12.86it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25156/306948 [38:21<6:43:12, 11.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25158/306948 [38:21<6:11:24, 12.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25160/306948 [38:21<5:52:05, 13.34it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25162/306948 [38:22<5:44:28, 13.63it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25164/306948 [38:22<6:16:22, 12.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25166/306948 [38:22<6:04:37, 12.88it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25168/306948 [38:22<6:09:10, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25170/306948 [38:22<5:45:05, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25172/306948 [38:22<6:04:07, 12.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25174/306948 [38:22<5:42:01, 13.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25176/306948 [38:23<6:20:23, 12.35it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25178/306948 [38:23<6:12:11, 12.62it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25180/306948 [38:23<6:14:38, 12.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25182/306948 [38:23<5:56:40, 13.17it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25184/306948 [38:23<5:45:50, 13.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25186/306948 [38:23<5:32:08, 14.14it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25188/306948 [38:24<5:32:48, 14.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25190/306948 [38:24<5:42:27, 13.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25192/306948 [38:24<5:50:41, 13.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25194/306948 [38:24<5:49:16, 13.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25196/306948 [38:24<6:01:17, 13.00it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25198/306948 [38:24<5:40:14, 13.80it/s]\u001b[A09/17/2021 06:11:31 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-617400\n",
            "09/17/2021 06:11:31 - INFO - __main__ -   Deleting older checkpoint [/content/models/roberta/output_base/checkpoint-616400] due to args.save_total_limit\n",
            "09/17/2021 06:11:35 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-617400\n",
            "\n",
            "Iteration:   8%|▊         | 25200/306948 [38:29<63:10:30,  1.24it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25202/306948 [38:29<45:59:53,  1.70it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25204/306948 [38:30<34:33:13,  2.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25206/306948 [38:30<25:54:45,  3.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25208/306948 [38:30<20:24:46,  3.83it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25210/306948 [38:30<16:14:57,  4.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25212/306948 [38:30<13:48:00,  5.67it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25214/306948 [38:31<11:42:25,  6.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25216/306948 [38:31<9:53:36,  7.91it/s] \u001b[A\n",
            "Iteration:   8%|▊         | 25218/306948 [38:31<8:33:22,  9.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25220/306948 [38:31<8:12:16,  9.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25222/306948 [38:31<7:28:00, 10.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25224/306948 [38:31<7:18:06, 10.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25226/306948 [38:31<6:46:26, 11.55it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25228/306948 [38:32<6:27:36, 12.11it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25230/306948 [38:32<6:04:04, 12.90it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25232/306948 [38:32<6:16:52, 12.46it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25234/306948 [38:32<5:55:54, 13.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25236/306948 [38:32<6:39:06, 11.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25238/306948 [38:32<6:26:27, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25240/306948 [38:33<6:15:53, 12.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25242/306948 [38:33<5:59:35, 13.06it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25244/306948 [38:33<6:06:17, 12.82it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25246/306948 [38:33<5:41:04, 13.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25248/306948 [38:33<6:30:37, 12.02it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25250/306948 [38:33<6:05:43, 12.84it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25252/306948 [38:34<6:28:21, 12.09it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25254/306948 [38:34<6:13:28, 12.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25256/306948 [38:34<6:26:19, 12.15it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25258/306948 [38:34<6:10:15, 12.68it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25260/306948 [38:34<6:43:27, 11.64it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25262/306948 [38:34<6:48:47, 11.48it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25264/306948 [38:35<6:48:00, 11.51it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25266/306948 [38:35<6:45:22, 11.58it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25268/306948 [38:35<6:59:41, 11.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25270/306948 [38:35<6:40:19, 11.73it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25272/306948 [38:35<6:55:40, 11.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25274/306948 [38:35<6:38:47, 11.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25276/306948 [38:36<6:33:03, 11.94it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25278/306948 [38:36<6:45:41, 11.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25280/306948 [38:36<6:28:34, 12.08it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25282/306948 [38:36<6:14:26, 12.54it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25284/306948 [38:36<6:21:40, 12.30it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25286/306948 [38:36<5:44:52, 13.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25288/306948 [38:36<5:42:22, 13.71it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25290/306948 [38:37<6:09:07, 12.72it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25292/306948 [38:37<6:12:09, 12.61it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25294/306948 [38:37<5:49:25, 13.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25296/306948 [38:37<5:44:00, 13.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25298/306948 [38:37<5:37:14, 13.92it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25300/306948 [38:37<5:30:41, 14.19it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25302/306948 [38:38<5:54:22, 13.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25304/306948 [38:38<6:13:20, 12.57it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25306/306948 [38:38<5:40:12, 13.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25308/306948 [38:38<6:13:41, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25310/306948 [38:38<5:50:39, 13.39it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25312/306948 [38:38<6:07:30, 12.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25314/306948 [38:38<5:54:15, 13.25it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25316/306948 [38:39<6:19:47, 12.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25318/306948 [38:39<6:04:02, 12.89it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25320/306948 [38:39<7:15:19, 10.78it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25322/306948 [38:39<6:48:28, 11.49it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25324/306948 [38:39<6:21:57, 12.29it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25326/306948 [38:39<6:17:37, 12.43it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25328/306948 [38:40<6:39:55, 11.74it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25330/306948 [38:40<6:14:35, 12.53it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25332/306948 [38:40<5:59:52, 13.04it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25334/306948 [38:40<6:09:58, 12.69it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25336/306948 [38:40<6:13:38, 12.56it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25338/306948 [38:40<5:58:22, 13.10it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25340/306948 [38:41<6:07:51, 12.76it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25342/306948 [38:41<5:49:07, 13.44it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25344/306948 [38:41<5:59:04, 13.07it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25346/306948 [38:41<5:34:05, 14.05it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25348/306948 [38:41<5:36:23, 13.95it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25350/306948 [38:41<5:17:08, 14.80it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25352/306948 [38:41<6:18:04, 12.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25354/306948 [38:42<6:01:01, 13.00it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25356/306948 [38:42<6:18:10, 12.41it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25358/306948 [38:42<5:51:13, 13.36it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25360/306948 [38:42<5:43:45, 13.65it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25362/306948 [38:42<6:07:22, 12.77it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25364/306948 [38:42<5:57:31, 13.13it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25366/306948 [38:42<5:29:02, 14.26it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25368/306948 [38:43<5:38:19, 13.87it/s]\u001b[A\n",
            "Iteration:   8%|▊         | 25370/306948 [38:43<5:36:39, 13.94it/s]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOG1ATElIB8k"
      },
      "source": [
        "## 8.2 Domain Command\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 2500 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 50 \\\n",
        "    --save_steps 100 \\\n",
        "    --logging_steps 5 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/domain_output_2k.txt\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS7-csNJGkYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "728302d6-3d55-4061-c6b6-5c0c7269b59a"
      },
      "source": [
        "## 8.3 Train RoBERTa Domain\n",
        "\n",
        "domains = ['cs', 'biomed', 'review', 'news']\n",
        "base = '/content/data/'\n",
        "for d in domains:\n",
        "    print(\"__________START \" + f\"{d}__________\")\n",
        "    train_file = f\"{d}/train.txt\"\n",
        "    dev_file = f\"{d}/dev.txt\"\n",
        "    \n",
        "    # COPY the latest model\n",
        "    !cp -R /content/drive/MyDrive/Adaptive_pretrain/sentenced/roberta_models_base9/models/ /content/\n",
        "    !cp -R /content/drive/MyDrive/Adaptive_pretrain/sentenced/roberta_models_base9/runs/ /content/\n",
        "    !cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/\n",
        "\n",
        "    with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "        json.dump(config, fp)\n",
        "\n",
        "    with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "        json.dump(tokenizer_config, fp)\n",
        "    \n",
        "    # Model Parameters\n",
        "    MODEL_TYPE = \"roberta\"\n",
        "    MODEL_DIR = \"/content/models/roberta\"\n",
        "    OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "    TRAIN_PATH = \"/content/data/\"+train_file\n",
        "    EVAL_PATH = \"/content/data/\"+dev_file\n",
        "\n",
        "    train_params = {\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "        \"model_type\": MODEL_TYPE,\n",
        "        \"config_name\": MODEL_DIR,\n",
        "        \"tokenizer_name\": MODEL_DIR,\n",
        "        \"train_path\": TRAIN_PATH,\n",
        "        \"eval_path\": EVAL_PATH,\n",
        "        \"num_train_epochs\": 1.0,\n",
        "        \"do_eval\": \"--do_eval\",\n",
        "        \"evaluate_during_training\": \"\",\n",
        "        \"line_by_line\": \"--line_by_line\",\n",
        "        \"should_continue\": \"--should_continue\",\n",
        "        \"model_name_or_path\": \"\",\n",
        "    }\n",
        "\n",
        "    !{cmd.format(**train_params)}\n",
        "\n",
        "    save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain_2k_{d}/'\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    !cp -R /content/models \"$save_dir\"\n",
        "    !cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________START cs__________\n",
            "^C\n",
            "^C\n",
            "__________START biomed__________\n",
            "^C\n",
            "09/07/2021 04:23:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Load checkpoint from /content/models/roberta/output_base/checkpoint-168300\n",
            "Traceback (most recent call last):\n",
            "  File \"run_lm.py\", line 806, in <module>\n",
            "    main()\n",
            "  File \"run_lm.py\", line 733, in main\n",
            "    cache_dir=args.cache_dir,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py\", line 594, in from_pretrained\n",
            "    return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/models/auto/auto_factory.py\", line 388, in from_pretrained\n",
            "    return model_class.from_pretrained(pretrained_model_name_or_path, *model_args, config=config, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\", line 1223, in from_pretrained\n",
            "    f\"Error no file named {[WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME + '.index', FLAX_WEIGHTS_NAME]} found in \"\n",
            "OSError: Error no file named ['pytorch_model.bin', 'tf_model.h5', 'model.ckpt.index', 'flax_model.msgpack'] found in directory /content/models/roberta/output_base/checkpoint-168300 or `from_tf` and `from_flax` set to False.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwxAikxOIOTE",
        "outputId": "4659cc44-3fe9-40cf-8e6b-2c8bb9203826"
      },
      "source": [
        "## 9 Cross domain\n",
        "## 9.1 cs -> biomed\n",
        "\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 2500 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 50 \\\n",
        "    --save_steps 100 \\\n",
        "    --logging_steps 5 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/cs2biomed_output_2k.txt\n",
        "\"\"\"\n",
        "\n",
        "train_file = \"biomed/train.txt\"\n",
        "dev_file = \"biomed/dev.txt\"\n",
        "\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain/roberta_models_domain_cs/models/ /content/\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain/roberta_models_domain_cs/runs/ /content/\n",
        "\n",
        "!cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/\n",
        "\n",
        "with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "\n",
        "# Model Parameters\n",
        "MODEL_TYPE = \"roberta\"\n",
        "MODEL_DIR = \"/content/models/roberta\"\n",
        "OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "TRAIN_PATH = \"/content/data/\"+train_file\n",
        "EVAL_PATH = \"/content/data/\"+dev_file\n",
        "\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"num_train_epochs\": 1.0,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"--should_continue\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}\n",
        "\n",
        "!{cmd.format(**train_params)}\n",
        "\n",
        "save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain_cs2biomed_2k/'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "!cp -R /content/models \"$save_dir\"\n",
        "!cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 11:13:42 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 11:13:50 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/biomed/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path='/content/models/roberta/output_base/checkpoint-20', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=True, tokenizer_name='/content/models/roberta', train_data_file='/content/data/biomed/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 11:13:50 - INFO - __main__ -   Creating features from dataset file at /content/data/biomed/train.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 11:14:24 - INFO - __main__ -   ***** Running training *****\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Num examples = 570338\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Num Epochs = 1\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Total optimization steps = 25\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Continuing training from epoch 0\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Continuing training from global step 20\n",
            "09/03/2021 11:14:24 - INFO - __main__ -     Will skip the first 20 steps in the first epoch\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/142585 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 21/142585 [00:00<36:10, 65.69it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0%|          | 28/142585 [00:00<1:15:58, 31.27it/s]\u001b[A\n",
            "Iteration:   0%|          | 32/142585 [00:01<1:28:16, 26.91it/s]\u001b[A\n",
            "Iteration:   0%|          | 35/142585 [00:01<1:33:01, 25.54it/s]\u001b[A\n",
            "Iteration:   0%|          | 38/142585 [00:01<1:48:20, 21.93it/s]\u001b[A\n",
            "Iteration:   0%|          | 41/142585 [00:01<1:55:37, 20.55it/s]\u001b[A\n",
            "Iteration:   0%|          | 43/142585 [00:01<1:36:08, 24.71it/s]\n",
            "Epoch:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "09/03/2021 11:14:26 - INFO - __main__ -    global_step = 26, average loss = 2.330418715110192\n",
            "09/03/2021 11:14:26 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 11:14:29 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 11:14:31 - INFO - __main__ -   Creating features from dataset file at /content/data/biomed/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 11:14:39 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 11:14:39 - INFO - __main__ -     Num examples = 142585\n",
            "09/03/2021 11:14:39 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35647/35647 [11:20<00:00, 52.42it/s]\n",
            "09/03/2021 11:25:59 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 11:25:59 - INFO - __main__ -     perplexity = tensor(24099.3711)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHMLRwd9fhur",
        "outputId": "fd114378-ae41-417f-f41c-ab8fc54a6c47"
      },
      "source": [
        "## 9.2 biomed -> news\n",
        "\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 2500 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 50 \\\n",
        "    --save_steps 100 \\\n",
        "    --logging_steps 5 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/biomed2news_output_2k.txt\n",
        "\"\"\"\n",
        "\n",
        "train_file = \"news/train.txt\"\n",
        "dev_file = \"news/dev.txt\"\n",
        "\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain/roberta_models_domain_biomed/models/ /content/\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain/roberta_models_domain_biomed/runs/ /content/\n",
        "\n",
        "!cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/\n",
        "\n",
        "with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "\n",
        "# Model Parameters\n",
        "MODEL_TYPE = \"roberta\"\n",
        "MODEL_DIR = \"/content/models/roberta\"\n",
        "OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "TRAIN_PATH = \"/content/data/\"+train_file\n",
        "EVAL_PATH = \"/content/data/\"+dev_file\n",
        "\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"num_train_epochs\": 1.0,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"--should_continue\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}\n",
        "\n",
        "!{cmd.format(**train_params)}\n",
        "\n",
        "save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain_biomed2news_2k/'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "!cp -R /content/models \"$save_dir\"\n",
        "!cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 11:52:47 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 11:52:56 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/news/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path='/content/models/roberta/output_base/checkpoint-20', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=True, tokenizer_name='/content/models/roberta', train_data_file='/content/data/news/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 11:52:56 - INFO - __main__ -   Creating features from dataset file at /content/data/news/train.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 11:53:26 - INFO - __main__ -   ***** Running training *****\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Num examples = 570339\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Num Epochs = 1\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Total optimization steps = 25\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Continuing training from epoch 0\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Continuing training from global step 20\n",
            "09/03/2021 11:53:26 - INFO - __main__ -     Will skip the first 20 steps in the first epoch\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/142585 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 21/142585 [00:00<32:23, 73.34it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0%|          | 29/142585 [00:00<1:17:52, 30.51it/s]\u001b[A\n",
            "Iteration:   0%|          | 33/142585 [00:01<1:34:20, 25.18it/s]\u001b[A\n",
            "Iteration:   0%|          | 36/142585 [00:01<1:47:38, 22.07it/s]\u001b[A\n",
            "Iteration:   0%|          | 39/142585 [00:01<1:53:26, 20.94it/s]\u001b[A\n",
            "Iteration:   0%|          | 42/142585 [00:01<1:42:57, 23.07it/s]\n",
            "Epoch:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "09/03/2021 11:53:28 - INFO - __main__ -    global_step = 26, average loss = 2.280203782595121\n",
            "09/03/2021 11:53:28 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 11:53:31 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 11:53:33 - INFO - __main__ -   Creating features from dataset file at /content/data/news/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 11:53:40 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 11:53:40 - INFO - __main__ -     Num examples = 142594\n",
            "09/03/2021 11:53:40 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35649/35649 [10:58<00:00, 54.13it/s]\n",
            "09/03/2021 12:04:39 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 12:04:39 - INFO - __main__ -     perplexity = tensor(20304.8945)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVwCi5Zxfhf-",
        "outputId": "332c03b9-e539-43aa-bbdb-adf808e1758d"
      },
      "source": [
        "## 9.3 biomed -> review\n",
        "\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 125000 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/biomed2review_output.txt\n",
        "\"\"\"\n",
        "\n",
        "train_file = \"review/train.txt\"\n",
        "dev_file = \"review/dev.txt\"\n",
        "\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain/roberta_models_domain_biomed/models/ /content/\n",
        "!cp -R /content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain/roberta_models_domain_biomed/runs/ /content/\n",
        "\n",
        "!cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/\n",
        "\n",
        "with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)\n",
        "\n",
        "# Model Parameters\n",
        "MODEL_TYPE = \"roberta\"\n",
        "MODEL_DIR = \"/content/models/roberta\"\n",
        "OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "TRAIN_PATH = \"/content/data/\"+train_file\n",
        "EVAL_PATH = \"/content/data/\"+dev_file\n",
        "\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"num_train_epochs\": 1.0,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"--evaluate_during_training\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"--should_continue\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}\n",
        "\n",
        "!{cmd.format(**train_params)}\n",
        "\n",
        "save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain_biomed2review/'\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.mkdir(save_dir)\n",
        "!cp -R /content/models \"$save_dir\"\n",
        "!cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Evaluating:  53%|█████▎    | 18992/35522 [05:34<04:31, 60.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53%|█████▎    | 18999/35522 [05:34<04:37, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19006/35522 [05:34<04:25, 62.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19013/35522 [05:34<04:27, 61.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19020/35522 [05:35<04:19, 63.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19027/35522 [05:35<04:40, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19033/35522 [05:35<04:40, 58.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19039/35522 [05:35<05:19, 51.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19046/35522 [05:35<04:55, 55.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19052/35522 [05:35<04:59, 55.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19059/35522 [05:35<04:43, 58.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19066/35522 [05:35<04:42, 58.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19073/35522 [05:35<04:33, 60.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19080/35522 [05:36<04:26, 61.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 19087/35522 [05:36<04:34, 59.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19094/35522 [05:36<04:30, 60.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19101/35522 [05:36<04:54, 55.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19107/35522 [05:36<04:50, 56.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19113/35522 [05:36<04:54, 55.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19120/35522 [05:36<04:44, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19126/35522 [05:36<04:46, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19133/35522 [05:36<04:36, 59.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19139/35522 [05:37<04:45, 57.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19146/35522 [05:37<04:33, 59.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19153/35522 [05:37<04:30, 60.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19160/35522 [05:37<04:37, 58.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19167/35522 [05:37<04:25, 61.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19174/35522 [05:37<04:31, 60.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19181/35522 [05:37<04:24, 61.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19188/35522 [05:37<04:36, 59.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19195/35522 [05:38<04:26, 61.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19202/35522 [05:38<04:37, 58.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19209/35522 [05:38<04:34, 59.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19215/35522 [05:38<04:45, 57.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19222/35522 [05:38<04:37, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19228/35522 [05:38<04:41, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19235/35522 [05:38<04:31, 59.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19242/35522 [05:38<04:24, 61.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19249/35522 [05:38<04:41, 57.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19256/35522 [05:39<04:35, 59.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19262/35522 [05:39<04:53, 55.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19269/35522 [05:39<04:40, 57.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19275/35522 [05:39<04:50, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19281/35522 [05:39<04:48, 56.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19287/35522 [05:39<04:56, 54.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19294/35522 [05:39<04:45, 56.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19300/35522 [05:39<04:54, 55.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19306/35522 [05:39<04:48, 56.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19313/35522 [05:40<05:00, 53.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19319/35522 [05:40<05:23, 50.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19325/35522 [05:40<05:10, 52.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19332/35522 [05:40<04:49, 55.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19339/35522 [05:40<04:35, 58.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19345/35522 [05:40<04:39, 57.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19352/35522 [05:40<04:28, 60.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 19359/35522 [05:40<04:32, 59.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19366/35522 [05:41<04:31, 59.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19372/35522 [05:41<04:35, 58.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19379/35522 [05:41<04:25, 60.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19386/35522 [05:41<04:32, 59.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19393/35522 [05:41<04:32, 59.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19400/35522 [05:41<04:41, 57.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19407/35522 [05:41<04:29, 59.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19414/35522 [05:41<04:19, 62.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19421/35522 [05:41<04:22, 61.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19428/35522 [05:42<04:20, 61.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19435/35522 [05:42<04:23, 61.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19442/35522 [05:42<04:17, 62.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19449/35522 [05:42<04:25, 60.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19456/35522 [05:42<04:21, 61.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19463/35522 [05:42<04:29, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19470/35522 [05:42<04:20, 61.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19477/35522 [05:42<04:29, 59.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19484/35522 [05:42<04:19, 61.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19491/35522 [05:43<04:12, 63.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19498/35522 [05:43<04:29, 59.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19505/35522 [05:43<04:26, 60.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19512/35522 [05:43<04:40, 57.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19518/35522 [05:43<05:22, 49.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19525/35522 [05:43<05:04, 52.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▍    | 19532/35522 [05:43<04:58, 53.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19538/35522 [05:43<05:25, 49.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19544/35522 [05:44<05:24, 49.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19550/35522 [05:44<05:08, 51.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19557/35522 [05:44<04:47, 55.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19563/35522 [05:44<04:43, 56.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19570/35522 [05:44<04:36, 57.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19576/35522 [05:44<05:08, 51.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19582/35522 [05:44<04:58, 53.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19588/35522 [05:44<04:59, 53.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19594/35522 [05:45<04:55, 53.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19600/35522 [05:45<04:59, 53.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19607/35522 [05:45<04:43, 56.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19613/35522 [05:45<04:53, 54.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19620/35522 [05:45<04:39, 56.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19627/35522 [05:45<05:36, 47.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19634/35522 [05:45<05:04, 52.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19640/35522 [05:45<05:01, 52.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19647/35522 [05:45<04:45, 55.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19653/35522 [05:46<04:52, 54.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19660/35522 [05:46<04:40, 56.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19667/35522 [05:46<04:38, 56.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19674/35522 [05:46<04:29, 58.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19681/35522 [05:46<04:25, 59.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19688/35522 [05:46<04:38, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19694/35522 [05:46<04:35, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19700/35522 [05:46<05:02, 52.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19707/35522 [05:47<04:44, 55.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 19713/35522 [05:47<04:50, 54.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19720/35522 [05:47<04:40, 56.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19726/35522 [05:47<04:42, 55.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19733/35522 [05:47<04:28, 58.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19739/35522 [05:47<04:33, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19746/35522 [05:47<04:24, 59.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19753/35522 [05:47<04:28, 58.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19760/35522 [05:47<04:26, 59.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19767/35522 [05:48<04:37, 56.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19773/35522 [05:48<04:53, 53.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19780/35522 [05:48<04:46, 54.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19787/35522 [05:48<04:32, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19794/35522 [05:48<04:26, 59.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19800/35522 [05:48<04:40, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19807/35522 [05:48<04:27, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19813/35522 [05:48<04:34, 57.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19819/35522 [05:49<04:31, 57.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19825/35522 [05:49<04:41, 55.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19832/35522 [05:49<04:26, 58.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19839/35522 [05:49<04:28, 58.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19845/35522 [05:49<04:29, 58.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19852/35522 [05:49<04:25, 59.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19858/35522 [05:49<04:38, 56.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19865/35522 [05:49<04:25, 58.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19871/35522 [05:49<04:27, 58.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19878/35522 [05:50<04:25, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19884/35522 [05:50<04:38, 56.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19891/35522 [05:50<04:31, 57.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19897/35522 [05:50<04:28, 58.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19903/35522 [05:50<04:31, 57.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19910/35522 [05:50<04:18, 60.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19917/35522 [05:50<04:29, 57.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19924/35522 [05:50<04:20, 59.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19931/35522 [05:50<04:27, 58.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19938/35522 [05:51<04:18, 60.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19945/35522 [05:51<04:24, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19952/35522 [05:51<04:15, 60.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19959/35522 [05:51<04:16, 60.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19966/35522 [05:51<04:16, 60.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19973/35522 [05:51<04:27, 58.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 19980/35522 [05:51<04:18, 60.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 19987/35522 [05:51<04:26, 58.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 19994/35522 [05:51<04:17, 60.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20001/35522 [05:52<04:28, 57.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20007/35522 [05:52<04:26, 58.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20014/35522 [05:52<04:23, 58.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20021/35522 [05:52<04:19, 59.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20028/35522 [05:52<04:15, 60.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20035/35522 [05:52<04:27, 57.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20042/35522 [05:52<04:22, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20048/35522 [05:52<04:22, 59.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20055/35522 [05:53<04:18, 59.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20061/35522 [05:53<04:25, 58.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▋    | 20068/35522 [05:53<04:16, 60.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20075/35522 [05:53<04:20, 59.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20082/35522 [05:53<04:14, 60.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20089/35522 [05:53<04:09, 61.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20096/35522 [05:53<04:28, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20103/35522 [05:53<04:15, 60.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20110/35522 [05:53<04:51, 52.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20116/35522 [05:54<04:56, 51.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20123/35522 [05:54<04:39, 55.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20130/35522 [05:54<04:29, 57.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20136/35522 [05:54<04:29, 57.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20143/35522 [05:54<04:23, 58.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20149/35522 [05:54<04:38, 55.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20155/35522 [05:54<04:40, 54.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20161/35522 [05:54<04:44, 53.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20168/35522 [05:55<04:32, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20174/35522 [05:55<04:34, 55.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20181/35522 [05:55<04:23, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20188/35522 [05:55<04:32, 56.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20195/35522 [05:55<04:22, 58.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20202/35522 [05:55<04:32, 56.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20209/35522 [05:55<04:27, 57.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20216/35522 [05:55<04:34, 55.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20223/35522 [05:55<04:24, 57.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20230/35522 [05:56<04:32, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20237/35522 [05:56<04:23, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20243/35522 [05:56<04:21, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20249/35522 [05:56<04:26, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20255/35522 [05:56<04:30, 56.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20261/35522 [05:56<04:34, 55.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20268/35522 [05:56<04:26, 57.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20274/35522 [05:56<04:34, 55.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20281/35522 [05:56<04:26, 57.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20288/35522 [05:57<04:21, 58.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20295/35522 [05:57<04:15, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20302/35522 [05:57<04:10, 60.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20309/35522 [05:57<04:14, 59.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20315/35522 [05:57<04:17, 59.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20321/35522 [05:57<04:27, 56.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20328/35522 [05:57<04:20, 58.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20334/35522 [05:57<04:34, 55.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20340/35522 [05:58<04:38, 54.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20346/35522 [05:58<04:43, 53.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20353/35522 [05:58<04:21, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20359/35522 [05:58<04:41, 53.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20366/35522 [05:58<04:28, 56.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20373/35522 [05:58<04:31, 55.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20379/35522 [05:58<05:02, 50.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20386/35522 [05:58<04:47, 52.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20392/35522 [05:58<04:51, 51.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20398/35522 [05:59<04:55, 51.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20405/35522 [05:59<04:38, 54.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20412/35522 [05:59<04:38, 54.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 20419/35522 [05:59<04:23, 57.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20426/35522 [05:59<04:31, 55.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20433/35522 [05:59<04:23, 57.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20439/35522 [05:59<04:21, 57.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20445/35522 [05:59<04:29, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20452/35522 [06:00<04:17, 58.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20458/35522 [06:00<04:21, 57.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20465/35522 [06:00<04:12, 59.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20471/35522 [06:00<05:12, 48.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20478/35522 [06:00<04:49, 51.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20484/35522 [06:00<04:47, 52.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20491/35522 [06:00<04:32, 55.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20497/35522 [06:00<04:39, 53.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20504/35522 [06:01<04:27, 56.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20510/35522 [06:01<04:28, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20517/35522 [06:01<04:19, 57.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20524/35522 [06:01<04:22, 57.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20531/35522 [06:01<04:13, 59.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20538/35522 [06:01<04:21, 57.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20545/35522 [06:01<04:07, 60.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20552/35522 [06:01<04:05, 60.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20559/35522 [06:01<04:11, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20566/35522 [06:02<04:07, 60.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20573/35522 [06:02<04:11, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20580/35522 [06:02<04:04, 61.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20587/35522 [06:02<04:14, 58.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20594/35522 [06:02<04:09, 59.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20601/35522 [06:02<04:14, 58.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20608/35522 [06:02<04:05, 60.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20615/35522 [06:02<04:37, 53.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20622/35522 [06:03<04:26, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20628/35522 [06:03<04:30, 55.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20635/35522 [06:03<04:18, 57.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20641/35522 [06:03<04:21, 56.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20647/35522 [06:03<04:18, 57.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20654/35522 [06:03<04:14, 58.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20660/35522 [06:03<04:21, 56.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20667/35522 [06:03<04:11, 59.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20673/35522 [06:03<04:22, 56.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20680/35522 [06:04<04:14, 58.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20686/35522 [06:04<04:21, 56.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20693/35522 [06:04<04:13, 58.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20699/35522 [06:04<04:15, 57.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20706/35522 [06:04<04:07, 59.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20713/35522 [06:04<04:01, 61.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20720/35522 [06:04<04:20, 56.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20726/35522 [06:04<04:25, 55.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20732/35522 [06:04<04:22, 56.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20738/35522 [06:05<04:20, 56.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20744/35522 [06:05<04:26, 55.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20751/35522 [06:05<04:19, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20757/35522 [06:05<04:29, 54.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20764/35522 [06:05<04:23, 56.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20770/35522 [06:05<04:26, 55.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 20777/35522 [06:05<04:15, 57.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20783/35522 [06:05<04:29, 54.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20790/35522 [06:05<04:17, 57.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20797/35522 [06:06<04:06, 59.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20804/35522 [06:06<04:15, 57.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20811/35522 [06:06<04:12, 58.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20817/35522 [06:06<04:25, 55.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20824/35522 [06:06<04:17, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20830/35522 [06:06<05:22, 45.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20835/35522 [06:06<05:29, 44.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20841/35522 [06:06<05:09, 47.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20848/35522 [06:07<04:55, 49.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20855/35522 [06:07<04:36, 53.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20862/35522 [06:07<04:39, 52.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 20869/35522 [06:07<04:25, 55.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20876/35522 [06:07<04:15, 57.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20882/35522 [06:07<04:16, 57.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20889/35522 [06:07<04:09, 58.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20895/35522 [06:07<04:24, 55.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20902/35522 [06:08<04:08, 58.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20908/35522 [06:08<04:17, 56.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20916/35522 [06:08<04:01, 60.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20923/35522 [06:08<04:12, 57.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20930/35522 [06:08<04:08, 58.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20936/35522 [06:08<04:17, 56.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20942/35522 [06:08<04:13, 57.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20949/35522 [06:08<04:02, 60.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20956/35522 [06:08<04:15, 57.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20963/35522 [06:09<04:09, 58.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20969/35522 [06:09<04:12, 57.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20976/35522 [06:09<04:04, 59.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20982/35522 [06:09<04:27, 54.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20988/35522 [06:09<04:37, 52.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 20994/35522 [06:09<04:34, 52.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21001/35522 [06:09<04:20, 55.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21007/35522 [06:09<04:22, 55.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21014/35522 [06:10<04:14, 56.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21020/35522 [06:10<04:16, 56.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21026/35522 [06:10<04:15, 56.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21033/35522 [06:10<03:59, 60.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21040/35522 [06:10<04:07, 58.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21047/35522 [06:10<04:00, 60.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21054/35522 [06:10<04:00, 60.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21061/35522 [06:10<03:58, 60.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21068/35522 [06:10<04:11, 57.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21074/35522 [06:11<04:19, 55.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21080/35522 [06:11<04:16, 56.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21087/35522 [06:11<04:05, 58.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21093/35522 [06:11<04:12, 57.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21099/35522 [06:11<04:42, 51.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21105/35522 [06:11<04:54, 48.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21112/35522 [06:11<04:28, 53.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21118/35522 [06:11<04:29, 53.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21125/35522 [06:11<04:14, 56.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 21131/35522 [06:12<04:13, 56.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21137/35522 [06:12<04:20, 55.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21145/35522 [06:12<04:01, 59.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21151/35522 [06:12<04:10, 57.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21158/35522 [06:12<04:05, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21164/35522 [06:12<04:18, 55.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21171/35522 [06:12<04:10, 57.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21177/35522 [06:12<04:23, 54.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21184/35522 [06:13<04:14, 56.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21190/35522 [06:13<04:12, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21197/35522 [06:13<04:03, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21204/35522 [06:13<03:54, 61.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21211/35522 [06:13<04:01, 59.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21218/35522 [06:13<03:59, 59.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21225/35522 [06:13<04:10, 57.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21232/35522 [06:13<04:02, 58.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21238/35522 [06:13<04:51, 49.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21245/35522 [06:14<04:39, 51.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21252/35522 [06:14<04:21, 54.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21259/35522 [06:14<04:24, 54.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21266/35522 [06:14<04:08, 57.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21273/35522 [06:14<04:01, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21280/35522 [06:14<04:07, 57.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21286/35522 [06:14<04:14, 56.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21292/35522 [06:14<04:19, 54.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21299/35522 [06:15<04:09, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21305/35522 [06:15<04:11, 56.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|█████▉    | 21312/35522 [06:15<03:57, 59.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21319/35522 [06:15<03:59, 59.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21325/35522 [06:15<04:00, 59.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21332/35522 [06:15<03:52, 60.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21339/35522 [06:15<03:57, 59.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21346/35522 [06:15<03:51, 61.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21353/35522 [06:16<04:41, 50.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21359/35522 [06:16<04:40, 50.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21366/35522 [06:16<04:17, 54.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21373/35522 [06:16<04:03, 58.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21380/35522 [06:16<04:01, 58.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21387/35522 [06:16<04:00, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21393/35522 [06:16<04:01, 58.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21400/35522 [06:16<03:57, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21407/35522 [06:16<03:59, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21414/35522 [06:17<03:57, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21420/35522 [06:17<03:59, 58.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21427/35522 [06:17<03:52, 60.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21434/35522 [06:17<03:58, 58.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21440/35522 [06:17<04:00, 58.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21446/35522 [06:17<04:04, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21452/35522 [06:17<04:15, 55.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21459/35522 [06:17<04:06, 57.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21466/35522 [06:17<03:59, 58.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21473/35522 [06:18<03:47, 61.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21480/35522 [06:18<04:02, 57.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 21487/35522 [06:18<03:51, 60.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21494/35522 [06:18<04:01, 58.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21501/35522 [06:18<03:58, 58.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21507/35522 [06:18<04:09, 56.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21514/35522 [06:18<04:03, 57.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21521/35522 [06:18<04:01, 58.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21527/35522 [06:18<04:01, 58.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21533/35522 [06:19<04:01, 57.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21539/35522 [06:19<04:03, 57.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21546/35522 [06:19<03:52, 60.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21553/35522 [06:19<03:47, 61.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21560/35522 [06:19<03:48, 61.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21567/35522 [06:19<04:00, 58.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21574/35522 [06:19<03:55, 59.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21580/35522 [06:19<03:59, 58.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21587/35522 [06:19<03:52, 59.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21594/35522 [06:20<03:52, 59.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21600/35522 [06:20<03:58, 58.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21607/35522 [06:20<03:51, 60.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21614/35522 [06:20<04:31, 51.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21621/35522 [06:20<04:24, 52.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21628/35522 [06:20<04:05, 56.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21635/35522 [06:20<03:53, 59.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21642/35522 [06:20<03:56, 58.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21649/35522 [06:21<04:07, 56.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21655/35522 [06:21<04:14, 54.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21662/35522 [06:21<04:01, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21668/35522 [06:21<04:06, 56.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21675/35522 [06:21<03:51, 59.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21682/35522 [06:21<03:52, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21689/35522 [06:21<03:44, 61.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21696/35522 [06:21<03:52, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21703/35522 [06:21<03:46, 60.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21710/35522 [06:22<03:40, 62.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21717/35522 [06:22<03:55, 58.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21724/35522 [06:22<03:46, 60.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21731/35522 [06:22<03:51, 59.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21738/35522 [06:22<03:45, 61.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21745/35522 [06:22<03:49, 60.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 21752/35522 [06:22<03:48, 60.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21759/35522 [06:22<03:51, 59.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21766/35522 [06:23<03:44, 61.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21773/35522 [06:23<03:49, 59.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21780/35522 [06:23<03:48, 60.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21787/35522 [06:23<03:55, 58.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21794/35522 [06:23<03:48, 59.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21801/35522 [06:23<03:56, 58.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21808/35522 [06:23<03:45, 60.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21815/35522 [06:23<03:44, 61.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21822/35522 [06:23<03:49, 59.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21829/35522 [06:24<03:47, 60.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21836/35522 [06:24<03:50, 59.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████▏   | 21843/35522 [06:24<03:49, 59.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21849/35522 [06:24<03:57, 57.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21856/35522 [06:24<03:48, 59.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21863/35522 [06:24<03:55, 58.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21870/35522 [06:24<03:47, 59.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21877/35522 [06:24<03:57, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21884/35522 [06:25<03:49, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21891/35522 [06:25<04:50, 46.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21898/35522 [06:25<04:42, 48.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21905/35522 [06:25<04:21, 52.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21912/35522 [06:25<04:21, 52.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21919/35522 [06:25<04:05, 55.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21926/35522 [06:25<03:52, 58.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21933/35522 [06:25<03:53, 58.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21940/35522 [06:26<03:47, 59.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21947/35522 [06:26<03:49, 59.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21954/35522 [06:26<03:47, 59.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21961/35522 [06:26<03:53, 57.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21968/35522 [06:26<03:46, 59.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21975/35522 [06:26<03:53, 58.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21982/35522 [06:26<03:48, 59.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21988/35522 [06:26<03:58, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 21995/35522 [06:27<03:46, 59.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22002/35522 [06:27<03:50, 58.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22009/35522 [06:27<03:44, 60.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22016/35522 [06:27<03:35, 62.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22023/35522 [06:27<03:51, 58.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22030/35522 [06:27<04:03, 55.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22036/35522 [06:27<04:06, 54.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22043/35522 [06:27<03:54, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22049/35522 [06:28<04:44, 47.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22055/35522 [06:28<04:40, 47.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22062/35522 [06:28<04:21, 51.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22068/35522 [06:28<04:21, 51.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22075/35522 [06:28<04:06, 54.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22082/35522 [06:28<03:54, 57.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22088/35522 [06:28<04:03, 55.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22095/35522 [06:28<03:52, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22101/35522 [06:28<04:06, 54.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22108/35522 [06:29<03:55, 56.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22114/35522 [06:29<04:03, 55.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22121/35522 [06:29<03:55, 56.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22127/35522 [06:29<04:28, 49.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22134/35522 [06:29<04:12, 53.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22140/35522 [06:29<04:12, 53.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22147/35522 [06:29<04:00, 55.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22153/35522 [06:29<04:04, 54.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22160/35522 [06:30<03:50, 57.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22166/35522 [06:30<03:58, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22173/35522 [06:30<03:47, 58.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22180/35522 [06:30<03:39, 60.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22187/35522 [06:30<03:45, 59.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22194/35522 [06:30<03:44, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 22200/35522 [06:30<03:46, 58.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22207/35522 [06:30<03:36, 61.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22214/35522 [06:30<03:47, 58.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22221/35522 [06:31<03:38, 60.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22228/35522 [06:31<03:50, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22234/35522 [06:31<03:48, 58.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22240/35522 [06:31<04:02, 54.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22247/35522 [06:31<03:53, 56.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22254/35522 [06:31<03:55, 56.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22260/35522 [06:31<03:55, 56.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22267/35522 [06:31<03:46, 58.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22273/35522 [06:31<03:57, 55.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22280/35522 [06:32<03:47, 58.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22286/35522 [06:32<04:03, 54.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22292/35522 [06:32<03:58, 55.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22298/35522 [06:32<04:02, 54.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22305/35522 [06:32<03:50, 57.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22311/35522 [06:32<04:03, 54.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22318/35522 [06:32<03:51, 57.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22325/35522 [06:32<03:44, 58.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22332/35522 [06:32<03:38, 60.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22339/35522 [06:33<03:36, 60.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22346/35522 [06:33<03:45, 58.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22352/35522 [06:33<03:45, 58.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22358/35522 [06:33<03:54, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22365/35522 [06:33<03:50, 57.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22371/35522 [06:33<04:04, 53.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22378/35522 [06:33<03:48, 57.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22384/35522 [06:33<03:59, 54.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22391/35522 [06:34<03:48, 57.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22397/35522 [06:34<03:59, 54.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22403/35522 [06:34<04:00, 54.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22409/35522 [06:34<04:02, 54.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22416/35522 [06:34<03:48, 57.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22423/35522 [06:34<03:38, 59.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22430/35522 [06:34<03:39, 59.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22437/35522 [06:34<03:30, 62.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22444/35522 [06:34<03:37, 60.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22451/35522 [06:35<03:36, 60.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22458/35522 [06:35<04:31, 48.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22464/35522 [06:35<04:23, 49.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22470/35522 [06:35<04:12, 51.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22477/35522 [06:35<03:58, 54.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22483/35522 [06:35<04:07, 52.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22490/35522 [06:35<03:51, 56.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22496/35522 [06:35<03:55, 55.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22503/35522 [06:36<03:47, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22509/35522 [06:36<03:51, 56.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22516/35522 [06:36<03:39, 59.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22522/35522 [06:36<03:48, 56.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22529/35522 [06:36<03:37, 59.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22536/35522 [06:36<03:32, 61.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22543/35522 [06:36<03:37, 59.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 22550/35522 [06:36<03:35, 60.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22557/35522 [06:36<03:35, 60.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22564/35522 [06:37<03:33, 60.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22571/35522 [06:37<03:46, 57.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22578/35522 [06:37<03:39, 59.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22584/35522 [06:37<03:39, 58.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22591/35522 [06:37<03:31, 61.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22598/35522 [06:37<03:33, 60.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22605/35522 [06:37<03:27, 62.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22612/35522 [06:37<03:26, 62.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22619/35522 [06:37<03:30, 61.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22626/35522 [06:38<03:28, 61.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22633/35522 [06:38<03:45, 57.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▎   | 22640/35522 [06:38<03:34, 60.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22647/35522 [06:38<04:00, 53.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22654/35522 [06:38<04:03, 52.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22661/35522 [06:38<03:48, 56.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22668/35522 [06:38<03:40, 58.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22674/35522 [06:39<04:24, 48.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22680/35522 [06:39<04:16, 50.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22687/35522 [06:39<03:57, 53.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22694/35522 [06:39<03:49, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22700/35522 [06:39<03:48, 56.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22707/35522 [06:39<03:45, 56.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22713/35522 [06:39<03:51, 55.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22719/35522 [06:39<03:51, 55.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22725/35522 [06:39<03:55, 54.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22732/35522 [06:40<03:47, 56.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22738/35522 [06:40<03:57, 53.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22745/35522 [06:40<03:45, 56.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22751/35522 [06:40<03:48, 55.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22758/35522 [06:40<03:37, 58.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22765/35522 [06:40<03:29, 60.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22772/35522 [06:40<04:09, 51.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22778/35522 [06:40<04:11, 50.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22785/35522 [06:41<03:57, 53.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22792/35522 [06:41<03:51, 55.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22799/35522 [06:41<03:41, 57.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22806/35522 [06:41<03:35, 59.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22812/35522 [06:41<03:40, 57.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22819/35522 [06:41<03:34, 59.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22825/35522 [06:41<03:34, 59.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22832/35522 [06:41<03:29, 60.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22839/35522 [06:41<03:29, 60.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22846/35522 [06:42<03:30, 60.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22853/35522 [06:42<03:37, 58.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22860/35522 [06:42<03:31, 60.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22867/35522 [06:42<03:34, 58.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22874/35522 [06:42<03:32, 59.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22880/35522 [06:42<03:31, 59.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22886/35522 [06:42<03:42, 56.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22893/35522 [06:42<03:32, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22899/35522 [06:42<03:35, 58.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 22906/35522 [06:43<03:29, 60.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22913/35522 [06:43<03:37, 58.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22919/35522 [06:43<03:35, 58.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22925/35522 [06:43<03:42, 56.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22932/35522 [06:43<03:35, 58.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22939/35522 [06:43<03:32, 59.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22945/35522 [06:43<03:39, 57.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22952/35522 [06:43<03:34, 58.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22958/35522 [06:43<03:38, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22965/35522 [06:44<03:30, 59.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22971/35522 [06:44<04:15, 49.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22978/35522 [06:44<03:58, 52.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22984/35522 [06:44<03:52, 53.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22991/35522 [06:44<03:41, 56.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 22997/35522 [06:44<03:48, 54.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23004/35522 [06:44<03:40, 56.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23010/35522 [06:44<03:48, 54.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23017/35522 [06:45<03:39, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23023/35522 [06:45<03:47, 54.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23030/35522 [06:45<03:34, 58.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23037/35522 [06:45<03:27, 60.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23044/35522 [06:45<03:32, 58.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23051/35522 [06:45<03:29, 59.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23057/35522 [06:45<03:39, 56.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23064/35522 [06:45<03:34, 58.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23070/35522 [06:45<03:41, 56.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23077/35522 [06:46<03:30, 59.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 23083/35522 [06:46<03:38, 57.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23090/35522 [06:46<03:28, 59.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23097/35522 [06:46<03:34, 57.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23104/35522 [06:46<03:30, 59.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23111/35522 [06:46<03:33, 58.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23118/35522 [06:46<03:26, 60.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23125/35522 [06:46<03:35, 57.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23132/35522 [06:46<03:29, 59.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23139/35522 [06:47<03:27, 59.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23145/35522 [06:47<03:38, 56.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23152/35522 [06:47<03:29, 59.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23158/35522 [06:47<03:37, 56.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23165/35522 [06:47<03:31, 58.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23171/35522 [06:47<03:35, 57.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23178/35522 [06:47<03:27, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23184/35522 [06:47<03:30, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23191/35522 [06:47<03:22, 60.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23198/35522 [06:48<03:17, 62.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23205/35522 [06:48<03:26, 59.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23212/35522 [06:48<03:26, 59.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23218/35522 [06:48<03:39, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23224/35522 [06:48<03:40, 55.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23230/35522 [06:48<03:46, 54.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23237/35522 [06:48<03:35, 57.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23243/35522 [06:48<03:39, 56.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23250/35522 [06:49<03:30, 58.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23256/35522 [06:49<03:34, 57.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 23263/35522 [06:49<03:28, 58.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23270/35522 [06:49<03:18, 61.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23277/35522 [06:49<03:19, 61.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23284/35522 [06:49<03:19, 61.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23291/35522 [06:49<03:23, 60.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23298/35522 [06:49<03:20, 60.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23305/35522 [06:49<03:28, 58.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23312/35522 [06:50<03:24, 59.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23319/35522 [06:50<03:38, 55.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23326/35522 [06:50<03:28, 58.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23332/35522 [06:50<03:28, 58.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23339/35522 [06:50<03:22, 60.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23346/35522 [06:50<03:27, 58.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23353/35522 [06:50<03:22, 59.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23360/35522 [06:50<03:18, 61.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23367/35522 [06:51<03:56, 51.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23373/35522 [06:51<03:57, 51.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23379/35522 [06:51<03:48, 53.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23385/35522 [06:51<03:53, 52.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23392/35522 [06:51<03:38, 55.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23399/35522 [06:51<03:30, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23405/35522 [06:51<03:37, 55.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23412/35522 [06:51<03:32, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23418/35522 [06:51<03:36, 56.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23425/35522 [06:52<03:27, 58.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23431/35522 [06:52<03:29, 57.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23437/35522 [06:52<03:27, 58.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23444/35522 [06:52<03:28, 57.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23451/35522 [06:52<03:24, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23457/35522 [06:52<03:23, 59.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23463/35522 [06:52<03:23, 59.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23470/35522 [06:52<03:16, 61.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23477/35522 [06:52<03:21, 59.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23484/35522 [06:53<03:19, 60.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23491/35522 [06:53<03:31, 56.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23498/35522 [06:53<03:23, 59.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23504/35522 [06:53<03:29, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23511/35522 [06:53<03:22, 59.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23518/35522 [06:53<03:26, 58.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23525/35522 [06:53<03:22, 59.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 23532/35522 [06:53<03:28, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23538/35522 [06:53<03:29, 57.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23544/35522 [06:54<03:50, 51.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23550/35522 [06:54<03:43, 53.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23557/35522 [06:54<03:30, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23563/35522 [06:54<03:43, 53.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23569/35522 [06:54<03:50, 51.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23575/35522 [06:54<03:49, 52.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23582/35522 [06:54<03:34, 55.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23588/35522 [06:54<03:41, 53.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23595/35522 [06:55<03:30, 56.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23601/35522 [06:55<03:29, 56.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23607/35522 [06:55<03:29, 56.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23614/35522 [06:55<03:17, 60.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 23621/35522 [06:55<03:23, 58.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23627/35522 [06:55<03:23, 58.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23633/35522 [06:55<04:09, 47.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23639/35522 [06:55<04:11, 47.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23646/35522 [06:56<03:48, 51.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23652/35522 [06:56<03:42, 53.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23658/35522 [06:56<03:38, 54.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23665/35522 [06:56<03:32, 55.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23671/35522 [06:56<03:35, 54.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23678/35522 [06:56<03:28, 56.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23684/35522 [06:56<03:41, 53.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23691/35522 [06:56<03:31, 56.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23697/35522 [06:56<03:33, 55.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23704/35522 [06:57<03:21, 58.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23710/35522 [06:57<03:27, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23717/35522 [06:57<03:18, 59.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23724/35522 [06:57<03:27, 56.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23731/35522 [06:57<03:20, 58.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23738/35522 [06:57<03:15, 60.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23745/35522 [06:57<03:23, 57.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23752/35522 [06:57<03:14, 60.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23759/35522 [06:57<03:27, 56.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23766/35522 [06:58<03:19, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23772/35522 [06:58<03:58, 49.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23778/35522 [06:58<04:01, 48.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23785/35522 [06:58<03:39, 53.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23792/35522 [06:58<03:27, 56.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23798/35522 [06:58<03:26, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23804/35522 [06:58<03:45, 52.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23810/35522 [06:58<03:45, 51.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23817/35522 [06:59<03:40, 53.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23823/35522 [06:59<03:38, 53.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23830/35522 [06:59<03:28, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23836/35522 [06:59<03:33, 54.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23843/35522 [06:59<03:22, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23849/35522 [06:59<03:32, 54.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23856/35522 [06:59<03:24, 57.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23863/35522 [06:59<03:26, 56.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23870/35522 [07:00<03:15, 59.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23877/35522 [07:00<03:13, 60.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23884/35522 [07:00<03:14, 59.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23891/35522 [07:00<03:19, 58.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23897/35522 [07:00<03:28, 55.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23904/35522 [07:00<03:20, 57.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23910/35522 [07:00<03:27, 56.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23917/35522 [07:00<03:19, 58.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23923/35522 [07:00<03:25, 56.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23930/35522 [07:01<03:17, 58.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23936/35522 [07:01<03:19, 58.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23943/35522 [07:01<03:14, 59.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23950/35522 [07:01<03:07, 61.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23957/35522 [07:01<03:16, 58.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23964/35522 [07:01<03:12, 60.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 23971/35522 [07:01<03:16, 58.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 23978/35522 [07:01<03:14, 59.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 23984/35522 [07:01<03:19, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 23991/35522 [07:02<03:15, 58.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 23997/35522 [07:02<03:18, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24004/35522 [07:02<03:12, 59.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24011/35522 [07:02<03:23, 56.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24018/35522 [07:02<03:14, 59.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24025/35522 [07:02<03:23, 56.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24032/35522 [07:02<03:18, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24038/35522 [07:02<03:28, 55.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24045/35522 [07:03<03:20, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24052/35522 [07:03<03:24, 56.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24059/35522 [07:03<03:19, 57.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24066/35522 [07:03<03:13, 59.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24072/35522 [07:03<03:18, 57.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24079/35522 [07:03<03:11, 59.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24086/35522 [07:03<03:19, 57.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24093/35522 [07:03<03:12, 59.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24099/35522 [07:03<03:17, 57.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24105/35522 [07:04<03:20, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24111/35522 [07:04<03:30, 54.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24117/35522 [07:04<03:30, 54.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24123/35522 [07:04<03:34, 53.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24130/35522 [07:04<03:20, 56.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24137/35522 [07:04<03:12, 59.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24143/35522 [07:04<03:15, 58.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24150/35522 [07:04<03:11, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24156/35522 [07:05<03:58, 47.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24163/35522 [07:05<03:45, 50.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24170/35522 [07:05<03:25, 55.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24177/35522 [07:05<03:17, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24184/35522 [07:05<03:13, 58.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24191/35522 [07:05<03:06, 60.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24198/35522 [07:05<03:16, 57.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24205/35522 [07:05<03:14, 58.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24211/35522 [07:05<03:20, 56.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24217/35522 [07:06<03:18, 57.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24223/35522 [07:06<03:29, 54.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24230/35522 [07:06<03:20, 56.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24236/35522 [07:06<03:22, 55.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24243/35522 [07:06<03:15, 57.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24250/35522 [07:06<03:10, 59.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24256/35522 [07:06<03:48, 49.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24262/35522 [07:06<03:48, 49.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24269/35522 [07:07<03:33, 52.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24275/35522 [07:07<03:30, 53.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24282/35522 [07:07<03:20, 55.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24288/35522 [07:07<03:19, 56.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24294/35522 [07:07<03:17, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24301/35522 [07:07<03:12, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24307/35522 [07:07<03:15, 57.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24314/35522 [07:07<03:05, 60.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24321/35522 [07:07<03:34, 52.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 24328/35522 [07:08<03:25, 54.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24334/35522 [07:08<03:30, 53.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24340/35522 [07:08<03:24, 54.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24346/35522 [07:08<03:27, 53.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24353/35522 [07:08<03:17, 56.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24360/35522 [07:08<03:19, 55.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24367/35522 [07:08<03:10, 58.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24374/35522 [07:08<03:06, 59.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24381/35522 [07:09<03:15, 57.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24388/35522 [07:09<03:11, 58.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24394/35522 [07:09<03:13, 57.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24400/35522 [07:09<03:13, 57.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24406/35522 [07:09<03:21, 55.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24412/35522 [07:09<03:17, 56.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▊   | 24418/35522 [07:09<03:21, 55.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24425/35522 [07:09<03:10, 58.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24432/35522 [07:09<03:16, 56.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24439/35522 [07:10<03:08, 58.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24446/35522 [07:10<03:05, 59.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24453/35522 [07:10<03:03, 60.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24460/35522 [07:10<03:01, 60.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24467/35522 [07:10<03:08, 58.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24474/35522 [07:10<03:06, 59.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24480/35522 [07:10<03:13, 57.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24487/35522 [07:10<03:05, 59.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24493/35522 [07:10<03:18, 55.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24500/35522 [07:11<03:10, 57.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24506/35522 [07:11<03:15, 56.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24512/35522 [07:11<03:19, 55.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24518/35522 [07:11<03:57, 46.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24524/35522 [07:11<03:41, 49.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24530/35522 [07:11<03:46, 48.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24536/35522 [07:11<03:35, 51.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24542/35522 [07:11<03:34, 51.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24548/35522 [07:12<03:27, 52.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24554/35522 [07:12<03:20, 54.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24560/35522 [07:12<03:23, 53.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24567/35522 [07:12<03:14, 56.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24573/35522 [07:12<03:18, 55.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24580/35522 [07:12<03:12, 56.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24586/35522 [07:12<03:20, 54.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24593/35522 [07:12<03:13, 56.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24599/35522 [07:12<03:22, 53.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24605/35522 [07:13<03:22, 53.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24611/35522 [07:13<03:32, 51.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24617/35522 [07:13<03:23, 53.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24623/35522 [07:13<03:25, 53.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24630/35522 [07:13<03:15, 55.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24637/35522 [07:13<03:09, 57.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24643/35522 [07:13<03:11, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24650/35522 [07:13<03:05, 58.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24656/35522 [07:13<03:19, 54.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24662/35522 [07:14<03:14, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24668/35522 [07:14<03:16, 55.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24675/35522 [07:14<03:07, 57.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 24681/35522 [07:14<03:10, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24688/35522 [07:14<03:05, 58.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24695/35522 [07:14<03:00, 59.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24701/35522 [07:14<03:02, 59.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24707/35522 [07:14<03:04, 58.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24713/35522 [07:14<03:09, 57.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24719/35522 [07:15<03:15, 55.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24725/35522 [07:15<03:24, 52.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24732/35522 [07:15<03:12, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24738/35522 [07:15<03:15, 55.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24745/35522 [07:15<03:05, 58.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24752/35522 [07:15<03:09, 56.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24759/35522 [07:15<03:05, 58.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24765/35522 [07:15<03:04, 58.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24771/35522 [07:15<03:13, 55.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24778/35522 [07:16<03:09, 56.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24784/35522 [07:16<03:16, 54.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24791/35522 [07:16<03:05, 57.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24797/35522 [07:16<03:11, 56.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24804/35522 [07:16<03:05, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24810/35522 [07:16<03:07, 57.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24816/35522 [07:16<03:05, 57.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24823/35522 [07:16<03:14, 55.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24829/35522 [07:17<03:14, 54.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24836/35522 [07:17<03:17, 54.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24842/35522 [07:17<03:37, 49.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24848/35522 [07:17<03:37, 49.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24854/35522 [07:17<03:31, 50.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 24860/35522 [07:17<03:31, 50.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24867/35522 [07:17<03:19, 53.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24874/35522 [07:17<03:21, 52.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24880/35522 [07:18<03:31, 50.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24886/35522 [07:18<03:27, 51.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24893/35522 [07:18<03:14, 54.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24900/35522 [07:18<03:05, 57.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24906/35522 [07:18<03:08, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24912/35522 [07:18<03:10, 55.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24918/35522 [07:18<03:12, 55.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24925/35522 [07:18<03:00, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24931/35522 [07:18<03:03, 57.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24937/35522 [07:19<03:02, 58.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24943/35522 [07:19<03:02, 57.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24949/35522 [07:19<03:04, 57.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24956/35522 [07:19<03:00, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24962/35522 [07:19<03:41, 47.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24969/35522 [07:19<03:30, 50.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24976/35522 [07:19<03:13, 54.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24983/35522 [07:19<03:03, 57.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24989/35522 [07:19<03:06, 56.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 24995/35522 [07:20<03:04, 57.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25001/35522 [07:20<03:09, 55.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25008/35522 [07:20<02:59, 58.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25014/35522 [07:20<03:33, 49.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25021/35522 [07:20<03:20, 52.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25027/35522 [07:20<03:25, 51.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25034/35522 [07:20<03:13, 54.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 25040/35522 [07:20<03:15, 53.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25047/35522 [07:21<03:08, 55.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25053/35522 [07:21<03:05, 56.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25059/35522 [07:21<03:04, 56.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25066/35522 [07:21<02:58, 58.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25072/35522 [07:21<03:07, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25079/35522 [07:21<02:58, 58.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25085/35522 [07:21<02:59, 58.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25092/35522 [07:21<02:52, 60.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25099/35522 [07:21<02:57, 58.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25106/35522 [07:22<02:54, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25112/35522 [07:22<03:03, 56.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25119/35522 [07:22<02:56, 58.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25126/35522 [07:22<03:03, 56.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25133/35522 [07:22<02:57, 58.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25140/35522 [07:22<03:04, 56.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25147/35522 [07:22<02:58, 58.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25154/35522 [07:22<02:53, 59.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25161/35522 [07:23<02:55, 59.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25168/35522 [07:23<02:52, 60.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25175/35522 [07:23<02:57, 58.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25182/35522 [07:23<02:48, 61.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25189/35522 [07:23<02:55, 58.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25196/35522 [07:23<02:47, 61.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25203/35522 [07:23<02:49, 60.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25210/35522 [07:23<02:46, 61.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25217/35522 [07:23<02:56, 58.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25224/35522 [07:24<02:51, 59.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25231/35522 [07:24<03:01, 56.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25238/35522 [07:24<02:53, 59.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25245/35522 [07:24<03:01, 56.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25252/35522 [07:24<02:54, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25259/35522 [07:24<02:55, 58.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25266/35522 [07:24<02:49, 60.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25273/35522 [07:24<02:44, 62.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25280/35522 [07:25<02:59, 57.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25287/35522 [07:25<02:55, 58.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25293/35522 [07:25<03:34, 47.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25299/35522 [07:25<03:26, 49.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 25306/35522 [07:25<03:10, 53.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25313/35522 [07:25<03:09, 53.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25320/35522 [07:25<02:57, 57.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25327/35522 [07:25<02:47, 60.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25334/35522 [07:26<02:55, 58.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25340/35522 [07:26<02:54, 58.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25346/35522 [07:26<03:00, 56.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25353/35522 [07:26<02:53, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25359/35522 [07:26<03:01, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25365/35522 [07:26<02:58, 56.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25371/35522 [07:26<03:03, 55.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25377/35522 [07:26<03:01, 55.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25383/35522 [07:26<02:57, 57.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25389/35522 [07:26<03:03, 55.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 25396/35522 [07:27<02:54, 58.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25403/35522 [07:27<02:51, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25410/35522 [07:27<02:47, 60.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25417/35522 [07:27<02:52, 58.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25425/35522 [07:27<02:42, 62.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25432/35522 [07:27<02:52, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25439/35522 [07:27<02:46, 60.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25446/35522 [07:27<02:51, 58.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25452/35522 [07:28<02:51, 58.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25459/35522 [07:28<02:46, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25466/35522 [07:28<02:47, 59.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25473/35522 [07:28<03:05, 54.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25480/35522 [07:28<02:55, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25487/35522 [07:28<02:49, 59.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25494/35522 [07:28<02:53, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25501/35522 [07:28<02:47, 59.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25508/35522 [07:28<02:53, 57.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25514/35522 [07:29<02:54, 57.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25520/35522 [07:29<03:03, 54.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25527/35522 [07:29<02:56, 56.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25533/35522 [07:29<03:03, 54.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25539/35522 [07:29<02:59, 55.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25545/35522 [07:29<03:39, 45.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25552/35522 [07:29<03:20, 49.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25558/35522 [07:29<03:14, 51.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25565/35522 [07:30<02:59, 55.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25572/35522 [07:30<02:52, 57.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25579/35522 [07:30<02:48, 58.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25586/35522 [07:30<02:44, 60.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25593/35522 [07:30<02:40, 61.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25600/35522 [07:30<02:38, 62.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25607/35522 [07:30<02:45, 59.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25614/35522 [07:30<02:41, 61.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25621/35522 [07:30<02:44, 60.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25628/35522 [07:31<02:40, 61.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25635/35522 [07:31<02:50, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25642/35522 [07:31<02:46, 59.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25649/35522 [07:31<03:12, 51.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25656/35522 [07:31<03:02, 54.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25662/35522 [07:31<02:59, 55.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25669/35522 [07:31<02:51, 57.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25675/35522 [07:31<02:56, 55.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25682/35522 [07:32<02:49, 58.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25688/35522 [07:32<02:56, 55.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25695/35522 [07:32<02:47, 58.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25702/35522 [07:32<02:45, 59.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25708/35522 [07:32<02:47, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25715/35522 [07:32<02:44, 59.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25721/35522 [07:32<02:49, 57.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25728/35522 [07:32<02:43, 60.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25735/35522 [07:32<02:52, 56.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25741/35522 [07:33<02:50, 57.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25747/35522 [07:33<02:57, 55.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 25753/35522 [07:33<02:54, 55.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25759/35522 [07:33<02:51, 56.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25765/35522 [07:33<02:51, 56.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25771/35522 [07:33<02:49, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25777/35522 [07:33<03:18, 49.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25784/35522 [07:33<03:10, 51.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25791/35522 [07:34<02:59, 54.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25798/35522 [07:34<02:49, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25804/35522 [07:34<03:01, 53.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25811/35522 [07:34<02:51, 56.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25817/35522 [07:34<02:56, 54.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25823/35522 [07:34<02:58, 54.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25829/35522 [07:34<03:03, 52.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25836/35522 [07:34<02:54, 55.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25842/35522 [07:34<02:58, 54.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25849/35522 [07:35<02:50, 56.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25855/35522 [07:35<02:55, 55.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25862/35522 [07:35<02:47, 57.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25869/35522 [07:35<02:48, 57.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25876/35522 [07:35<02:41, 59.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25883/35522 [07:35<02:36, 61.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25890/35522 [07:35<02:37, 61.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25897/35522 [07:35<02:33, 62.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25904/35522 [07:35<02:34, 62.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25911/35522 [07:36<02:36, 61.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25918/35522 [07:36<02:39, 60.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25925/35522 [07:36<02:37, 60.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25932/35522 [07:36<02:40, 59.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25939/35522 [07:36<02:36, 61.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25946/35522 [07:36<02:41, 59.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25953/35522 [07:36<02:39, 59.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25960/35522 [07:36<02:46, 57.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25967/35522 [07:37<02:43, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25974/35522 [07:37<02:50, 56.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25980/35522 [07:37<02:48, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25987/35522 [07:37<02:44, 57.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 25993/35522 [07:37<02:51, 55.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26000/35522 [07:37<02:41, 58.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26006/35522 [07:37<02:43, 58.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26012/35522 [07:37<02:45, 57.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26018/35522 [07:38<03:18, 47.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26025/35522 [07:38<03:02, 51.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26031/35522 [07:38<02:58, 53.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26038/35522 [07:38<02:50, 55.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26044/35522 [07:38<02:58, 53.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26051/35522 [07:38<02:50, 55.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26057/35522 [07:38<02:47, 56.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26064/35522 [07:38<02:39, 59.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26071/35522 [07:38<02:37, 59.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26078/35522 [07:39<02:42, 58.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26085/35522 [07:39<02:35, 60.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26092/35522 [07:39<02:38, 59.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26098/35522 [07:39<02:39, 59.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 26104/35522 [07:39<02:47, 56.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26110/35522 [07:39<02:44, 57.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26116/35522 [07:39<02:52, 54.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26123/35522 [07:39<02:45, 56.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26129/35522 [07:39<02:54, 53.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26135/35522 [07:40<02:52, 54.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26142/35522 [07:40<02:53, 54.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26148/35522 [07:40<02:49, 55.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26154/35522 [07:40<02:47, 56.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26160/35522 [07:40<02:47, 55.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26167/35522 [07:40<02:40, 58.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26173/35522 [07:40<02:44, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26180/35522 [07:40<02:37, 59.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26186/35522 [07:40<02:42, 57.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▎  | 26193/35522 [07:41<02:41, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26200/35522 [07:41<02:44, 56.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26206/35522 [07:41<02:42, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26213/35522 [07:41<02:38, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26219/35522 [07:41<02:45, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26226/35522 [07:41<02:37, 58.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26232/35522 [07:41<03:08, 49.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26239/35522 [07:41<03:04, 50.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26246/35522 [07:42<02:51, 54.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26252/35522 [07:42<02:49, 54.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26258/35522 [07:42<02:53, 53.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26265/35522 [07:42<02:43, 56.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26271/35522 [07:42<02:44, 56.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26278/35522 [07:42<02:37, 58.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26284/35522 [07:42<02:44, 56.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26291/35522 [07:42<02:37, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26297/35522 [07:42<02:41, 57.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26303/35522 [07:43<02:44, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26310/35522 [07:43<02:39, 57.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26316/35522 [07:43<02:46, 55.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26322/35522 [07:43<02:43, 56.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26328/35522 [07:43<02:44, 55.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26335/35522 [07:43<02:38, 58.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26341/35522 [07:43<02:44, 55.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26348/35522 [07:43<02:37, 58.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26354/35522 [07:43<02:42, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26360/35522 [07:44<03:07, 48.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26366/35522 [07:44<03:02, 50.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26373/35522 [07:44<02:46, 55.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26380/35522 [07:44<02:42, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26387/35522 [07:44<02:37, 57.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26394/35522 [07:44<02:32, 59.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26401/35522 [07:44<02:37, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26408/35522 [07:44<02:31, 60.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26415/35522 [07:45<03:03, 49.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26421/35522 [07:45<03:00, 50.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26428/35522 [07:45<02:46, 54.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26435/35522 [07:45<02:48, 53.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26442/35522 [07:45<02:41, 56.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26449/35522 [07:45<02:44, 55.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26455/35522 [07:45<02:42, 55.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 26462/35522 [07:45<02:37, 57.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26468/35522 [07:46<02:39, 56.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26475/35522 [07:46<02:31, 59.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26482/35522 [07:46<02:36, 57.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26489/35522 [07:46<02:31, 59.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26495/35522 [07:46<02:36, 57.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26502/35522 [07:46<02:34, 58.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26508/35522 [07:46<02:39, 56.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26515/35522 [07:46<02:30, 60.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26522/35522 [07:46<02:23, 62.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26529/35522 [07:47<02:36, 57.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26536/35522 [07:47<02:31, 59.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26543/35522 [07:47<02:39, 56.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26550/35522 [07:47<02:31, 59.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26557/35522 [07:47<02:37, 56.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26564/35522 [07:47<02:36, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26570/35522 [07:47<02:38, 56.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26577/35522 [07:47<02:34, 58.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26583/35522 [07:47<02:39, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26589/35522 [07:48<02:37, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26595/35522 [07:48<02:43, 54.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26601/35522 [07:48<02:42, 55.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26607/35522 [07:48<02:46, 53.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26614/35522 [07:48<02:40, 55.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26621/35522 [07:48<02:41, 55.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26627/35522 [07:48<02:51, 51.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26633/35522 [07:48<02:48, 52.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▍  | 26640/35522 [07:49<02:38, 56.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26646/35522 [07:49<02:35, 57.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26652/35522 [07:49<02:34, 57.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26659/35522 [07:49<02:30, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26665/35522 [07:49<02:34, 57.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26672/35522 [07:49<02:29, 59.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26678/35522 [07:49<02:33, 57.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26685/35522 [07:49<02:30, 58.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26691/35522 [07:49<02:30, 58.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26697/35522 [07:50<02:36, 56.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26703/35522 [07:50<02:34, 57.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26709/35522 [07:50<02:35, 56.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26716/35522 [07:50<02:27, 59.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26722/35522 [07:50<02:29, 58.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26729/35522 [07:50<02:26, 59.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26736/35522 [07:50<02:32, 57.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26743/35522 [07:50<02:25, 60.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26750/35522 [07:50<02:20, 62.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26757/35522 [07:51<02:25, 60.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26764/35522 [07:51<02:24, 60.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26771/35522 [07:51<02:25, 60.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26778/35522 [07:51<02:22, 61.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26785/35522 [07:51<02:27, 59.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26792/35522 [07:51<02:23, 60.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26799/35522 [07:51<02:28, 58.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26806/35522 [07:51<02:24, 60.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26813/35522 [07:51<02:27, 58.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 26819/35522 [07:52<02:30, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26825/35522 [07:52<02:41, 53.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26831/35522 [07:52<02:38, 54.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26838/35522 [07:52<02:30, 57.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26844/35522 [07:52<02:45, 52.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26850/35522 [07:52<02:41, 53.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26856/35522 [07:52<02:39, 54.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26863/35522 [07:52<02:31, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26869/35522 [07:52<02:42, 53.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26876/35522 [07:53<02:34, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26882/35522 [07:53<02:40, 53.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26889/35522 [07:53<02:36, 55.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26895/35522 [07:53<02:38, 54.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26901/35522 [07:53<02:34, 55.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26907/35522 [07:53<02:37, 54.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26914/35522 [07:53<02:33, 56.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26921/35522 [07:53<02:35, 55.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26928/35522 [07:54<02:29, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26934/35522 [07:54<02:29, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26940/35522 [07:54<02:37, 54.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26947/35522 [07:54<02:26, 58.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26953/35522 [07:54<02:29, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26960/35522 [07:54<02:21, 60.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26967/35522 [07:54<02:25, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26974/35522 [07:54<02:20, 60.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26981/35522 [07:54<02:22, 59.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26988/35522 [07:55<02:22, 60.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 26995/35522 [07:55<02:28, 57.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27001/35522 [07:55<02:44, 51.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27007/35522 [07:55<02:43, 51.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27014/35522 [07:55<02:35, 54.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27021/35522 [07:55<02:36, 54.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27028/35522 [07:55<02:29, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27035/35522 [07:55<02:32, 55.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27042/35522 [07:56<02:26, 58.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27049/35522 [07:56<02:30, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27056/35522 [07:56<02:23, 59.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27063/35522 [07:56<02:21, 59.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27070/35522 [07:56<02:30, 56.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27077/35522 [07:56<02:24, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 27083/35522 [07:56<02:31, 55.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27090/35522 [07:56<02:24, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27096/35522 [07:56<02:31, 55.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27102/35522 [07:57<02:33, 54.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27108/35522 [07:57<02:38, 52.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27115/35522 [07:57<02:28, 56.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27121/35522 [07:57<02:29, 56.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27128/35522 [07:57<02:24, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27135/35522 [07:57<02:24, 58.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27141/35522 [07:57<02:24, 58.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27148/35522 [07:57<02:20, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27154/35522 [07:58<02:54, 47.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27160/35522 [07:58<02:55, 47.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27167/35522 [07:58<02:41, 51.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▋  | 27173/35522 [07:58<02:40, 52.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27180/35522 [07:58<02:29, 55.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27187/35522 [07:58<02:22, 58.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27193/35522 [07:58<02:25, 57.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27200/35522 [07:58<02:20, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27206/35522 [07:58<02:24, 57.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27213/35522 [07:59<02:17, 60.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27220/35522 [07:59<02:17, 60.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27227/35522 [07:59<02:14, 61.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27234/35522 [07:59<02:25, 56.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27241/35522 [07:59<02:19, 59.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27248/35522 [07:59<02:13, 62.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27255/35522 [07:59<02:19, 59.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27262/35522 [07:59<02:13, 61.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27269/35522 [08:00<02:16, 60.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27276/35522 [08:00<02:15, 60.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27283/35522 [08:00<02:16, 60.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27290/35522 [08:00<02:13, 61.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27297/35522 [08:00<02:41, 50.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27304/35522 [08:00<02:33, 53.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27310/35522 [08:00<02:30, 54.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27316/35522 [08:00<02:28, 55.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27322/35522 [08:00<02:29, 54.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27329/35522 [08:01<02:23, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27335/35522 [08:01<02:28, 55.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27342/35522 [08:01<02:24, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27348/35522 [08:01<02:27, 55.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27355/35522 [08:01<02:19, 58.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27362/35522 [08:01<02:13, 60.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27369/35522 [08:01<02:24, 56.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27376/35522 [08:01<02:19, 58.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27383/35522 [08:02<02:17, 59.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27390/35522 [08:02<02:12, 61.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27397/35522 [08:02<02:17, 59.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27404/35522 [08:02<02:13, 60.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27411/35522 [08:02<02:17, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27418/35522 [08:02<02:14, 60.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27425/35522 [08:02<02:20, 57.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27432/35522 [08:02<02:16, 59.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27438/35522 [08:02<02:19, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27445/35522 [08:03<02:16, 59.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27452/35522 [08:03<02:17, 58.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27459/35522 [08:03<02:11, 61.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27466/35522 [08:03<02:22, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27473/35522 [08:03<02:15, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27480/35522 [08:03<02:10, 61.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27487/35522 [08:03<02:16, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27494/35522 [08:03<02:11, 61.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27501/35522 [08:04<02:13, 60.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27508/35522 [08:04<02:14, 59.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27515/35522 [08:04<02:22, 56.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27522/35522 [08:04<02:17, 58.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 27528/35522 [08:04<02:23, 55.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27535/35522 [08:04<02:16, 58.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27541/35522 [08:04<02:20, 56.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27548/35522 [08:04<02:16, 58.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27555/35522 [08:04<02:19, 56.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27562/35522 [08:05<02:17, 58.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27568/35522 [08:05<02:22, 55.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27575/35522 [08:05<02:18, 57.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27582/35522 [08:05<02:15, 58.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27588/35522 [08:05<02:40, 49.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27594/35522 [08:05<02:39, 49.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27601/35522 [08:05<02:28, 53.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27608/35522 [08:05<02:29, 52.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27615/35522 [08:06<02:22, 55.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27622/35522 [08:06<02:23, 55.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27629/35522 [08:06<02:15, 58.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27636/35522 [08:06<02:13, 59.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27642/35522 [08:06<02:18, 56.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27649/35522 [08:06<02:14, 58.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27655/35522 [08:06<02:18, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27661/35522 [08:06<02:18, 56.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27667/35522 [08:06<02:27, 53.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27674/35522 [08:07<02:20, 55.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27680/35522 [08:07<02:19, 56.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27687/35522 [08:07<02:15, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27694/35522 [08:07<02:17, 57.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27700/35522 [08:07<02:16, 57.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27707/35522 [08:07<02:13, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27713/35522 [08:07<02:19, 56.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27719/35522 [08:07<02:20, 55.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27725/35522 [08:08<02:20, 55.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27732/35522 [08:08<02:16, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27738/35522 [08:08<02:19, 55.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27745/35522 [08:08<02:15, 57.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27751/35522 [08:08<02:18, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27758/35522 [08:08<02:11, 58.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27765/35522 [08:08<02:05, 61.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27772/35522 [08:08<02:08, 60.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27779/35522 [08:08<02:06, 61.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27786/35522 [08:09<02:09, 59.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27793/35522 [08:09<02:09, 59.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27799/35522 [08:09<02:10, 59.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27806/35522 [08:09<02:10, 59.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27812/35522 [08:09<02:15, 57.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27819/35522 [08:09<02:12, 58.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27825/35522 [08:09<02:18, 55.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27832/35522 [08:09<02:15, 56.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27838/35522 [08:09<02:23, 53.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27845/35522 [08:10<02:17, 56.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27852/35522 [08:10<02:43, 47.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27859/35522 [08:10<02:30, 50.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27865/35522 [08:10<02:26, 52.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27872/35522 [08:10<02:16, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 27878/35522 [08:10<02:19, 54.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27885/35522 [08:10<02:13, 57.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27891/35522 [08:10<02:18, 55.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27897/35522 [08:11<02:17, 55.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27904/35522 [08:11<02:11, 58.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27910/35522 [08:11<02:18, 55.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27917/35522 [08:11<02:11, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27923/35522 [08:11<02:18, 54.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27929/35522 [08:11<02:15, 56.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27935/35522 [08:11<02:19, 54.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27942/35522 [08:11<02:14, 56.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27948/35522 [08:11<02:18, 54.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27954/35522 [08:12<02:17, 55.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27961/35522 [08:12<02:19, 54.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 27967/35522 [08:12<02:16, 55.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 27974/35522 [08:12<02:11, 57.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 27980/35522 [08:12<02:14, 56.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 27986/35522 [08:12<02:11, 57.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 27992/35522 [08:12<02:14, 56.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 27999/35522 [08:12<02:11, 57.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28005/35522 [08:12<02:14, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28012/35522 [08:13<02:06, 59.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28018/35522 [08:13<02:12, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28025/35522 [08:13<02:07, 59.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28032/35522 [08:13<02:04, 60.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28039/35522 [08:13<02:07, 58.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28046/35522 [08:13<02:06, 59.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28052/35522 [08:13<02:35, 48.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28058/35522 [08:13<02:32, 48.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28065/35522 [08:14<02:21, 52.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28072/35522 [08:14<02:18, 53.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28078/35522 [08:14<02:29, 49.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28084/35522 [08:14<02:28, 49.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28091/35522 [08:14<02:20, 52.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28097/35522 [08:14<02:23, 51.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28104/35522 [08:14<02:13, 55.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28110/35522 [08:14<02:11, 56.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28116/35522 [08:15<02:16, 54.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28123/35522 [08:15<02:10, 56.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28129/35522 [08:15<02:11, 56.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28136/35522 [08:15<02:06, 58.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28142/35522 [08:15<02:16, 53.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28148/35522 [08:15<02:22, 51.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28154/35522 [08:15<02:28, 49.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28161/35522 [08:15<02:19, 52.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28167/35522 [08:15<02:19, 52.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28173/35522 [08:16<02:14, 54.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28179/35522 [08:16<02:17, 53.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28185/35522 [08:16<02:16, 53.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28192/35522 [08:16<02:18, 53.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28198/35522 [08:16<02:35, 47.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28203/35522 [08:16<02:33, 47.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28210/35522 [08:16<02:21, 51.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28216/35522 [08:16<02:18, 52.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28222/35522 [08:17<02:21, 51.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28229/35522 [08:17<02:11, 55.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 28235/35522 [08:17<02:13, 54.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28242/35522 [08:17<02:06, 57.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28248/35522 [08:17<02:11, 55.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28254/35522 [08:17<02:10, 55.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28260/35522 [08:17<02:09, 55.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28267/35522 [08:17<02:05, 57.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28274/35522 [08:17<02:10, 55.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28280/35522 [08:18<02:08, 56.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28287/35522 [08:18<02:03, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28293/35522 [08:18<02:06, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28300/35522 [08:18<02:02, 58.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28306/35522 [08:18<02:09, 55.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28312/35522 [08:18<02:08, 56.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28318/35522 [08:18<02:10, 55.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28325/35522 [08:18<02:05, 57.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28331/35522 [08:18<02:06, 57.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28338/35522 [08:19<02:02, 58.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28345/35522 [08:19<01:57, 60.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28352/35522 [08:19<02:10, 54.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28358/35522 [08:19<02:20, 50.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28364/35522 [08:19<02:27, 48.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28369/35522 [08:19<02:34, 46.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28375/35522 [08:19<02:33, 46.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28380/35522 [08:19<02:39, 44.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28386/35522 [08:20<02:30, 47.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28392/35522 [08:20<02:26, 48.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28399/35522 [08:20<02:12, 53.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28405/35522 [08:20<02:10, 54.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 28411/35522 [08:20<02:09, 55.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28418/35522 [08:20<02:05, 56.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28424/35522 [08:20<02:09, 54.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28431/35522 [08:20<02:04, 57.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28437/35522 [08:21<02:12, 53.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28444/35522 [08:21<02:06, 55.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28450/35522 [08:21<02:11, 53.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28457/35522 [08:21<02:06, 55.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28463/35522 [08:21<02:10, 54.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28470/35522 [08:21<02:03, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28477/35522 [08:21<02:04, 56.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28484/35522 [08:21<01:59, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28491/35522 [08:21<01:55, 60.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28498/35522 [08:22<02:04, 56.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28504/35522 [08:22<02:02, 57.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28510/35522 [08:22<02:28, 47.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28516/35522 [08:22<02:25, 48.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28523/35522 [08:22<02:11, 53.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28530/35522 [08:22<02:03, 56.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28536/35522 [08:22<02:02, 57.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28543/35522 [08:22<01:59, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28549/35522 [08:23<02:03, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28555/35522 [08:23<02:10, 53.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28561/35522 [08:23<02:11, 52.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28567/35522 [08:23<02:08, 53.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28573/35522 [08:23<02:08, 53.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28580/35522 [08:23<02:02, 56.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28587/35522 [08:23<02:06, 54.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 28594/35522 [08:23<02:02, 56.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28601/35522 [08:23<02:06, 54.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28607/35522 [08:24<02:04, 55.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28614/35522 [08:24<02:06, 54.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28620/35522 [08:24<02:07, 54.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28627/35522 [08:24<02:09, 53.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28634/35522 [08:24<02:02, 56.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28640/35522 [08:24<02:00, 57.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28646/35522 [08:24<02:11, 52.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28653/35522 [08:24<02:03, 55.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28659/35522 [08:25<02:02, 55.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28665/35522 [08:25<02:04, 54.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28671/35522 [08:25<02:10, 52.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28677/35522 [08:25<02:06, 54.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28683/35522 [08:25<02:08, 53.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28690/35522 [08:25<02:03, 55.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28696/35522 [08:25<02:08, 52.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28702/35522 [08:25<02:05, 54.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28709/35522 [08:25<01:58, 57.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28715/35522 [08:26<01:58, 57.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28721/35522 [08:26<01:58, 57.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28727/35522 [08:26<02:02, 55.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28734/35522 [08:26<01:57, 58.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28740/35522 [08:26<02:02, 55.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28747/35522 [08:26<01:57, 57.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28753/35522 [08:26<01:58, 57.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28760/35522 [08:26<01:54, 59.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28767/35522 [08:26<01:50, 61.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28774/35522 [08:27<02:15, 49.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28780/35522 [08:27<02:14, 50.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28787/35522 [08:27<02:06, 53.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28793/35522 [08:27<02:09, 51.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28800/35522 [08:27<02:02, 54.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28807/35522 [08:27<02:00, 55.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28814/35522 [08:27<01:53, 59.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28821/35522 [08:27<01:50, 60.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28828/35522 [08:28<01:56, 57.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28835/35522 [08:28<01:53, 58.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28841/35522 [08:28<01:59, 56.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28848/35522 [08:28<01:54, 58.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28854/35522 [08:28<01:59, 56.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 28861/35522 [08:28<01:53, 58.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28867/35522 [08:28<01:53, 58.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28873/35522 [08:28<01:52, 59.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28880/35522 [08:28<01:48, 61.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28887/35522 [08:29<01:51, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28894/35522 [08:29<01:48, 60.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28901/35522 [08:29<01:47, 61.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28908/35522 [08:29<01:47, 61.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28915/35522 [08:29<02:05, 52.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28922/35522 [08:29<01:57, 56.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28928/35522 [08:29<01:57, 56.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28935/35522 [08:29<01:53, 57.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28941/35522 [08:30<01:58, 55.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████▏ | 28948/35522 [08:30<01:54, 57.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28954/35522 [08:30<01:55, 56.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28961/35522 [08:30<01:53, 57.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28967/35522 [08:30<01:55, 56.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28974/35522 [08:30<01:50, 59.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28981/35522 [08:30<01:50, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28987/35522 [08:30<01:53, 57.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 28994/35522 [08:30<01:51, 58.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29000/35522 [08:31<01:55, 56.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29007/35522 [08:31<01:51, 58.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29013/35522 [08:31<01:54, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29020/35522 [08:31<01:50, 59.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29026/35522 [08:31<01:51, 58.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29033/35522 [08:31<01:48, 59.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29040/35522 [08:31<01:46, 61.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29047/35522 [08:31<01:50, 58.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29054/35522 [08:31<01:48, 59.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29060/35522 [08:32<01:54, 56.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29066/35522 [08:32<01:53, 56.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29072/35522 [08:32<02:10, 49.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29078/35522 [08:32<02:18, 46.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29084/35522 [08:32<02:10, 49.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29090/35522 [08:32<02:04, 51.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29096/35522 [08:32<02:01, 52.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29103/35522 [08:32<01:54, 55.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29109/35522 [08:33<01:58, 54.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29116/35522 [08:33<01:52, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29122/35522 [08:33<01:59, 53.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29128/35522 [08:33<01:57, 54.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29135/35522 [08:33<01:54, 55.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29142/35522 [08:33<01:50, 57.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29149/35522 [08:33<01:47, 59.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29155/35522 [08:33<01:47, 58.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29162/35522 [08:33<01:46, 59.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29168/35522 [08:34<01:48, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29175/35522 [08:34<01:46, 59.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29181/35522 [08:34<01:48, 58.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29187/35522 [08:34<01:48, 58.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29193/35522 [08:34<01:47, 58.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29199/35522 [08:34<01:47, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29205/35522 [08:34<01:47, 58.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29211/35522 [08:34<01:48, 58.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29218/35522 [08:34<01:45, 59.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29224/35522 [08:34<01:54, 55.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29231/35522 [08:35<01:51, 56.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29237/35522 [08:35<01:55, 54.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29244/35522 [08:35<01:49, 57.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29251/35522 [08:35<01:52, 55.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29258/35522 [08:35<01:47, 58.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29265/35522 [08:35<01:43, 60.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29272/35522 [08:35<01:56, 53.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29278/35522 [08:35<01:58, 52.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29284/35522 [08:36<02:04, 50.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29291/35522 [08:36<01:58, 52.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29298/35522 [08:36<01:51, 55.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 29305/35522 [08:36<01:47, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29311/35522 [08:36<01:49, 56.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29317/35522 [08:36<01:47, 57.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29324/35522 [08:36<01:45, 58.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29331/35522 [08:36<01:42, 60.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29338/35522 [08:37<01:45, 58.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29345/35522 [08:37<01:43, 59.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29352/35522 [08:37<01:47, 57.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29359/35522 [08:37<01:44, 59.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29365/35522 [08:37<01:47, 57.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29372/35522 [08:37<01:44, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29379/35522 [08:37<01:48, 56.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29386/35522 [08:37<01:44, 58.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29393/35522 [08:37<01:39, 61.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29400/35522 [08:38<01:43, 59.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29407/35522 [08:38<01:42, 59.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29413/35522 [08:38<01:46, 57.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29420/35522 [08:38<01:42, 59.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29426/35522 [08:38<01:44, 58.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29433/35522 [08:38<01:41, 60.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29440/35522 [08:38<01:40, 60.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29447/35522 [08:38<01:38, 61.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29454/35522 [08:38<01:36, 63.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29461/35522 [08:39<01:40, 60.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29468/35522 [08:39<01:37, 61.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29475/35522 [08:39<01:41, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29482/35522 [08:39<01:40, 60.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29489/35522 [08:39<01:44, 57.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29496/35522 [08:39<01:39, 60.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29503/35522 [08:39<01:39, 60.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29510/35522 [08:39<01:37, 61.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29517/35522 [08:39<01:37, 61.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29524/35522 [08:40<01:38, 61.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29531/35522 [08:40<01:43, 58.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29538/35522 [08:40<01:41, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29545/35522 [08:40<01:42, 58.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29552/35522 [08:40<01:39, 60.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29559/35522 [08:40<01:36, 61.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29566/35522 [08:40<01:40, 59.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29573/35522 [08:40<01:38, 60.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29580/35522 [08:41<01:43, 57.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29587/35522 [08:41<01:41, 58.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29593/35522 [08:41<01:42, 57.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29600/35522 [08:41<01:40, 59.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29606/35522 [08:41<01:41, 58.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29613/35522 [08:41<01:36, 60.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29620/35522 [08:41<01:36, 60.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29627/35522 [08:41<01:50, 53.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29633/35522 [08:42<01:50, 53.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29640/35522 [08:42<01:46, 55.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29646/35522 [08:42<01:49, 53.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29652/35522 [08:42<01:47, 54.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 29659/35522 [08:42<01:47, 54.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29666/35522 [08:42<01:39, 58.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29673/35522 [08:42<01:37, 60.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29680/35522 [08:42<01:39, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29687/35522 [08:42<01:37, 59.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29694/35522 [08:43<01:36, 60.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29701/35522 [08:43<01:34, 61.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29708/35522 [08:43<01:34, 61.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29715/35522 [08:43<01:33, 61.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29722/35522 [08:43<01:38, 58.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29729/35522 [08:43<01:34, 61.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29736/35522 [08:43<01:37, 59.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29742/35522 [08:43<01:37, 59.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 29749/35522 [08:43<01:36, 59.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29756/35522 [08:44<01:38, 58.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29763/35522 [08:44<01:44, 55.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29769/35522 [08:44<02:04, 46.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29774/35522 [08:44<02:04, 46.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29781/35522 [08:44<01:52, 50.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29787/35522 [08:44<01:53, 50.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29794/35522 [08:44<01:45, 54.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29801/35522 [08:44<01:46, 53.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29808/35522 [08:45<01:42, 55.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29815/35522 [08:45<01:43, 55.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29822/35522 [08:45<01:40, 56.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29829/35522 [08:45<01:36, 58.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29835/35522 [08:45<01:36, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29842/35522 [08:45<01:32, 61.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29849/35522 [08:45<01:34, 60.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29856/35522 [08:45<01:33, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29863/35522 [08:46<01:37, 57.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29869/35522 [08:46<01:43, 54.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29875/35522 [08:46<01:42, 54.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29882/35522 [08:46<01:39, 56.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29888/35522 [08:46<01:41, 55.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29895/35522 [08:46<01:36, 58.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29902/35522 [08:46<01:32, 60.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29909/35522 [08:46<01:34, 59.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29916/35522 [08:46<01:32, 60.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29923/35522 [08:47<01:46, 52.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29930/35522 [08:47<01:43, 54.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29937/35522 [08:47<01:36, 57.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29944/35522 [08:47<01:33, 59.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29951/35522 [08:47<01:34, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29958/35522 [08:47<01:31, 61.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29965/35522 [08:47<01:35, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29972/35522 [08:47<01:32, 60.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29979/35522 [08:48<01:32, 59.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29986/35522 [08:48<01:32, 60.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 29993/35522 [08:48<01:35, 57.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 30000/35522 [08:48<01:33, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 30006/35522 [08:48<01:35, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 30013/35522 [08:48<01:32, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30020/35522 [08:48<01:37, 56.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30027/35522 [08:48<01:33, 58.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30033/35522 [08:48<01:33, 58.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30039/35522 [08:49<01:37, 56.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30046/35522 [08:49<01:33, 58.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30052/35522 [08:49<01:37, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30058/35522 [08:49<01:38, 55.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30064/35522 [08:49<01:42, 53.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30071/35522 [08:49<01:35, 56.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30077/35522 [08:49<01:37, 56.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30083/35522 [08:49<01:36, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30089/35522 [08:49<01:40, 53.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30096/35522 [08:50<01:36, 56.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30102/35522 [08:50<01:42, 52.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30109/35522 [08:50<01:36, 55.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30116/35522 [08:50<01:33, 57.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30122/35522 [08:50<01:35, 56.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30129/35522 [08:50<01:30, 59.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30135/35522 [08:50<01:42, 52.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30142/35522 [08:50<01:36, 55.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30148/35522 [08:51<01:39, 53.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30155/35522 [08:51<01:34, 56.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30161/35522 [08:51<01:37, 54.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30168/35522 [08:51<01:33, 57.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30174/35522 [08:51<01:37, 54.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30181/35522 [08:51<01:33, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 30187/35522 [08:51<01:50, 48.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30194/35522 [08:51<01:41, 52.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30200/35522 [08:52<01:38, 53.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30207/35522 [08:52<01:34, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30214/35522 [08:52<01:34, 56.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30221/35522 [08:52<01:29, 59.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30227/35522 [08:52<01:29, 59.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30233/35522 [08:52<01:31, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30240/35522 [08:52<01:28, 60.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30247/35522 [08:52<01:29, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30254/35522 [08:52<01:25, 61.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30261/35522 [08:53<01:29, 59.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30268/35522 [08:53<01:26, 60.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30275/35522 [08:53<01:27, 59.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30282/35522 [08:53<01:32, 56.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30288/35522 [08:53<01:34, 55.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30295/35522 [08:53<01:29, 58.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30301/35522 [08:53<01:37, 53.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30308/35522 [08:53<01:32, 56.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30315/35522 [08:53<01:27, 59.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30322/35522 [08:54<01:31, 57.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30329/35522 [08:54<01:27, 59.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30336/35522 [08:54<01:30, 57.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30343/35522 [08:54<01:25, 60.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30350/35522 [08:54<01:27, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30357/35522 [08:54<01:25, 60.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30364/35522 [08:54<01:29, 57.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 30370/35522 [08:54<01:28, 58.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30376/35522 [08:55<01:29, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30383/35522 [08:55<01:25, 60.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30390/35522 [08:55<01:28, 58.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30397/35522 [08:55<01:25, 60.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30404/35522 [08:55<01:21, 62.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30411/35522 [08:55<01:25, 60.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30418/35522 [08:55<01:21, 62.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30425/35522 [08:55<01:25, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30432/35522 [08:55<01:23, 60.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30439/35522 [08:56<01:25, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30446/35522 [08:56<01:23, 60.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30453/35522 [08:56<01:26, 58.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30460/35522 [08:56<01:25, 59.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30466/35522 [08:56<01:44, 48.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30473/35522 [08:56<01:36, 52.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30479/35522 [08:56<01:35, 52.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30486/35522 [08:56<01:31, 55.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30492/35522 [08:57<01:32, 54.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30498/35522 [08:57<01:30, 55.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30504/35522 [08:57<01:30, 55.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30511/35522 [08:57<01:25, 58.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30518/35522 [08:57<01:23, 60.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30525/35522 [08:57<01:27, 57.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30531/35522 [08:57<01:35, 52.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30538/35522 [08:57<01:28, 56.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30545/35522 [08:57<01:26, 57.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30551/35522 [08:58<01:26, 57.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30557/35522 [08:58<01:26, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30563/35522 [08:58<01:29, 55.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30569/35522 [08:58<01:27, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30575/35522 [08:58<01:30, 54.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30582/35522 [08:58<01:25, 57.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30589/35522 [08:58<01:23, 59.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30595/35522 [08:58<01:27, 56.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30602/35522 [08:58<01:22, 59.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30609/35522 [08:59<01:23, 58.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30616/35522 [08:59<01:20, 61.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30623/35522 [08:59<01:32, 53.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30630/35522 [08:59<01:27, 56.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 30636/35522 [08:59<01:28, 55.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30643/35522 [08:59<01:24, 57.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30649/35522 [08:59<01:24, 57.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30656/35522 [08:59<01:22, 59.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30662/35522 [08:59<01:24, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30669/35522 [09:00<01:20, 60.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30676/35522 [09:00<01:19, 60.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30683/35522 [09:00<01:26, 55.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30690/35522 [09:00<01:28, 54.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30697/35522 [09:00<01:24, 57.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30703/35522 [09:00<01:23, 57.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30709/35522 [09:00<01:26, 55.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30716/35522 [09:00<01:23, 57.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▋ | 30722/35522 [09:01<01:22, 57.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30728/35522 [09:01<01:22, 58.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30734/35522 [09:01<01:22, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30740/35522 [09:01<01:21, 58.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30747/35522 [09:01<01:19, 59.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30753/35522 [09:01<01:22, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30760/35522 [09:01<01:20, 59.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30766/35522 [09:01<01:37, 48.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30773/35522 [09:01<01:35, 49.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30780/35522 [09:02<01:28, 53.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30787/35522 [09:02<01:24, 56.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30793/35522 [09:02<01:23, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30800/35522 [09:02<01:21, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30806/35522 [09:02<01:22, 56.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30813/35522 [09:02<01:19, 59.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30819/35522 [09:02<01:21, 58.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30826/35522 [09:02<01:19, 59.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30833/35522 [09:02<01:20, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30840/35522 [09:03<01:18, 59.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30847/35522 [09:03<01:16, 61.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30854/35522 [09:03<01:17, 60.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30861/35522 [09:03<01:16, 60.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30868/35522 [09:03<01:16, 60.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30875/35522 [09:03<01:15, 61.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30882/35522 [09:03<01:18, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30889/35522 [09:03<01:17, 59.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30895/35522 [09:04<01:22, 55.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30902/35522 [09:04<01:19, 57.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30908/35522 [09:04<01:23, 55.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30915/35522 [09:04<01:17, 59.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30921/35522 [09:04<01:21, 56.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30928/35522 [09:04<01:17, 59.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30935/35522 [09:04<01:14, 61.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30942/35522 [09:04<01:15, 60.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30949/35522 [09:04<01:14, 61.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30956/35522 [09:05<01:34, 48.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30962/35522 [09:05<01:29, 50.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30969/35522 [09:05<01:22, 55.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30975/35522 [09:05<01:27, 51.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30982/35522 [09:05<01:21, 55.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30989/35522 [09:05<01:17, 58.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 30996/35522 [09:05<01:21, 55.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31002/35522 [09:05<01:20, 56.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31008/35522 [09:06<01:20, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31015/35522 [09:06<01:17, 58.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31021/35522 [09:06<01:20, 55.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31027/35522 [09:06<01:19, 56.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31033/35522 [09:06<01:21, 55.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31039/35522 [09:06<01:19, 56.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31046/35522 [09:06<01:16, 58.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31052/35522 [09:06<01:20, 55.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31059/35522 [09:06<01:16, 58.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31065/35522 [09:07<01:28, 50.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31071/35522 [09:07<01:29, 49.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 31077/35522 [09:07<01:26, 51.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31084/35522 [09:07<01:21, 54.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31090/35522 [09:07<01:20, 55.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31097/35522 [09:07<01:17, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31103/35522 [09:07<01:20, 55.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31109/35522 [09:07<01:18, 56.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31116/35522 [09:08<01:16, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31122/35522 [09:08<01:15, 58.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31129/35522 [09:08<01:12, 60.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31136/35522 [09:08<01:15, 58.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31143/35522 [09:08<01:11, 61.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31150/35522 [09:08<01:13, 59.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31157/35522 [09:08<01:12, 60.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31164/35522 [09:08<01:15, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31171/35522 [09:08<01:13, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31178/35522 [09:09<01:13, 59.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31185/35522 [09:09<01:11, 60.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31192/35522 [09:09<01:15, 57.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31199/35522 [09:09<01:13, 59.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31205/35522 [09:09<01:14, 58.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31212/35522 [09:09<01:12, 59.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31219/35522 [09:09<01:11, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31226/35522 [09:09<01:13, 58.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31233/35522 [09:09<01:12, 58.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31239/35522 [09:10<01:26, 49.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31245/35522 [09:10<01:24, 50.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31252/35522 [09:10<01:18, 54.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31259/35522 [09:10<01:14, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31265/35522 [09:10<01:16, 55.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31272/35522 [09:10<01:12, 58.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31278/35522 [09:10<01:14, 56.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31285/35522 [09:10<01:11, 59.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31291/35522 [09:11<01:13, 57.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31298/35522 [09:11<01:10, 59.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31305/35522 [09:11<01:11, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31311/35522 [09:11<01:14, 56.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31317/35522 [09:11<01:21, 51.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31323/35522 [09:11<01:20, 52.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31330/35522 [09:11<01:19, 52.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31336/35522 [09:11<01:16, 54.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31343/35522 [09:11<01:13, 57.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31349/35522 [09:12<01:15, 55.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31356/35522 [09:12<01:11, 58.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31362/35522 [09:12<01:11, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31368/35522 [09:12<01:12, 57.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31374/35522 [09:12<01:15, 55.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31381/35522 [09:12<01:10, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31388/35522 [09:12<01:12, 57.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31395/35522 [09:12<01:10, 58.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31402/35522 [09:13<01:13, 56.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31408/35522 [09:13<01:19, 51.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31414/35522 [09:13<01:18, 52.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31421/35522 [09:13<01:14, 55.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31428/35522 [09:13<01:10, 57.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 31434/35522 [09:13<01:12, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31441/35522 [09:13<01:09, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31447/35522 [09:13<01:13, 55.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31454/35522 [09:13<01:11, 57.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31460/35522 [09:14<01:13, 55.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31467/35522 [09:14<01:10, 57.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31473/35522 [09:14<01:10, 57.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31480/35522 [09:14<01:07, 60.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31487/35522 [09:14<01:09, 58.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31495/35522 [09:14<01:05, 61.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31502/35522 [09:14<01:09, 58.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31509/35522 [09:14<01:06, 60.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31516/35522 [09:14<01:03, 62.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▊ | 31523/35522 [09:15<01:07, 59.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31530/35522 [09:15<01:05, 60.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31537/35522 [09:15<01:05, 60.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31544/35522 [09:15<01:04, 61.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31551/35522 [09:15<01:06, 59.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31558/35522 [09:15<01:04, 61.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31565/35522 [09:15<01:06, 59.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31572/35522 [09:15<01:05, 60.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31579/35522 [09:16<01:06, 58.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31586/35522 [09:16<01:05, 59.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31593/35522 [09:16<01:06, 59.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31600/35522 [09:16<01:05, 59.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31607/35522 [09:16<01:04, 60.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31614/35522 [09:16<01:07, 57.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31621/35522 [09:16<01:05, 59.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31627/35522 [09:16<01:07, 57.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31634/35522 [09:16<01:05, 58.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31640/35522 [09:17<01:07, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31647/35522 [09:17<01:05, 59.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31653/35522 [09:17<01:19, 48.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31660/35522 [09:17<01:13, 52.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31666/35522 [09:17<01:13, 52.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31673/35522 [09:17<01:10, 54.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31679/35522 [09:17<01:13, 52.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31685/35522 [09:17<01:11, 53.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31691/35522 [09:18<01:12, 53.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31697/35522 [09:18<01:10, 54.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31703/35522 [09:18<01:09, 54.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31710/35522 [09:18<01:04, 59.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31716/35522 [09:18<01:04, 59.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31722/35522 [09:18<01:05, 58.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31729/35522 [09:18<01:01, 61.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31736/35522 [09:18<01:03, 59.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31743/35522 [09:18<01:01, 61.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31750/35522 [09:19<01:03, 59.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31757/35522 [09:19<01:02, 60.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31764/35522 [09:19<01:04, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31771/35522 [09:19<01:02, 60.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31778/35522 [09:19<01:00, 61.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31785/35522 [09:19<01:01, 60.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 31792/35522 [09:19<01:03, 58.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31798/35522 [09:19<01:04, 57.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31805/35522 [09:19<01:02, 59.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31811/35522 [09:20<01:15, 49.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31818/35522 [09:20<01:08, 53.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31824/35522 [09:20<01:09, 53.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31830/35522 [09:20<01:07, 54.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31836/35522 [09:20<01:11, 51.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31843/35522 [09:20<01:06, 55.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31849/35522 [09:20<01:06, 55.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31856/35522 [09:20<01:02, 58.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31862/35522 [09:21<01:04, 56.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31868/35522 [09:21<01:03, 57.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31874/35522 [09:21<01:05, 56.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31880/35522 [09:21<01:08, 53.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31887/35522 [09:21<01:03, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31893/35522 [09:21<01:07, 54.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31900/35522 [09:21<01:02, 57.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31906/35522 [09:21<01:04, 56.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31913/35522 [09:21<01:01, 58.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31919/35522 [09:22<01:04, 55.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31926/35522 [09:22<01:00, 59.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31933/35522 [09:22<01:02, 57.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31940/35522 [09:22<01:00, 59.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31947/35522 [09:22<00:58, 61.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31954/35522 [09:22<00:59, 60.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31961/35522 [09:22<01:05, 54.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 31968/35522 [09:22<01:02, 57.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 31975/35522 [09:23<01:04, 54.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 31982/35522 [09:23<01:02, 56.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 31989/35522 [09:23<00:59, 59.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 31995/35522 [09:23<01:02, 56.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32002/35522 [09:23<00:59, 59.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32008/35522 [09:23<01:00, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32015/35522 [09:23<00:58, 59.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32022/35522 [09:23<01:00, 57.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32028/35522 [09:23<01:00, 57.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32034/35522 [09:24<01:03, 54.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32041/35522 [09:24<01:00, 57.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32048/35522 [09:24<01:01, 56.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32055/35522 [09:24<00:58, 59.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32062/35522 [09:24<00:58, 59.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32068/35522 [09:24<00:59, 58.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32075/35522 [09:24<00:57, 60.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32082/35522 [09:24<01:10, 48.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32088/35522 [09:25<01:10, 48.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32095/35522 [09:25<01:04, 52.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32101/35522 [09:25<01:08, 49.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32108/35522 [09:25<01:05, 52.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32114/35522 [09:25<01:05, 51.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32121/35522 [09:25<01:01, 55.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32128/35522 [09:25<00:59, 57.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32134/35522 [09:25<00:59, 57.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32141/35522 [09:25<00:56, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 32147/35522 [09:26<00:59, 56.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32154/35522 [09:26<00:56, 59.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32160/35522 [09:26<00:58, 57.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32167/35522 [09:26<00:56, 59.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32173/35522 [09:26<00:59, 56.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32180/35522 [09:26<00:57, 57.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32187/35522 [09:26<00:55, 60.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32194/35522 [09:26<00:56, 58.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32201/35522 [09:26<00:55, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32208/35522 [09:27<00:55, 60.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32215/35522 [09:27<00:53, 61.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32222/35522 [09:27<00:55, 59.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32229/35522 [09:27<00:54, 60.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32236/35522 [09:27<00:55, 58.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32243/35522 [09:27<00:55, 58.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32249/35522 [09:27<00:56, 57.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32256/35522 [09:27<00:54, 60.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32263/35522 [09:28<00:57, 56.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32270/35522 [09:28<00:54, 59.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32277/35522 [09:28<00:52, 61.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32284/35522 [09:28<01:01, 52.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32290/35522 [09:28<01:01, 52.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32297/35522 [09:28<00:56, 56.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32304/35522 [09:28<00:53, 59.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32311/35522 [09:28<00:55, 58.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32318/35522 [09:28<00:54, 58.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32324/35522 [09:29<00:57, 55.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32331/35522 [09:29<00:54, 58.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32337/35522 [09:29<00:57, 55.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32344/35522 [09:29<00:54, 57.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32350/35522 [09:29<00:55, 57.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32357/35522 [09:29<00:53, 59.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32363/35522 [09:29<00:54, 57.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32370/35522 [09:29<00:52, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32377/35522 [09:30<00:55, 56.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32383/35522 [09:30<01:00, 52.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32389/35522 [09:30<00:59, 52.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32396/35522 [09:30<00:55, 56.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32403/35522 [09:30<00:53, 58.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 32409/35522 [09:30<00:54, 56.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32416/35522 [09:30<00:52, 59.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32422/35522 [09:30<00:53, 58.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32429/35522 [09:30<00:51, 59.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32435/35522 [09:31<00:54, 56.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32442/35522 [09:31<00:52, 58.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32448/35522 [09:31<00:54, 56.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32454/35522 [09:31<00:54, 56.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32461/35522 [09:31<00:51, 59.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32467/35522 [09:31<00:52, 57.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32474/35522 [09:31<00:51, 59.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32480/35522 [09:31<00:52, 57.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32487/35522 [09:31<00:50, 59.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32493/35522 [09:32<00:51, 59.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 32500/35522 [09:32<00:49, 60.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32507/35522 [09:32<00:48, 62.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32514/35522 [09:32<00:49, 60.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32521/35522 [09:32<00:55, 54.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32528/35522 [09:32<00:52, 57.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32535/35522 [09:32<00:50, 58.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32541/35522 [09:32<00:52, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32547/35522 [09:32<00:52, 56.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32553/35522 [09:33<00:52, 56.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32560/35522 [09:33<00:51, 57.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32566/35522 [09:33<00:51, 57.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32573/35522 [09:33<00:49, 59.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32580/35522 [09:33<00:49, 58.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32587/35522 [09:33<00:47, 61.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32594/35522 [09:33<00:51, 56.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32601/35522 [09:33<00:50, 58.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32608/35522 [09:33<00:48, 60.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32615/35522 [09:34<00:49, 58.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32622/35522 [09:34<00:48, 59.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32629/35522 [09:34<00:50, 57.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32636/35522 [09:34<00:49, 58.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32642/35522 [09:34<00:50, 57.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32649/35522 [09:34<00:49, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32655/35522 [09:34<00:49, 57.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32662/35522 [09:34<00:47, 59.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32668/35522 [09:35<00:50, 56.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32675/35522 [09:35<00:49, 58.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32682/35522 [09:35<00:51, 55.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32689/35522 [09:35<00:49, 57.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32695/35522 [09:35<00:51, 55.18it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32702/35522 [09:35<00:48, 58.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32709/35522 [09:35<00:47, 59.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32716/35522 [09:35<00:54, 51.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32722/35522 [09:36<00:53, 52.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32728/35522 [09:36<00:52, 53.33it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32735/35522 [09:36<00:52, 52.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32742/35522 [09:36<00:49, 56.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32748/35522 [09:36<00:48, 56.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32754/35522 [09:36<00:49, 55.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32761/35522 [09:36<00:46, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32767/35522 [09:36<00:48, 56.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32773/35522 [09:36<00:49, 55.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32779/35522 [09:37<00:50, 54.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32786/35522 [09:37<00:48, 56.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32793/35522 [09:37<00:48, 56.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32800/35522 [09:37<00:46, 58.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32806/35522 [09:37<00:46, 58.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32812/35522 [09:37<00:48, 56.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32819/35522 [09:37<00:46, 58.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32825/35522 [09:37<00:49, 54.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32832/35522 [09:37<00:48, 55.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32838/35522 [09:38<00:47, 55.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32845/35522 [09:38<00:44, 59.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 32852/35522 [09:38<00:45, 58.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32859/35522 [09:38<00:43, 61.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32866/35522 [09:38<00:45, 58.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32873/35522 [09:38<00:44, 59.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32880/35522 [09:38<00:43, 60.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32887/35522 [09:38<00:45, 58.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32893/35522 [09:38<00:45, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32899/35522 [09:39<00:45, 57.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32905/35522 [09:39<00:45, 57.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32911/35522 [09:39<00:46, 56.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32918/35522 [09:39<00:44, 57.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32924/35522 [09:39<00:46, 56.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32931/35522 [09:39<00:44, 57.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32938/35522 [09:39<00:47, 54.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32944/35522 [09:39<00:48, 52.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32950/35522 [09:40<00:49, 52.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32957/35522 [09:40<00:45, 55.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32964/35522 [09:40<00:46, 54.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32971/35522 [09:40<00:43, 58.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32978/35522 [09:40<00:41, 61.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32985/35522 [09:40<00:49, 51.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32991/35522 [09:40<00:52, 47.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 32998/35522 [09:40<00:48, 51.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33004/35522 [09:41<00:48, 51.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33011/35522 [09:41<00:44, 56.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33018/35522 [09:41<00:45, 55.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33025/35522 [09:41<00:42, 58.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33032/35522 [09:41<00:41, 59.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33039/35522 [09:41<00:42, 58.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33046/35522 [09:41<00:41, 59.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33052/35522 [09:41<00:42, 57.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33059/35522 [09:41<00:41, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33065/35522 [09:42<00:43, 56.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33072/35522 [09:42<00:41, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33078/35522 [09:42<00:42, 56.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33085/35522 [09:42<00:41, 58.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33092/35522 [09:42<00:41, 58.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33099/35522 [09:42<00:40, 60.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33106/35522 [09:42<00:39, 61.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33113/35522 [09:42<00:40, 59.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33120/35522 [09:43<00:44, 54.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33126/35522 [09:43<00:49, 48.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33132/35522 [09:43<00:48, 49.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33139/35522 [09:43<00:44, 53.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33145/35522 [09:43<00:43, 54.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33151/35522 [09:43<00:45, 52.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33158/35522 [09:43<00:42, 56.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33164/35522 [09:43<00:42, 55.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33171/35522 [09:43<00:40, 57.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33177/35522 [09:44<00:42, 55.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33183/35522 [09:44<00:41, 56.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33189/35522 [09:44<00:42, 55.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33196/35522 [09:44<00:40, 56.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33202/35522 [09:44<00:42, 55.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 33208/35522 [09:44<00:41, 56.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33215/35522 [09:44<00:39, 58.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33221/35522 [09:44<00:46, 49.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33228/35522 [09:45<00:44, 51.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33235/35522 [09:45<00:42, 53.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33242/35522 [09:45<00:42, 53.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33248/35522 [09:45<00:41, 54.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33255/35522 [09:45<00:39, 57.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33261/35522 [09:45<00:40, 56.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33268/35522 [09:45<00:38, 58.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33274/35522 [09:45<00:38, 58.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33281/35522 [09:45<00:37, 60.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33288/35522 [09:46<00:37, 59.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33294/35522 [09:46<00:37, 59.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▎| 33301/35522 [09:46<00:35, 62.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33308/35522 [09:46<00:37, 59.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33315/35522 [09:46<00:36, 61.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33322/35522 [09:46<00:37, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33329/35522 [09:46<00:35, 61.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33336/35522 [09:46<00:38, 56.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33342/35522 [09:46<00:38, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33348/35522 [09:47<00:40, 53.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33355/35522 [09:47<00:38, 56.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33361/35522 [09:47<00:40, 52.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33368/35522 [09:47<00:38, 55.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33374/35522 [09:47<00:39, 54.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33381/35522 [09:47<00:37, 57.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33387/35522 [09:47<00:38, 55.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33394/35522 [09:47<00:37, 57.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33401/35522 [09:48<00:37, 56.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33407/35522 [09:48<00:37, 56.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33414/35522 [09:48<00:35, 58.66it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33420/35522 [09:48<00:37, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33427/35522 [09:48<00:35, 58.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33433/35522 [09:48<00:40, 51.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33440/35522 [09:48<00:38, 54.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33446/35522 [09:48<00:37, 55.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33453/35522 [09:48<00:35, 58.27it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33459/35522 [09:49<00:36, 55.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33466/35522 [09:49<00:35, 57.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33472/35522 [09:49<00:35, 57.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33479/35522 [09:49<00:34, 59.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33486/35522 [09:49<00:33, 61.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33493/35522 [09:49<00:34, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33500/35522 [09:49<00:33, 60.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33507/35522 [09:49<00:34, 59.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33514/35522 [09:49<00:32, 61.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33521/35522 [09:50<00:34, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33528/35522 [09:50<00:32, 61.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33535/35522 [09:50<00:33, 59.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33542/35522 [09:50<00:32, 60.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33549/35522 [09:50<00:32, 61.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33556/35522 [09:50<00:32, 61.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 33563/35522 [09:50<00:32, 59.64it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33569/35522 [09:50<00:34, 56.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33575/35522 [09:51<00:37, 52.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33581/35522 [09:51<00:37, 51.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33588/35522 [09:51<00:36, 53.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33595/35522 [09:51<00:34, 55.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33602/35522 [09:51<00:33, 58.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33608/35522 [09:51<00:34, 55.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33616/35522 [09:51<00:31, 60.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33623/35522 [09:51<00:32, 58.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33629/35522 [09:51<00:32, 58.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33635/35522 [09:52<00:33, 57.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33642/35522 [09:52<00:32, 58.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33648/35522 [09:52<00:32, 57.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33655/35522 [09:52<00:31, 59.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33661/35522 [09:52<00:34, 53.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33667/35522 [09:52<00:34, 53.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33674/35522 [09:52<00:34, 53.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33681/35522 [09:52<00:32, 56.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33687/35522 [09:53<00:35, 51.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33693/35522 [09:53<00:35, 51.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33700/35522 [09:53<00:34, 53.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33706/35522 [09:53<00:37, 47.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33712/35522 [09:53<00:36, 49.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33718/35522 [09:53<00:34, 52.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33725/35522 [09:53<00:32, 55.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33731/35522 [09:53<00:31, 56.48it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33737/35522 [09:53<00:31, 57.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 33743/35522 [09:54<00:33, 53.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33749/35522 [09:54<00:33, 52.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33755/35522 [09:54<00:33, 53.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33761/35522 [09:54<00:32, 54.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33767/35522 [09:54<00:31, 55.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33774/35522 [09:54<00:30, 58.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33781/35522 [09:54<00:29, 59.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33787/35522 [09:54<00:30, 57.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33794/35522 [09:54<00:29, 58.65it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33800/35522 [09:55<00:35, 49.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33806/35522 [09:55<00:36, 47.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33813/35522 [09:55<00:33, 51.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33820/35522 [09:55<00:30, 55.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33827/35522 [09:55<00:29, 57.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33834/35522 [09:55<00:28, 59.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33841/35522 [09:55<00:28, 59.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33848/35522 [09:55<00:29, 57.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33854/35522 [09:56<00:29, 57.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33861/35522 [09:56<00:27, 59.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33868/35522 [09:56<00:28, 57.58it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33875/35522 [09:56<00:27, 59.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33882/35522 [09:56<00:33, 49.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33889/35522 [09:56<00:30, 52.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33895/35522 [09:56<00:30, 52.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33902/35522 [09:56<00:29, 55.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33908/35522 [09:57<00:30, 53.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33915/35522 [09:57<00:28, 55.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 33921/35522 [09:57<00:29, 54.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33928/35522 [09:57<00:27, 57.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33934/35522 [09:57<00:28, 56.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33941/35522 [09:57<00:26, 58.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33948/35522 [09:57<00:26, 59.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33955/35522 [09:57<00:27, 57.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33962/35522 [09:58<00:25, 60.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33969/35522 [09:58<00:26, 58.38it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33976/35522 [09:58<00:26, 58.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33982/35522 [09:58<00:26, 57.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33990/35522 [09:58<00:24, 61.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 33997/35522 [09:58<00:26, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34004/35522 [09:58<00:25, 60.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34011/35522 [09:58<00:25, 59.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34018/35522 [09:58<00:24, 60.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34025/35522 [09:59<00:25, 58.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34032/35522 [09:59<00:24, 61.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34039/35522 [09:59<00:24, 59.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34046/35522 [09:59<00:24, 61.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34053/35522 [09:59<00:23, 61.73it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34060/35522 [09:59<00:24, 59.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34067/35522 [09:59<00:23, 61.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34074/35522 [09:59<00:24, 58.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34080/35522 [09:59<00:24, 58.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34086/35522 [10:00<00:24, 58.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34093/35522 [10:00<00:24, 59.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34099/35522 [10:00<00:27, 51.15it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34106/35522 [10:00<00:25, 55.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34112/35522 [10:00<00:26, 53.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34119/35522 [10:00<00:25, 55.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34125/35522 [10:00<00:25, 54.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34132/35522 [10:00<00:24, 56.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34138/35522 [10:01<00:25, 54.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34145/35522 [10:01<00:24, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34152/35522 [10:01<00:22, 59.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34159/35522 [10:01<00:23, 58.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34166/35522 [10:01<00:22, 60.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34173/35522 [10:01<00:23, 58.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34180/35522 [10:01<00:22, 59.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 34187/35522 [10:01<00:23, 57.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34194/35522 [10:01<00:22, 59.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34200/35522 [10:02<00:23, 55.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34207/35522 [10:02<00:22, 58.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34213/35522 [10:02<00:23, 55.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34220/35522 [10:02<00:22, 58.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34226/35522 [10:02<00:22, 57.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34233/35522 [10:02<00:21, 60.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34240/35522 [10:02<00:20, 62.55it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34247/35522 [10:02<00:21, 59.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34254/35522 [10:02<00:20, 61.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34261/35522 [10:03<00:21, 59.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34268/35522 [10:03<00:20, 61.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 34275/35522 [10:03<00:20, 60.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34282/35522 [10:03<00:20, 61.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34289/35522 [10:03<00:20, 59.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34296/35522 [10:03<00:20, 60.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34303/35522 [10:03<00:21, 57.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34310/35522 [10:03<00:20, 59.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34316/35522 [10:04<00:20, 57.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34322/35522 [10:04<00:20, 57.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34329/35522 [10:04<00:20, 59.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34335/35522 [10:04<00:20, 57.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34342/35522 [10:04<00:19, 59.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34348/35522 [10:04<00:20, 56.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34355/35522 [10:04<00:19, 58.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34361/35522 [10:04<00:20, 57.24it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34368/35522 [10:04<00:19, 59.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34375/35522 [10:05<00:19, 58.08it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34382/35522 [10:05<00:18, 61.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34389/35522 [10:05<00:18, 62.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34396/35522 [10:05<00:18, 59.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34403/35522 [10:05<00:18, 60.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34410/35522 [10:05<00:18, 59.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34417/35522 [10:05<00:18, 60.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34424/35522 [10:05<00:18, 60.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34431/35522 [10:05<00:17, 62.53it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34438/35522 [10:06<00:18, 58.50it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34445/35522 [10:06<00:17, 60.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34452/35522 [10:06<00:21, 50.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34458/35522 [10:06<00:20, 51.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34464/35522 [10:06<00:20, 51.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34471/35522 [10:06<00:19, 54.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34477/35522 [10:06<00:19, 54.32it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34484/35522 [10:06<00:18, 57.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34490/35522 [10:07<00:18, 56.75it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34497/35522 [10:07<00:17, 59.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34504/35522 [10:07<00:17, 57.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34511/35522 [10:07<00:17, 59.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34518/35522 [10:07<00:16, 60.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34525/35522 [10:07<00:17, 56.14it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34532/35522 [10:07<00:16, 58.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34538/35522 [10:07<00:17, 55.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34545/35522 [10:08<00:16, 57.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34551/35522 [10:08<00:17, 56.43it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34558/35522 [10:08<00:16, 57.92it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34564/35522 [10:08<00:16, 56.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34571/35522 [10:08<00:16, 58.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34577/35522 [10:08<00:16, 55.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34584/35522 [10:08<00:16, 58.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34590/35522 [10:08<00:16, 56.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34597/35522 [10:08<00:15, 58.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34604/35522 [10:09<00:15, 59.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34610/35522 [10:09<00:16, 54.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34617/35522 [10:09<00:15, 56.80it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34623/35522 [10:09<00:19, 46.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 34628/35522 [10:09<00:18, 47.34it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34634/35522 [10:09<00:17, 50.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34641/35522 [10:09<00:15, 55.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34647/35522 [10:09<00:16, 53.88it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34654/35522 [10:09<00:15, 57.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34660/35522 [10:10<00:15, 56.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34667/35522 [10:10<00:14, 58.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34674/35522 [10:10<00:14, 59.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34681/35522 [10:10<00:13, 61.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34688/35522 [10:10<00:14, 58.26it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34695/35522 [10:10<00:13, 59.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34702/35522 [10:10<00:13, 61.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34709/35522 [10:10<00:13, 60.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34716/35522 [10:11<00:13, 61.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34723/35522 [10:11<00:14, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34730/35522 [10:11<00:13, 59.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34737/35522 [10:11<00:15, 51.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34744/35522 [10:11<00:14, 52.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34751/35522 [10:11<00:13, 56.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34758/35522 [10:11<00:13, 54.59it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34765/35522 [10:11<00:13, 57.36it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34772/35522 [10:12<00:12, 59.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34779/35522 [10:12<00:12, 58.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34787/35522 [10:12<00:11, 61.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34794/35522 [10:12<00:11, 61.99it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34801/35522 [10:12<00:11, 62.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34808/35522 [10:12<00:12, 59.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34815/35522 [10:12<00:11, 60.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34822/35522 [10:12<00:11, 61.93it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34829/35522 [10:12<00:10, 63.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34836/35522 [10:13<00:11, 59.17it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34843/35522 [10:13<00:11, 60.86it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34850/35522 [10:13<00:11, 57.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34857/35522 [10:13<00:11, 60.09it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34864/35522 [10:13<00:11, 59.52it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34871/35522 [10:13<00:11, 58.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34878/35522 [10:13<00:10, 60.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34885/35522 [10:13<00:11, 57.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34892/35522 [10:14<00:10, 58.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34898/35522 [10:14<00:10, 56.82it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34905/35522 [10:14<00:10, 58.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34911/35522 [10:14<00:10, 56.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34918/35522 [10:14<00:10, 58.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34924/35522 [10:14<00:10, 55.10it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34931/35522 [10:14<00:10, 57.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34937/35522 [10:14<00:10, 56.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34944/35522 [10:14<00:09, 58.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34951/35522 [10:15<00:09, 59.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34957/35522 [10:15<00:09, 58.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34963/35522 [10:15<00:09, 56.20it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34969/35522 [10:15<00:10, 54.01it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34976/35522 [10:15<00:09, 57.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34982/35522 [10:15<00:11, 47.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  98%|█████████▊| 34988/35522 [10:15<00:10, 50.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 34994/35522 [10:15<00:10, 52.21it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35000/35522 [10:15<00:09, 53.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35006/35522 [10:16<00:09, 52.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35013/35522 [10:16<00:09, 56.12it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35019/35522 [10:16<00:09, 53.57it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35025/35522 [10:16<00:09, 54.97it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35031/35522 [10:16<00:09, 53.00it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35038/35522 [10:16<00:08, 56.16it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35045/35522 [10:16<00:08, 57.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35051/35522 [10:16<00:08, 56.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35058/35522 [10:17<00:07, 59.83it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35065/35522 [10:17<00:08, 56.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▊| 35072/35522 [10:17<00:07, 58.90it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35078/35522 [10:17<00:07, 56.29it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35086/35522 [10:17<00:07, 60.35it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35093/35522 [10:17<00:07, 59.61it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35100/35522 [10:17<00:06, 62.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35107/35522 [10:17<00:07, 59.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35114/35522 [10:17<00:06, 60.22it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35121/35522 [10:18<00:07, 56.41it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35128/35522 [10:18<00:06, 58.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35135/35522 [10:18<00:06, 57.87it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35142/35522 [10:18<00:06, 60.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35149/35522 [10:18<00:06, 62.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35156/35522 [10:18<00:06, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35163/35522 [10:18<00:05, 62.13it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35170/35522 [10:18<00:05, 59.70it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35177/35522 [10:18<00:05, 61.60it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35184/35522 [10:19<00:05, 59.25it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35191/35522 [10:19<00:05, 60.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35198/35522 [10:19<00:05, 56.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35205/35522 [10:19<00:05, 57.71it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35211/35522 [10:19<00:05, 54.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35218/35522 [10:19<00:05, 57.72it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35224/35522 [10:19<00:05, 56.44it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35230/35522 [10:19<00:05, 56.91it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35237/35522 [10:20<00:04, 59.46it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35243/35522 [10:20<00:04, 59.23it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35250/35522 [10:20<00:04, 60.37it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35257/35522 [10:20<00:04, 59.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35264/35522 [10:20<00:04, 59.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35270/35522 [10:20<00:04, 59.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35277/35522 [10:20<00:04, 59.94it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35283/35522 [10:20<00:04, 58.68it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35289/35522 [10:20<00:04, 51.51it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35295/35522 [10:21<00:04, 52.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35302/35522 [10:21<00:03, 55.28it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35308/35522 [10:21<00:04, 52.76it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35315/35522 [10:21<00:03, 55.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35322/35522 [10:21<00:03, 58.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35328/35522 [10:21<00:03, 56.69it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35334/35522 [10:21<00:03, 57.49it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  99%|█████████▉| 35340/35522 [10:21<00:03, 54.98it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35347/35522 [10:21<00:03, 56.96it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35353/35522 [10:22<00:03, 55.79it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35359/35522 [10:22<00:02, 56.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35366/35522 [10:22<00:02, 57.02it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35373/35522 [10:22<00:02, 59.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35380/35522 [10:22<00:02, 60.67it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35387/35522 [10:22<00:02, 56.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35394/35522 [10:22<00:02, 60.04it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35401/35522 [10:22<00:02, 57.03it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35408/35522 [10:23<00:01, 58.63it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35414/35522 [10:23<00:01, 57.74it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35420/35522 [10:23<00:01, 57.78it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35426/35522 [10:23<00:02, 46.07it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35433/35522 [10:23<00:01, 48.54it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35440/35522 [10:23<00:01, 53.30it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35447/35522 [10:23<00:01, 57.06it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35453/35522 [10:23<00:01, 52.77it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35460/35522 [10:24<00:01, 55.05it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35466/35522 [10:24<00:01, 54.85it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35473/35522 [10:24<00:00, 56.89it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35479/35522 [10:24<00:00, 57.56it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35486/35522 [10:24<00:00, 58.62it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35492/35522 [10:24<00:00, 56.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35499/35522 [10:24<00:00, 58.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35505/35522 [10:24<00:00, 58.84it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|█████████▉| 35511/35522 [10:24<00:00, 56.11it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 35522/35522 [10:25<00:00, 56.83it/s]\n",
            "09/03/2021 14:21:59 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 14:21:59 - INFO - __main__ -     perplexity = tensor(21630.3047)\n",
            "Iteration:   0%|          | 42/142027 [31:30<1775:42:50, 45.02s/it]\n",
            "Epoch:   0%|          | 0/1 [31:30<?, ?it/s]\n",
            "09/03/2021 14:21:59 - INFO - __main__ -    global_step = 26, average loss = 2.2687561145195594\n",
            "09/03/2021 14:21:59 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 14:22:02 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 14:22:04 - INFO - __main__ -   Creating features from dataset file at /content/data/review/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 14:22:10 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 14:22:10 - INFO - __main__ -     Num examples = 142086\n",
            "09/03/2021 14:22:10 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35522/35522 [10:26<00:00, 56.66it/s]\n",
            "09/03/2021 14:32:37 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 14:32:37 - INFO - __main__ -     perplexity = tensor(21515.1152)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHUm-8KColGc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmUXfpyUok7z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkVzMJU0IOEG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h58PZJntm3cq"
      },
      "source": [
        "!rm -r /content/models/roberta/output_base\n",
        "!rm -r /content/runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUm0yl_RDIPi"
      },
      "source": [
        "!split -n 10 /content/data/train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8LUOXn0mI4j"
      },
      "source": [
        "!mv /content/xaa /content/data/train00.txt\n",
        "!mv /content/xab /content/data/train01.txt\n",
        "!mv /content/xac /content/data/train02.txt\n",
        "!mv /content/xad /content/data/train03.txt\n",
        "!mv /content/xae /content/data/train04.txt\n",
        "!mv /content/xaf /content/data/train05.txt\n",
        "!mv /content/xag /content/data/train06.txt\n",
        "!mv /content/xah /content/data/train07.txt\n",
        "!mv /content/xai /content/data/train08.txt\n",
        "!mv /content/xaj /content/data/train09.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgwATN7SDmAC"
      },
      "source": [
        "!split -n 10 /content/data/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX2nvfMimqsp"
      },
      "source": [
        "!mv /content/xaa /content/data/dev00.txt\n",
        "!mv /content/xab /content/data/dev01.txt\n",
        "!mv /content/xac /content/data/dev02.txt\n",
        "!mv /content/xad /content/data/dev03.txt\n",
        "!mv /content/xae /content/data/dev04.txt\n",
        "!mv /content/xaf /content/data/dev05.txt\n",
        "!mv /content/xag /content/data/dev06.txt\n",
        "!mv /content/xah /content/data/dev07.txt\n",
        "!mv /content/xai /content/data/dev08.txt\n",
        "!mv /content/xaj /content/data/dev09.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NptUZaoSv3P",
        "outputId": "280bb54f-554a-4b4e-b40b-9488e8b2dd77"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 85.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.1 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 87.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySZk6XRPjnDP",
        "outputId": "173daa7a-5d32-4317-9069-9c9355f0af6c"
      },
      "source": [
        "%%writefile run_lm.py\n",
        "\n",
        "### THIS FILE IS COPIED FROM THE HUGGINGFACE REPOSITORY FOR CONVENIENCE.\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "\n",
        "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size) + \"_\" + filename\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "            for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"wb\") as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)\n",
        "\n",
        "\n",
        "class LineByLineTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "        # Here, we do not cache the features, operating under the assumption\n",
        "        # that we will soon use fast multithreaded tokenizers from the\n",
        "        # `tokenizers` repo everywhere =)\n",
        "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return LineByLineTextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
        "    else:\n",
        "        return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
        "    ordering_and_checkpoint_path = []\n",
        "\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
        "\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    return checkpoints_sorted\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def mask_tokens(inputs: torch.Tensor, tokenizer: PreTrainedTokenizer, args) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "\n",
        "    if tokenizer.mask_token is None:\n",
        "        raise ValueError(\n",
        "            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "        )\n",
        "\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
        "    special_tokens_mask = [\n",
        "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "    ]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
        "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate\n",
        "    )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if (\n",
        "        args.model_name_or_path\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for epoch in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "        if args.local_rank != -1:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                    os.makedirs(output_dir, exist_ok=True)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prefix=\"\") -> Dict:\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir, exist_ok=True)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate\n",
        "    )\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", position=0, leave=True):\n",
        "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\"perplexity\": perplexity}\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--train_data_file\", default=None, type=str, required=True, help=\"The input training data file (a text file).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_type\", type=str, required=True, help=\"The model architecture to be trained or fine-tuned.\",\n",
        "    )\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--eval_data_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--line_by_line\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--should_continue\", action=\"store_true\", help=\"Whether to continue from latest checkpoint in output_dir\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--mlm\", action=\"store_true\", help=\"Train with masked-language modeling loss instead of language modeling.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mlm_probability\", type=float, default=0.15, help=\"Ratio of tokens to mask for masked language modeling loss\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--config_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional directory to store the pre-trained models downloaded from s3 (instead of the default one)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--block_size\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"Optional input sequence length after tokenization.\"\n",
        "        \"The training dataset will be truncated in block of this size for training.\"\n",
        "        \"Default to the model max input length for single sentence inputs (take into account special tokens).\",\n",
        "    )\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=1.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--save_total_limit\",\n",
        "        type=int,\n",
        "        default=None,\n",
        "        help=\"Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not args.mlm:\n",
        "        raise ValueError(\n",
        "            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
        "            \"flag (masked language modeling).\"\n",
        "        )\n",
        "    if args.eval_data_file is None and args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "    if args.should_continue:\n",
        "        sorted_checkpoints = _sorted_checkpoints(args)\n",
        "        if len(sorted_checkpoints) == 0:\n",
        "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
        "        else:\n",
        "            args.model_name_or_path = sorted_checkpoints[-1]\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "        and not args.should_continue\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    if args.config_name:\n",
        "        config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    elif args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
        "    else:\n",
        "        # When we release a pip version exposing CONFIG_MAPPING,\n",
        "        # we can do `config = CONFIG_MAPPING[args.model_type]()`.\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --config_name\"\n",
        "        )\n",
        "\n",
        "    if args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    elif args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --tokenizer_name\"\n",
        "        )\n",
        "\n",
        "    if args.block_size <= 0:\n",
        "        # args.block_size = tokenizer.max_len\n",
        "        args.block_size = 512\n",
        "        # Our input block size will be the max possible for the model\n",
        "    else:\n",
        "        # args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "        args.block_size = min(args.block_size, 512)\n",
        "\n",
        "    if args.model_name_or_path:\n",
        "        model = AutoModelWithLMHead.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            config=config,\n",
        "            cache_dir=args.cache_dir,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        # Create output directory if needed\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run_lm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TbVPA3tjnJm"
      },
      "source": [
        "import os\n",
        "from tokenizers import ByteLevelBPETokenizer as bpe\n",
        "\n",
        "corpus = '/content/drive/MyDrive/RoBERTa_base_data/combined_corpus.txt'\n",
        "tokenizer = bpe()\n",
        "tokenizer.train(files=corpus,\n",
        "                vocab_size=50265,\n",
        "                min_frequency=2,\n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "if not os.path.isdir('/content/models/roberta/'):\n",
        "    !mkdir -p \"/content/models/roberta\"\n",
        "tokenizer.save('/content/models/roberta/tokenizer.json', pretty=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgQQbRprj1NZ"
      },
      "source": [
        "!mkdir -p \"/content/models/roberta\"\n",
        "!cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcXIQqurS4fH"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfo48wB6wWbo",
        "outputId": "522c1421-35dc-4725-dbdd-113382d4c1e1"
      },
      "source": [
        "print(tf.config.list_physical_devices(\"GPU\"))\n",
        "print(torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeiODE27jnHJ",
        "outputId": "b82b4de6-7a0e-4543-cc62-62ff02082519"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "test_tokenizer = Tokenizer.from_file('/content/models/roberta/tokenizer.json')\n",
        "encoded = test_tokenizer.encode(\"This is a test!!\")\n",
        "print(encoded.tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'Ġis', 'Ġa', 'Ġtest', '!!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V48ineTjnFD"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey6feYLKjnAw"
      },
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('/content/data/'):\n",
        "    !mkdir /content/data/\n",
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_train.txt /content/data\n",
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_dev.txt /content/data\n",
        "!mv /content/data/combined_train.txt /content/data/train.txt\n",
        "!mv /content/data/combined_dev.txt /content/data/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ3pMxn2jm-Z"
      },
      "source": [
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"/content/models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"/content/models/roberta/output_base\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"/content/data/train01.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"/content/data/dev01.txt\" #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4W_Csxwjm7h"
      },
      "source": [
        "# Command line\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 25 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/domain_output.txt\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaJqxeBZjm49"
      },
      "source": [
        "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
        "#   line_by_line, should_continue, and model_name_or_path.\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"num_train_epochs\": 3.0,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJQ3z0Kmjmus",
        "outputId": "cf83d8c1-cab0-4840-bc19-22f5aed176d7"
      },
      "source": [
        "!{cmd.format(**train_params)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "08/31/2021 03:39:53 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "08/31/2021 03:39:53 - INFO - __main__ -   Training new model from scratch\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:892: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "08/31/2021 03:40:03 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/dev01.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='/content/models/roberta', train_data_file='/content/data/train01.txt', warmup_steps=10, weight_decay=0.01)\n",
            "08/31/2021 03:40:03 - INFO - __main__ -   Creating features from dataset file at /content/data/train01.txt\n",
            "tcmalloc: large alloc 2133999616 bytes == 0x5608af756000 @  0x7fbedc54e1e7 0x5607a0555a18 0x5607a056cd65 0x5607a056cae8 0x5607a057a8bf 0x5607a056b5eb 0x5607a05f8b93 0x5607a0523ea9 0x5607a0523da0 0x5607a05982f9 0x5607a0592c35 0x5607a0525dd1 0x5607a0525280 0x5607a0527208 0x5607a0604141 0x5607a06a29f1 0x5607a0523ea9 0x5607a0615c0d 0x5607a05980d8 0x5607a0592c35 0x5607a0525fec 0x5607a0566bc9 0x5607a0563ac4 0x5607a05248a9 0x5607a0598b0a 0x5607a0592c35 0x5607a052573a 0x5607a059493b 0x5607a0593235 0x5607a052573a 0x5607a0593b0e\n",
            "tcmalloc: large alloc 2130395136 bytes == 0x56092f404000 @  0x7fbedc54e1e7 0x5607a0556488 0x5607a06041a7 0x5607a06a29f1 0x5607a0523ea9 0x5607a0615c0d 0x5607a05980d8 0x5607a0592c35 0x5607a0525fec 0x5607a0566bc9 0x5607a0563ac4 0x5607a05248a9 0x5607a0598b0a 0x5607a0592c35 0x5607a052573a 0x5607a059493b 0x5607a0593235 0x5607a052573a 0x5607a0593b0e 0x5607a0592c35 0x5607a0592933 0x5607a065c402 0x5607a065c77d 0x5607a065c626 0x5607a0634313 0x5607a0633fbc 0x7fbedb338bf7 0x5607a0633e9a\n",
            "tcmalloc: large alloc 2130395136 bytes == 0x5608500fa000 @  0x7fbedc54e1e7 0x5607a0555a18 0x5607a056cd65 0x5607a058524c 0x5607a06042a6 0x5607a06a29f1 0x5607a0523ea9 0x5607a0615c0d 0x5607a05980d8 0x5607a0592c35 0x5607a0525fec 0x5607a0566bc9 0x5607a0563ac4 0x5607a05248a9 0x5607a0598b0a 0x5607a0592c35 0x5607a052573a 0x5607a059493b 0x5607a0593235 0x5607a052573a 0x5607a0593b0e 0x5607a0592c35 0x5607a0592933 0x5607a065c402 0x5607a065c77d 0x5607a065c626 0x5607a0634313 0x5607a0633fbc 0x7fbedb338bf7 0x5607a0633e9a\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "08/31/2021 03:45:39 - INFO - __main__ -   ***** Running training *****\n",
            "08/31/2021 03:45:39 - INFO - __main__ -     Num examples = 1034160\n",
            "08/31/2021 03:45:39 - INFO - __main__ -     Num Epochs = 1\n",
            "08/31/2021 03:45:39 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "08/31/2021 03:45:39 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "08/31/2021 03:45:39 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "08/31/2021 03:45:39 - INFO - __main__ -     Total optimization steps = 25\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/258540 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/258540 [00:00<31:36:02,  2.27it/s]\u001b[A\n",
            "Iteration:   0% 2/258540 [00:00<21:36:18,  3.32it/s]\u001b[A\n",
            "Iteration:   0% 4/258540 [00:00<15:24:46,  4.66it/s]\u001b[A\n",
            "Iteration:   0% 6/258540 [00:01<11:28:45,  6.26it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0% 8/258540 [00:01<9:53:03,  7.27it/s] \u001b[A\n",
            "Iteration:   0% 10/258540 [00:01<8:32:31,  8.41it/s]\u001b[A\n",
            "Iteration:   0% 12/258540 [00:01<7:46:59,  9.23it/s]\u001b[A\n",
            "Iteration:   0% 14/258540 [00:01<7:00:48, 10.24it/s]\u001b[A\n",
            "Iteration:   0% 16/258540 [00:02<8:17:25,  8.66it/s]\u001b[A\n",
            "Iteration:   0% 17/258540 [00:02<9:46:10,  7.35it/s]\u001b[A\n",
            "Iteration:   0% 18/258540 [00:02<9:39:58,  7.43it/s]\u001b[A\n",
            "Iteration:   0% 19/258540 [00:02<9:27:39,  7.59it/s]\u001b[A\n",
            "Iteration:   0% 20/258540 [00:02<9:13:12,  7.79it/s]\u001b[A\n",
            "Iteration:   0% 22/258540 [00:02<8:20:11,  8.61it/s]\u001b[A\n",
            "Iteration:   0% 23/258540 [00:03<8:33:35,  8.39it/s]\u001b[A\n",
            "Iteration:   0% 24/258540 [00:03<8:47:03,  8.17it/s]\u001b[A\n",
            "Iteration:   0% 25/258540 [00:03<8:36:07,  8.35it/s]\u001b[A\n",
            "Iteration:   0% 26/258540 [00:03<9:02:48,  7.94it/s]\u001b[A\n",
            "Iteration:   0% 28/258540 [00:03<9:07:19,  7.87it/s]\u001b[A\n",
            "Iteration:   0% 29/258540 [00:03<9:20:46,  7.68it/s]\u001b[A\n",
            "Iteration:   0% 30/258540 [00:04<9:49:01,  7.31it/s]\u001b[A\n",
            "Iteration:   0% 31/258540 [00:04<9:33:50,  7.51it/s]\u001b[A\n",
            "Iteration:   0% 32/258540 [00:04<10:02:10,  7.15it/s]\u001b[A\n",
            "Iteration:   0% 33/258540 [00:04<10:54:00,  6.59it/s]\u001b[A\n",
            "Iteration:   0% 35/258540 [00:04<8:39:06,  8.30it/s] \u001b[A\n",
            "Iteration:   0% 36/258540 [00:04<10:26:06,  6.88it/s]\u001b[A\n",
            "Iteration:   0% 37/258540 [00:05<10:21:11,  6.94it/s]\u001b[A\n",
            "Iteration:   0% 38/258540 [00:05<10:19:10,  6.96it/s]\u001b[A\n",
            "Iteration:   0% 39/258540 [00:05<10:20:35,  6.94it/s]\u001b[A08/31/2021 03:45:46 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-10\n",
            "08/31/2021 03:45:50 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-10\n",
            "\n",
            "Iteration:   0% 40/258540 [00:10<110:08:21,  1.53s/it]\u001b[A\n",
            "Iteration:   0% 42/258540 [00:10<64:37:53,  1.11it/s] \u001b[A\n",
            "Iteration:   0% 43/258540 [00:10<51:08:25,  1.40it/s]\u001b[A\n",
            "Iteration:   0% 44/258540 [00:10<40:35:40,  1.77it/s]\u001b[A\n",
            "Iteration:   0% 46/258540 [00:11<27:17:35,  2.63it/s]\u001b[A\n",
            "Iteration:   0% 47/258540 [00:11<24:08:43,  2.97it/s]\u001b[A\n",
            "Iteration:   0% 48/258540 [00:11<20:18:31,  3.54it/s]\u001b[A\n",
            "Iteration:   0% 49/258540 [00:11<17:40:17,  4.06it/s]\u001b[A\n",
            "Iteration:   0% 50/258540 [00:11<17:03:49,  4.21it/s]\u001b[A\n",
            "Iteration:   0% 51/258540 [00:11<15:22:49,  4.67it/s]\u001b[A\n",
            "Iteration:   0% 52/258540 [00:12<14:02:29,  5.11it/s]\u001b[A\n",
            "Iteration:   0% 54/258540 [00:12<11:14:49,  6.38it/s]\u001b[A\n",
            "Iteration:   0% 55/258540 [00:12<10:55:07,  6.58it/s]\u001b[A\n",
            "Iteration:   0% 56/258540 [00:12<10:19:33,  6.95it/s]\u001b[A\n",
            "Iteration:   0% 57/258540 [00:12<9:37:13,  7.46it/s] \u001b[A\n",
            "Iteration:   0% 58/258540 [00:12<9:05:27,  7.90it/s]\u001b[A\n",
            "Iteration:   0% 60/258540 [00:13<8:28:35,  8.47it/s]\u001b[A\n",
            "Iteration:   0% 61/258540 [00:13<8:21:24,  8.59it/s]\u001b[A\n",
            "Iteration:   0% 62/258540 [00:13<8:18:58,  8.63it/s]\u001b[A\n",
            "Iteration:   0% 63/258540 [00:13<8:19:42,  8.62it/s]\u001b[A\n",
            "Iteration:   0% 64/258540 [00:13<10:24:14,  6.90it/s]\u001b[A\n",
            "Iteration:   0% 65/258540 [00:13<10:35:45,  6.78it/s]\u001b[A\n",
            "Iteration:   0% 66/258540 [00:13<9:45:58,  7.35it/s] \u001b[A\n",
            "Iteration:   0% 68/258540 [00:14<9:52:06,  7.28it/s]\u001b[A\n",
            "Iteration:   0% 69/258540 [00:14<9:55:02,  7.24it/s]\u001b[A\n",
            "Iteration:   0% 71/258540 [00:14<8:14:16,  8.72it/s]\u001b[A\n",
            "Iteration:   0% 72/258540 [00:14<8:40:04,  8.28it/s]\u001b[A\n",
            "Iteration:   0% 73/258540 [00:14<8:18:28,  8.64it/s]\u001b[A\n",
            "Iteration:   0% 75/258540 [00:14<9:27:13,  7.59it/s]\u001b[A\n",
            "Iteration:   0% 76/258540 [00:15<9:37:42,  7.46it/s]\u001b[A\n",
            "Iteration:   0% 77/258540 [00:15<9:30:27,  7.55it/s]\u001b[A\n",
            "Iteration:   0% 78/258540 [00:15<10:40:03,  6.73it/s]\u001b[A\n",
            "Iteration:   0% 79/258540 [00:15<10:09:21,  7.07it/s]\u001b[A08/31/2021 03:45:56 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base/checkpoint-20\n",
            "08/31/2021 03:46:05 - INFO - __main__ -   Saving optimizer and scheduler states to /content/models/roberta/output_base/checkpoint-20\n",
            "\n",
            "Iteration:   0% 80/258540 [00:25<213:36:16,  2.98s/it]\u001b[A\n",
            "Iteration:   0% 81/258540 [00:25<155:07:04,  2.16s/it]\u001b[A\n",
            "Iteration:   0% 82/258540 [00:26<112:35:16,  1.57s/it]\u001b[A\n",
            "Iteration:   0% 83/258540 [00:26<82:38:20,  1.15s/it] \u001b[A\n",
            "Iteration:   0% 84/258540 [00:26<63:33:45,  1.13it/s]\u001b[A\n",
            "Iteration:   0% 85/258540 [00:26<47:22:52,  1.52it/s]\u001b[A\n",
            "Iteration:   0% 86/258540 [00:26<35:33:43,  2.02it/s]\u001b[A\n",
            "Iteration:   0% 87/258540 [00:26<27:06:28,  2.65it/s]\u001b[A\n",
            "Iteration:   0% 88/258540 [00:27<24:03:03,  2.99it/s]\u001b[A\n",
            "Iteration:   0% 89/258540 [00:27<20:53:54,  3.44it/s]\u001b[A\n",
            "Iteration:   0% 91/258540 [00:27<13:36:54,  5.27it/s]\u001b[A\n",
            "Iteration:   0% 92/258540 [00:27<13:57:59,  5.14it/s]\u001b[A\n",
            "Iteration:   0% 93/258540 [00:27<13:37:03,  5.27it/s]\u001b[A\n",
            "Iteration:   0% 94/258540 [00:27<12:07:00,  5.92it/s]\u001b[A\n",
            "Iteration:   0% 95/258540 [00:28<12:23:35,  5.79it/s]\u001b[A\n",
            "Iteration:   0% 96/258540 [00:28<11:19:44,  6.34it/s]\u001b[A\n",
            "Iteration:   0% 98/258540 [00:28<10:04:22,  7.13it/s]\u001b[A\n",
            "Iteration:   0% 100/258540 [00:28<9:11:10,  7.81it/s]\u001b[A\n",
            "Iteration:   0% 101/258540 [00:28<8:55:45,  8.04it/s]\u001b[A\n",
            "Iteration:   0% 103/258540 [00:29<20:15:14,  3.54it/s]\n",
            "Epoch:   0% 0/1 [00:29<?, ?it/s]\n",
            "08/31/2021 03:46:09 - INFO - __main__ -    global_step = 26, average loss = 10.124158373245827\n",
            "08/31/2021 03:46:09 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:902: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "08/31/2021 03:46:12 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "08/31/2021 03:46:14 - INFO - __main__ -   Creating features from dataset file at /content/data/dev01.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8go0lTxukxJn"
      },
      "source": [
        "## Transfer Trained model back to drive\n",
        "if not os.path.isdir('/content/drive/MyDrive/Adaptive_pretrain/roberta_models_base/'):\n",
        "    !mkdir '/content/drive/MyDrive/Adaptive_pretrain/roberta_models_base'\n",
        "!cp -R /content/models /content/drive/MyDrive/Adaptive_pretrain/roberta_models_base/\n",
        "!cp -R /content/runs /content/drive/MyDrive/Adaptive_pretrain/roberta_models_base/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puiZL8BukxBt"
      },
      "source": [
        "## Check\n",
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model='/content/models/roberta/output_base',\n",
        "    tokenizer='/content/models/roberta/output_base'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2rM_X0Lngqk",
        "outputId": "dbe47f8c-3b08-4153-c7f9-ace0014288d1"
      },
      "source": [
        "# Train RoBERTa base\n",
        "\n",
        "import os\n",
        "# from distutils.dir_util import copy_tree\n",
        "for num_corpus in range(0, 10):\n",
        "    print(\"__________START \" + f\"train0{num_corpus}__________\")\n",
        "    train_file = f\"train0{num_corpus}.txt\"\n",
        "    dev_file = f\"dev0{num_corpus}.txt\"\n",
        "    # Model paths\n",
        "    MODEL_TYPE = \"roberta\"\n",
        "    MODEL_DIR = \"/content/models/roberta\"\n",
        "    OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "    TRAIN_PATH = \"/content/data/\"+train_file\n",
        "    EVAL_PATH = \"/content/data/\"+dev_file\n",
        "\n",
        "    train_params = {\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "        \"model_type\": MODEL_TYPE,\n",
        "        \"config_name\": MODEL_DIR,\n",
        "        \"tokenizer_name\": MODEL_DIR,\n",
        "        \"train_path\": TRAIN_PATH,\n",
        "        \"eval_path\": EVAL_PATH,\n",
        "        \"num_train_epochs\": 1.0,\n",
        "        \"do_eval\": \"--do_eval\",\n",
        "        \"evaluate_during_training\": \"\",\n",
        "        \"line_by_line\": \"--line_by_line\",\n",
        "        \"should_continue\": \"\",\n",
        "        \"model_name_or_path\": \"\",\n",
        "    }\n",
        "\n",
        "    if num_corpus != 0:\n",
        "        train_params['should_continue'] = \"--should_continue\"\n",
        "    \n",
        "    !{cmd.format(**train_params)}\n",
        "\n",
        "    save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_base_3ep{num_corpus}/'\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    !cp -R /content/models \"$save_dir\"\n",
        "    !cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________START train00__________\n",
            "09/03/2021 01:41:35 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "09/03/2021 01:41:36 - INFO - __main__ -   Training new model from scratch\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:582: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 01:41:45 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/dev00.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=3.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='/content/models/roberta', train_data_file='/content/data/train00.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 01:41:45 - INFO - __main__ -   Creating features from dataset file at /content/data/train00.txt\n",
            "tcmalloc: large alloc 2026496000 bytes == 0x5644756b2000 @  0x7fe15426c1e7 0x5643434caa18 0x5643434e1d65 0x5643434e1ae8 0x5643434ef8bf 0x5643434e05eb 0x56434356db93 0x564343498ea9 0x564343498da0 0x56434350d2f9 0x564343507c35 0x56434349add1 0x56434349a280 0x56434349c208 0x564343579141 0x5643436179f1 0x564343498ea9 0x56434358ac0d 0x56434350d0d8 0x564343507c35 0x56434349afec 0x5643434dbbc9 0x5643434d8ac4 0x5643434998a9 0x56434350db0a 0x564343507c35 0x56434349a73a 0x56434350993b 0x564343508235 0x56434349a73a 0x564343508b0e\n",
            "tcmalloc: large alloc 2006777856 bytes == 0x5644ee350000 @  0x7fe15426c1e7 0x5643434cb488 0x5643435791a7 0x5643436179f1 0x564343498ea9 0x56434358ac0d 0x56434350d0d8 0x564343507c35 0x56434349afec 0x5643434dbbc9 0x5643434d8ac4 0x5643434998a9 0x56434350db0a 0x564343507c35 0x56434349a73a 0x56434350993b 0x564343508235 0x56434349a73a 0x564343508b0e 0x564343507c35 0x564343507933 0x5643435d1402 0x5643435d177d 0x5643435d1626 0x5643435a9313 0x5643435a8fbc 0x7fe153056bf7 0x5643435a8e9a\n",
            "tcmalloc: large alloc 2006777856 bytes == 0x5644756b2000 @  0x7fe15426c1e7 0x5643434caa18 0x5643434e1d65 0x5643434fa24c 0x5643435792a6 0x5643436179f1 0x564343498ea9 0x56434358ac0d 0x56434350d0d8 0x564343507c35 0x56434349afec 0x5643434dbbc9 0x5643434d8ac4 0x5643434998a9 0x56434350db0a 0x564343507c35 0x56434349a73a 0x56434350993b 0x564343508235 0x56434349a73a 0x564343508b0e 0x564343507c35 0x564343507933 0x5643435d1402 0x5643435d177d 0x5643435d1626 0x5643435a9313 0x5643435a8fbc 0x7fe153056bf7 0x5643435a8e9a\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "cp: cannot stat '/content/runs': No such file or directory\n",
            "__________START train01__________\n",
            "Traceback (most recent call last):\n",
            "  File \"run_lm.py\", line 795, in <module>\n",
            "    main()\n",
            "  File \"run_lm.py\", line 628, in main\n",
            "    raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
            "ValueError: Used --should_continue but no checkpoint was found in --output_dir.\n",
            "cp: cannot stat '/content/runs': No such file or directory\n",
            "__________START train02__________\n",
            "Traceback (most recent call last):\n",
            "  File \"run_lm.py\", line 795, in <module>\n",
            "    main()\n",
            "  File \"run_lm.py\", line 628, in main\n",
            "    raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
            "ValueError: Used --should_continue but no checkpoint was found in --output_dir.\n",
            "cp: cannot stat '/content/runs': No such file or directory\n",
            "__________START train03__________\n",
            "Traceback (most recent call last):\n",
            "  File \"run_lm.py\", line 795, in <module>\n",
            "    main()\n",
            "  File \"run_lm.py\", line 628, in main\n",
            "    raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
            "ValueError: Used --should_continue but no checkpoint was found in --output_dir.\n",
            "cp: cannot stat '/content/runs': No such file or directory\n",
            "__________START train04__________\n",
            "Traceback (most recent call last):\n",
            "  File \"run_lm.py\", line 795, in <module>\n",
            "    main()\n",
            "  File \"run_lm.py\", line 628, in main\n",
            "    raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
            "ValueError: Used --should_continue but no checkpoint was found in --output_dir.\n",
            "cp: cannot stat '/content/runs': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo1drOh5yCCy"
      },
      "source": [
        "output_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_base{num_corpus}/'\n",
        "!cp -R /content/models \"$output_dir\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qee95NzQjmKK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49ec0a5-256b-46e2-fb08-215506d6fa3d"
      },
      "source": [
        "!echo \"START\" |& tee /content/drive/MyDrive/Adaptive_pretrain/base_output.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "START\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "978CcxNGI-nK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_CbtybaI-bl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TbyXKDqS-QK"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJOhrwnrXwQG"
      },
      "source": [
        "class LineByLineDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "        logger.info(\"Creating features from dataset file at %s.\", file_path)\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)['input_ids']\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.example[i], dtype=torch.long)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdKe1oFn_Jfx"
      },
      "source": [
        "!rm -r /content/models\n",
        "!rm -r /content/runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl2Av2RIWVyX"
      },
      "source": [
        "!mkdir -p \"/content/models/roberta\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B2dLyQwbY8ah"
      },
      "source": [
        "import os\n",
        "from tokenizers import ByteLevelBPETokenizer as bpe\n",
        "\n",
        "corpus = '/content/drive/MyDrive/RoBERTa_base_data/combined_train.txt'\n",
        "tokenizer = bpe()\n",
        "tokenizer.train(files=corpus,\n",
        "                vocab_size=51200,\n",
        "                min_frequency=2,\n",
        "                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "if not os.path.isdir('/content/models/roberta/'):\n",
        "    !mkdir -p \"/content/models/roberta\"\n",
        "tokenizer.save('/content/models/roberta/tokenizer.json', pretty=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQFWN2SVtw5X",
        "outputId": "9c204e7d-eb7e-4bf1-f97c-70254c533960"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "test_tokenizer = Tokenizer.from_file('/content/models/roberta/tokenizer.json')\n",
        "encoded = test_tokenizer.encode(\"This is a test\")\n",
        "print(encoded.tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['This', 'Ġis', 'Ġa', 'Ġtest']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91R2f8N3Ewdf"
      },
      "source": [
        "from typing import Dict, List, Tuple\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxU7plf6Etn4"
      },
      "source": [
        "def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duCxzxZjxPt4"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 51200\n",
        "}\n",
        "\n",
        "with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "    json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "    json.dump(tokenizer_config, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixjR23c16VuL",
        "outputId": "bbfddfe3-a4a4-4ef0-8afe-305eb936425a"
      },
      "source": [
        "%%writefile run_lm.py\n",
        "\n",
        "### THIS FILE IS COPIED FROM THE HUGGINGFACE REPOSITORY FOR CONVENIENCE.\n",
        "\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\n",
        "\"\"\"\n",
        "Fine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\n",
        "GPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa are fine-tuned\n",
        "using a masked language modeling (MLM) loss.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    MODEL_WITH_LM_HEAD_MAPPING,\n",
        "    WEIGHTS_NAME,\n",
        "    AdamW,\n",
        "    AutoConfig,\n",
        "    AutoModelWithLMHead,\n",
        "    AutoTokenizer,\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "\n",
        "\n",
        "try:\n",
        "    from torch.utils.tensorboard import SummaryWriter\n",
        "except ImportError:\n",
        "    from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "MODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\n",
        "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "\n",
        "        block_size = block_size - tokenizer.num_special_tokens_to_add(pair=False)\n",
        "\n",
        "        directory, filename = os.path.split(file_path)\n",
        "        cached_features_file = os.path.join(\n",
        "            directory, args.model_type + \"_cached_lm_\" + str(block_size) + \"_\" + filename\n",
        "        )\n",
        "\n",
        "        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n",
        "            logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"rb\") as handle:\n",
        "                self.examples = pickle.load(handle)\n",
        "        else:\n",
        "            logger.info(\"Creating features from dataset file at %s\", directory)\n",
        "\n",
        "            self.examples = []\n",
        "            with open(file_path, encoding=\"utf-8\") as f:\n",
        "                text = f.read()\n",
        "\n",
        "            tokenized_text = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))\n",
        "\n",
        "            for i in range(0, len(tokenized_text) - block_size + 1, block_size):  # Truncate in block of block_size\n",
        "                self.examples.append(tokenizer.build_inputs_with_special_tokens(tokenized_text[i : i + block_size]))\n",
        "            # Note that we are loosing the last truncated example here for the sake of simplicity (no padding)\n",
        "            # If your dataset is small, first you should loook for a bigger one :-) and second you\n",
        "            # can change this behavior by adding (model specific) padding.\n",
        "\n",
        "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "            with open(cached_features_file, \"wb\") as handle:\n",
        "                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item], dtype=torch.long)\n",
        "\n",
        "\n",
        "class LineByLineTextDataset(Dataset):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer, args, file_path: str, block_size=512):\n",
        "        assert os.path.isfile(file_path)\n",
        "        # Here, we do not cache the features, operating under the assumption\n",
        "        # that we will soon use fast multithreaded tokenizers from the\n",
        "        # `tokenizers` repo everywhere =)\n",
        "        logger.info(\"Creating features from dataset file at %s\", file_path)\n",
        "\n",
        "        with open(file_path, encoding=\"utf-8\") as f:\n",
        "            lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]\n",
        "\n",
        "        self.examples = tokenizer.batch_encode_plus(lines, add_special_tokens=True, max_length=block_size)[\"input_ids\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return torch.tensor(self.examples[i], dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, evaluate=False):\n",
        "    file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "    if args.line_by_line:\n",
        "        return LineByLineTextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
        "    else:\n",
        "        return TextDataset(tokenizer, args, file_path=file_path, block_size=args.block_size)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n",
        "    ordering_and_checkpoint_path = []\n",
        "\n",
        "    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n",
        "\n",
        "    for path in glob_checkpoints:\n",
        "        if use_mtime:\n",
        "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
        "        else:\n",
        "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
        "            if regex_match and regex_match.groups():\n",
        "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
        "\n",
        "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
        "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
        "    return checkpoints_sorted\n",
        "\n",
        "\n",
        "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n",
        "    if not args.save_total_limit:\n",
        "        return\n",
        "    if args.save_total_limit <= 0:\n",
        "        return\n",
        "\n",
        "    # Check if we should delete older checkpoint(s)\n",
        "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
        "    if len(checkpoints_sorted) <= args.save_total_limit:\n",
        "        return\n",
        "\n",
        "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n",
        "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
        "    for checkpoint in checkpoints_to_be_deleted:\n",
        "        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n",
        "        shutil.rmtree(checkpoint)\n",
        "\n",
        "\n",
        "def mask_tokens(inputs: torch.Tensor, tokenizer: PreTrainedTokenizer, args) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "\n",
        "    if tokenizer.mask_token is None:\n",
        "        raise ValueError(\n",
        "            \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "        )\n",
        "\n",
        "    labels = inputs.clone()\n",
        "    # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "    probability_matrix = torch.full(labels.shape, args.mlm_probability)\n",
        "    special_tokens_mask = [\n",
        "        tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "    ]\n",
        "    probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "    if tokenizer._pad_token is not None:\n",
        "        padding_mask = labels.eq(tokenizer.pad_token_id)\n",
        "        probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "    labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "    # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "    inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "\n",
        "    # 10% of the time, we replace masked input tokens with random word\n",
        "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "    random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
        "    inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels\n",
        "\n",
        "\n",
        "def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer = SummaryWriter()\n",
        "\n",
        "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate\n",
        "    )\n",
        "\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
        "\n",
        "    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\n",
        "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": args.weight_decay,\n",
        "        },\n",
        "        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
        "    )\n",
        "\n",
        "    # Check if saved optimizer or scheduler states exist\n",
        "    if (\n",
        "        args.model_name_or_path\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n",
        "        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n",
        "    ):\n",
        "        # Load in optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        "    if args.fp16:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
        "\n",
        "    # multi-gpu training (should be after apex fp16 initialization)\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Distributed training (should be after apex fp16 initialization)\n",
        "    if args.local_rank != -1:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(\n",
        "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
        "        )\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
        "    logger.info(\n",
        "        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "        args.train_batch_size\n",
        "        * args.gradient_accumulation_steps\n",
        "        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n",
        "    )\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    epochs_trained = 0\n",
        "    steps_trained_in_current_epoch = 0\n",
        "    # Check if continuing training from a checkpoint\n",
        "    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n",
        "        try:\n",
        "            # set global_step to gobal_step of last saved checkpoint from model path\n",
        "            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n",
        "            global_step = int(checkpoint_suffix)\n",
        "            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n",
        "\n",
        "            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n",
        "            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
        "            logger.info(\"  Continuing training from global step %d\", global_step)\n",
        "            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n",
        "        except ValueError:\n",
        "            logger.info(\"  Starting fine-tuning.\")\n",
        "\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(\n",
        "        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
        "    )\n",
        "    set_seed(args)  # Added here for reproducibility\n",
        "    for epoch in train_iterator:\n",
        "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
        "\n",
        "        if args.local_rank != -1:\n",
        "            train_sampler.set_epoch(epoch)\n",
        "\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "\n",
        "            # Skip past any already trained steps if resuming training\n",
        "            if steps_trained_in_current_epoch > 0:\n",
        "                steps_trained_in_current_epoch -= 1\n",
        "                continue\n",
        "\n",
        "            inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "            inputs = inputs.to(args.device)\n",
        "            labels = labels.to(args.device)\n",
        "            model.train()\n",
        "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
        "\n",
        "            if args.n_gpu > 1:\n",
        "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    # Log metrics\n",
        "                    if (\n",
        "                        args.local_rank == -1 and args.evaluate_during_training\n",
        "                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(args, model, tokenizer)\n",
        "                        for key, value in results.items():\n",
        "                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
        "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
        "                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n",
        "                    logging_loss = tr_loss\n",
        "\n",
        "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    checkpoint_prefix = \"checkpoint\"\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n",
        "                    os.makedirs(output_dir, exist_ok=True)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "\n",
        "                    _rotate_checkpoints(args, checkpoint_prefix)\n",
        "\n",
        "                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                epoch_iterator.close()\n",
        "                break\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            train_iterator.close()\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        tb_writer.close()\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n",
        "\n",
        "def evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prefix=\"\") -> Dict:\n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    eval_output_dir = args.output_dir\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(args, tokenizer, evaluate=True)\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(eval_output_dir, exist_ok=True)\n",
        "\n",
        "    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
        "    # Note that DistributedSampler samples randomly\n",
        "\n",
        "    def collate(examples: List[torch.Tensor]):\n",
        "        if tokenizer._pad_token is None:\n",
        "            return pad_sequence(examples, batch_first=True)\n",
        "        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(\n",
        "        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate\n",
        "    )\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if args.n_gpu > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "        inputs, labels = mask_tokens(batch, tokenizer, args) if args.mlm else (batch, batch)\n",
        "        inputs = inputs.to(args.device)\n",
        "        labels = labels.to(args.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # outputs = model(inputs, masked_lm_labels=labels) if args.mlm else model(inputs, labels=labels)\n",
        "            outputs = model(inputs, labels=labels)\n",
        "            lm_loss = outputs[0]\n",
        "            eval_loss += lm_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
        "\n",
        "    result = {\"perplexity\": perplexity}\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--train_data_file\", default=None, type=str, required=True, help=\"The input training data file (a text file).\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_dir\",\n",
        "        type=str,\n",
        "        required=True,\n",
        "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_type\", type=str, required=True, help=\"The model architecture to be trained or fine-tuned.\",\n",
        "    )\n",
        "\n",
        "    # Other parameters\n",
        "    parser.add_argument(\n",
        "        \"--eval_data_file\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"An optional input evaluation data file to evaluate the perplexity on (a text file).\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--line_by_line\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--should_continue\", action=\"store_true\", help=\"Whether to continue from latest checkpoint in output_dir\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"The model checkpoint for weights initialization. Leave None if you want to train a model from scratch.\",\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--mlm\", action=\"store_true\", help=\"Train with masked-language modeling loss instead of language modeling.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--mlm_probability\", type=float, default=0.15, help=\"Ratio of tokens to mask for masked language modeling loss\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--config_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional pretrained config name or path if not the same as model_name_or_path. If both are None, initialize a new config.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--tokenizer_name\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional pretrained tokenizer name or path if not the same as model_name_or_path. If both are None, initialize a new tokenizer.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cache_dir\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        help=\"Optional directory to store the pre-trained models downloaded from s3 (instead of the default one)\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--block_size\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"Optional input sequence length after tokenization.\"\n",
        "        \"The training dataset will be truncated in block of this size for training.\"\n",
        "        \"Default to the model max input length for single sentence inputs (take into account special tokens).\",\n",
        "    )\n",
        "    parser.add_argument(\"--do_train\", action=\"store_true\", help=\"Whether to run training.\")\n",
        "    parser.add_argument(\"--do_eval\", action=\"store_true\", help=\"Whether to run eval on the dev set.\")\n",
        "    parser.add_argument(\n",
        "        \"--evaluate_during_training\", action=\"store_true\", help=\"Run evaluation during training at each logging step.\"\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\"--per_gpu_train_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for training.\")\n",
        "    parser.add_argument(\n",
        "        \"--per_gpu_eval_batch_size\", default=4, type=int, help=\"Batch size per GPU/CPU for evaluation.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--gradient_accumulation_steps\",\n",
        "        type=int,\n",
        "        default=1,\n",
        "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
        "    )\n",
        "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "    parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "    parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_train_epochs\", default=1.0, type=float, help=\"Total number of training epochs to perform.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--max_steps\",\n",
        "        default=-1,\n",
        "        type=int,\n",
        "        help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\",\n",
        "    )\n",
        "    parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=500, help=\"Log every X updates steps.\")\n",
        "    parser.add_argument(\"--save_steps\", type=int, default=500, help=\"Save checkpoint every X updates steps.\")\n",
        "    parser.add_argument(\n",
        "        \"--save_total_limit\",\n",
        "        type=int,\n",
        "        default=None,\n",
        "        help=\"Limit the total amount of checkpoints, delete the older checkpoints in the output_dir, does not delete by default\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--eval_all_checkpoints\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Evaluate all checkpoints starting with the same prefix as model_name_or_path ending and ending with step number\",\n",
        "    )\n",
        "    parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_output_dir\", action=\"store_true\", help=\"Overwrite the content of the output directory\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--overwrite_cache\", action=\"store_true\", help=\"Overwrite the cached training and evaluation sets\"\n",
        "    )\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"random seed for initialization\")\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--fp16\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--fp16_opt_level\",\n",
        "        type=str,\n",
        "        default=\"O1\",\n",
        "        help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "        \"See details at https://nvidia.github.io/apex/amp.html\",\n",
        "    )\n",
        "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
        "    parser.add_argument(\"--server_ip\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    parser.add_argument(\"--server_port\", type=str, default=\"\", help=\"For distant debugging.\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.model_type in [\"bert\", \"roberta\", \"distilbert\", \"camembert\"] and not args.mlm:\n",
        "        raise ValueError(\n",
        "            \"BERT and RoBERTa-like models do not have LM heads but masked LM heads. They must be run using the --mlm \"\n",
        "            \"flag (masked language modeling).\"\n",
        "        )\n",
        "    if args.eval_data_file is None and args.do_eval:\n",
        "        raise ValueError(\n",
        "            \"Cannot do evaluation without an evaluation data file. Either supply a file to --eval_data_file \"\n",
        "            \"or remove the --do_eval argument.\"\n",
        "        )\n",
        "    if args.should_continue:\n",
        "        sorted_checkpoints = _sorted_checkpoints(args)\n",
        "        if len(sorted_checkpoints) == 0:\n",
        "            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n",
        "        else:\n",
        "            args.model_name_or_path = sorted_checkpoints[-1]\n",
        "\n",
        "    if (\n",
        "        os.path.exists(args.output_dir)\n",
        "        and os.listdir(args.output_dir)\n",
        "        and args.do_train\n",
        "        and not args.overwrite_output_dir\n",
        "        and not args.should_continue\n",
        "    ):\n",
        "        raise ValueError(\n",
        "            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n",
        "                args.output_dir\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # Setup distant debugging if needed\n",
        "    if args.server_ip and args.server_port:\n",
        "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
        "        import ptvsd\n",
        "\n",
        "        print(\"Waiting for debugger attach\")\n",
        "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
        "        ptvsd.wait_for_attach()\n",
        "\n",
        "    # Setup CUDA, GPU & distributed training\n",
        "    if args.local_rank == -1 or args.no_cuda:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
        "        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n",
        "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
        "        torch.cuda.set_device(args.local_rank)\n",
        "        device = torch.device(\"cuda\", args.local_rank)\n",
        "        torch.distributed.init_process_group(backend=\"nccl\")\n",
        "        args.n_gpu = 1\n",
        "    args.device = device\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n",
        "    )\n",
        "    logger.warning(\n",
        "        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
        "        args.local_rank,\n",
        "        device,\n",
        "        args.n_gpu,\n",
        "        bool(args.local_rank != -1),\n",
        "        args.fp16,\n",
        "    )\n",
        "\n",
        "    # Set seed\n",
        "    set_seed(args)\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    if args.config_name:\n",
        "        config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n",
        "    elif args.model_name_or_path:\n",
        "        config = AutoConfig.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
        "    else:\n",
        "        # When we release a pip version exposing CONFIG_MAPPING,\n",
        "        # we can do `config = CONFIG_MAPPING[args.model_type]()`.\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new config instance from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --config_name\"\n",
        "        )\n",
        "\n",
        "    if args.tokenizer_name:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n",
        "    elif args.model_name_or_path:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, cache_dir=args.cache_dir)\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            \"You are instantiating a new tokenizer from scratch. This is not supported, but you can do it from another script, save it,\"\n",
        "            \"and load it from here, using --tokenizer_name\"\n",
        "        )\n",
        "\n",
        "    if args.block_size <= 0:\n",
        "        # args.block_size = tokenizer.max_len\n",
        "        args.block_size = 512\n",
        "        # Our input block size will be the max possible for the model\n",
        "    else:\n",
        "        # args.block_size = min(args.block_size, tokenizer.max_len)\n",
        "        args.block_size = min(args.block_size, 512)\n",
        "\n",
        "    if args.model_name_or_path:\n",
        "        model = AutoModelWithLMHead.from_pretrained(\n",
        "            args.model_name_or_path,\n",
        "            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "            config=config,\n",
        "            cache_dir=args.cache_dir,\n",
        "        )\n",
        "    else:\n",
        "        logger.info(\"Training new model from scratch\")\n",
        "        model = AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "    model.to(args.device)\n",
        "\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()  # End of barrier to make sure only the first process in distributed training download model & vocab\n",
        "\n",
        "    logger.info(\"Training/evaluation parameters %s\", args)\n",
        "\n",
        "    # Training\n",
        "    if args.do_train:\n",
        "        if args.local_rank not in [-1, 0]:\n",
        "            torch.distributed.barrier()  # Barrier to make sure only the first process in distributed training process the dataset, and the others will use the cache\n",
        "\n",
        "        train_dataset = load_and_cache_examples(args, tokenizer, evaluate=False)\n",
        "\n",
        "        if args.local_rank == 0:\n",
        "            torch.distributed.barrier()\n",
        "\n",
        "        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
        "        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
        "\n",
        "    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n",
        "    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
        "        # Create output directory if needed\n",
        "        if args.local_rank in [-1, 0]:\n",
        "            os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
        "        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "        # They can then be reloaded using `from_pretrained()`\n",
        "        model_to_save = (\n",
        "            model.module if hasattr(model, \"module\") else model\n",
        "        )  # Take care of distributed/parallel training\n",
        "        model_to_save.save_pretrained(args.output_dir)\n",
        "        tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "        # Good practice: save your training arguments together with the trained model\n",
        "        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n",
        "\n",
        "        # Load a trained model and vocabulary that you have fine-tuned\n",
        "        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n",
        "        model.to(args.device)\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    if args.do_eval and args.local_rank in [-1, 0]:\n",
        "        checkpoints = [args.output_dir]\n",
        "        if args.eval_all_checkpoints:\n",
        "            checkpoints = list(\n",
        "                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n",
        "            )\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n",
        "            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n",
        "\n",
        "            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, tokenizer, prefix=prefix)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing run_lm.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soIItJ6RLjP_"
      },
      "source": [
        "cs = []\n",
        "with open('/content/drive/MyDrive/Adaptive_pretrain/cs.txt', 'r') as f:\n",
        "    for txt in f.readlines():\n",
        "        cs.append(txt)\n",
        "\n",
        "max_len = int(len(cs) * 0.8)\n",
        "train_cs = cs[:max_len]\n",
        "dev_cs = cs[max_len+1:]\n",
        "\n",
        "with open('/content/drive/MyDrive/Adaptive_pretrain/train.txt', 'w') as f:\n",
        "    for txt in train_cs:\n",
        "        f.write(txt)\n",
        "\n",
        "with open('/content/drive/MyDrive/Adaptive_pretrain/dev.txt', 'w') as f:\n",
        "    for txt in dev_cs:\n",
        "        f.write(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "id": "2AZUKTJgn2Ft",
        "outputId": "77dab0ca-0881-45bd-c03e-b1c00bc46bfd"
      },
      "source": [
        "cs[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Estimating 3D human pose from monocular images demands large amounts of 3D pose and in-the-wild 2D pose annotated datasets which are costly and require sophisticated systems to acquire. In this regard, we propose a metric learning based approach to jointly learn a rich embedding and 3D pose regression from the embedding using multi-view synchronised videos of human motions and very limited 3D pose annotations. The inclusion of metric learning to the baseline pose estimation framework improves the performance by 21% when 3D supervision is limited. In addition, we make use of a person-identity based adversarial loss as additional weak supervision to outperform state-of-the-art whilst using a much smaller network. Lastly, but importantly, we demonstrate the advantages of the learned embedding and establish view-invariant pose retrieval benchmarks on two popular, publicly available multi-view human pose datasets, Human 3.6M and MPI-INF-3DHP, to facilitate future research.\\n'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7iBau1wMMNp"
      },
      "source": [
        "!mkdir /content/data/\n",
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_train.txt /content/data\n",
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_dev.txt /content/data\n",
        "!mv /content/data/combined_train.txt /content/data/train.txt\n",
        "!mv /content/data/combined_dev.txt /content/data/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5D2jNfXMNFQ"
      },
      "source": [
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"/content/models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"/content/models/roberta/output\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"/content/data/train.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"/content/data/dev.txt\" #@param {type: \"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdMqKeqjMSZQ"
      },
      "source": [
        "# Command line\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 25 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2LOXn0DOwv8"
      },
      "source": [
        "# Arguments for training from scratch. I turn off evaluate_during_training,\n",
        "#   line_by_line, should_continue, and model_name_or_path.\n",
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"--evaluate_during_training\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OYl04zpPIjS",
        "outputId": "0a00496e-a513-4dea-dba3-99f28ef1682c"
      },
      "source": [
        "!{cmd.format(**train_params)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "08/30/2021 10:12:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "08/30/2021 10:12:23 - INFO - __main__ -   Training new model from scratch\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:892: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "08/30/2021 10:12:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='/content/models/roberta', train_data_file='/content/data/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "08/30/2021 10:12:33 - INFO - __main__ -   Creating features from dataset file at /content/data/train.txt\n",
            "tcmalloc: large alloc 4850917376 bytes == 0x55dd401a2000 @  0x7f89d9fa01e7 0x55dc85095a18 0x55dc85060987 0x55dc851df335 0x55dc85179a48 0x55dc85064252 0x55dc8514296e 0x55dc851799c2 0x55dc85064252 0x55dc85067496 0x55dc851e29c3 0x55dc85063ea9 0x55dc85155c0d 0x55dc850d80d8 0x55dc850d2c35 0x55dc85065fec 0x55dc850a6bc9 0x55dc850a3ac4 0x55dc850648a9 0x55dc850d8b0a 0x55dc850d2c35 0x55dc8506573a 0x55dc850d493b 0x55dc850d3235 0x55dc8506573a 0x55dc850d3b0e 0x55dc850d2c35 0x55dc850d2933 0x55dc8519c402 0x55dc8519c77d 0x55dc8519c626\n",
            "tcmalloc: large alloc 4850917376 bytes == 0x55de61bd4000 @  0x7f89d9fa01e7 0x55dc85095a18 0x55dc850aaf6c 0x55dc85138b93 0x55dc85063ea9 0x55dc85063da0 0x55dc850d82f9 0x55dc850d2c35 0x55dc85065dd1 0x55dc85065280 0x55dc85067208 0x55dc85144141 0x55dc851e29f1 0x55dc85063ea9 0x55dc85155c0d 0x55dc850d80d8 0x55dc850d2c35 0x55dc85065fec 0x55dc850a6bc9 0x55dc850a3ac4 0x55dc850648a9 0x55dc850d8b0a 0x55dc850d2c35 0x55dc8506573a 0x55dc850d493b 0x55dc850d3235 0x55dc8506573a 0x55dc850d3b0e 0x55dc850d2c35 0x55dc850d2933 0x55dc8519c402\n",
            "tcmalloc: large alloc 9701834752 bytes == 0x55df82e06000 @  0x7f89d9fa01e7 0x55dc85095a18 0x55dc850ace06 0x55dc850acae8 0x55dc850ba8bf 0x55dc850ab5eb 0x55dc85138b93 0x55dc85063ea9 0x55dc85063da0 0x55dc850d82f9 0x55dc850d2c35 0x55dc85065dd1 0x55dc85065280 0x55dc85067208 0x55dc85144141 0x55dc851e29f1 0x55dc85063ea9 0x55dc85155c0d 0x55dc850d80d8 0x55dc850d2c35 0x55dc85065fec 0x55dc850a6bc9 0x55dc850a3ac4 0x55dc850648a9 0x55dc850d8b0a 0x55dc850d2c35 0x55dc8506573a 0x55dc850d493b 0x55dc850d3235 0x55dc8506573a 0x55dc850d3b0e\n",
            "tcmalloc: large alloc 19403669504 bytes == 0x55e1c626a000 @  0x7f89d9fa01e7 0x55dc85095a18 0x55dc850acd65 0x55dc850acae8 0x55dc850ba8bf 0x55dc850ab5eb 0x55dc85138b93 0x55dc85063ea9 0x55dc85063da0 0x55dc850d82f9 0x55dc850d2c35 0x55dc85065dd1 0x55dc85065280 0x55dc85067208 0x55dc85144141 0x55dc851e29f1 0x55dc85063ea9 0x55dc85155c0d 0x55dc850d80d8 0x55dc850d2c35 0x55dc85065fec 0x55dc850a6bc9 0x55dc850a3ac4 0x55dc850648a9 0x55dc850d8b0a 0x55dc850d2c35 0x55dc8506573a 0x55dc850d493b 0x55dc850d3235 0x55dc8506573a 0x55dc850d3b0e\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hzFLitkjPLmY",
        "outputId": "c9093ec7-7676-439b-827a-e35f8f8f9866"
      },
      "source": [
        "# !mkdir '/content/drive/MyDrive/Adaptive_pretrain/roberta_models'\n",
        "!cp -R /content/models /content/drive/MyDrive/Adaptive_pretrain/roberta_models/\n",
        "!cp -R /content/runs /content/drive/MyDrive/Adaptive_pretrain/roberta_models/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/runs': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1k6TjnfUL_T"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fill_mask = pipeline(\n",
        "    \"fill-mask\",\n",
        "    model='/content/models/roberta/output',\n",
        "    tokenizer='/content/models/roberta/output'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3USaIskusy",
        "outputId": "ad546faf-316d-4bef-d795-6931a82cefa8"
      },
      "source": [
        "fill_mask(\"Natural language processing (NLP) or computational linguistics is one of the most <mask> technologies of the information age.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'score': 0.0019420207245275378,\n",
              "  'sequence': 'Natural language processing (NLP) or computational linguistics is one of the most. technologies of the information age.',\n",
              "  'token': 18,\n",
              "  'token_str': '.'},\n",
              " {'score': 0.0014330908888950944,\n",
              "  'sequence': 'Natural language processing (NLP) or computational linguistics is one of the most, technologies of the information age.',\n",
              "  'token': 16,\n",
              "  'token_str': ','},\n",
              " {'score': 0.0013845879584550858,\n",
              "  'sequence': 'Natural language processing (NLP) or computational linguistics is one of the most the technologies of the information age.',\n",
              "  'token': 267,\n",
              "  'token_str': ' the'},\n",
              " {'score': 0.0009771912591531873,\n",
              "  'sequence': 'Natural language processing (NLP) or computational linguistics is one of the most of technologies of the information age.',\n",
              "  'token': 292,\n",
              "  'token_str': ' of'},\n",
              " {'score': 0.0002799422072712332,\n",
              "  'sequence': 'Natural language processing (NLP) or computational linguistics is one of the most those technologies of the information age.',\n",
              "  'token': 956,\n",
              "  'token_str': ' those'}]"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8u4quvVmJeP"
      },
      "source": [
        "!cp /content/drive/MyDrive/RoBERTa_base_data/wiki.txt /content/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnB_ykKEvG3t"
      },
      "source": [
        "corpus = []\n",
        "with open('/content/drive/MyDrive/RoBERTa_base_data/combined_corpus.txt', 'r') as f:\n",
        "    for txt in f.readlines():\n",
        "        corpus.append(txt)\n",
        "\n",
        "max_len = int(len(corpus) * 0.8)\n",
        "train_corpus = corpus[:max_len]\n",
        "dev_corpus = corpus[max_len+1:]\n",
        "\n",
        "with open('/content/data/train.txt', 'w') as f:\n",
        "    for txt in train_corpus:\n",
        "        f.write(txt)\n",
        "\n",
        "with open('/content/data/dev.txt', 'w') as f:\n",
        "    for txt in dev_corpus:\n",
        "        f.write(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBk-TBZ_E3Wo"
      },
      "source": [
        "!sed -i '/^ *$/d' /content/drive/MyDrive/Adaptive_pretrain/.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe9CqWwbezW-",
        "outputId": "ca50ce86-b70c-4d34-e00b-df8279887a76"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 35.9 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 51 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 61 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 102 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 112 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 122 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 133 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 143 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 153 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 163 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 174 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 184 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 194 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 204 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 215 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 225 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 235 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 245 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 256 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 264 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▊                             | 10 kB 47.8 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 20 kB 54.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 30 kB 58.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 40 kB 62.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 51 kB 64.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 61 kB 67.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 71 kB 68.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 81 kB 71.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 92 kB 75.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 102 kB 78.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 112 kB 78.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 118 kB 78.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 70.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, fsspec, datasets\n",
            "Successfully installed datasets-1.11.0 fsspec-2021.7.0 xxhash-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMf4vi-hh_Qt"
      },
      "source": [
        "!cat /content/drive/MyDrive/RoBERTa_base_data/cc_news.txt /content/drive/MyDrive/RoBERTa_base_data/openweb.txt /content/drive/MyDrive/RoBERTa_base_data/all_books.txt /content/drive/MyDrive/RoBERTa_base_data/wiki.txt > /content/drive/MyDrive/RoBERTa_base_data/combined_corpus.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQLcrBJviyk-",
        "outputId": "c87da0ea-3fdb-4910-de7a-db6b76b8ff0a"
      },
      "source": [
        "!wc -l /content/drive/MyDrive/RoBERTa_base_data/combined_sents.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50172275 /content/drive/MyDrive/RoBERTa_base_data/combined_sents.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsfz7ZcrjMq7"
      },
      "source": [
        "max_len = int(50172275 * 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf1GI1wclAnk",
        "outputId": "2e454529-b317-4619-c4d6-9fa9972e4ebf"
      },
      "source": [
        "max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40137820"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIkrp1FblTXw",
        "outputId": "37a6d566-927b-4b08-86d3-e9eb10f0e8fa"
      },
      "source": [
        "50172275 - max_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10034455"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOb3D_XmlKL_"
      },
      "source": [
        "TRAIN_SIZE =  40137820#@param {type:\"integer\"}\n",
        "VAL_SIZE =  10034455#@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQJjws5rllqA"
      },
      "source": [
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_base_data/combined_sents.txt) > /content/drive/MyDrive/RoBERTa_base_data/combined_train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_base_data/combined_sents.txt) > /content/drive/MyDrive/RoBERTa_base_data/combined_dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Np9RCfyAmBHL",
        "outputId": "61512e27-1f34-45ac-a69e-309de4b4aa16"
      },
      "source": [
        "!wc -l /content/drive/MyDrive/RoBERTa_base_data/combined_train.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2620530 /content/drive/MyDrive/RoBERTa_base_data/combined_train.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVslZmGYmHrs",
        "outputId": "744315a0-3a57-4770-8fca-52edef78b9ec"
      },
      "source": [
        "!wc -l /content/drive/MyDrive/RoBERTa_base_data/combined_dev.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "655133 /content/drive/MyDrive/RoBERTa_base_data/combined_dev.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEstKu1EmKY1"
      },
      "source": [
        "!rm -r /content/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM_XybYVdlB0"
      },
      "source": [
        "!rm -r /content/runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftJD_8ars1um"
      },
      "source": [
        "!mkdir /content/drive/MyDrive/RoBERTa_base_data/test_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clHhiaIetVcb"
      },
      "source": [
        "!cp /content/drive/MyDrive/RoBERTa_base_data/combined_corpus.txt /content/drive/MyDrive/RoBERTa_base_data/test_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9k2XLpQdnxU"
      },
      "source": [
        "!split -n 10000 /content/drive/MyDrive/RoBERTa_base_data/test_corpus/combined_corpus.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-VasbbZt63J",
        "outputId": "feaeb8d3-0957-4511-d44e-649bd7b6c8a5"
      },
      "source": [
        "!cat xbmf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "annister. So, which Premier League team has a deep seated hatred of Chelsea but are not contenders for the league crown? A good answer is Fulham. The West London club are traditional rivals of Chelsea and are not contenders for the Premier League crown.\n",
            "2 House Tyrell - Manchester City\n",
            "Over the first couple of season's of the Game of Thrones series, House Tyrell of Highgarden were on the periphery of ultimate power of Westeros. After the Battle of the Blackwater, the Tyrell's emerged as one of the dominant houses in Westeros. The most distinguishing characteristic of the Tyrell's is their wealth. They are responsible for feeding most of King's Landing. Another characteristic is their burgeoning rivalry with the Lannisters. The one Premier League team that closely resembles these characteristics is Manchester City. Manchester City were not a dominant team in the Premier League until very recently. In the last few years, however, they have emerged as a power in the English game. Their greatest asset is their wealth. The exchanges between Manuel Pellegrini and Jose Mourinho highlight their growing rivalry with Chelsea, who are the best representatives of House Lannister.\n",
            "1 House Lannister - Chelsea\n",
            "The similarities between House Lannister and Chelsea are striking. House Lannister are one of the dominant houses in Westeros and are distinguished by their wealth and ruthlessness. Chelsea are a dominant Premier League team that are noted for their financial clout and their ruthlessness. Jose Mourinho is a calculating and ruthlessly efficient manager similar to Tywin Lannister. Chelsea's captain John Terry is one of the faces of the team and has a rare talent in that everything he does manages to alienate the public and other teams. This characteristic is very similar to that of Cersei Lannister. Tywin Lannister cannot stand his second son, Tyrion Lannister and struggles to hide his disdain. If that does not describe the relationship between Jose Mourinho and Fernando Torres, I don't know what does.\n",
            "Next Chapter >\n",
            "The Porsche connection\n",
            "One of the things I love about aircooled Volkswagens is the way they share DNA with Porsche models of the same era. Sure, the 356s, 914s and 911s might have been sleeker, sexier and faster than their Volkswagen cousins, but the fact they shared so many traits provided many with the inspiration to build their Beetles into something more than their low price and humble image might suggest.\n",
            "In 1974 when Volkswagen introduced the first generation Golf as the eventual replacement for the Beetle, I’m not sure if anyone knew it would go on to become the icon that is today. Whether serving as basic transportation for people around the world or a platform to enjoy the automotive hobby, the Golf would very much become the people’s car of a new generation.\n",
            "But even after decades of watching people build some genuinely badass Golfs and watching VW itself continually develop faster and more luxurious versions of the iconic hatchback, it’s hard to look at a Golf and a Porsche side by side and see any sort of meaningful connection.\n",
            "These days it’s not that you expect a compact hatchback and a high end sports car to be built from the same stuff, but that connection is one of the things that made aircooled VWs so fun to play with.\n",
            "That brings me to 034Motorsport’s GTI RS – a car which thanks to some pretty gnarly engineering has injected some of that Porsche spirit right back into VW’s popular compact car. And no I’m not talking about having styling influenced by Porsche – this is a Golf that’s both mid-engined and turbocharged.\n",
            "If you think this sounds something like you’d expect to find at a Gatebil event in Scandanavia, you aren’t alone. This is the exact sort of thing those crazy folks up there like to do, but this car isn’t from Northern Europe. It’s actually from right here in California, where 034Motorsport has been building some wickedly quick Audis and VWs since the late ‘90s.\n",
            "The wild recipe\n",
            "When you’re running a high performance shop, the best thing you can do is build something to get people talking – and really, what better way to do that than by doing something crazy like a mid-engined VW Golf?\n",
            "So starting with a basic 2001 GTI, the team at 034Motorsport got to work creating one of the craziest Golfs the world has ever seen.\n",
            "As you’d expect, converting a front-engined, front-wheel drive car to mid-engined, rear-wheel drive is not the easiest of tasks, and the first thing the guys did was fabricate a custom rear tubular subframe using suspension components from a Porsche 996 Turbo.\n",
            "Along with the 996 suspension, the new subframe was also built with custom pickup points, custom mounts for the engine and transmission mounts, and it even includes the stock 996 Turbo rear sway bar.\n",
            "Next up came the task of selecting and building an engine that would occupy the space where the GTI’s rear passenger area once was. Given 034’s experience with Audi, it’s not surprising that they went with a 2.7 liter turbocharged V6 from a 2001 S4.\n",
            "Not only was the S4 motor a perfect fit with the shop’s image, but in many ways the relationship between VW and Audi today is the same as that of VW and Porsche way back when. You might be buying an entry level Golf, but you don’t need to look far to see the resemblance to the sexy Audis in the dealership down the street.\n",
            "Of course the S4 motor in this particular car is far from factory spec. For starters, it’s been bored and stroked to a displacement of three liters and the bottom end has been fully strengthened with a forged crank, JE pistons and other heavy duty bits. Up top, there are ported 2.8 liter heads from a B5 A4, a valvetrain from Supertech and a set of high performance CAT cams.\n",
            "The big story though, is the turbo set-up, which depending on the driver’s mood can be switched between two different snails: either a Precision Turbo CEA 62mm GT35R or a Garrett GT42RS.\n",
            "Either way, the turbo sits on a custom 034 manifold and is backed by a one-off intake and exhaust set-ups along with a custom air-to-water intercooler. The whole set-up is then controlled by an 034EFI stand-alone ECU with two different maps available. With the GT42 turbine fitted, the GTI-RS made an impressive 807 horesepower and 596 foot pounds of torque to the rear wheels.\n",
            "It also makes for a pretty wicked sound, as you can see in this short video clip that Larry grabbed when he shot the car.\n",
            "Not quite a sleeper\n",
            "And how exactly does all that power get to the rear wheels? It goes through an Audi 01E six-speed transmission with custom ratios, an 034 Motorsport LSD and heavy duty axles from the Drive Shaft Shop.\n",
            "As for the suspension, the Golf is equipped with custom-valved Ohlins three-way dampers in the front and rear with H2Sport front spindles and control arms up front.\n",
            "Braking duties are handled by the factory 996 Turbo brakes in the rear, while the front calipers have been swapped out for those from a 993 Turbo, along with custom 034 Motorsport rotors.\n",
            "In keeping with the Porsche theme, the car is running a set of 996 Turbo ‘hollow spoke’ wheels 18”x8 in the front and 18”x11 in the rear with Continental ExtremeContact DW tires all around.\n",
            "If someone asks to pop the hood, they’ll find the place where the GTI’s original engine once sat is now occupied by a large ATL fuel cell.\n",
            "While the body of the GTI has for the most part been kept stock, I’m not quite sure if you could call this thing a sleeper. Especially when you see things like the big vents in front of the rear wheels or the widebody conversion, which was custom-built out of metal by the crew at 034.\n",
            "Inside, the cockpit is surprisingly original with an original dash, factory GTI leather seats and an R32 steering wheel.\n",
            "But if somehow you didn’t realise there was a wild turbocharged motor sitting behind you, the NHRA legal roll hoop and rear chassis reinforcement might raise some suspicions.\n",
            "In the end, there’s a lot to like about 034’s crazy project. First off there’s the fact that this GTI is both very unusual and very fast – two things we always dig.\n",
            "Then there’s the idea that these guys have taken a basic Volkswagen and mixed it perfectly with some high tech Audi horsepower and some old fashioned Porsche DNA. This is really what Speedhunting is all about.\n",
            "Mike Garrett\n",
            "Instagram: speedhunters_mike\n",
            "mike@speedhunters.com\n",
            "Photos by Larry Chen\n",
            "Instagram: larry_chen_foto\n",
            "larry@speedhunters.com\n",
            "Grant Robertson.\n",
            "Labour finance spokesman Grant Robertson wasted no time yesterday taking aim at the Government’s financial performance after economic growth figures came in lower than expected.\n",
            "The second quarterly fall in gross domestic product (GDP) per person growth showed the need for a fresh approach to give all New Zealanders a fair share in prosperity, he said.\n",
            "Statistics New Zealand figures showed GDP rose 0.5% in March, following an increase of 0.4% in December.\n",
            "Much lower building activity, combined with mixed results for the service sector, took the shine off higher dairy production. That meant a second quarter of moderate overall GDP growth, national accounts senior manager Gary Dunnet said.\n",
            "At an industry level, 11 out of 16 industries increased in the quarter. Agriculture and retail trade had the largest increases. Construction was significantly down.\n",
            "GDP per capital decreased 0.1% in March following a 0.2% fall in the December quarter, he said.\n",
            "Annual GDP growth for the year ended March was 3% and the size of the economy in current prices was $265 billion.\n",
            "Mr Robertson said National was asleep at the wheel after nine years in Government.\n",
            "\"The economy is going backwards on a per person basis, the export sector is in recession and, now, construction is falling, too.\n",
            "\"Relying on population growth and an overheated housing market to prop up the economy is a dangerously complacent approach.\"\n",
            "The fall in the construction sector was particularly worrying, he said.\n",
            "There was already a shortage of 60,000 houses and that was growing daily. Houses were needed more than ever but building sector activity was shrinking.\n",
            "The complete failure to address the housing crisis was National’s biggest economic legacy, Mr Robertson said.\n",
            "Finance Minister Steven Joyce said the moderate GDP growth during the past six months was a reminder every economic gain was hard won in what was still a challenging international environment.\n",
            "\"This is not the time to rest on our laurels and start considering policies that would damage growth in key sectors or across the economy.\"\n",
            "The GDP figures followed on from Wednesday’s release of New Zealand’s external accounts for the March year, which showed a current account deficit of 3.1% of GDP, the same as the March 2016 year.\n",
            "ANZ senior economist Philip Borkin said the GDP result was well below consensus expectations and the Reserve Bank’s forecast of 0.9% and represented only a mild rebound from the soft end to last year.\n",
            "Mr Borkin acknowledged the poor growth in per capita terms but took a more sanguine view.\n",
            "Migrants, the key driver of population growth, did not instantly slot into the economic system. Like any worker who started a new job, they took time to fit in and deliver added value.\n",
            "The disruptive impact of natural disasters should also not be discounted when viewing the economy’s disappointing productivity performance, he said.\n",
            "Hours worked contracting in the first quarter implied productivity growth did post a \"decent bounce\".\n",
            "\"Stepping back, although the headline result is a clear disappointment, we don’t believe it is a true reflection of growth momentum across the economy at present.\"\n",
            "The lack of reaction on the part of the New Zealand dollar to the figures suggested the market was taking a similar view.\n",
            "Importantly, putting aside the historical nature of March’s figures, forward indicators remained positive overall, he said.\n",
            "Despite a backdrop of softer housing activity, difficulties finding staff and increasing capital constraints, momentum was likely to accelerate in the quarters ahead to a rate that would continue to eat into spare capacity.\n",
            "The figures allowed the Reserve Bank to remain cautious and watchful, Mr Borkin said.\n",
            "At a glance\n",
            "• Economic growth slowed again in March.\n",
            "• Agricultural production rose by 4.3%.\n",
            "• Poor growth in per capita terms highlighted.\n",
            "• Forward momentum expected to be strong.\n",
            "The Elder Scrolls Online Tamriel Unlimited (PC/Mac) The Elder Scrolls Online: Tamriel Unlimited brings the world of Elder Scrolls to a brilliantly crafted MMORPG. Buy The Elder Scrolls Online Tamriel Unlimited (PC/Mac) and get your Key for PC activation within minutes via email! Use the keys to access your highspeed download of your received game.\n",
            "Don't miss out this offer, buy The Elder Scrolls Online Tamriel Unlimited (PC/Mac) now!\n",
            "About The Elder Scrolls Online Tamriel Unlimited (PC/Mac)\n",
            "The Elder Scrolls Online: Tamriel Unlimited, the latest chapter of the award-winning series, brings the legendary experience online for the first time. Explore the vast world with friends or embark upon the adventure alone - the choices you will make will shape your destiny.\n",
            "The award-winning fantasy role-playing series, The Elder Scrolls goes online – no game subscription required. Experience this multiplayer role-playing game on your own or together with your friends, guild mates, and thousands of alliance members. Explore dangerous caves and dungeons in Skyrim, or craft quality goods to sell in the city of Daggerfall. Embark upon adventurous quests across Tamriel and engage in massive player versus player battles, or spend your days at the nearest fishing hole or reading one of many books of lore. The choices are yours in the persistent world of The Elder Scrolls Online: Tamriel Unlimited.\n",
            "Purchase the game and enjoy your adventures without a monthly fee. Three alliances vie for control of the Empire. As these powers battle for supremacy, darker forces move to destroy the world. Battle, craft, fish, steal, siege, or explore. The choice is yours to make in a persistent Elder Scrolls world. Adventure alone, quest with friends, or join an army of thousands in epic PVP battles.\n",
            "You've never seen Elder Scrolls quite like this, and through beauty of no subscription, and a vast world to explore, the possibilities are endless. The Elder Scrolls Online: Tamriel Unlimited is what fans have wanted for a long time. Total freedom to shape the game in any way they want, and the power to choose not only their character, but their path free of pushing or pulling from outside forces.\n",
            "Plus, with the latest DLC like the Homestead pack, the game will be brimming with content for some time. So join in the Elder Scrolls experience and see what the world of Tamriel is really like. The world is waiting for you.\n",
            "The Elder Scrolls Online Tamriel Unlimited (PC/Mac) Features\n",
            "NO SUBSCRIPTION REQUIRED - Purchase the game and enjoy your adventures without a monthly fee\n",
            "THE FIGHT FOR TAMRIEL BEGINS! - Three alliances vie for control of the Empire. As these powers battle for supremacy, darker forces move to destroy the world\n",
            "PLAY THE WAY YOU LIKE - Battle, craft, fish, steal, siege, or explore. The choice is yours to make in a persistent Elder Scrolls world\n",
            "A MULTIPLAYER RPG - Adventure alone, quest with friends, or join an army of thousands in epic PVP battles\n",
            "TELL YOUR OWN STORY - Discover the secrets of Tamriel as you quest to regain your lost soul and save the world from Oblivion\n",
            "NEW DELHI: An across-the-board increase in railway fares is unlikely to be announced in the Railway Budget on Thursday, with Railway Minister Suresh Prabhu set to rely on higher support from the main Budget, joint ventures with the private sector and soft financing from other countries to put the financially derailed organisation back on track.As Prabhu, whose appointment as railway minister last November triggered hopes of radical change in a ministry long regarded as a fount of populist thinking, prepares for what arguably will be the biggest day in his ministerial career so far on February 26, sources have told ET that the fare hike option had all but been junked for now and the overall thrust would be incremental.“An across-the-board hike is unlikely since passenger growth has been negative and even freight has not grown as per expectations,” one official involved with the budget-making process told ET. However, “minor adjustments” in fares of Tatkal and premium special trains that run on dynamic pricing could not be ruled out, the official added. He requested anonymity given the sensitivity of the issue.Faced with a near-bankrupt organisation, Prabhu, whose budget speech is being closely watched for clues on the tone and thrust of the Union Budget on Saturday, is believed to have taken the longest time to decide whether fares should be increased or not. He left office late on Monday after giving final touches to the Railway Budget for 2015-16.The budget document went for printing early Tuesday morning. Known to be a hardworking ‘technocrat’ minister, accountant-turned-banker-turned politician Prabhu has averaged 16-hour days since taking charge of the ministry. He worked through the last weekend, wrestling to balance traditional populism with much-needed reforms that are expected from him.The sources said Prabhu was personally in favour of a fare increase to generate much-needed resources, but senior railway officials managed to convince him that it would be difficult to justify such a move given that fares had been raised steeply only last year.“Fares were increased by the government in June last year, three weeks before the then railway minister Sadananda Gowda presented his budget. It was a steep hike — 14.2% in passenger fares and 6.5% in freight. With diesel prices down, the government does not have a basis to go for a hike,” one official said.He said it would be difficult for the minister to explain politically that even though diesel prices are down, the fuel bill of railways has gone up because of high electricity rates. In any case, officials said a fare increase, if it became absolutely unavoidable, could always be done anytime during the year and attributed to the Fuel Adjustment Component.But Prabhu faces a tough challenge to raise resources to fund critical infrastructure projects.Officials said the ministry is likely to seek a Gross Budgetary Support of Rs 50,000 crore, up from Rs 38,000 crore last year. Prabhu will also take the joint venture and publicprivate partnership (PPP) route to raise another Rs 70,000 crore and is likely to go in for bilateral financing and technological support from countries such as China Japan and France, all of which are major suppliers and have evinced interest in participating in India’s railway expansion.Railway ministry sources said Prabhu could resort to bilateral cooperation with interested nations to take up specific initiatives such as upgrade of network, safety-related infrastructure, dedicated freight corridors and high-speed trains.Prabhu has received a lot of suggestions from the prime minister’s office and officials spoke of him frequently consulting a blue plastic folder marked ‘Prime Minister’s Office’. Prime Minister Narendra Modi has in the past spoken of the railways being a lynchpin in India’s economic progress as well as its political integration.Sources said the railway minister was also unlikely to announce any major organisational reform. “He is likely to go in for small incremental steps rather than corporatisation in a single stroke,” one official in the Railway Board said.Prabhu’s immediate focus will be to finish existing projects rather announce new big-ticket schemes.He needs at least Rs 1.8 lakh crore just to complete 359 pending rail projects. Even among the new lines, the emphasis is likely to be on improving connectivity in border areas, especially in the Northeastern states along the border with China.“They will be mostly announced as national or strategic lines since the railways don’t have to pay for such lines from their own budget,” one official said.With foreign investors yet to show any interest in railway projects under the FDI route, Prabhu has been eyeing other sources of funds. He has asked the finance ministry to allow the railways access the Rs 6 lakh crore of public pension funds. He also made a case for channeling corporate social responsibility funds for railway schemes.Prabhu is also likely to announce a slew of green initiatives, notably the use of solar energy, CNG, setting up water recycling plants and waste-to-energy projects.\n",
            "Many graduates have been shocked this week to see just how their debt is escalating, with interest charged at up to 3.9%. That’s more than the typical rate on a first-time buyer mortgage. Have they been mis-sold a dodgy loan?\n",
            "University of Nottingham graduate Simon Crowther’s post on Facebook went viral this week, after he revealed how much interest is being added to his debt. He’s part of the first wave of graduates to have left university after paying £9,000-a-year fees. His total debt, a year after leaving college, jumped to £41,976 by the end of March, with the interest racking up by as much as £180 a month. Crowther claims he was mis-sold the loan and “cheated by a government who encouraged many of us to undertake higher education, despite trebling the cost of attending university”.\n",
            "Judging by the huge response to his post, a lot of recent graduates feel the same way. But were loans actually mis-sold? When the Financial Ombudsman Service looks at product sales, a number of tests will apply. Was the individual given suitable advice? Were the risks explained? Were you given the information needed to make a proper decision?\n",
            "Crucially, this is all about what you are told at the point of sale. For students, that means when they were just 17 or 18 and at school.\n",
            "But teachers are not regulated financial advisers, and nor should they have to be. They are naturally keen to take as many pupils as possible through to university; the money discussion barely comes into it. When my colleagues on the Money desk spoke to teachers this week, we found some who were just as bewildered as Crowther when it came to the interest rate applied.\n",
            "Maybe 17- and 18-year-olds should be reading the terms and conditions more carefully. Ignorance, as lawyers like to say, is no defence. As one student wrote below Crowther’s post: “I was always under the impression that the interest rate would be inflation plus 3% – no prospective students were lied to. Yes, this is an extremely high interest rate but we have not been led on to believe it would be anything less.”\n",
            "Other recent graduates recall the loan details with much less perspicacity. “It was years ago, I was only 17, I was out boozing, I can’t remember …” sums up the views of ones I spoke to. Should they be condemned for that? Before we older graduates rush to judge, remember we were the lucky generation that never had to pay tuition fees.\n",
            "Let’s turn to the explanation of risks. First, students can’t be certain how much they have to repay. It will be whatever the RPI inflation figure is in the future, plus up to 3%. The September 2015 RPI figure was 0.9%, which is why some graduates face an interest rate today of 3.9%. In April it was 1.3%, and if it stays that way the student rate will rise to 4.3%. Borrowers who take out a mortgage or personal loan can fix their rates, but students can’t, and have to carry an unfair risk of future rises.\n",
            "Second, the government has linked repayments to RPI, which is generally higher than CPI, and which even the ONS says is no longer a proper national statistic. It is a great irony that the government is suggesting Tata Steel can slash the cost of its pension scheme by switching to CPI from RPI, yet young adults – burdened by huge debts and absurd rents and house prices – are told RPI is what they must pay.\n",
            "The most serious case for mis-selling rests on a decision taken by George Osborne last November. In 2010 the government committed to uprate the £21,000 repayment threshold in line with average earnings. But in November 2015 it was frozen for five years, a move that will affect current students and graduates who took out post-2012 loans, including Crowther. A change to loan conditions, made after a loan is taken out? A mortgage company can’t legally do that to borrowers – so why is the government doing it to students?\n",
            "Many Christians, unfortunately, believe their faith requires a \"first man\" who sinned and brought trouble on the world (feminists can thank two millennia of patriarchy for getting the \"first woman\" off the hook). The central Christian theme is \"Creation-Fall-Redemption\": God creates a perfect world; Adam \"falls\" by sinning, wrecks everything, and God curses the creation with death and suffering; and Christ redeems the world. In this picture Adam and Christ function as symmetrical \"bookends\": Adam breaks everything and Christ fixes it.\n",
            "In the book I show how Adam evolved from an obscure character in Jewish literature to the \"Central Myth of the West,\" and now stands as an almost impenetrable barrier to millions of Christians accepting modern science.\n",
            "I was reminded of this a few days ago when Beacon Press released my book, Saving the Original Sinner: How Christians Have Used the Bible's First Man to Oppress, Inspire, and Make Sense of the World . The book was inspired by the controversy raging in evangelical Christianity about whether Adam and Eve were actual historical figures. Recent advances in genetics have made it quite clear that the human race never consisted of just two individuals and Christian geneticists have been working overtime trying to convince their respective faith communities they need to pay attention. Most of them are not.\n",
            "Equating science with atheism is one of the most dangerous byproducts of America's culture wars. This strange polarization portends disaster, as the country divides into factions that cannot find common ground on the way the world operates. And it goes without saying that there will be no agreement on what should be done when scientifically significant issues need political action.\n",
            "Adam and Eve thus stand on the bullseye of the controversy about evolution—a controversy that has taken on new urgency over the past few decades as the human genome has been mapped. This progress has established with near certainty that humans are closely related to chimps and bonobos, with whom they share a common ancestor; that the human race originated in Africa millennia before the events in Genesis took place; and that the human race never consisted of only two people. The conclusion is clear: The couple described in the opening pages of the Bible never existed—and thus could not have precipitated the disaster known as \"The Fall.\"\n",
            "Without Adam, the traditional formula that has long defined Christianity must be reinvented and many Christians are convinced that this is impossible. Millions of Americans would prefer to reject science, rather than bid farewell to the first man: \"The denial of an historical Adam and Eve as the first parents of all humanity and the solitary first human pair,\" warns the influential and widely followed Southern Baptist theologian Al Mohler, \"severs the link between Adam and Christ which is so crucial to the Gospel.\"\n",
            "I was reminded of the starkness of this division this week when the first discussion of my book began to appear. The journal Books & Culture ran an interview with me discussing the book and the first comment to appear was the following:\n",
            "\"All this is a good example of believing man's word instead of God's Word. That is, making a compromise with atheists.\"\n",
            "If this were a single voice it could be ignored. But the sad reality is that this view runs through much of evangelical Christianity in America. It has taken up residence in the GOP, where denying various sciences—evolution, geology, climate science—has become a de facto requirement for election. Many evangelical colleges have it in their faith statement. Public school teachers find themselves embroiled in controversy simply teaching the material in the Biology text. Ken Ham's entire Answers in Genesis project is based on it. The starting point for so many Christian has become the absolute truth of a particular interpretation of the Genesis creation story. And any alternative viewpoint is now understood to be a \"compromise with atheists.\"\n",
            "About the Author\n",
            "Karl Giberson teaches science and religion at Stonehill College and is a leading voice in America’s creation/evolution controversy. He is the author of ten books, including Saving Darwin, a Washington Post “Best Book of 2008,” Saving the Original Sinner: How Christians Have Used the Bible's First Man to Oppress, Inspire, and Make Sense of the World, and The Anointed: Evangelical Truth in a Secular Age, with Randall Stephens. He lives in Hingham, Massachusetts. He lives on the web at www.karlgiberson.com. Follow him on Twitter at @gibersok.\n",
            "Bharatiya Janata Party (BJP) leader Giriraj Singh on Tuesday rejected reports appearing in the media of him crying before Prime Minister Narendra Modi and seeking forgiveness for his recent racist remarks against Congress Party president Sonia Gandhi.\n",
            "He said that he had had no meeting with the Prime Minister, and questioned as to who saw him crying at that reported meeting.\n",
            "He said and asked, \"I had no meeting with the PM and who said I cried? Who saw?\"\n",
            "Union Minister Giriraj Singh on Tuesday invited wrath from Prime Minister Narendra Modi. This comes a day after Giriraj expressed regret in Lok Sabha for his racist remarks against Congress President Sonia Gandhi.\n",
            "Sources also revealed that Girirak Singh broke down after being rebuked by PM Modi\n",
            "However, when asked about the reprimanding, Giriraj Singh denied and maintained that nothing of such sort has happened.\n",
            "The minister of state for micro, small and medium enterprises was under fire for his racist barb at Gandhi asking whether Congress would have accepted her leadership had she not been white-skinned.\n",
            "“Had Rajiv Gandhi married a Nigerian woman and if she was not a white-skinned woman, would the Congress have then accepted her (Sonia’s) leadership,” he had said on March 31.\n",
            "Blade Runner in the style of ‘Starry Night’ by Van Gogh\n",
            "Video: https://vimeo.com/168731830\n",
            "Here’s a few short clips from the 1982 scifi classic Blade Runner rendered in the style of Starry Night by Van Gogh (1853-90). I’m in love with the world that Syd Mead and Doug Trumbull created for the movie, and I think it’s strange but satisfying seeing some of the special effects rendered using brush-strokes.\n",
            "To create these I used a hacked-up version of Style Transfer by the indestructible Frank Liu. The technique is an implementation of A Neural Algorithm of Artistic Style which uses Deep Neural Networks to copy the artistic rendering from one image to another.\n",
            "Feel free to ping me with any questions here or @bhautikj on twitter. I’ve got a few more of these coming :]\n",
            "Youngsters at Vinnie’s Theatre School in Northampton got the chance to meet a real life version of the Disney princess Elsa.\n",
            "The Snow Queen from the hugely popular film Frozen visited the theatre school, based in The Mounts, on Tuesday morning.\n",
            "Elsa from Frozen visited Vinnie's Theatre School in Northampton\n",
            "The children were able to get their picture taken with Elsa and were taught how to perform one of her songs.\n",
            "Lyndsay Aldred, who runs the theatre school, said the children had been very excited to meet the a real life version of the Disney princess.\n",
            "She said: “Hardly any of them kissed their mum’s goodbye this morning as they were so excited about meeting Elsa.\n",
            "“They know all the words to the songs and have watched the film numerous times.”\n",
            "Vinnie’s Theatre School offers place to children aged from two-and-a-half upwards. For more information or to book a place email vinniesagency@aol.com.\n",
            "Looking for news you can trust?\n",
            "Subscribe to our free newsletters.\n",
            "Puerto Rico Gov. Ricardo Rosselló said Friday that Republicans’ decision to leave a pair of provisions in the tax reform legislation that experts say will hammer Puerto Rico’s already struggling economy was “unconscionable.”\n",
            "“It is devastating and unconscionable that Congress would do this at this juncture,” Rosselló told NBC News after it was clear the provisions remained in the bill.\n",
            "Republicans released the latest version of their bill Friday evening and plan to vote it out of the House and Senate early next week. The tax bill, as written, would include taxes on payments between US companies and their foreign subsidiaries and profits from intellectual property. At a Friday news conference in San Juan, Rosselló called the tax reform plan “a huge blow for Puerto Rico,” according to Caribbean Business, and, the paper writes, the bill would have an “adverse impact” on 50 percent of the island’s gross domestic product, 30 percent of the government’s revenue, and 250,000 direct and indirect jobs.\n",
            "Rosselló’s administration estimates that recovery from Hurricanes Irma and María will cost roughly $95 billion. The island was already grappling with more than $72 billion in outstanding debt, $49 billion in unfunded pension liabilities, and a 45 percent unemployment rate, the result of a decade-long economic downturn. That crisis had already fueled an exodus of roughly 400,000 people over the last decade, a trend that has only intensified since the hurricanes.\n",
            "Carlos Mercader, the director of Rosselló’s office in Washington, DC, tells Mother Jones the Republican decision was “shameful” and reflects the island’s lack of status in Washington when big decisions are made. “We’ve had many congressmen coming down to Puerto Rico, visiting the island, being amazed by the incredible devastation,” says Mercader. “They’ve been the most empathetic people in the world. But it’s all words, no actions. When actions need to come, this is what they do.”\n",
            "In an attempt to make it harder for US companies to avoid US taxes via foreign subsidiaries, the bill would impose a 20 percent excise tax on payments from US companies to their foreign subsidiaries. For tax purposes, the IRS sometimes considers Puerto Rico and the other territories foreign countries. That shift could cause the US pharmaceutical industry, which generates billions of dollars in revenue and employs tens of thousands of workers on the island, to shift production out of Puerto Rico.\n",
            "According to BloombergPolitics, the way the current law works allows US companies to buy their own products from Puerto Rican subsidiaries and avoid regular income taxes, and pay just 4 percent in excise tax to the island’s government, as long as the money from that subsidiary is kept offshore. The arrangement has been a “paradise” for US drug makers and, the Food and Drug Administration estimates that drug companies and medical device manufacturers account for nearly 30 percent of the island’s GDP.\n",
            "Rosselló said Republicans like House Speaker Paul Ryan (R-Wis.) and Sen. Marco Rubio (R-Fla.) “turned a blind eye” on Puerto Rico. “I will be very active and I’m sure my colleagues will be very active, in different Puerto Rican populations or Latino populations and make sure everyone knows we were treated as second-class citizens,” the governor said.\n",
            "A Study in Emerald\n",
            "A Study in Emerald is a board game inspired by the Neil Gaiman story of the same name. The premise is simple, the year is 1881 and the ‘Old Ones’ have been ruling the earth for over seven hundred years. Although most of humanity has accepted these monstrous rulers, there is a growing underground movement to overthrow the regime, labeled the Restorationists. A secret war is being fought around the cities of the Europe and the New World between agents of the Restorationists and those loyal to the powers that be.\n",
            "The award winning short story combines the worlds of Sherlock Holmes and H.P Lovecraft. The board game has gone one step further to bring in real historical figures to flesh out this alternate reality, allowing players to examine an interesting ‘what-if’ situation.\n",
            "At the beginning of the game you are assigned a secret identity, which will either be that of a Restorationist or a Loyalist. Your identity matters as it determines what you need to do to achieve victory.\n",
            "There is a deck-building component to the game. You start with a deck of ten cards and you add to this set as the game goes on. There are twelve decks of cards on the board, one in each city. You can gain these cards through the use of influence cubes, in what is in effect a bidding system. As an action you can place influence cubes on a card that you would like to draft. If at the beginning of your next turn you have the most influence on that card then you can choose to claim it and place it on your discard pile. You can also use influence to gain control of cities, which will earn your victory points as well as the benefits on the City card.\n",
            "Many of the cards that are available to draft are agents. When you draft one of these cards you also take control of the agent counter, which remains on the board. Agents have a range of uses, from acting as additional influence to allowing you to assassinate ‘royal’ persons or other agents.\n",
            "The 1880s was a period of political unrest, with anarchist factions turning to violence as a means of achieving their ends. It is no different in this game, problems are generally solved with dynamite. The Restorationists wish to assassinate their nightmare rulers, while Loyalist’s hunt them down and try to kill them in turn.\n",
            "Another aim of the Restorationists is revolution. If they can convince the people to rise up and attack their masters then the world, possibly, can be set to rights. The Loyalists also have an agenda, which is to engineer a world war. By doing so they will create enough madness to feed their masters, giving them the strength to bring more of their kind to Earth from the terrible dimensions in which they reside.\n",
            "There are many directions a player can take in this game, some of which will be determined by the cards that are available. As some cards are removed from the set at the beginning of the game and the rest are arranged in random piles then no two games will be exactly the same. The cards allow for the appearance of zombies and vampires (the inclusion of both can be justified, as will be stated in the player notes). You can employ the services of the Russian secret police, otherwise known as the Okhrana, to harass your opponents, or call on the Old Ones themselves to appear. You must be careful, though, when dealing with such terrible forces as your sanity will be tested to its limit.\n",
            "To win the game it helps to know the identities of your fellow players. When the game ends all players reveal their identities. If it is a four-player game then each side (Restorationist and Loyalist) totals their points and the side with the lower score is eliminated. If there is an odd number of players then the side which has lowest scoring individual player is eliminated. The remaining player with the highest score is then declared the victor. Thus you have to be careful to end the game when you are assured of victory. It will do you no good if you are ahead on points while another player languishes in last place, as he will bring you down with him.\n",
            "A Study in Emerald is designed for two to five players and should take around two hours to play.\n",
            "Errata\n",
            "Unfortunately there is a minor printing error in A Study in Emerald. The reverse (vampire) sides of William Morris and Wilhelm Stieber have been accidentally transposed. This only becomes an issue if one or the other agent becomes a vampire, which will not actually happen that often.\n",
            "For the moment we can offer you a hi-res PDF download which you can use to produce a stand-in counter or stick on the back of the counter. (Right-click (control-click on a Mac) the image and choose “Save Link As…” to save the zip file to your computer.)\n",
            "I have looked into producing replacement counters but this cannot be done until the end of January next year. The big problem is shipping out these counters to individual addresses, which would be very expensive. I am looking at various options, such as using BGG or having the counters included in a range of future releases.\n",
            "SANDY, Utah — The smallest player in Major League Soccer came up huge Saturday night.\n",
            "Joao Plata, all 5-foot-2 of him, assisted on both Luis Gil and Javier Morales' goals in Real Salt Lake's 2-0 victory against the Vancouver Whitecaps in front of 17,480 at Rio Tinto Stadium.\n",
            "Gil headed home Plata’s cross just two minutes after halftime, and Morales put the game out of reach with 20 minutes still on the clock as RSL moved to 4-4-2 and Vancouver dropped to 2-4-3 after starting 2013 with back-to-back wins.\n",
            "Perhaps it was the Canadian side coming out with tired legs after putting in so much effort to beat Edmonton in their Amway Canadian Championship semifinal second leg on Wednesday, but the opening minutes belonged to RSL.\n",
            "CHECK OUT THE FULL LINEUPS AND BOX SCORE\n",
            "At the half-hour mark, the possession was in favor of the home side – a dominant 72 percent – with the RSL midfield connecting and combining to great effect. Still, once either attack hit the final third, things broke down.\n",
            "The first shot on goal of the match didn't come until the 32nd minute. And while RSL had only two shots on target in the first half, Vancouver finished without an attempt on goal and only four total shots.\n",
            "If the opening half was a little lackluster, the second half began with a bang. Real burst from the locker room with a goal in the 47th minute when Nick Rimando delivered a ball that perhaps no other 'keeper in the league could have provided.\n",
            "From an goal kick, Rimando opted for a quick restart and hit the ball into space 60-yards upfield to an open Plata. The diminutive Ecuadorian settled and hit a beautiful, bending ball to the center of the box. Gil, meanwhile, timed his run to perfection and beat goalkeeper Joe Cannon to the ball, heading it in off the right post and into the net for the 1-0 lead.\n",
            "Unfortunately, Gil was injured on the play as he and Cannon collided. The scorer subbed out only minutes later for Sebastian Velasquez.\n",
            "The goal opened things up as Vancouver began pressing for an equalizer, and the Whitecaps came inches away from leveling things in the 62nd minute.\n",
            "OPTA Chalkboard: 'Caps flood box with crosses but fail to replicate RSL's final touch\n",
            "A long ball found Corey Hertzog and with Chris Schuler chasing, the striker got a touch on the ball just as Rimando and the trailing defender reached him. The ball snuck through and was headed toward the net before a sliding Nat Borchers cleared it off the line.\n",
            "Vancouver continued to press and created a few more quality chances in the process, but left their lines stretched and opened themselves up to the counterattack.\n",
            "RSL took advantage of the space and scored the insurance goal in the 71st minute. Morales started and finished the play, sending a ball wide to Velasquez, who found Plata in time for the midfielder to deliver a ball directly to the feet of the Argentine. All it took from there was a professional finish to tuck the ball away into the bottom left corner.\n",
            "Following the win, Real will now prepare for a midweek clash at New England on Wednesday. Vancouver will host the defending MLS Cup champions LA Galaxy next Saturday.\n",
            "MLSsoccer.com Men of the Match\n",
            "Spain's banking association announced on Monday it would freeze eviction orders for the next two years in cases of \"extreme hardship\", following widespread alarm and protests after a woman killed herself on Friday moments before she was due to be evicted, the second such death in less than a month.\n",
            "\"This cannot be allowed to go on,\" said Juan Carlos Mediavilla, a judge who attended the scene after Amaia Egaña, 53, leapt from her fourth-floor flat in the northern city of Bilbao.\n",
            "\"It's a problem which has been talked about for some time. The time for talk is over and steps must be taken for something to happen.\"\n",
            "Within hours of Egaña's death noisy protesters had gathered on the streets of Bilbao. Stickers saying \"murderers\" were fixed to cash machines, while the governing People's party and opposition Socialists pledged to hold an emergency meeting on Monday to agree on reforming mortgage laws.\n",
            "Maria Cumbicus, who has fallen behind on her mortgage payments since she lost her job as a cleaner three months ago, is in danger of joining the list of 400,000 who have been evicted in Spain since a property boom ended in 2008 and the country sank into economic crisis.\n",
            "\"It is just terrible never knowing if you'll be able to get back into your home every time you go out,\" said Cumbicus, 50, who is originally from Ecuador, one of many who moved to Spain when its economy boomed but have since fallen on hard times in a country where one in four are on the dole. Her husband was forced to return to Ecuador because he was self-employed and not entitled to unemployment benefit after losing his job.\n",
            "Cumbicus's flat on the outskirts of Pamplona, a city famous for its annual bull-running festival, was put up for auction in court on Mondaymorning, but no bids were accepted, so she now has 20 days in which to negotiate with the bank and see if she can transfer the property in lieu of payment. Under Spanish law, mortgage holders are still liable for outstanding debts even if they are evicted.\n",
            "\"All these years I've been paying and now I'm going to be on the street,\" she said. \"Halting evictions is all very well after these suicides, but there is very little you can do on your own unless someone comes to your aid.\"\n",
            "Spain's banks have come under fire from protesters and opposition politicians for continuing to carry out evictions even after some received part of a European bailout negotiated by Madrid which could amount to €100bn (£80bn).\n",
            "Details of the cross-party deal to reform the mortgage law by decree remain under discussion, but the minister for the economy, Luis de Guindos, said: \"No family of good faith should become homeless because of the crisis.\"\n",
            "The Platform of those Affected by Mortgages (PAH) grassroots group, which has been campaigning to change the law for nearly three years, says a moratorium on evictions would not be enough.\n",
            "\"This measure would not affect foreclosures under way and so leave out hundreds of thousands of families still swamped by proceedings. We demand an immediate halt to all foreclosures, as long as they affect first homes and debtors in good faith,\" a PAH statement said.\n",
            "The United Police Union said it would back any of its members who conscientiously objected to enforcing eviction orders.\n",
            "\"We're not robots, we're human beings and this is like the soldier in a firing squad who refuses to shoot, even knowing he will take the place of the one to be shot,\" said Jose Manuel Sanchez, the union's secretary general.\n",
            "I assume this Washington Post story is true: “FBI obtained FISA warrant to monitor former Trump adviser Carter Page.” It confirms what has been sporadically reported since late last year, that the Obama administration sought and ultimately received a FISA order to spy on at least one associate of Donald Trump. So Trump’s famous tweets were, in substance, true.\n",
            "The FBI obtained a secret court order last summer to monitor the communications of an adviser to presidential candidate Donald Trump, part of an investigation into possible links between Russia and the campaign, law enforcement and other U.S. officials said.\n",
            "Do the leaks come from the same Obama administration holdovers who have leaked in the past, trying to get ahead of disclosures that will confirm that President Trump’s suspicions were correct? Or do they come from officials appointed by Trump? I don’t know, but the Post’s illicit sources are pretty much always Democrats.\n",
            "The FBI and the Justice Department obtained the warrant targeting Carter Page’s communications after convincing a Foreign Intelligence Surveillance Court judge that there was probable cause to believe Page was acting as an agent of a foreign power, in this case Russia, according to the officials.\n",
            "That’s a strong charge, but I doubt that there is evidence to support it. Carter Page “worked in Moscow for Merrill Lynch a decade ago and … has said he invested in Russian energy giant Gazprom.” He never had any official association with the Trump campaign, but has been referred to as an “informal adviser.” He has asked to testify before a Congressional committee to clear his name.\n",
            "The current leakers, whoever they are, described the Obama administration’s FISA application in detail. Or else the Post reporters have seen it.\n",
            "The government’s application for the surveillance order targeting Page included a lengthy declaration that laid out investigators’ basis for believing that Page was an agent of the Russian government and knowingly engaged in clandestine intelligence activities on behalf of Moscow, officials said. Among other things, the application cited contacts that he had with a Russian intelligence operative in New York City in 2013, officials said. Those contacts had earlier surfaced in a federal espionage case brought by the Justice Department against another Russian agent. In addition, the application said Page had other contacts with Russian operatives that have not been publicly disclosed, officials said.\n",
            "The Obama administration was already trying, last Summer, to find evidence that Russia’s government was “meddling” in our presidential election:\n",
            "The application also showed that the FBI and the Justice Department’s national security division have been seeking since July to determine how broad a network of accomplices Russia enlisted in attempting to influence the 2016 presidential election, the officials said.\n",
            "I find it hard to believe that Russia’s rulers, from Vladimir Putin on down, wanted to help elect a president who vowed to rebuild America’s dwindling military strength, and to put America first, in place of an administration that was consistently supine in the face of Russian aggression and was borderline anti-American. Possibly Putin and his advisers are that dumb, but I doubt it.\n",
            "In any event, the Obama administration failed to find any evidence that anyone associated with Trump was somehow cooperating with the Russians–not even a “junior member of the [Trump] campaign’s foreign policy advisory group,” as Page described himself. If they had, we would have learned about it long before now.\n",
            "We haven’t heard the last of this story, but for the moment one thing is clear: a great many people, inside and outside of the media, owe President Trump an apology. Assuming that President Obama knew of, and approved, the FISA application–a safe assumption, I think–Trump’s much-reviled tweet was true:\n",
            "Obama had my “wires tapped” in Trump Tower just before the victory. Nothing found.\n",
            "How much of this Trump knew all along is, at this point, unclear.\n",
            "UPDATE: We are now starting to get a picture of how sinister this whole Democratic Party misinformation campaign is. Through the last half of 2016, the Obama administration was desperately searching for evidence of some link between the Trump presidential campaign and Russia. They went to the length of seeking (twice, reportedly) and finally obtaining a FISA order that allowed them to spy on at least one insignificant Trump associate.\n",
            "In addition, we now know that Susan Rice headed up an operation whereby raw NSA intelligence was sifted for names of Trump associates, no doubt in hopes of uncovering dirt of some sort.* And we also know that these efforts came up dry. The Obama administration found no compromising information about Trump or any of his associates.\n",
            "Nevertheless, ever since the Inauguration the Democratic Party, especially its press wing in Washington and New York, has relentlessly pushed the Trump/Russia story. What story? There isn’t one. But that hasn’t stopped Democrats in the press from talking about little else for the last three months.\n",
            "And yet, all along, the Democrats have known that their spying produced nothing. This whole story is almost unbelievably sordid. The relevant Congressional committees should investigate thoroughly, and criminal prosecutions should follow where laws have been broken.\n",
            "It is time to get to the bottom of the Obama spy scandal.\n",
            "___________________________\n",
            "* All of this is reminiscent of Watergate, in this sense: after the fact, no one could figure out why the Plumbers bugged the Democratic National Committee, given that President Nixon was obviously going to be re-elected anyway. (The answer to that question may still be unknown, but that is another story.) Similarly, Barack Obama and his minion Susan Rice no doubt were confident that Hillary Clinton would win the election and serve Obama’s third term. Yet, they weren’t taking any chances.\n",
            "Russia on Friday dismissed constitutional amendments proposed by Kiev to decentralise Ukraine as part of the peace process as merely an \"imitation\" of compliance.\n",
            "Ukraine's parliament on Thursday voted in favour of sending to the Constitutional Court a package of reforms that would grant greater autonomy to the war-ravaged separatist east.\n",
            "The constitutional reforms will have to be voted on twice more by the Verkhovna Rada parliament -- and approved by 300 of 450 lawmakers -- but Moscow has indicated its displeasure with the measures already.\n",
            "\"The attempt to present the constitutional amendments... as some kind of fulfilment by Kiev of its Minsk obligations is just an imitation and should not fool anybody,\" said a statement on the foreign ministry website.\n",
            "A peace accord struck in February in the Belarussian capital Minsk called on Kiev to see through a raft of measures to grant a special status to the rebel-controlled areas of Donetsk and Lugansk regions, including constitutional reform.\n",
            "The Russian statement called the proposed amendments \"political demagoguery\" which aim to \"mislead the Ukrainian and international public.\"\n",
            "The vote ignited a storm of criticism in the Ukrainian parliament, including physical scuffles.\n",
            "Kiev and the West blame Moscow for continuing violence in Ukraine's east, where some 6,500 people have been killed since the conflict between the separatists and Kiev's forces began last April.\n",
            "Moscow has always denied sending troops or arms across the border.\n",
            "Allāh سبحانه و تعالى has 2 methods of creation. One method is that things are created over time i.e. fī sittati ayyāmin.\n",
            "إنَّ رَبَّكُمُ اللَّـهُ الَّذِي خَلَقَ السَّمَاوَاتِ وَالْأَرْضَ فِي سِتَّةِ أَيَّامٍ ثُمَّ اسْتَوَىٰ عَلَى الْعَرْشِ يُغْشِي اللَّيْلَ النَّهَارَ يَطْلُبُهُ حَثِيثًا وَالشَّمْسَ وَالْقَمَرَ وَالنُّجُومَ مُسَخَّرَاتٍ بِأَمْرِهِ أَلَا لَهُ الْخَلْقُ وَالْأَمْرُ تَبَارَكَ اللَّـهُ رَبُّ الْعَالَمِينَ\n",
            "Indeed, your Lord is Allah, who created the heavens and earth in six days and then established Himself above the Throne. He covers the night with the day, [another night] chasing it rapidly; and [He created] the sun, the moon, and the stars, subjected by His command. Unquestionably, His is the creation and the command; blessed is Allah, Lord of the worlds. (7:54)\n",
            "The second method is that things are created instantaneously with the command “kun”, and the result is “fa ya kun“.\n",
            "إِنَّمَا أَمْرُهُ إِذَا أَرَادَ شَيْئًا أَن يَقُولَ لَهُ كُن فَيَكُونُ\n",
            "His command is only when He intends a thing that He says to it, “Be,” and it is. (36: 82).\n",
            "So these are 2 different spheres of the creation of Allāh سبحانه و تعالى. The sphere in which Allāh سبحانه و تعالى says ‘kun fa ya kun‘; this is called `alam al‑’amr. The second sphere which Allāh سبحانه و تعالى creates over time in stages, and that is called `alam al‑khalq; the created world.\n",
            "Human beings are very unique type of beings in that they actually composite of both of these worlds. So when we consider ourselves, we actually have to consider two aspects of ourselves. The first is that we have a rūḥ, i.e. a soul, and that soul comes from the commanded world i.e. `alam al‑’amr. At the same time, we have a jism i.e. a body. This jism actually comes from the create world i.e. alam al‑khalq. So we are a composite of both of these worlds.\n",
            "From this we derive several principles. The first and most foundational is that that which comes from `alam al‑’amr needs to be sustained by amri things. So for example we say that the soul or rūḥ comes from `alam al‑’amr and that its sustenance will also come from amri things, spiritual things, so that the sustenance of the soul is actually derived from things that are spiritual.\n",
            "At the same time, the body comes from`alam al‑khalq, from the created world. Therefore the things which sustain the body will be khalqi i.e. will come from the created world. The simple example will be that of bodies are made from mud and everything that we use to sustain our bodies happen to also come from mud. For example, the food we eat grows from mud, the clothes we wear grow from mud, cotton for example. At the same time one can say that we get our food and clothing from animals e.g. if we wear wool. However both of those also come from mud because they are also created in a sense from mud; dirt. They also eat crops which come from dirt.\n",
            "So essentially what we are trying to highlight is that the body requires its sustenance from bodily things i.e. from tangible and physical things; and that the soul requires its sustenance from spiritual things. This is a very simple principle which is not too hard to understand. However it forms the key foundational idea for the vast majority of human failure in this day and age. Why? Because when you look at mankind as a whole, most people in this day and age have made the erroneous assumption that peace will be attained by gaining physical things. So whether you look at a Muslim or a non-Muslim in this day and age, we have begun to think that somehow our happiness is tied to physical things. For example, if we have a big house, we will somehow be happy; if we have nice clothing, we will somehow be happy; if we have a nice car, we will be happy; if we eat good food, we will be happy. However what you have to recognize is that happiness is not a characteristic of the body. It is a characteristic of the soul.\n",
            "However what you have to recognize is that happiness is not a characteristic of the body. It is a characteristic of the soul.\n",
            "So here we are trying to feed the soul with houses, clothing, ice cream and chocolate, nice cars, and what ends up happening is that we are feeding the body instead of the soul. The body turns into a monster. It crushes and leaves no space for growth of the soul, and it produces depression within a human being. It’s so amazing obvious that in this day and age, the wealthiest people tend to be the most depressed. You look at the wealthiest societies and within them you find the greatest degree of depression. You can travel to some very poor places in the world and you’ll see happy people. It’s funny because you’ll read these articles not from Muslims, but from non-Muslims who actually say “I travelled to Tibet or some village in China or Afghanistan.” And they’ll say that “When I went there, I was shocked to see that people were happy. They were smiling, laughing and enjoying life.” That’s because this simple principle has not destroyed them. You can say that this mistaken principle has not destroyed them.\n",
            "We all desire to be happy in the life and in the hereafter. What contents the human being is the soul we are providing for. Why? Because our souls are from jannah. That is who we are. The body is not who we are. The body will be put in the grave and will return back to the dirt and disintegrate. It’s the soul that will be raised up and will be judged on the Day of Judgment.\n",
            "The soul actually grew up in jannah. We were in jannah. We were in the presence of Allah سبحانه و تعالى. Then we got banished down to the earth for a period of time and we happened to be wrapped up in this thing called the body while we are here. The body is what deceives us because all the tests in this world are bodily for example, lowering the gaze, eating halāl, earning halāl income. These are all bodily tests.\n",
            "The body is sort of the vehicle that tests the soul. If the soul is fed with the things that it was used to having e.g. the presence of Allah سبحانه و تعالى and qurb (nearness) to Allah سبحانه و تعالى, the soul becomes very content and happy; and the human being becomes happy irrespective of what their body has.\n",
            "A great example is that of one who has spent any time studying the dīn and in any type of Islamic environment will tell you that when students study at madāris of our dīn, they are so content despite the fact that it is one of the hardest things to do. You can ask them what their toughest struggle was. They’ll say, “Well, when I studied in madrassa that was really tough.” And if you ask them “When were you happiest in your life?” They’ll say, “When I studied in madrassa, there is no greater happiness that I can recall.” This is the common theme. You can ask anyone and I don’t have a doubt that they will share the exact same with you if they truly benefitted from their teachers and they were doing what they were supposed to do.\n",
            "Just as the body requires a balanced slack of nutrients that people need to consume in order to grow the body properly, in the same way there is balance of nutrients that is required for the growth of the soul. So you need to give it some Qur’ān, some salawāt on Rasulullāh ﷺ, ṣuhbaḥ of good company, and putting it in the masjid as much as possible. So these are the multiple nutrients that are required in order to grow the soul.\n",
            "The first basic thing about the rūḥ is that it needs to be given space to grow. The body (jism) and the soul rūḥ exist in one closed contained space. If the body is big, the soul has no space to grow. If the emphasis of the human being is on the body all the time then what ends up happening is the body grows into an animal, into a monster, and it provides very little room for the soul to grow. So the first thing that has to happen before the soul can grow is that you have to contain the growth of the body.\n",
            "Nafs is the name of the inner portion of the body. It’s the key command center of the jism. So if the nafs is out of control, it takes up the vast majority of space and the soul becomes very crushed and depressed. One thing that has to happen is the nafs has to be curtailed. The simple curtailing of the nafs will cause the soul to expand.\n",
            "The simple curtailing of the nafs will cause the soul to expand.\n",
            "How comes that you find people who practice yoga or people who live in a Buddhist monastery become very spiritual? How comes they themselves say they experience something and that they feel spiritual and good about themselves? This is a very simple principle. The body and the soul exist in a contained space. The vast majority of human beings have grown their body and their nafs so much their soul is automatically constricted and depressed. So if any human being using any mechanism begins to cut off the bodily desires and the body begins to shrink down to its normal size, the soul will expand and they will feel spiritual. So it’s not that only the Muslim can feel spiritual. There is some degree of spirituality that will arise just from maintaining a normal balance of being human. So we shouldn’t be confused that how come they can be spiritual? Is that some form of ‘ayb on Islam? Is that some source of defect in Islam? No! This is just part of being a natural human being.\n",
            "Now the ‘ajīb (strange) thing however is those things will never make the soul to grow. See there are 2 separate issues. One is to just come and maintain the balance of the body and the soul which if any human being does they will be at some degree of peace. But actually beyond that is to cause the soul to grow and elevate. That requires Islām; that requires the dīn. When a person does that, they go far beyond this basic spirituality that you see arise in various settings. They go far beyond that and attain a degree of spirituality, righteousness, and closeness to Allāh سبحانه و تعالى that can never be described in words. Our challenge is not only to contain the body. We want to contain the body and knock it back in to its cage. So you put it back in to the cage to create space for the soul. Once you create space, the soul automatically expands and you’ll feel good about yourself. Then the next thing is to grow the soul.\n",
            "So we know we have to grow the soul and we know that as we grow and feed the soul, we will become people who are close to Allāh سبحانه و تعالى. When we say grow the soul, what we actually mean is bringing it closer to Allāh سبحانه و تعالى. We should ask ourselves, what is the best mechanism by which we can do that? The best mechanism is the dīn of Allāh سبحانه و تعالى. In this day and age, that dīn will be the dīn of Islām. In this day and age the mechanism of that will be the sunnah of Prophet Muḥammad ﷺ. If you want to summarize it in one word, it’s the sunnah. The sunnah is the mechanism to get closeness to Allāh سبحانه و تعالى.\n",
            "The sunnah is the mechanism to get closeness to Allāh سبحانه و تعالى.\n",
            "Now it is unfortunate that in this day and age this has become the most confusing principle. It is confusing to people that the sunnah is the actual mechanism by which we attain wilāyah. Even this concept has been lost. This is how deluded we have become. We have to be very clear about this one principle because it’s the failure of the vast majority of people who want to pursue this path.\n",
            "The premise of all of this lies in an āyah of The Holy Qur’an in which Allāh سبحانه و تعالى says;\n",
            "قُلْ إِنْ كُنْتُمْ تُحِبُّونَ اللَّهَ فَاتَّبِعُونِي يُحْبِبْكُمْ اللَّهُ وَيَغْفِرْ لَكُمْ ذُنُوبَكُمْ وَاللَّهُ غَفُورٌ رَحِيمٌ\n",
            "Say, [O Muḥammad], “If you should love Allāh, then follow me, [so] Allāh will love you and forgive you your sins. And Allāh is Forgiving and Merciful.” (3:31)\n",
            "This is our equation. Now what does the equation say?\n",
            "قُلْ\n",
            "The messenger of Allāh سبحانه و تعالى is being commanded by His Lord to say:\n",
            "إِنْ كُنْتُمْ تُحِبُّونَ اللَّه\n",
            "Say to who? Say to your companions and all of the generations of Muslims to witness this discussion as well until the Day of Judgment. Say to your companions:\n",
            "إِنْ كُنْتُمْ تُحِبُّونَ اللَّه\n",
            "If you love Allāh سبحانه و تعالى\n",
            "فَاتَّبِعُونِي\n",
            "Follow me\n",
            "يُحْبِبْكُمْ اللَّهُ\n",
            "Allāh سبحانه و تعالى will love you.\n",
            "So here is a very simple equation. If you love Allāh سبحانه و تعالى and you desire to attract the attention of Allāh سبحانه و تعالى then you have to recognize the only mechanism by which you can do that is to follow the sunnah of Rasulullah ﷺ.\n",
            "Now actually as a corollary to this, we have to realize that the food of the soul comes from Allāh’s attention. The more the soul is focused on Allah سبحانه و تعالى and the more the barakāt, fuyūdhāt, and tawajjuḥāt come from Allāh سبحانه و تعالى, the more the soul becomes elevated and eventually develops into a worthy human being who will eventually enter in to jannah; someone who will attain contentment in this life and who will attain falāḥ in the hereafter.\n",
            "What does falāḥ mean? According to scholars, falāḥ is that success after which there can never be failure. Now why is that condition required? Because in this world every success is followed by failure or at least the potential for it. For example, what are billionaires doing all the time? They are always worried about their money. It is funny a guy makes a billion dollars yet he is always worried about his money. He is worried about his asset preservation. Even the wealthy man worries about preservation of capital. Why? Because every wealthy man know it is easy to become poor again, quote and quote, relatively poor again. You can become the most successful person in the community but you’ll always know there is a potential for failure. You’ll always be up at night thinking what the next step forward is because if you do not take a step forward then you are taking a step back.\n",
            "Even if you were to have a perfect life, no one can deny that you will die which is the ultimate failure if you were living a perfect life. So in this life there is no permanent success. There is limited success. But the word falāḥ means a permanent lasting success after which there will never be failure. So what is meant by that? It means that once you go into jannah, you never have to worry again about being sick, about losing your wealth, about losing your house, and the best is you never ever have to worry about jahannam. Because for the mu’min that is the failure we always worry about. Even if we are practicing properly, we worry about the potential for jahannam which is always on our neck. The possibility always exists.\n",
            "Now this falāḥ is so established that what will happen after people are put into jannah? When all the people have been put in either jannah or jahannam and everybody’s balance has been taken into account, an animal will be brought in front the people and it will be sacrificed. The people will be asked on if they know what the animal is. The people will say they do not know. They will be told that this is death itself. Death itself has been sacrificed. When people of jahannam will realize that the door is closed on them, they thought they could die because they were going through hell. So even death will have been removed as an escape. As for the people in jannah, they will now know that there is no more worry because they got permanent success and the possibility of death has been removed. So this is what basically defines falāḥ.\n",
            "(The above is a partial summary of a talk given by Shaykh Husain Abdul Sattar (Fundamentals of Tasawwuf part 2). The audio of the talk can be downloaded from HERE. To listen to more talks of Shaykh Husain Abdul Sattar, please visit sacredlearning.org)\n",
            "Advertisements\n",
            "This week in the news a high school senior named Suzy Lee Weiss wrote a piece in the Wall Street Journal satirizing the college admissions process of Ivy League colleges. She was upset that Harvard, Yale, and Princeton rejected her application and decided to speak out for those being left out of their top college choices. Unfortunately for students across the country, she might be the worst voice to represent seniors trying to get into college.\n",
            "I don’t want to go after Suzy Lee Weiss too much, I think she has every right to voice her frustration with the farce of applying to Harvard, Princeton, and Yale. You go smart-upper-middle-class girlfriend. She can’t help being privileged, and to her this is a real problem she can cry about in her own room, in a house her parents own in a really nice neighborhood. She can text her tears to her friends on her iPhone, and Facebook them on her own Macbook Air. What bothers me is that of all the voices out there striving to get a college education, it is this girl’s story that goes viral.\n",
            "This is just another example of the misplaced lens of the corporate media. This isn’t surprising coming from the Wall Street Journal, which doesn’t even pretend to have a clue about the problems of regular people. But the fact that everyone is talking about this highlights a disconnect with the reality of most middle-class and poor students in this country. Ms. Weiss’ problems aren’t really problems to most people, they are White People Problems.\n",
            "For the record, I am white. When my colleague introduced me to the concept of White People Problems, she was talking about the struggles she and her husband were having with their financial advisor and how best to maximize their savings. “It has been a nightmare,” she said, before couching it by saying, “Well, as much a nightmare can be while living comfortably and planning on how to be even more comfortable.” She went on to give me other examples of instances when upper-middle-class white people really struggle: What to do when your yoga instructor moves to Costa Rica; does Trader Joe’s provide enough gluten free products; it’s Friday evening already and you haven’t made dinner reservations in San Francisco and all the good places will definitely be booked for 8:00 tapas; deciding where to put your money that is currently just sitting there not making a good interest rate; which organic dog food is safest. The problem Suzy Lee Weiss, and the Wall Street Journal, and everyone covering this has is they think this is an actual problem.\n",
            "Poor Suzy Lee Weiss, a girl who has been admitted into Indiana, Penn State, Michigan, and Wisconsin, who, like 97% of applicants to Harvard this year, didn’t get in.\n",
            "By her own admission Weiss isn’t a competitive applicant. Aside from a 4.5 GPA and an SAT score over 2100, she doesn’t have much. While those two factoids might make some people think, “Well, a 4.5 should get you into any school,” those of us who work with seniors every year know that a 4.5 means nothing. Remember, Harvard has full time, very smart people whose job it is to determine who out there is the best of the best, and in their estimation, it wasn’t her. Suzy Lee Weiss didn’t deserve to get into an Ivy League college, and the fact that she and others think she does is ridiculous.\n",
            "Every year I could have a dozen seniors write letters about their struggle to get into top universities, not that anyone will publish them. Last year a student of mine with a 4.2 was accepted into St. Mary’s highly impacted nursing program only to have to turn it down because she couldn’t afford to go. She was looking at taking out over $75,000 in loans for her UNDERGRADUATE degree. The biggest problem facing students today is the COST of college, a little fact Ms. Weiss doesn’t even mention—which shows she isn’t even worried about that part of the game.\n",
            "I am always reminded of a presentation I received, along with dozens of other educators, from the admissions office at Stanford University. They presented us with three applications and asked us which student we would take. They were all excellent applicants, but our top pick was a girl from Chicago who was a published poet, a national figure in slam poetry, who had a 4.5 GPA with a killer personal statement. She had meaningful volunteer positions and held down multiple jobs to support her and her mom. At the end of the presentation the admissions people admitted they had taken our third choice. When we asked why, they said it was because his father had gone to Stanford. That was it. Read that little anecdote again—THAT IS A TRUE STORY. This is a planned presentation Stanford gives to EDUCATORS. Their very simple and intentional lesson is that the game is rigged. No one lies about that, there isn’t any beating around the bush. To get into Harvard you have to be the son of a member of congress, the head of the CIA, or the CEO of Wells Fargo—or Mark Zuckerberg. A 4.5 won’t get anybody into Harvard and there isn’t anyone out there saying it will.\n",
            "It annoys me that I am even writing about Suzy Lee Weiss because there are so many more kids out there with real problems, and when it comes to college access her’s might be the most unimportant perspective possible.\n",
            "Really poor kids get financial aid, but that still isn’t enough to cover the cost of college. We know the middle class is being squeezed out of college because they don’t get enough financial aid and their parents can’t afford to make up the difference. Tuition rises on an exorbitant slope every year. How about the kid who was brought to this country when they were 1, has a higher GPA and SAT scores than Ms. Weiss, who has ACTUAL volunteer hours and holds down ACTUAL jobs (not fake jobs or pity projects), but can’t even apply to most colleges? We have bought into Weiss’ pity story about parents who have stopped parenting her because she is the 4th child; what about kids who don’t have parents? What about my students who are homeless? What about my boy Arthur who can’t pass Algebra? What about my students who have been SHOT this year?\n",
            "That brings me to the final irony of this sad, sad episode. How is it that we have even heard about Suzy Lee Weiss and her White People Problems? Oh yeah, her sister is a former assistant editor for the Wall Street Journal. This poor girl’s suffering is unending—every time she gets a rejection letter all she has to do is call up her sister to get her “problems” published in one of the largest publications in the world.\n",
            "Again, I’m not mad at Suzy Lee Weiss—in her world this must be difficult. But when will the media cover the problems of real students with real problems who live in the real world? They won’t, because none of those kids have relatives at the New York Times.\n",
            "Oh, and because of her little tirade, Yale has decided to accept little Suzy. I guess now she will be overcome with the stress of deciding which color cardigan goes best with the autumn leaves in Connecticut–somebody call the associated press.\n",
            "BMW Team RBM drivers Maxime Martin and Tom Blomqvist headed to Vallelunga last week to complete a three day test of the new BMW M4 DTM.\n",
            "“It was great to be back behind the wheel after the long break,” said Martin. “The new regulations promise to make this an interesting season: our car is new, the tyres are different and there is a lot to familiarise ourselves with. The test went well. We still have work to do, but we can be satisfied with how it has gone so far.”\n",
            "A change to DTM regulations for 2017 mean ‘More Power, Less Downforce’, with engines now producing over 500hp, changes to aerodynamics and a new softer tyre from Hankook.\n",
            "“After the winter break, it is fun to return to the racetrack and prepare for the season with the team. I was able to get well acquainted with the new BMW M4 DTM and the way the new tyres work during the test. Overall, these were useful and productive days.” Blomqvist added.\n",
            "While he didn’t complete any laps in the new car, Bruno Spengler was in Vallelunga to get to know the team that he moves to after two seasons with BMW Team MTEK.\n",
            "“It was good to spend three days with the team. I get on well with everyone. It was important to sit down with the engineers and start our analysis. The new regulations, the new car and the new tyres mean there is a lot to learn from a technical point of view. I am looking forward to the season with the BMW Team RBM.”\n",
            "A final test will take place from 3-6 April at Hockenheim before the season starts a month later on 6 May.\n",
            "President Rodrigo Duterte on Monday extended his deadly drug war up to the last day of his term in 2022, and conceded the police force acting as his frontline troops was “corrupt to the core.”\n",
            "Thousands of people have died in the crackdown that began when Duterte took office in the middle of last year, with rights groups warning police are carrying out extrajudicial killings not just to fight crime but to aid their own corrupt activities.\n",
            "ADVERTISEMENT\n",
            "Duterte won the presidential election largely on a law-and-order platform headlined by a vow to eliminate the illegal drug trade in three to six months.\n",
            "Once in office Duterte extended the timeframe until March of this year, but on Monday he said there would be no end while he was in power.\n",
            "“I will extend it to the last day of my term,” Duterte told reporters.\n",
            "“March no longer applies.”\n",
            "In the Philippines, presidents are allowed to serve only a single term of six years.\n",
            "Duterte said on Monday he believed almost 40 percent of all police officers around the country were involved in graft.\n",
            "“You policemen are the most corrupt. You are corrupt to the core. It’s in your system,” Duterte told reporters as he railed against the officers who allegedly masterminded the murder of a South Korean businessman.\n",
            "He said he wanted to “cleanse” the police force by doing a review of all the police officers that had previously been involved in extortion.\n",
            "Duterte has been unrepentant in the face of fierce criticism of the drug war from various Western governments, UN agencies and rights groups, saying he must take extreme measures to stop the Philippines from becoming a narco state.\n",
            "ADVERTISEMENT\n",
            "However a series of scandals involving the police using the drug war as a cover for extortion, including the abduction and murder of a South Korean businessmen, have fuelled fears that rogue cops are on the rampage.\n",
            "Police chiefs repeatedly insisted in recent weeks that those crimes were isolated cases and that they did not signal a larger problem.\n",
            "However Duterte has insisted on many occasions he will not allow any police officer to go to jail for killing people in the name of his drug war.\n",
            "He also said last year he would be “happy to slaughter” three million drug addicts as part of his crime war.\n",
            "Police have reported killing more than 2,500 people they accused of being drug suspects, while nearly 4,000 others have died in unexplained circumstances in the crackdown, according to official figures./rga\n",
            "Read Next\n",
            "LATEST STORIES\n",
            "MOST READ\n",
            "1868 – Prescott, Arizona\n",
            "Ex-Union cavalry officer, Captain Leander Lincoln kicked the saloon doors open and entered with both guns drawn!\n",
            "“I’m looking for the Stuart boys!” he shouted.\n",
            "Three men slowly stood up from the card table. The rest of the saloon was silent as the oldest spoke, “You found them. Now what are you going to do?” he asked as his right hand slithered down to hover over his Colt 45.\n",
            "Lincoln, laughed and said, “I’m going to kill all three of you fools if you all don’t unbuckle your gun belts very carefully and let them drop to the ground.\n",
            "“Here’s the thing. Your wanted dead, or alive. I’d just as soon shoot your sorry asses so you better make a quick decision!”\n",
            "Three gun belts fell to the wooden floor.\n",
            "The US Army drove the Navajo people from their ancestorial lands in Arizona Territory and Western New Mexico, and marched them on the infamous Long Walk to imprisonment in Bosque Redondo when Leander was still in the Army and stationed in Washington DC.\n",
            "When the treaty of 1868 was signed the Navajo left Bosque Redondo, and were relocated to eastern New Mexico. That was the year Leander mustered out of the Army and went West to see his mother and half brother.\n",
            "Hundreds of Navajo men, women, and children died on the Long Walk. The survivors were put on a reservation. The horror of the relocation was firmly embedded in their minds.\n",
            "Some wanted revenge. The rest went on with their hardscrabble lives.\n",
            "Hashkeh Naabah greeted Leander warmly.\n",
            "“What has my white son Ahiga brought me?” he politely asked.\n",
            "“Three more white men who won’t be missed. Your men are taking them off the horses and tying them to stakes as we speak.”\n",
            "“No one will come and say we killed them then?” Hashkeh inquired.\n",
            "“No. They are wanted men. They are yours now. I will continue to bring you white men as long as I can. As long as I live.”\n",
            "“You are a lot like your mother, and my sister, Yanaha. He bravery inspired us all on the Long Walk. We still mourn her death.”\n",
            "“As do I, Uncle.”\n",
            "“Come, let us go watch the squaws torture these white eyes. The big one looks like he may last for a long time.”\n",
            "The prisoners screams pierced the night.\n",
            "Leander’s anger at the US Army, and what they did to his mother, burned his soul and left a charred husk of a human thirsting for revenge. Posing as a bounty hunter was a stroke of genius.\n",
            "He knew he couldn’t start killing Union soldiers and hope to get away with it. In his mind he ceased being a “white man” and embraced his Navajo heritage. He was Ahiga, son of Yanaha. As such, he had no qualms about killing any white men.\n",
            "After roaming from town-to-town looking for wanted men throughout the west he acquired a reputation. Folks knew Captain Lincoln never brought anyone back alive. Just their heads.\n",
            "His hunt lasted two years, before he was shot to death in a saloon by a drunken ex-Confederate soldier who refused to believe the war was over.\n",
            "The elders at the Navajo Reservation told Ahiga’s story to each new generation. It was a story however, that was never shared with outsiders.\n",
            "As It Stands, historical fiction is a good way to tell stories that could have been true, but aren’t.\n",
            "Share this: Google\n",
            "Twitter\n",
            "Facebook\n",
            "Reddit\n",
            "Pocket\n",
            "LinkedIn\n",
            "Telegram\n",
            "Skype\n",
            "Tumblr\n",
            "WhatsApp\n",
            "More\n",
            "Pinterest\n",
            "Like this: Like Loading...\n",
            "Sergey Vasiliev grew up idolizing his older brother. Alexander was the serious one, the better student, but fun-loving Sergey followed him everywhere, hanging out with his friends and treating his every word like the gospel truth.\n",
            "The two also remained close as adults, even after Sergey left the Black Sea port of more than a million and moved 500 kilometres north to Kiev to start a construction business and be with his girlfriend. Then, four months ago, Ukraine's political turbulence suddenly drove them apart. They reacted very differently to the pro-Western revolution in the capital's Independence Square – the \"Maidan.\"\n",
            "Sergey, furious at government corruption, charged to the front line in January, clashing with the infamous Berkut riot police. The solution to Ukraine's problems, he had come to believe, lay in ripping up its Soviet foundation and lingering deference to Russia, and building something new, more European in their place.\n",
            "Story continues below advertisement\n",
            "This was anathema to all that Alexander, by then a prominent pro-Russian figure, held dear. When the uprising his brother had embraced succeeded, he began to agitate for a counter-revolution.\n",
            "Three weeks ago, 46 people died here in a fiery partisan clash, but by then Alexander was gone. He had come under police scrutiny as a \"separatist,\" and in March moved with his young family to Crimea. When President Vladimir Putin announced that the peninsula had been annexed to the Russian Federation, he was delighted.\n",
            "Now, the brothers live on different sides of Europe's new Cold War – and a burgeoning conflict in Ukraine that Russian President Vladimir Putin declared a \"civil war\" on Friday.\n",
            "The political divide is often portrayed as splitting the country along linguistic and cultural lines, pitting the Ukrainian-speaking and heavily Catholic centre and west against the Russian-speaking and Orthodox south and east.\n",
            "That is how the Kremlin and its allies want the fight to be seen. But the real cause of Ukraine's unrest is the one driving the Vasiliev family apart.\n",
            "On Sunday, a much-anticipated presidential election will show just how sharply divided this country, which once had a population of 45 million, has become. Turnout is expected to be high in the centre and the west, where many are anxious to replace the interim government that has ruled since the uprising with a president who can stitch up the nation's many wounds.\n",
            "But voting will prove dangerous if not impossible in Donetsk and Lugansk, eastern provinces where government buildings are under the control of pro-Russian separatists. And this time the only Crimeans who will cast ballots are the ones who have fled to escape Russian rule.\n",
            "Story continues below advertisement\n",
            "Story continues below advertisement\n",
            "Ukrainian-speakers know how they want the country to proceed, but there isn't nearly as much unity among the country's many Russian-speakers. In many ways, the violence in the south and east is less an inter-communal conflict than an intra-communal one, splitting the Russian-speaking population in two and turning families and friends against each other.\n",
            "The tale of the Vasiliev brothers could have been torn from the pages of Taras Bulba, the famous Nikolai Gogol short story about two Ukrainian brothers who wind up on opposite sides of a 16th-century war against occupying Poland.\n",
            "But if Taras Bulba told the tale of the early struggle for an independent Ukraine, the rift between the Vasilievs illustrates how split Russian-speakers are on whether, and how, they belong in an independent Ukraine.\n",
            "\"The main reason for our disagreement is that I think of myself as a proud Ukrainian. My brother thinks of himself as Russian because we are ethnically Russian, and Odessa has always been a Russian city,\" says Sergey, 32, now married with an 11-year-old adopted daughter.\n",
            "In an e-mail written from his new home in Sevastopol, the home port of Russia's Black Sea fleet, Alexander, 35, insists that Ukraine is now in a \"full-force\" civil war. He despairs of ever again being able to visit his beloved Odessa, and worries that his brother and parents will find it difficult to come to Crimea as tensions in Ukraine turn more and more acrimonious.\n",
            "The threat of violence is constant – two days ago, 13 Ukrainian soldiers were gunned down in a battle with pro-Russian militants near the Donetsk town of Volnovakha.\n",
            "Story continues below advertisement\n",
            "But that was far from the bloodiest day since the uprising ended in February.\n",
            "Odessa's trial by fire\n",
            "The charred husk of the Trade Union building in the centre of Odessa stands as a ghostly monument to how dangerous and deadly this country's divisions have become.\n",
            "The five-storey Soviet-era structure overlooks Kulikovo Field, a paved plaza where a small pro-Russian protest camp sprang up as soon as President Viktor Yanukovych's government was toppled.\n",
            "Other than the few hundred pro-Russians there, and occasional pro-Maidan gatherings at the top of this city's famous Potemkin stairs, Odessa remained quiet throughout the upheaval in Kiev. Tensions grew, however, as neighbouring Crimea declared its independence and then was quickly annexed by Russia.\n",
            "Like Crimea, the port founded in 1794 by Catherine the Great to be the southern gateway to her empire retained strong cultural ties to Russia after the Soviet Union fell. Some in the city believed passionately, and still do, that Odessa also must return to Mother Russia some day.\n",
            "Story continues below advertisement\n",
            "On May 2, a sporting event lit the fuse. Soccer here, as in other parts of Europe, often leads to violence. Fans brawl frequently, and the \"Ultras\" – truly hard-core supporters – are known both for their rabid devotion and for being Ukrainian nationalists.\n",
            "But this time the Ultras for Odessa Chernomoretz and visiting Kharkiv Metallist decided that, rather than fight, they would join forces and stage a Ukrainian unity march through the centre of the city. En route, they were attacked by Russian supporters, some armed with guns, and an Odessa Ultra was shot and killed.\n",
            "Outraged, the Ultras battled back, joined by veterans of the \"self-defence forces\" who had fought on Maidan against Mr. Yanukovych. Forced to flee, the pro-Russians retreated to Kulikovo Field and regrouped.\n",
            "Konstantin Ivanov, a 34-year-old businessman and self-defence veteran (shot four times during the battle for Kiev, he is alive only because of a bulletproof vest), says he wanted the conflict to end there. Although ethnic Russian himself, he despised those on the field, but thought clearing their camp was a job for the police.\n",
            "However, the police were mostly bystanders that day, and the soccer fans, having lost one of their own, were \"very aggressive,\" Mr. Ivanov says. \"It wasn't possible to stop the Ultras.\"\n",
            "Rather than desert their allies, Mr. Ivanov and the Maidan forces marched with them to Kulikovo, where the pro-Russians quickly disappeared into the Trade Union building. Mr. Ivanov believes this was planned, since people were already on the roof, shooting fireworks down on the crowd. Molotov cocktails were next, thrown by both sides. Soon the building was ablaze, and it was clear that lives were at stake.\n",
            "Story continues below advertisement\n",
            "But hatred ruled. As people leaped from windows, some suffering from smoke inhalation, others unconscious and badly wounded from the fall, the crowd set on them. Of the 46 who died that day, 32 were victims of the inferno.\n",
            "Mr. Ivanov is sympathetic, to a point. \"I'm sad for esthetic reasons – it doesn't look good to beat people suffering from inhalation and had jumped out of a building with second– and third-degree burns. They were not dangerous to us,\" he says.\n",
            "\"But from an ethical position, I think the Ultras had the right to do it. This wasn't a duel. There were no rules, no gentlemen's agreement between the parties. It began with them attacking us with automatic weapons.\"\n",
            "Russian state television described the tragedy as \"genocide\" launched by ethnic Ukrainians. However, in videos on YouTube both sides can be heard screaming at each other and the inert police – in Russian. The battle of Odessa was a fratricidal conflict, the city's Russian-speakers shooting, beating and burning each other, everyone convinced they were doing battle with dangerous stooges manipulated by foreign powers.\n",
            "\"I knew people on both sides,\" says Sergey, who had come from Kiev to attend the game. Despite his passionate pro-Ukrainian stance, he was too aghast at what he was seeing to join the fracas. Friends were fighting friends, and he knew two of the pro-Russians who died, one in the street fight and the other in the fire.\n",
            "\"What happened in Odessa was very, very sad, and I'm still very, very sad, because I think this will not stop,\" he says. \"It's a civil war now because there are people on both sides who lost their relatives, and they will seek vengeance.\"\n",
            "Story continues below advertisement\n",
            "One people or two?\n",
            "Ukraine's Russian population arrived in waves, and its history is both complicated and politicized. At one time, Russians and Ukrainians (as well as Belarusians) were considered almost indistinguishable, all rooted in the Kievan Rus, a federation of Slavic tribes that converted to Orthodox Christianity in the 10th century. They emerged as separate entities only in the 13th century after the arrival of the Mongols' \"golden horde,\" which conquered Moscow while much of western Ukraine fell to Poland and Lithuania.\n",
            "Ukrainian historians emphasize the cultural and religious differences that emerged from that point, and describe periods when the country was part of the Russian Empire and later the Soviet Union as prolonged occupations.\n",
            "Some Russian historians, however, emphasize the ethnic similarity of the people and portray the independent nations that emerged from the Soviet collapse as unnatural. Many Russians still refer to Kiev as the \"mother city\" of them and their culture.\n",
            "Stamping out Ukrainian nationalism was an obsession both for the czars and Soviet rulers . One tactic – a ban on teaching Ukrainian – was so successful that even now Russian is the dominant language in much of Kiev's public life. (Although allowed under Soviet rule, Ukrainian-language schools flourished only in the west. The 1989 census showed that 100 per cent of Crimean students were being taught in Russian.)\n",
            "The \"Russification\" effort also saw ethnic Russians encouraged to settle in Ukraine. They moved in to repopulate farmlands abandoned after the Stalin-engineered Holodomor, or Great Famine, of the 1930s, doing the same in Crimea after the 1944 expulsion of its ethnic Tatar population. Jobs in the sensitive defence-industry factories of eastern Ukraine were – in effect, if not by official writ – reserved for ethnic Russians.\n",
            "By the time of the Soviet collapse in 1991, just under a quarter of the country's population was ethnic Russian, and more than 40 per cent of citizens identified themselves as Russian-speakers – numbers that have declined only slightly since then.\n",
            "Shortly after Ukraine declared its independence in December, 1991, Crimea – which had been part of Soviet Ukraine only since a 1954 decree by Nikita Khrushchev – voted to join the Russian Federation, but the new government in Moscow was too preoccupied to respond.\n",
            "Regional-autonomy movements soon sprang up in the south and east, but became a threat to national unity only after the 2004 Orange Revolution, when Mr. Yanukovych was ousted (for the first time) over election fraud and pro-Western Viktor Yushchenko swept to power with support largely from western and central Ukraine.\n",
            "Alexander Vasiliev says that was when he got involved in politics, joining a pro-Russian party called Motherland. To him, and much of Russified Ukraine, the revolution was a Western-sponsored coup, with Russian-speakers fearing they would somehow be made to speak Ukrainian. It's a narrative that Crimeans clung to in March as they voted to join Russia, and one repeated by the masked gunmen on the barricades in the self-declared Donetsk People's Republic.\n",
            "Brother Sergey calls the Orange Revolution a \"global divorce\" – the moment Russia and the West stopped trying to work together, and returned to something like Cold War footing.\n",
            "In any event, Ukraine has spent the past decade refighting that battle. When Mr. Yushchenko's \"orange\" coalition fell apart, Mr. Yanukovych improbably returned to power in 2010, this time winning a vote that international monitors called \"an impressive display of democratic elections.\" He swept eastern and southern Ukraine, taking 74 per cent of the vote in Odessa, and – ominously – even higher shares in Crimea, Donetsk and Lugansk.\n",
            "But last year, despite the political support he received from Moscow, Mr. Yanukovych toyed with signing a trade deal that could have led the country to become a member of the European Union. It was likely a political feint – he risked losing both his alliance with the Kremlin and his base at home – but when Mr. Yanukovych abruptly reversed course in November in favour of closer ties with Russia, he rekindled the political war in earnest.\n",
            "This time, however, the two sides were far angrier and, rather than seeking a fair election, the Maidan protesters wanted to throw out a fairly elected leader with a year remaining in his mandate. The civil-society groups who had led the Orange Revolution were joined by fringe nationalists, including Right Sector – the ideological descendants of the Ukrainian Patriotic Army, a group led by the legendary and controversial Stepan Bandera that fought both the Nazis and the Soviets in an early bid for independence. They weren't demanding a recount, they wanted to live in a very different country.\n",
            "Sergey's first taste of combat came late in January during the \"Hrushevskoho street riots,\" when 200,000 pro-Europe demonstrators marched through central Kiev in defiance of new \"anti-protest\" laws that effectively criminalized the rolling protests on Independence Square. They used clubs and Molotov cocktails on police, who battled back with truncheons, tear gas and, in several instances, live ammunition.\n",
            "Thin but muscular, he smiles shyly when asked just what took place. \"I played an active role. I don't want to say more than that.\" Four people were killed – three of them shot by police snipers – and the Maidan had its first martyrs. It was the point of no return and, like Odessa later on, the rules had evaporated. There was no more room for compromise.\n",
            "By chance, both brothers were in Odessa at the end of February when Mr. Yanukovych ordered his riot police to use all means to end the uprising on the Maidan, provoking a furious gun battle that left 100 people dead, nearly all of them protesters. But those defending the square held their ground and – after an EU-brokered peace deal fell apart – the President was forced to flee.\n",
            "Sergey watched the battle on television with his heart in his throat. He wanted to be there, and says only his brother kept him from heading straight to Kiev. Alexander's suspicions increased the next day, when parliament – anxious to please the angry crowds still on the streets – moved to repeal a Yanukovych-era law that allowed regional governments to adopt Russian as a second official language.\n",
            "Interim President Oleksandr Turchynov refused to sign the new bill, but it was still a propaganda gift to the Kremlin – proof to Russian-speakers in Odessa, Kharkiv, Donetsk and Lugansk of the new government's hostility to them and their culture.\n",
            "With Kiev still reeling from the upheaval, Russia sent soldiers in disguise to Crimea to oversee the controversial referendum on separation and then moved tens of thousands of troops to its border with Ukraine. \"The essential issue is how to ensure the legitimate rights and interests of ethnic Russians and Russian speakers,\" Mr. Putin said, questioning whether the belt from Kharkiv in the northeast to Odessa in the south had ever truly been part of Ukraine.\n",
            "\"I would like to remind you that what was called Novorossiya [New Russia] back in the czarist days … was not part of Ukraine back then,\" he added in remarks televised across Russia and much of eastern Ukraine. \"These territories were given to Ukraine in the 1920s by the Soviet government. Why? Who knows?\"\n",
            "So Mr. Putin deliberately sowed the seeds of the separatism movement now roiling the Donetsk and Lugansk regions. In fact, Alexander Vasiliev was forced to flee Odessa after he too referred to the city as part of \"Novorossiya\" in a public speech. \"I think it is the most appropriate name for these lands,\" he argues, but asking him and his brother how they feel about the Russian President is perhaps the easiest way to see what sets them apart.\n",
            "Although he admits to concerns about corruption in Russia and the lack of dissent, Alexander says that \"Putin is undoubtedly one of the most influential world leaders today. There's no doubt that under his rule Russia is strengthened, and the standard of living of the majority of citizens has increased significantly.\"\n",
            "Mention Mr. Putin to Sergey, however, and he responds only in expletives.\n",
            "How their parents feel Sunday's election is likely to see Petro Porashenko, a billionaire chocolatier and key financial backer of the Maidan, swept to power ahead of former prime minister Yulia Tymoshenko, another Maidan idol. But Sergey is the only member of his family planning to vote, and even he, in a clear sign of how far Mr. Porashenko must go to win over the Russian-speaking minority. says he'll spoil his ballot by marking \"against all.\"\n",
            "There is no serious candidate from the south or east who can claim to represent those who backed Mr. Yanukovych in 2010 (he took 49 per cent of the final national vote versus 45 per cent for Ms. Tymoshenko). For many Russian-speakers, the election is simply to decide which tycoon who supported the Maidan (Ms. Tymoshenko is also believed to be a billionaire) will reap the rewards.\n",
            "Also unwilling to exercise their franchise, the Vasiliev brothers' parents fear that Ukraine's swirling politics may have broken up their once-close family forever. Their granddaughter, Alexander's 3-year-old Sofiya, now lives on the other side of a de facto international border, and it isn't clear if, when or where they will be able to see her again.\n",
            "\"It's sad,\" says Natalya Vasilieva, 60, a former oceanographer, sniffling as she shuffles photographs of her family.\n",
            "\"We don't know when Alexander will be able to come back. It's not safe for him and his family in Odessa.\"\n",
            "She and her 57-year-old husband – also named Alexander – have retired to a village outside Kherson, a city 200 kilometres east of Odessa, where they live on her pension while he tends a hobby vineyard. Like many older Russian-speakers, they share their first-born son's pro-Putin views and pine for the stable life they say they had before the Soviet breakup.\n",
            "They consider the rise of a government with ties to Right Sector an insult to their own parents, who fought in the Soviet army when it \"liberated\" Ukraine from the Nazis. There are statues to Stepan Bandera in Western Ukraine, but on the outskirts of Kherson, calling people \"Banderites\" is the same as calling them the enemy.\n",
            "Their younger son's front-line role during the revolution leaves them confused and angry.\n",
            "\"I was very upset when I found out Sergey was involved in the events at Hrushevskoho Street,\" his father says, staring at the kitchen table.\n",
            "\"He was never in the army. He doesn't understand what it means to take an oath. He was fighting against guys in the police who were upholding their oaths. An oath is a sacred thing.\"\n",
            "Mr. Vasiliev says his is not the only family divided by the conflict – some have \"much bigger divisions: People are getting divorced.\"\n",
            "His sons recoil at the idea of taking up arms against each other, as the brothers in Taras Bulba did, insisting they have decided to stop discussing politics with each other and will always put family above all else.\n",
            "But Alexander says he could see himself taking part in the \"insurrectionary movement\" in eastern and southern Ukraine, while Sergey vows to do his duty if the army ever calls up men his age to fight for their country.\n",
            "Why are the brothers so diametrically opposed? Their parents think it's because of their slight difference in age.\n",
            "Alexander was born in 1979, Sergey in 1982. \"The two boys are only three years apart, but they belong to different generations,\" their mother explains. \"They were in their youth when they lived through a turning point in history.\"\n",
            "Because he was 12 when the Soviet Union fell, Alexander was old enough to be angry when told he suddenly lived in another country. \"If someone came to your country and changes its name, would you stop loving your motherland?\" Natalya asks.\n",
            "Sergey, however, was just 9 and \"didn't feel that loss – he doesn't remember the transition,\" she adds.\n",
            "\"For him, Ukraine is his motherland.\"\n",
            "No matter what you think of present-day New York, the city has undeniably influenced the world. New York is a cultural hub—one of the greatest cities of human civilization.\n",
            "In a post in Reddit’s Pics community, user TheZackAttack01 took us all the way back to the birth of New York’s culture of diversity by sharing a century-old photo collection of immigrants arriving to New York’s Ellis Island.\n",
            "The photos were taken by Augustus Sherman, a Chief Registry Clerk who worked on Ellis Island from 1892 until 1925, according to the Public Domain Review.\n",
            "The photos were published in National Geographic magazine in 1907, giving us a glimpse of what it felt like to be one of America’s early immigrants.\n",
            "A Romanian shepherd (or Flea?)\n",
            "Some redditors thought this was the most powerful image from the whole collection. Others were more amused by the man’s striking similarity to a modern-day celebrity.\n",
            "A German stowaway\n",
            "Scandinavian children\n",
            "Italian woman\n",
            "A boy from India\n",
            "A Danish man\n",
            "A Guadeloupean woman\n",
            "Click here for the entire collection.\n",
            "The Federal Bureau of Investigation has interviewed senior aides to Hillary Clinton about her use of a private email server to conduct official State Department business, an indication the politically charged probe is nearing its end, according to people familiar with the matter.\n",
            "Those interviewed include longtime Clinton aide Huma Abedin, according to people familiar with the matter. Abedin’s lawyer didn’t immediately respond to messages seeking comment.\n",
            "The investigation has centered on why and how Clinton used her own email server for government business while she worked as secretary of state from 2009 to 2013, and whether that setup compromised classified information.\n",
            "A review of Clinton’s emails has found classified information in some of the messages, though her campaign has argued that those determinations are the result of interagency disputes about what information is properly classified.\n",
            "“From the start, Hillary Clinton has offered to answer any questions that would help the Justice Department complete its review, and we hope and expect that anyone else who is asked would do the same,’’ a Clinton campaign spokesman said Thursday. ”We are confident the review will conclude that nothing inappropriate took place.’’\n",
            "An expanded version of this report appears on WSJ.com.\n",
            "In a RogueLike the game view (GV) is a rectangular area of the map occupied by the player that is displayed on screen, an example of which is shown below. A gameview consists of two parts: a size and an origin (the x and y coordinates which define the top left corner). The origin is calculated from the player's current coordinates by subtracting half the GV width from player X and half the GV height from player Y, and making adjustments to them under certain conditions described below.\n",
            "This article describes how to calculate the coordinates required for a game view.\n",
            "Terminology\n",
            "The following terms are required in order to calculate the GV origin coordinates GVOriginX and GVOriginY:\n",
            "PlayerX, PlayerY - The coordinates of the player's current location GVWidth, GVHeight ?- The size of the game view MapWidth, MapHeight - The size of the map the player is exploring.\n",
            "It is assumed that MapWidth > GVWidth and MapHeight > GVHeight.\n",
            "For the player to be displayed dead centre in the GV GVWidth and GVHeight must be odd numbers.\n",
            "Calculations\n",
            "This origin of the GV is defined as:\n",
            "GVOriginX = playerX - GVWidth / 2\n",
            "GVOriginY = playerY - GVHeight / 2\n",
            "Therefore, the bottom right corners coordinates of the GV are GVOriginX + GVWidth and GVOriginY + GVHeight.\n",
            "However, there are obvious conditions where GVOriginX and / or GVOriginY are less than 0, or the bottom right coordinates exceed the MapHeight and / or MapWidth, so we need to make the followings checks and correct as appropriate after calculating generating GVOriginX and GVOriginY:\n",
            "Check Correction if true GVOriginX < 0 GVOriginX = 0 GVOriginY < 0 GVOriginY = 0 GVOriginX + GVWidth > MapWidth GVOriginX -= (GVOriginX + iViewWidth - MapWidth) GVOriginY + GWHeigtht > MapHeight GVOriginY -= (GVOriginY + iViewHeight - MapHeight)\n",
            "The effect of making these changes will cause the player to be displayed off centre and closer to the edge being moved towards, as shown below. If none of the above corrections are required, the player will be shown in centre of GV as shown in the picture at the start of this article.\n",
            "Code\n",
            "A Visual Studio demonstrating the above method in a simple demo which allows a player to explore a map using the keys Q,W,E,A,S,D,Z,X and C can be found here.\n",
            "Github: here.\n",
            "Have I seen this before?\n",
            "The observant amongst you will notice that this code comes from my Evil Science article?Field of Vision using recursive shadow casting: C# .Net 3.5 implementation, but I thought I'd use the code again with emphasis on how to draw the Game View.\n",
            "AP Photo / Ohio Department of Rehabilitation and Correction This undated photo provided by the Ohio Department of Rehabilitation and Correction shows Dennis McGuire. McGuire was executed Thursday in Ohio using a mix of drugs never used for capital punishment before. His family announced Friday they plan to sue since his execution was cruel and unusual punishment.\n",
            "The family of the Ohio man executed with a new lethal mixture of drugs on Thursday is pursuing a lawsuit to assure that other death row inmates do not experience the same unusual execution circumstances he did, the family and their attorney said Friday.\n",
            "\"I can't think of any other way to describe it than torture,\" said Amber McGuire, the adult daughter of Dennis McGuire, who was put to death on Thursday by use of a combination of drugs that had never been used for capital punishment before.\n",
            "McGuire, 53, made loud snorting noises during one of the longest executions since Ohio resumed capital punishment in 1999. Nearly 25 minutes passed between the time the lethal drugs began flowing and McGuire was pronounced dead at 10:53 a.m.\n",
            "McGuire was sentenced to death for the 1989 rape and murder of a 22-year-old pregnant newlywed, Joy Stewart.\n",
            "McGuire’s family plans to file a lawsuit against the Ohio Department of Rehabilitation and Corrections “to make sure that this procedure is not utilized on anyone else ever,” the family’s lawyer, Jon Paul Rion told NBC News.\n",
            "Before the execution, McGuire’s lawyers tried to delay his death penalty by arguing that the new drug method could lead to a medical phenomenon known as \"air hunger\" and could cause him to suffer \"agony and terror.”\n",
            "Assistant Attorney General Thomas Madden had argued that while the U.S. Constitution bans cruel and unusual punishment, \"you're not entitled to a pain-free execution.\"\n",
            "While the court acknowledged the execution would be an “experiment,” U.S. District Judge Gregory Frost sided with the state.\n",
            "But Rion said the procedure violated the Eighth Amendment because “the facts would demonstrate that it was cruel and very unusual.” Rion said McGuire’s son and daughter “watched their father gasping for over 19 minutes.”\n",
            "Rion said McGuire “spoke to his son and made his son promise that if things don’t go right — because everyone feared that it wouldn’t — he would pursue this to make sure that it wouldn’t happen to anyone else.”\n",
            "The family is not looking for money but will file a suit against the Ohio Department of Rehabilitation and Corrections and possibly the drug companies, Rion said.\n",
            "Prison officials gave deathly doses of two drugs, the sedative midazolam and the painkiller hydromorphone. The drug the state previously used for executions is unavailable because the manufacturers will not allow for it to be used for executions.\n",
            "Officials at the Ohio Department of Corrections did not respond to requests for comments, but Ohio prisons spokeswoman JoEllen Smith said Thursday that a review would be conducted, as is the case in all executions.\n",
            "Before the execution, McGuire's attorney called on Republican Gov. John Kasich to halt any future executions in the state if Ohio could not secure a humane method. The next execution pending in Ohio is that of a condemned Cleveland-area killer in March.\n",
            "Kasich's office did not reply to calls for comment, and the Ohio Attorney General’s office declined to comment, since the case is ongoing.\n",
            "\"This doesn’t sound like it was a complete disaster, but they don’t want anything that even has the appearance of someone suffering or a delay in death being carried out,\" said Richard Dieter, executive director of the Death Penalty Information Center, which opposes capital punishment.\n",
            "Dieter said the execution in Ohio will force other states that allow capital punishment to reevaluate their methods. \"States will now have more of a burden to show that they are using a well thought out best practice,\" he said.\n",
            "Rion said he hopes McGuire’s case will at least change practices in Ohio.\n",
            "“Are we really using humans as experiments?” he asked.\n",
            "NBC News’ Tracy Connor and The Associated Press contributed to this report.\n",
            "Related:\n",
            "Ohio killer executed with untested two-drug cocktail\n",
            "On a cold, blustery day in late February, a group of killer whales known as “K Pod” was detected swimming down the coast of Northern California from an area around Fort Bragg. The route the pod embarked on that day was straight south, hugging the coastline. As they neared Point Reyes, a National Oceanographic and Atmospheric Administration (NOAA) fisheries researcher in Washington State, Brad Hanson, who had been following the whales using satellite tag information, sent out an email alert to a group of marine scientists along the coast. This was a thrilling call to action for a marine scientist who has lived near Point Reyes her whole life, yet had seen killer whales only twice from land, and I didn’t want to miss this rare opportunity. I grabbed my binoculars and spotting scope and headed out to the lighthouse to see if I could catch sight of this wayward pod so far from its spring and summer home in the Pacific Northwest, and so near to shore.\n",
            "Killer whales are toothed members of the order Cetacea (which includes whales, dolphins, and porpoises), but as members of the Delphinidae family, they’re more closely related to dolphins than to true whales. Globally, they’re lumped together into one species, Orcinus orca. Also called orcas, they’re one of the world’s fastest-moving marine mammals, able to swim at speeds approaching 35 miles per hour, easily outpacing slower whales such as the grays and minkes they sometimes feed upon. Their size (males can range up to 32 feet and weigh 22,000 pounds) strength, and ability to echolocate all contribute to their prowess as hunters.\n",
            "But while killer whales are found in all of the world’s oceans, their lives in the wild are poorly understood, in part because there are tremendous differences between different groups of orcas. Though the species’ range spans the globe from pole to pole, individual orcas belong to regional ecological groups, called ecotypes, that have distinct ranges and behaviors. Scientists recognize at least 10 ecotypes for the species worldwide, three of which can be found off California: Southern Resident, Transient, and Offshore. While our classification of orcas is likely to change as we learn more, our growing understanding of these orca ecotypes — bolstered by recent advances in research technology and protocols — has been a major key to unlocking the mystery of the killer whales of the eastern North Pacific.\n",
            "K-25’s California Journey: On Dec. 29, 2012, researchers in Washington attached a satellite tag to the dorsal fin of a male Southern Resident orca known as K-25. Over the next few months K-25 made three visits to Point Reyes, the first a three-week January trip that halted just north of the point (red line). In February K-25 hovered off the Humboldt Coast for several weeks and twice dipped south to Point Reyes. Data and K-25 tag photo from NOAA Northwest Fisheries Science Center; for more information see NOAA’s K-25 page.\n",
            "The Southern Resident ecotype was one of the first groups of Pacific orcas to be classified. In the 1970s, scientists learned how to identify individual whales by taking photographs and then studying the images to identify distinguishing characteristics such as size and shape of fins, nicks, scars, and markings on the saddle patch, the distinctive mark on an orca’s back just behind the dorsal fin. The photos were collated into catalogs and used to compare whales and pods (groups of whales that are closely associated by behavior and genetics). In 1976, Ken Balcomb of the Center for Whale Research used this technique to identify all the individuals in the three pods (J, K, and L) of killer whales that live around the San Juan Islands of Washington. Thanks to this research, we know that the Southern Resident ecotype is composed of three pods consisting of 20 to 40 individuals each and genetically tied through their mothers. The population size of this ecotype (as of summer 2013) is 82, small enough to census all the individuals within it each year.\n",
            "But even as the photos helped identify the individuals of the Southern Resident ecotype and document the relationships be-tween them, another mystery soon emerged: Where did these Southern Resident whales go in the winter? Photo identification only works when the whales are present, breaching the surface in front of a human with a camera. For years, anecdotal sightings off the coasts of Oregon and Northern California, including some from as far south as Monterey Bay by marine biologist Nancy Black of Monterey Bay Whale Watch and others, suggested the whales moved south along the coast, but documentation was lacking.\n",
            "We’ve recently started to fill this knowledge gap with the help of satellite tagging. To tag an orca on the open water, though, is no easy task; it requires a special understanding of the whales’ behavior and ecology, as well as extraordinary finesse. Greg Schorr from Cascadia Research Collective has both, and in 2012 he and NOAA’s Hanson were able to attach a tag to the whale that was heading toward Point Reyes. Hanson told me how Schorr tagged the tall dorsal fin of this Southern Resident K Pod male with a dart gun at a distance of around 25 feet in the waters off the Washington coast. The tag was an ARGOS (Advanced Research and Global Observation Satellite) system device, programmed to record the location of the orca for short periods throughout the day and transmit that information to satellites, which were then accessed online by biologists.\n",
            "Transient orcas are regularly seen in Monterey Bay, but it’s much less common to spot them in the open water between the coast and the Farallon Islands. (Photo by David Wimpfheimer, calnaturalist.com)\n",
            "The effort to tag and follow the Southern Resident whales matters because, while the population was listed as endangered under the Endangered Species Act in 2005, marine resource managers had little information on which to base protective measures outside of the whales’ Washington/British Columbia home range. With the tag affixed, scientists were able to follow K Pod south to Fort Bragg, where the whales appeared to be feeding near the shore. The tag continued transmitting for 94 days, mapping the transit of the whale in near real time and allowing Hanson’s group to follow along the coast by car and intersect the pod by boat, collecting even more precious information about the pod’s winter habits and location. Email alerts, like the one I received, enlisted more trained observers in the study. Through photographs of other individuals in the pod, Hanson and his group were able to identify the male’s traveling companions. And from collected whale poop samples they were even able to determine what the whales had been eating on their journey. This new information will greatly expand our understanding of Southern Residents and help guide recovery efforts.\n",
            "Want even more stories about Bay Area nature? Sign up for our weekly newsletter!\n",
            "While Southern Residents stick mostly to their home in the Pacific Northwest and are rarely spotted in California, whales from the Transient ecotype are much more commonly seen in our waters. Transients can be identified visually by their robust size, solid saddle patch, and tall straight dorsal fin of the male, compared to the open saddle patch and slightly curved tip of a Southern Resident male’s dorsal fin. The females’ dorsal fins in all ecotypes are much shorter than the males’ and they’re more curved, similar to those of dolphins. Transients are frequently observed in and around Monterey Bay, though they have been observed as far north as southeastern Alaska and as far south as Southern California. According to noaa, there are around 100 individual Transient orcas off the California coast. As researchers have begun to better understand the Transients, they have divided the ecotype into West Coast Transients that range from Southern California to southeast Alaska and a second group that occurs farther west through Prince William Sound, Kodiak, and the Aleutians. Transients of either group generally occur in small pods of 10 whales or fewer.\n",
            "Scientists use differences in morphology, diet, range, behavior, and genetics to distinguish between ecotypes. They also increasingly use acoustic differences. With an underwater hydrophone, researchers can hear the echolocation clicks, whistles, and pulsed calls of the whales as they communicate, then compare the calls with those of other groups. This method of detection can also be quite useful for finding whales, especially when rough seas make sightings difficult.\n",
            "Combining acoustic signatures with behavior has yielded some remarkable insights into the attack patterns of Transient whales in Monterey Bay. In contrast to the fish diet of Southern Residents, Transient orcas feed primarily on marine mammals. Over the past decade, Nancy Black and other researchers have documented Transient orcas attacking and eating gray whales in Monterey Bay. It’s a perfect setup for the orcas: In April and May, mother and calf pairs of gray whales migrate slowly north from their breeding lagoons in Baja and warm waters around the southern Channel Islands, generally hugging the coastline. But rather than follow the long perimeter of Monterey Bay, they strike out more or less directly across the bay, using knocking sounds to navigate back up the slope on the north shore after crossing over the deep canyon. This gives Transient orcas the opportunity to detect the gray whale pairs in the open water. In 2004, Black documented 16 attacks by Transient orcas on gray whale calves. She noted that the several core family groups of orcas, numbering seven or less, gathered into groups of 15 to 32 individuals, all working together and sharing the kill. The adult reproducing females did most of the work while the young closely observed, though some males also participated.\n",
            "Researchers also discovered, by listening with hydrophones, that the Transient whales go silent before attacking; successful attacks are followed by a burst of vocal calls and whistles. Southern Resident whales do not go silent before preying on fish, presumably because fish don’t detect their vocalizations. Harbor seals, though, know the difference between these two ecotypes and are known to ignore vocalizations of fish-eating Residents but react strongly to those of their Transient cousins.\n",
            "Much less is known about the vocalizations — or any other aspect — of the third ecotype, Offshore orcas, because they are so rarely encountered. However, when they have been detected, scientists have noted that their vocalizations are distinct from those of the other two ecotypes.\n",
            "O319 lies on the beach at Point Reyes. (Photo by: Richard James, coastodian.org)\n",
            "One final way of learning about orcas is to study the rare cases in which whales have died and washed ashore. Perhaps it was fitting, then, that researchers here were able to learn about the rarest of the ecotypes through this rarest of study methods.\n",
            "In November 2011, while most of the region’s orca specialists were away at a marine mammal conference, a local beach walker discovered a dead whale that had washed ashore on a remote beach at Point Reyes. Generally, when a dead orca washes ashore, scientists try to gather data before the carcass decomposes, and fortunately there was one local marine mammal biologist not at the conference. Moe Flannery of the California Academy of Sciences (CAS) gathered a corps of volunteers to collect information and tissue (blubber, skin, and muscle) from the whale while it was fresh. Flannery also shared photos of the dead orca via email with other researchers, hoping for possible identification. The response was prompt and unexpected: Graeme Ellis at the Department of Fisheries and Oceans Canada wrote back identifying the whale as a young male called o319.\n",
            "The “O” stands for offshore, by far the rarest and least-understood of the three Pacific orca ecotypes and something extremely remarkable to find washed up on the beach in Point Reyes — or anywhere. Offshore whales are generally a smaller form than those of the other two ecotypes and their saddle patch is fainter. As with the other ecotypes, the female’s dorsal fin is much shorter than the male’s, is rounded at the tip, and often has nicks, and the male’s fin is shorter than those of the other two ecotypes and additionally has a rounded tip. Researchers first identified the ecotype off the Pacific Northwest, but Offshore individuals and pods have subsequently been observed ranging far across the eastern North Pacific from the Bering Sea south to California. Once, they were observed and photographed by a volunteer at Point Reyes, exuberantly leaping after rounding the point. Orca researchers were able to identify the pod from the photograph of a male, adding the observation to their database. Researchers believe that Offshore orcas feed primarily on schooling fish and sharks such as Pacific sleeper sharks, blue sharks, and opah. Their teeth are often heavily worn, perhaps be-cause sharks have abrasive skin.\n",
            "The orca known as O319 had originally been sighted in 2002 as a juvenile and had been photographed in the Pacific Northwest numerous times — including just two months earlier off the west coast of Vancouver Island. Based on its size and lack of secondary sexual characteristics like curled flukes and large dorsal fin, Graeme estimated that O319 was no more than 15 years old at the time of death, an age when males are starting to mature sexually. (Orcas can live as long as 90 years, though the average age of death for females is 50 years and for males 30, so this male died relatively young.)\n",
            "The initial plan was to keep just the tissue samples and maybe the skull, but once Graeme had identified the dead orca as an Offshore, Flannery realized she needed to try and recover the entire skeleton, to archive it in the CAS research collection.\n",
            "Researchers and volunteers worked for days to cut up the stranded O319 and haul it off the beach and to the California Academy of Sciences. (Photo by mojoscoast)\n",
            "Tides, waves, and steep cliffs limited access to the beached whale, but this was a rare opportunity that couldn’t be passed up. It was a monumental task, requiring the assistance of many individuals and institutions. So the group of scientists and volunteers mobilized by the Marine Mammal Stranding Network made the trip to the beach to undertake the long, stinky process. The first step was to cut the whale into pieces to get it off the beach. Volunteers with ropes, stretchers, and strong backs then hefted the bones and body parts up a steep, narrow path to the top of a ridge and then along the path to the parking area. Sarah Codde, a national park biologist who was one of the first to see the dead whale and participated in the whole removal process, said that packing out the skull was the hardest task of all, requiring six people using a stretcher and ropes. The necropsy, begun in the field and completed later in the lab, revealed hemorrhaging in the head, a broken rib, blood-stained vertebrae, and blood in the pleural cavity, indicating that the orca had died from trauma, perhaps caused by another orca or a boat strike.\n",
            "Once the full skeleton had been brought to CAS, scientists and volunteers used a process called maceration, in which bones are soaked in water for long periods, to remove the remaining flesh and grease. Soaking off the flesh took two months, and degreasing to whiten the bones with sodium perborate took another four months. Finally, in March 2013, the skull was moved to the CAS roof so the sun could complete the whitening and degreasing.\n",
            "Lee Post was chosen to orchestrate the next stage, the articulation of the bones. Post, known as “the Boneman,” is a resident of Homer, Alaska, who wrote the series The Bone Building Books and had worked on a similar project at the Marine Science Center in Port Townsend, Washington. The CAS project was unique, however, because the entire articulation process was done in full view of the public inside the museum. Under the gaze of fascinated visitors, CAS staff and volunteers carefully assembled all 286 pieces of the skeleton—a jigsaw puzzle of whale-size proportions—over the course of two months. John Eleby, a volunteer articulator and national park ranger at Point Reyes, participated in the entire process of handling the cleaned bones, including sorting, identifying, measuring, weighing, and mapping, and finally attaching the skull and lower jaw to the remainder of the skeleton.\n",
            "Once completed, O319’s skeleton was put on display as part of the “Built for Speed” exhibit at CAS during the summer of 2013. It is now on permanent exhibit in the lobby at CAS.\n",
            "Articulating O319 For more about the reconstruction of O319, read our story about Lee Post, head over to the California Academy of Science’s orca articulation blog, or watch the Academy’s time-lapse video story below:\n",
            "While we have learned a lot from this rare beach-cast Offshore orca and from the satellite-tagged Southern Resident, there is still much to be discovered about these remarkable whales so we can properly manage our marine resources and give orcas a chance to survive in a rapidly changing marine environment subject to rising water temperature, acidification, and declining food resources. If, for example, scientists discover that Southern Residents feed only on Chinook salmon all the way south to Monterey Bay, should fisheries biologists revise how these salmon are managed along the Pacific coast? Whether orcas can adapt to such changes will be the subject of future scientists’ research, applying constantly evolving technologies not even imagined today.\n",
            "Many orca researchers believe that there may be two new ecotypes in the eastern Pacific. An “L.A. Pod” was identified in the 1980 and ’90s near Los Angeles with individuals that are much smaller and display more superficial gashes compared to the other three ecotypes. (Members of this pod were filmed in 1997 just off the Farallon Islands attacking and killing a great white shark, a behavior not observed before or since in orcas.) And a research group led by Jaime Jahncke, a marine biologist with Point Blue Conservation Science, observed another previously unidentified pod of five killer whales during a research cruise in July 2013, directly west of the continental shelf near the Farallon Islands. The whales looked similar to Transients but members of the pod did not fit any of the photo-IDs for California. With the continued discovery and potential identification of new ecotypes, efforts to protect and restore these populations become both more illuminating and more challenging.\n",
            "Back at Point Reyes, I waited out near the lighthouse for several hours on that raw February day, straining to see the bold black-and-white markings against the gray seas, but I never saw the tagged male of K Pod. I found out later that not long after I had arrived at the lighthouse, the pod had turned around near Tomales Point, just beyond the range of my spotting scope, and headed back toward Fort Bragg. Why? We’ll never know. But we know they did eventually come back. According to the satellite data, the K Pod did visit Point Reyes another time that winter — at night when no one was looking.\n",
            "Everyone should want to boycott Israel. They are a plague on this planet. They have sunk their fangs into every major national government, specifically the USA, where Jewish/Israel Elite are the ones who call the shots in the US Government. Jews, not US politicians, run the ship.\n",
            "Governments are too afraid to rise up against the jews after the last leader who tried to do so, Hitler, was utterly destroyed in doing so, despite being just in his actions to take on the jews and their banks. The jews crushed Hitler to send a message - we will destroy any nation's leadership who tries to stand up to us. Then, the jews faked and embellished the holocaust to allow themselves to play perpetual victim for the indefinite future. This makes it to where you basically can not criticize jews in any way, because \"holocaust\".\n",
            "Mechanics Series - #10 Colony Production\n",
            "Production, the total industrial output of a planet, is a central part of Lord of Rigel’s economy. This is based on the population assigned to industry, any improvements on a planet, and most importantly the mineral content of a planet. Planet mineral content varies from ultra-poor (the worst) to ultra-rich (the best). The mineral content that a planet has is based on star type and whether a planet is in a nebula. Generally, hotter and younger stars (Blue and White) tend to have more mineral rich planets while colder and older stars (Yellow, Orange, Red) tend to be mineral poor but more rich in organics. This overall leads to a natural breakdown of having industrial worlds supported by agricultural worlds. Mineral content affects the base production of each population unit assigned to industry.\n",
            "Industry points are then applied directly to producing whatever active item is in the build queue. If there isn’t an active item, then industry points can instead be applied to housing, thus granting a population bonus, or to trade goods where half of the industry points are converted into billions of credits (BCs). Increasing taxation is another means of converting production into credits - higher taxation results in lower production but more BCs.\n",
            "BCs can be used to purchase production, enabling something to be built in one turn if there are sufficient funds. The cost in BCs decreases the further along an item is in construction. Overall, purchasing production can be prohibitively expensive and should be used sparingly.\n",
            "One final mechanic tied to production is pollution. Planets have a built in “environmental tolerance” that is tied to the size of a planet. Basically, larger planets tend to have more places to stash pollution. Once production exceeds this tolerance level, planets begin to build up pollution. Each point of pollution results in a morale and farming penalty.\n",
            "If the pollution of a planet is well over the environmental tolerance, a planet can have environmental degradation. This has stiffer food and morale penalties and planet climate types can degrade (ex. a Terran world to an Arid one). Reducing pollution can stabilize a planet, but terraforming may be needed to restore it. In extreme conditions where an environment has degraded to an insane degree and a tiny planet is being over harvested, it may even be outright destroyed!\n",
            "Join us next week!\n",
            "When it comes to heart health, is marijuana any safer for you than tobacco?\n",
            "A new study published today in the European Journal of Preventive Cardiology concluded that marijuana use is associated with a threefold risk of death from hypertension.\n",
            "“This is not surprising since marijuana is known to have a number of effects on the cardiovascular system. Marijuana stimulates the sympathetic nervous system, leading to increases in heart rate, blood pressure, and oxygen demand,” said Barbara A. Yankey, study lead author, and PhD student in the School of Public Health at Georgia State University, in a press statement.\n",
            "Their results were based on a specially designed retrospective study.\n",
            "Researchers analyzed data from 1,213 participants who were considered marijuana users based on their responses to the 2005-2006 National Health and Nutrition Examination Survey (NHANES).\n",
            "This data was cross referenced with mortality data from 2011 from the National Center for Health Statistics.\n",
            "“Steps are being taken toward legalization and decriminalization of marijuana in the United States, and rates of recreational marijuana use may increase substantially as a result,” said Yankey. “We found higher estimated cardiovascular risks associated with marijuana use than cigarette smoking.”\n",
            "Advocates question study results\n",
            "Marijuana advocates are skeptical of the research.\n",
            "Paul Armentano, the deputy director of the National Organization for Reform of Marijuana Laws (NORML), not only disputes the results of the study, but the methodology of it as well.\n",
            "Researchers explain that their “retrospective study” was utilized due to the lack of longitudinal studies on the long-term cardiovascular effects of marijuana use — something Armentano says is patently false.\n",
            "In his response to Healthline, Armentano pointed out two separate longitudinal studies that examined the relationship between marijuana use and some common cardiovascular ailments.\n",
            "The first, published this year, followed more than 5,000 individuals for 25 years, beginning in the mid-1980s.\n",
            "“Compared with no marijuana use, cumulative lifetime and recent marijuana use showed no association with incident CVD [cardiovascular disease], stroke, or transient ischemic attacks, coronary heart disease, or CVD mortality,” the authors of that study wrote.\n",
            "Another longitudinal study from last year involving 1,037 individuals who were followed for 38 years came to a similar conclusion.\n",
            "“We found no association between cannabis and cardiovascular risks [e.g., high blood pressure, higher cholesterol], which may appear at odds with evidence that cannabis use increases risk for cardiovascular complications,” the authors wrote.\n",
            "Armentano also argued that this new study’s results are also jeopardized because of the ambiguous definition of “marijuana user.”\n",
            "The only qualification to meet this definition was if NHANES respondents answered “yes” when asked if they had ever used marijuana. The study did not ascertain frequency of use, or if a respondent even continued to use marijuana at all through the documented time period.\n",
            "Some cautions\n",
            "Armentano acknowledged that cannabinoids (a class of chemicals found in marijuana, — the most well-known is THC, a psychoactive) do affect blood pressure — a subject that he has written about previously.\n",
            "Other studies have also linked smoking marijuana to heart attack risk, stating it is a “rare trigger of myocardial infarction.”\n",
            "Armentano cautioned that, “Potential high-risk populations may wish to refrain from cannabis inhalation because of these concerns.”\n",
            "Ah! Delhi. What can I say about it that hasn’t already been said before? Well, it’s not new things I want to talk about. They’re actually all quite old!\n",
            "I write blogs and then I let them die. So as I make a comeback- one that I hope is a real one (yes, I say that every time) – the first thing I write about is my city. I love it as much as I love to hate it, and one of the main reasons why I love it is the layers and layers of history that it is home to. It is the same history (and its art) that I hope to write about bit by bit, hopefully breaking the stereotype of the “boring and drab” nature of such posts.\n",
            "A part of the city that Delhites proudly announce as their own to non-Delhites is Chandni Chowk. What we understand as Chandni Chowk in today’s Delhi is actually a section of Shahjahanabad, a city that Shah Jahan established as his capital in 1648 on the banks of the river Yamuna. Shahjahanbad as a whole is what we now refer to as ‘Old Delhi’- the city of Mughal Delhi that extended between the now surviving Kashmiri Gate on the north, Turkman Gate on the south, Delhi Gate on the south-east, Ajmeri Gate on the South West and Nigambodh Gate on the north-east.\n",
            "Chandni Chowk refers to a square on the street that runs through the centre of Shahjahanabad. This street extends between the Red Fort on one end, and the Fatehpuri Masjid on the other. The ‘Chandni Chowk’ was commissioned to be built by Jahanara Begum- one of the daughters of Shah Jahan and Mumtaz Mahal. The name Chandni Chowk (‘Moonlight Square’) is believed to have been chosen because at the centre of this square was a little pond of water that reflected the moonlight. While we often hark back to the splendour of Shah Jahan’s court and think of Chandni Chowk in historical terms as a Great Mughal neighbourhood, we ought not to forget that it was the very same city that continued to be occupied by the Mughals until the last Mughal emperor of Delhi, Bahadur Shah Zafar was deposed in 1858.\n",
            "Today, Chandni Chowk is among Delhi’s, and perhaps the country’s biggest wholesale markets selling everything from fabric to jewellery, from lehengas to utensils, and from almonds to bangles. But if one visits with a keen, discerning eye something historical pops up after every few metres. While the Red Fort is well known and visited by a lot of locals and tourists alike, it is the lesser known and sometimes neglected buildings in the area that are equally significant and very often magnificent, that add a charm to the locale. There are literally dozens of these scattered all over the vicinity but I’ll pick just a few.\n",
            "Fatehpuri Masjid: Not as popular as the Jama Masjid, but definitely as old, the Fatehpuri Masjid lies at one end of the long road that leads to the Red Fort. It was erected in 1650, soon after the Mughal capital was established at Shahjahanabad. It was built by one of Shah Jahan’s wives, Fatehpuri Begum, from whom it derives its name. The façade of the mosque, much like the Jama Masjid and the Red Fort is made from red sandstone. It has a simple, undecorated pishtaq (the monumental gateway) through which one enters the mosque. The scale of the building is much smaller than the Jama Masjid. The sahn (courtyard) has a little pool with a fountain for wadu (ablutions) which is typical to most mosques. Interestingly, after the revolt of 1857 the British who had taken over the entire city of Shahjahanabad and deposed the Mughals sold the mosque to a local banker, Lala Chunnamal (whose haveli survives to this day in Chandni Chowk). It was only in 1877 that the government under Lord Lytton acquired it back, and restored it to the Muslim community.\n",
            "Image source: http://www.skyscrapercity.com/showthread.php?t=255181\n",
            "Lala Chunnamal’s Haveli: Located in Katra Neel, this haveli is still occupied by the descendants of Lala Chunnamal. While the ground floor is now occupied by shops, the erstwhile grandeur of the haveli continues to be intact on the floors above. It is an example of a haveli built around two courtyards. It has high ceilings and one can still see decorative chandeliers and tile work from the 19th century and an arched, highly decorated fireplace which, believe it or not, is still functional.\n",
            "Image source: http://www.the-south-asian.com/may2007/Chunnamal-haveli-interiorDSC00287.jpg\n",
            "Kalan Masjid/ Kaali Masjid: Some buildings in the area predate Shahjahanabad (now you know what I mean when I say layers and layers of history!) Near the Turkman gate stands a mosque that was erected in 1387 by Khan-i- Jahan Junan Shah, a principal minister at the court of Feroz Shah Tughlaq (r. 1351-1388). This is one of a series of seven mosques that he is said to have built. The word ‘kalan’ means large and true to its name, the mosque is a very prominent structure, built on moderately high ground. It is a two-storeyed building made of common quartzite sandstone and was pretty heavily decorated in white, pale blue and green paint. However, in the recent times it has been covered with brightly coloured, glossy paints. The bays of the mosque are covered by thirty domes that can still be seen when one climbs up to the roof of the mosque.\n",
            "Image source: http://olddelhiheritage.in/kalan-masjid-kali-masjid/\n",
            "Shri Marwari Public Library: I don’t think too many of us Delhites believe that there are any public libraries in our city. Well, we’re wrong. Not only is Chandni Chowk home to a public library, it is home to one that was built in 1915 and played an important part in India’s freedom struggle. Located on the main road between the Red Fort and the Fatehpuri Masjid, this library was set up by late Seth Kedarnath Goenka who himself was a freedom fighter of his time. The library reflects an amalgam of colonial and classical architectural styles. The arched windows and pillars keep the old world charm alive. This library is home to three thousand rare Hindi and Urdu books and their prized possessions include handwritten manuscripts of freedom fighters Bal Krishna Gokhale, Bal Gangadhar Tilak and Pt. Madan Mohan Malviya, among others.\n",
            "Image source: http://marwarilibrary.com/PhotoGallery.aspx\n",
            "The next time you’re on a rickshaw down the main axial road of Chandni Chowk, look around. Somewhere in between the hustle bustle and chaos, the thousands of people, the noise of the traffic and the smoke from an auto, are buildings that belong to times much older. The Town Hall which is now an MCD office was built in 1864 (and is officially the building that replaced the remnants of Jahanara’s serai). The Allahabad Bank building was built in the 1930s, the Punjab National Bank building in the 1920s and the Gaurishankar temple where people are still offering worship, in the 1760s. And in that moment when you reflect on these things, you will be overwhelmed by the history and heritage that we live in the midst of- every single day.\n",
            "For books on Delhi, and Chandni Chowk check: Lucy Peck (2005) Delhi: A Thousand Years of Building, Roli Books; Vijay Goel (2003) Delhi the Emperor’s City: Rediscovering Chandni Chowk and its environs, Roli; William Dalrymple (1993) City of Djinns, Penguin; TGP Spear (1937) Delhi: A Historical Sketch, OUP.\n",
            "Advertisements\n",
            "Introduction:\n",
            "When loading up De_nuke on competitive MM, players generally wish to start on the CT side due to the nature of the map heavily favouriting CT's. Even when it comes to professional leagues (ESEA, ESL, Star Ladder) after the knife round the winner of the round generally opts to !stay/!switch to the CT side. As the events unfold, 5 non-premades can have major problems winning a round or two as T's even in the pro-league (full-premade) teams can have major problems. For this reason here is a small list of different ways of taking bomb sites as T's on Nuke.\n",
            "Basic strategies:\n",
            "A Rush Lobby:\n",
            "A rush is a classical strategy I have seen (and used myself) on eco rounds. It is a very effective surprise strategy since nobody usually awaits a mindless rush onto A. The key in this rush is a quick bomb plant and entry frags, remember the whole point of this rush is a solid surprise for the CT's. Generally to avoid getting sprayed down when entering A site, you should split up, go 3 blue box and 2 squeaky. You can also throw a HE into the squeeky to make entering easier and a few smokes. This could also work as a distraction (potentially could win T's in blue box 1-2 seconds).\n",
            "The general equipment I'd recommend is:\n",
            "Eco vs Eco: 2 armour buys, 2 flashes 1 smoke, 1 HE (optional: p250).\n",
            "Eco vs Norm: 2-4 p250's, 1-3 smokes, 2 flashes, 1 HE (If you have Juan Deag on your team then 1 deagle).\n",
            "Norm vs Norm: Not recommended unless you can out aim your opponents.\n",
            "A Main:\n",
            "A main push is essentially the same beginning setup as B outside, but instead of pushing for secret push into main and take bomb site A by number. A fair thumb rule is to expect two CT's in heaven, one ontop of main and one next to ladder/cat. Flashes are adviced and after the bomb plant smoking off heaven, main and/or blue box is essential.\n",
            "Equipment recommended:\n",
            "Eco vs Eco: 2 flashes, 1 smoke (optional: p250)\n",
            "Eco vs Norm: Not recommend unless you are sure that you can out aim your opponents.\n",
            "Norm vs Norm: 2-3 smokes, 4 flashes,\n",
            "B Outside:\n",
            "B rush through yard is a standard, normal buy strategy. Outside is generally held by an awper with one teammate ready to help from either main or heaven. The awper is relatively easy to deal with, a few correctly placed smokes should do the job. A flash bang doesn't hurt either! Once you've mauled through yard you should split up 1/2 into vents, one window, two squeeky doors on b.\n",
            "Equipment recommended:\n",
            "Eco vs Eco: 1 flash, 1 smoke (optional: pistols of your choice)\n",
            "Eco vs Norm: 3 smokes, 1 flash, 2-4 p250, (Once again if you have Juan Deag on your team go nuts)\n",
            "Norm vs Norm: 3 smokes, 2 flashes, 1 awp (Optional: molotov into main)\n",
            "B Ramp:\n",
            "B ramps is a tough one since the entrance onto ramps is a major choke point where two CT's can easily fend off a swarm of T's. In this case grenades of sorts are vital to the success of entering ramp (Or really good accuracy, you should prior to entering ramp try and pick off the person at the back boxes, unless he's an awp). You should try and always smoke off back boxes on ramp and upon entry chuck a flash or two AND possibly to avert getting back stabbed by CT's smoke hell (the area underneath heaven).\n",
            "Equipment recommended:\n",
            "Eco vs Eco: 1-2 smokes, 2 flashes, 1 armour (optional: p250)\n",
            "Eco vs Norm: 2 smokes, 2-4 p250, 3 flashes, 1 HE (Juan Deag still applies)\n",
            "Norm vs Norm: 2 smokes, 3 flashes, 2 HE's and a good entry fragger.\n",
            "B Vent Rush:\n",
            "B vent rush is once again a surprise strategy, but this strategy is the riskiest of all since it can fail before you even get past squeeky. The quickest way is to just rush in, but if you want a tad bit more safety while executing this rush you can HE the door follow it with a smoke and before you enter flash. It is muy importante to get the plant as soon as possible. This will pressure the CT's and cause potential mistakes in your favour.\n",
            "Equipment recommended:\n",
            "Eco vs Eco: 1 smoke, 2 flashes, 1 HE (optional: p250)\n",
            "Eco vs Norm: 1 smoke 2 flashes 1 HE, 2-4 P250/cz-75 (Still works if you have Mr. Juan Deag on your team)\n",
            "Norm vs Norm: Not recommend in a full buy vs full buy round.\n",
            "B Suicide Rush:\n",
            "B suicide rush is an extremely risky strategy! It is the twin brother of the B vents rush where instead of running in through squeeky into vents you go from outside into main into vents. There is no additional description available, just pray to whatever you can and as crazy as it may be... it might just work. You could also try to smoke off the entire main to A entrance to create as much cover as possible!\n",
            "Slightly more advance strategies:\n",
            "A Surround:\n",
            "A surround strategy is what it sounds like. It is a high risk high reward strategy and needs correct execution to make it work, BUT if done correctly the round is yours! In basis A surround is a deviation of A split, where as in a split you push 2/3 players through lobby into A and subsequently 3/2 players from outside into main. Here you will send one man into hell to either try and pick off rotators from ramp or try and push into heaven. Smokes are extremely important in this strategy since you want to keep your opponents guessing as long as possible, picking off the garage/main player is as important as smoking off the correct areas. Assuming your awp eliminated the outside holder you should smoke off main entrance and heaven from the outside. From here you can either decide to wait for the main smoke to dispurse or move in with your two players in lobby drawing aim away from main allowing the two players at main to enter through smoke (use flashes). Here the ideal situation would be to take out the player closer to the heaven ladder first so your lurking player can assist you from heaven if he chooses to go up. Once bomb is planted just hold the plant and the round should be yours and if executed correctly you should have low amounts of casualties!\n",
            "Equipment recommended:\n",
            "Awp, 4 smokes, 4 flashes, 2 HE's, 1 molotov (the molotov can be used by the lurking player to stop the rotation from ramps into heaven).\n",
            "A Split:\n",
            "A split is the big brother of the A surround strategy. The A split is the easier of the two since it require less precise team co-ordination and correct timing. An A split execution is a (depends on your preference) 2 lobby 3 outside into main or 3 lobby and 2 outside into main. The key here is to gain control of the yard area and shift the opponents attention towards yard allowing the lobby players to catch one of the two heaven players off guard. Once lobby has entered the outside crew should enter within seconds. This can create an element of surprise for the CT's. When going outside you should smoke off garage and between garage red box and under cat walk. This will eliminate most of the vision for anyplayer trying to rotate from ramps to outside/hell and garage. Another smoke that can come in handy is from blue box onto squeeky or straight from squeeky room to front of squeeky creating a bit of cover for the player coming from squeeky. Another way is for the outside crew to enter a few seconds after the smoke outside of squeeky deploys so the aim will shift towards blue box/squeeky for a moment allowing the first shot to be made by the players in main (obviously followed by the 2/3 men in lobby).\n",
            "Equipment recommended:\n",
            "4 Smokes, 4 flashes and a molotov (the molotov should be used to barbeque the ladders into heaven).\n",
            "A Silo Push:\n",
            "A silo push is a special strategy that can be used in loosing streak rounds (not eco keep in mind). This is a genuine strategy no one would expect to be used because it is a bit ridiculous. You want to hoist up your awp onto the silo to try and get the first scout over outside area and possibly a pick. After a succesful pick you hoist the rest of the team up onto the silo from where you jump onto the main roof. If you want to you can smoke off certain areas to create an illusion of a secret push. After waiting for about 5-15 seconds you jump down off the main roof and into main and onto A site. This should be done as fast as possible to create an element of surprise!\n",
            "Equipment recommended:\n",
            "An awp, 2 smokes, 1 molotov and at least a few flashes.\n",
            "B Marathon:\n",
            "B marathon is a strategy seen played by professional teams where they move from outside yard into hell and into ramps (Marathon is just a made up name). This can create confusion since when moving with five players through yard the CT's would expect either a secret push or main push creating a fun element of backstabbing surprise. Depending onwhich illusion you want to create (Setting up for a main push or secret) you should smoke areas off appropriately.\n",
            "Equipment recommended:\n",
            "Mix of molotovs and smokes to fit your needs and possibly an awp if you feel like it although it is not recommended.\n",
            "B Ramps Split:\n",
            "The B ramps split now is identical to the A split, but instead of going to A through lobby/outside you go to B through ramps. Once again you split your team into 3/2 comms room and 2/3 yard. The two players in communications wait for the players outside to make their way to hell where they can now create a sandwich to the poor players that try and hold ramps. Once executed correctly push ramps into B and plant. The key here is to pick any CT that is outside and shift walk towards hell through garage, don't forget to smoke off main!\n",
            "Equipment recommended:\n",
            "An awp, 2 smokes, 2 decoys (which can be thrown into secret or instead send a player into secret).\n",
            "B Outside Split:\n",
            "B outside split has the essense of the other split strategies, but (as usual) instead of pushing through comms those 2/3 players push through secret and the 3/2 others push through hell into ramps onto B. This can catch rotators off guard and especially the ramp holders. You sould smoke main and heaven to cut off extra vision so the players who move into ramps are not detected. Once again try and pick off the outside player with an awp. The less info the better.\n",
            "Equipment recommended:\n",
            "An awp, 3 smokes, 3 flashes.\n",
            "These are all risky strategies, but if executed correctly can pay off greatly. Hopefully these gave you a good idea on some variation of tactics on Nuke when playing as terrorists. Remember execution is key!\n",
            "Basic Callout Map:\n",
            "Aaron Dobson is Mr. June. Year after year, the young Patriots wide receiver makes big plays in the spring practice program. This year is no different.\n",
            "Each day, Dobson has made at least one spectacular catch. He had the play of the day on Tuesday with a one-handed, left-handed leaping touchdown catch against two defensive backs on a long pass in the end zone. On Wednesday and Thursday, he made acrobatic catches on the sideline against Malcolm Butler in tight coverage. He had seven catches on Tuesday, eight on Wednesday, and 12 on Thursday, with just one drop over the course of minicamp.\n",
            "Advertisement\n",
            "Unsurprisingly, excitement is tempered. In the past, there’s always been something that stands in his way on his path to asserting himself as a top wide receiver in the Patriots offense. Most recently, it’s been injuries. A foot injury ended Dobson’s rookie season, and cut into his preparation for his second season, which ended with a hamstring injury. In 2015, he went on the shelf with a hip injury.\n",
            "With those injuries weighing him down, Dobson knows that staying healthy has to be his primary objective.\n",
            "“I’m just trying to stay healthy to help my team and get better as a player,” he said after practice on Thursday.\n",
            "But it’s not the only thing that will go into whether or not Dobson sticks around for 2016 — and yes, there is doubt that the former second-round pick will make it to September as one of the wide receivers on the final 53-man roster.\n",
            "In order to make the final cut, he’ll have to outperform the other wide receivers fighting to make the roster, and there are plenty. Keshawn Martin, Devin Lucien, DeAndre Carter, Chris Harper, and Nate Washington are among the receivers Dobson will be battling.\n",
            "Making those big plays has been helpful for Dobson’s confidence.\n",
            "Advertisement\n",
            "“I feel like you’ve always got to have confidence anyway, but that definitely can boost your confidence [when] you make a great play,” Dobson said. “As a receiver, that’s what we try to do, but be consistent and try to make all those plays.”\n",
            "Meet the Patriots’ 2016 draft class\n",
            "“The Obama administration has secretly been ramping up on long-term counter terrorism policies that will essentially institutionalize the tracking, targeting, and killing of individuals—at will—for years and years.” - www.commondreams.org\n",
            "Gregg Miller over at The Washington Post today had a very detailed 5 page story on Obama’s ‘kill list’ and the expanding nature of it. I highly suggest anyone and everyone go and read it.\n",
            "In it Miller explains that over the last 4 years, Obama and his Administration have created a detailed list of “terrorist” suspects; this list has come to be known inside the white house as a “disposition matrix”.\n",
            "Greg Miller also writes that:\n",
            "Although the matrix is a work in progress, the effort to create it reflects a reality setting in among the nation’s counter terrorism ranks: The United States’ conventional wars are winding down, but the government expects to continue adding names to kill or capture lists for years.\n",
            "It is clear that the Presidents promise of 'change’ has certainly come true. President Obama has changed the way America dispenses justice. Due process free, drone deployed justice is now looking to be the new normal in the Washington beltway.\n",
            "Let us all not forget about a joint report issues by the Stanford Law School and the New York School of Law titled “living under drones”. It details the threat that civilians in Pakistan live under constantly from the fear of death by the drones deployed by the United States. It also explains that the number of civilians killed by the United States government (my government) is far greater than any Obama administration official will admit.\n",
            "Also the official definition now being used to target known militants is so vague it literally can include any male over the age of 16. The official definition is stated as “all military-age males in a strike zone.”\n",
            "Mills also points out that:\n",
            "Meanwhile, a significant milestone looms: The number of militants and civilians killed in the drone campaign over the past 10 years will soon exceed 3,000 by certain estimates, surpassing the number of people al-Qaeda killed in the Sept. 11 attacks.\n",
            "Is America’s bloody thirst quenched yet? No not in the least. America under the direction of Obama is now institutionalizing the killing of human beings in other countries even if those humans are American Citizens. It has happened to “teenager, Abdulrahman al-Awlaki, a U.S. citizen who was born in Denver in 1995”, and it can and most certainly will continue to happen unless fierce and swift push back is brought against Obama and his counter terrorism cronies.\n",
            "Drones have been deployed in Yemen, Pakistan, Somalia, Afghanistan and soon Africa, and they will eventually be flying over head in the United States soon.\n",
            "This disposition matrix created by the White House has left two options for the American Citizen:\n",
            "Option A) you can take the red pill and forget all about this kill list, you can wake up tomorrow morning as if everything is fine and act like America doesn’t kill innocent people by the use of remote controlled drones.\n",
            "Or option B) you can take the blue pill and wake up to the realization that the drone war is wrong, creating more enemies than friends, heartless, while also being disastrous foreign policy. The choice is yours…What pill will you take? I have chosen to swallow the blue pill, and there is no turning back.\n",
            "on a side note:\n",
            "* Do not mistake my anger with the Obama Administration over this kill list as showing some type of support for the Republican Party Nominee Mitt Romney. In the final presidential debate on Monday 10/22/12, it was clear that good'ol boy Mitt was a avid supporter of drones.\n",
            "Mitt Romney: “I believe we should use any and all means necessary,” Romney said when asked if he agreed with the president’s policy. “I support that entirely and feel the president was right to up the usage of that policy.” – Mitt and Barack Love drones\n",
            "Both parties have become nothing more than different sides of the same coin.\n",
            "** The wording of this KILL LIST is curious. It seems vague enough to not slap you in the face yet interesting enough to grab your attention. If you were to hear that former President Bush has something known as a 'disposition matrix’ one might not think twice about whether it was a list of humans to be hunted, targeted, and killed by the use of drones. I feel that the wording of this list was deliberately chosen as to not sound too many alarms.\n",
            "*** picture from: http://dissociatedpress.com/wp-content/uploads/2012/10/obama-drone-photo-225.jpg\n",
            "WASHINGTON (Reuters) - Civil rights advocates took extraordinary steps over the last three months to persuade the city of St. Paul, Minn., to withdraw a fair-housing case the U.S. Supreme Court had already agreed to hear, reflecting their expressed fears about the court under Chief Justice John Roberts.\n",
            "Their successful effort, actively joined by the U.S. Justice Department and former Vice President Walter Mondale, underscores how liberals today often try to avoid the court while conservatives believe the timing is right for them.\n",
            "The case that was to be heard on Wednesday, Magner v. Gallagher, centered on local housing-code enforcement but would have tested federal Fair Housing Act protections for racial minorities against predatory lending and other abusive mortgage practices.\n",
            "“There was a real concern that this court would weaken effective enforcement” of the act, said Wade Henderson, president of the Leadership Conference on Civil and Human Rights.\n",
            "The St. Paul episode stands out as the first major civil rights matter in years to be withdrawn after the justices had agreed to take it up. Dismissal of a Supreme Court case through either a settlement by the parties or, in this situation, a decision by a petitioner to drop its appeal, is extremely rare. It has happened only twice this term, and last session not at all.\n",
            "St. Paul’s actions arise in the context of a changed court. After Roberts became chief justice in September 2005 and Justice Samuel Alito succeeded moderate conservative Sandra Day O’Connor in January 2006, the high court moved further to the right, notably in disputes over race-based policies and abortion rights. A five-justice conservative majority on the nine-member court also reversed precedent in the politically charged campaign-finance area, as with Citizens United v. Federal Election Commission, which opened the door to more corporate money in elections.\n",
            "The St. Paul case began after the city stepped up enforcement of its housing code against conditions such as rodent infestation, missing dead-bolt locks and inadequate heat. In a set of lawsuits challenging the city’s action, landlords contended that “selective” code enforcement was driving up their costs and forcing them to abandon or sell buildings.\n",
            "In the part of their case that prompted national civil rights leaders to intervene, the landlords claimed the city was engaged in illegal housing discrimination because the tough enforcement ended up mostly hurting African Americans and other racial minorities who make up the majority of low-income renters. The landlords, in an unusual twist on a fair-housing claim, said the city’s enforcement was driving them to sell or abandon property, exacerbating the shortage of rentals for racial minorities.\n",
            "“DISPARATE IMPACT”\n",
            "At issue before the Supreme Court was the use of “disparate impact” claims targeting seemingly neutral practices that have a discriminatory effect. Such claims have for decades been a way that lawyers representing African Americans and other minorities tried to attack policies that do not directly discriminate yet have the effect of putting certain groups of people at a disadvantage.\n",
            "The Fair Housing Act, passed in 1968 to prohibit bias based on race in the sale or rental of housing and related services, does not explicitly allow such disparate-impact claims.\n",
            "Yet lower appeals courts, including the U.S. Court of Appeals for the Eighth Circuit in the St. Paul case, have interpreted the act broadly and declared such “disparate impact” claims covered by the act. Federal housing officials have also said the law prohibits practices that have a discriminatory effect even when no intention to discriminate exists.\n",
            "The city, backed by conservative groups such as the Eagle Forum and Pacific Legal Foundation, had appealed to the Supreme Court, urging that only direct bias violated the law.\n",
            "CIVIL RIGHTS CONCERN\n",
            "When the Supreme Court announced on November 7 that it would hear St. Paul’s appeal, national civil rights advocates and lawyers in the Justice Department civil rights division became concerned. They feared a potent weapon against housing bias could disappear, particularly at a time when it was being used against subprime lending and other mortgage abuses.\n",
            "Under the disparate-impact theory, judges can examine aggregate data on lending practices to find disparities that might otherwise remain concealed. The approach, the American Civil Liberties Union told the court in a filing, “is uniquely powerful as a means of smoking out illegitimate discrimination.”\n",
            "Henderson, of the Leadership Conference on Civil and Human Rights, flew to St. Paul to meet with Mayor Chris Coleman to encourage the city to drop its appeal. The civil rights coalition had submitted a “friend of the court” brief against the city’s appeal.\n",
            "Henderson said he told the mayor the local case could have large national consequences. “This court has shown a real propensity to rule against remedies that had been used by civil rights advocates, especially those related to race-based remedies,” Henderson said this week.\n",
            "Mayor Coleman referred requests for an interview to City Attorney Sara Grewing, who said Coleman became convinced over a series of conversations with advocates such as Henderson and those in the U.S. Justice Department that the city should seriously consider ending its appeal related to Fair Housing Act claims.\n",
            "Assistant Attorney General Thomas Perez, who runs the U.S. Justice Department civil rights division, personally called Coleman and Grewing, Grewing said. Perez emphasized the use of the disparate-impact theory in mortgage cases, Grewing said.\n",
            "Such Justice Department back-channel calls rarely become public, and department officials declined to provide details of Perez’s involvement. Its filing in the case had stressed “the courts of appeals for decades have uniformly and correctly concluded” that racial minorities could sue when disproportionately hurt by a seemingly neutral law.\n",
            "Former Vice President Mondale became involved after University of Minnesota law professor Myron Orfield, an expert on race and poverty issues, alerted him to the potential consequences of the city’s case.\n",
            "Mondale, who as a U.S. senator from Minnesota in 1968 was a lead author of the Fair Housing Act, said in an interview that he understood the risk to the act and agreed to call Mayor Coleman, who is a friend.\n",
            "“I said I would hope that there’s some way of working on this and not risking a Supreme Court decision that ruins the act,” Mondale said. Of a disparate-impact claim, he said: “It’s the only way that you can really effectively enforce laws against housing discrimination.”\n",
            "CITY PULLS OUT\n",
            "On February 10, the city announced it was pulling the plug. Coleman said in a statement that if the city had prevailed in its plea to the high court to prohibit disparate-impact claims, an “important tool in fighting predatory lending and economic injustice” could be lost.\n",
            "The Supreme Court officially dismissed the case without comment on February 14. The lawsuit brought by the landlords against St. Paul officials will now go to trial barring any local settlement. Grewing said officials remain convinced they will be able to defend the housing code enforcement.\n",
            "Thomas Goldstein, a Washington, D.C., lawyer who represented St. Paul landlords after the case reached the high court, had urged the justices to dismiss it and avoid setting any rule that would have conflicted with the lower court trend.\n",
            "“We’re perfectly satisfied,” Goldstein said of the turn of events. “These plaintiffs weren’t here to set a national precedent. Once the Supreme Court stepped in, there was a chance they could lose” if it changed the rules and barred their claim.\n",
            "Kevin Decker, a Minneapolis lawyer who sided with the city and filed a brief on behalf of the International Municipal Lawyers Association, said dismissal of the case was a lost opportunity for a clear national standard. “There’s a lack of certainty that cities face,” he said.\n",
            "AVOIDING THE COURT\n",
            "The St. Paul case is conspicuous because of how far it got and the fact it was removed just weeks before the justices were to hear it.\n",
            "Yet in recent years liberals have sought to avoid going to the Supreme Court in cases ranging from affirmative action to voting rights. Advocates for liberal concerns such as abortion rights and gay marriage have also kept a wary eye on the justices while devising strategy in lower courts. Some abortion-rights advocates, for example, have so far declined to challenge state restrictions on abortion based on the notion that a fetus can feel pain, even though they believe the restrictions unconstitutional.\n",
            "Those on the other side have taken the opposite tack. Conservatives who have labored to get their cases to the court include Edward Blum, director of the Project on Fair Representation, founded in 2005 to challenge race-based policies in education and voting. He recently helped lawyers bring an appeal by a white student who said she was denied admission to the University of Texas because of a policy favoring minorities.\n",
            "The justices on February 21 agreed to hear the case of student Abigail Fisher and to revisit a 2003 ruling, written by Justice O’Connor, allowing universities to consider race in admissions. The case will be heard in the term that begins in October.\n",
            "“The timing is fortuitous,” said Blum, who for two decades has worked with lawyers to challenge racial policies in education and voting districts. Citing the makeup of the Supreme Court, he said: “It’s well-known that there are three members of a conservative bloc who have already expressed opinions on this and it’s likely that the two new members of the conservative bloc will fall into that camp as well.”\n",
            "Remission from depression is delayed in adults who have experienced childhood physical abuse or parental addictions, a new study by University of Toronto researchers has found. The study is published this week in the journal Social Psychiatry and Psychiatric Epidemiology.\n",
            "University of Toronto investigators examined a range of factors associated with remission in a sample of 1,128 depressed Canadian adults, drawn from the National Population Health Survey. Depressed individuals were followed every other year until remission occurred, for up to 12 years. \"Our findings indicated that most people bounce back. In fact, three-quarters of individuals were no longer depressed after two years,\" reported co-author and Professor Emeriti Tahany M. Gadalla. However, not everyone recovered at the same rate.\n",
            "\"Early adversities have far-reaching consequences. The average time to recovery from depression was 9 months longer for adults who had been physically abused during their childhood and about 5 months longer for those whose parents had addiction problems\" says lead author Esme Fuller-Thomson, Sandra Rotman Endowed Chair in the University of Toronto's Factor-Inwentash Faculty of Social Work.\n",
            "\"Numerous studies have shown that childhood abuse and parental addictions make individuals more vulnerable to depression,\" says co-author and MSW graduate Marla Battiston. \"Our research highlights that these factors also slow the recovery time among those who become depressed.\"\n",
            "Although this study could not determine why childhood adversities are associated with poor depression outcomes, the researchers speculate that negative experiences may interrupt the normal development of the hypothalamic-pituitary-adrenal (HPA) axis, which affects stress regulation. \"In many studies, adult depression has been characterized by HPA axis hyperactivity,\" says co-author and recent PhD graduate, Sarah Brennenstuhl. \"This link is an important avenue for future research.\"\n",
            "Early design documents for the original Deus Ex reveal an X-Files influenced game that never came to be, according to a recent report from Eurogamer.\n",
            "This early iteration of the project, then called Majestic Revelations, is described in its original design document as a \"near future science fiction\" game, which featured \"elements of conspiracy theory and X-File weirdness.\" The \"RPG adventure\" game was first slated for a holiday release in 1998; However, its eventual release as Deus Ex didn't happen until mid-2000.\n",
            "Early scripts feature the familiar JC Denton, who would later appear in the final version of the game. Other aspects of the title changed, however. In earlier drafts, Deus Ex's anti-terrorist group UNATCO is called TLC, or Terrorist Limitation Coalition. Characters such as hacker Tracer Tong are similarly altered, described in the document not as the anarchist of Deus Ex but rather as a \"mercenary ally.\" UNATCO head Joseph Manderley is described as a \"ruthless bastard\" who pursues your character across the globe, and appears to be less like the bureaucrat seen in the final version.\n",
            "In Revelations, secret committee Majestic 12 shifts from an untouchable group attempting to control the world into an aggressive force who are easily killed by AI. This Majestic 12 plans to begin an invasion of Texas via Mexico, followed by assassinating the cabinet of the U.S. government.\n",
            "Some aspects of the game were cut entirely, including a mission within the White House. Others, set in a war-ridden Texas and around Denver airport were never prototyped, while an underwater city set in a flooded valley in Hollywood was re-purposed to become a secret research base of Majestic 12.\n",
            "Levels designed around the White House and the President's nuclear bunker were, however, developed but cut from the game later on. According to lead writer Sheldon Pacotti it is \"possible\" these scenes exist on DVD \"in someone's attic, somewhere.\"\n",
            "Business secretary Vince Cable announced this week that he wants to \"ration\" British science, potentially eliminating the 46% of UK research that is not defined as world class. My immediate reaction was: you must be crazy.\n",
            "Cable's narrow interpretation of quality is an astonishing insult to the thousands of British scientists who help this country (and its ministers) have a well above average reputation and global influence. Cable didn't mention that most of the 46% he considers less worthy is actually classed (by the independently run Research Assessment Exercise) as internationally or nationally recognised for its \"originality, significance, and rigour\". In fact, in 2008 it found that only 2% of UK research \"falls below the standard of nationally recognised work\". It is this work that should be cut.\n",
            "If Cable goes further, what will be the impact? The two biggest killers in Britain are heart disease and cancer. If the recognised work was eliminated, that would lead to a 40% cut in research for heart disease and a 28% cut in cancer research. Britain's ability to tackle the biggest threats to national health would be significantly disabled.\n",
            "Cable acknowledged that investing in science is critical to economic performance. He cited OECD evidence showing that investment in innovative research was crucial for future economic success, and quoted the organisation's conclusion that cutting back investment in innovation \"will damage the foundations of long-term growth\". I couldn't have put it better myself. And, further, he pointed out that in the face of financial crisis, smart governments – such as the US and Sweden – were actually increasing investment in research.\n",
            "But when he says we will \"economise\", to \"screen out mediocrity\", this is exactly what the research councils do already. The Medical Research council, for example, spent £704m in 2008/09 on research. In the past year, 1,475 research grant applications were made to the council, but only 279 were funded. The research councils ensure that taxpayers' money is spent only on the best research, with the potentially highest impact on improving human health and welfare.\n",
            "Of course, there is waste in science. In medical research, for example, Iain Chalmers and Paul Glasziou from Oxford have argued that sometimes the wrong questions are asked by scientists, poor experimental designs are used, studies remain unreported, and those that are published are incomplete. But it's not a matter of cutting research. It's a matter of improving it. In medicine that means including patients in decisions about what questions to answer, improving the quality of research designs, and ensuring the results of research are fully and freely reported.\n",
            "There is another reason to question Cable's rationing plan: his impending attack on universities. The university – the main centre for UK science – is funded from two sources: the research councils (which distribute £2.8bn annually) and the Higher Education Funding Council for England (£7.4bn). This dual funding gives universities stability. In the latest independent rankings of the world's universities, the UK is second only to the US in the quality of its education and research. This week, Cambridge beat Harvard to first place in one important league table. Yet it is the depth and breadth of our university sector that is extraordinary. The UK had 30 of the 200 best universities in the world. For a small nation of just 62 million people, that is extraordinary: Germany had 12 places, Japan 10 places.\n",
            "Does this success mean that we can safely trim a little fat from our science budget? No. The Shanghai Jiao Tong ranking system, which is especially sensitive to success in the sciences – reveals that Asia, and especially China, is rapidly catching up with western centres of excellence. Cable pointed out this week that China has increased government funds for science by 25%. Any deceleration in UK investment will inevitably diminish our relative success against a major competitor.\n",
            "Cable says the government's spending priorities \"will help to define what we value as a nation\". One of Britain's last remaining comparative advantages in the world is its science, and the universities which support that science. It's hard to believe that intelligent ministers who understand the arguments for science are going to preside over cuts that could not only set UK science back a generation, but also inflict a needless wound on our physical, mental, social and economic health. But that is what Cable has prepared us for.\n",
            "Richard Horton is editor of the Lancet\n",
            "Films, Stand-Up, Comedy and TV Show Appearances\n",
            "Could you please send me the complete script for...?\n",
            "We have the complete digital scripts of Dead Poets Society, Being Human, AI: Artificial Intelligence and Good Will Hunting. If all you need is a transcript, we have the following titles available: Mrs. Doubtfire, Patch Adams, Bicentennial Man, Fathers' Day, What Dreams May Come and Jakob the Liar.\n",
            "Could you forward the following message to Robin's family?\n",
            "No, we can't. We're sorry. If you'd like to write to Robin's family, please use the fanmail address. Please know that emails sent to the fansite won't be read or forwarded to Robin's family. You have to use the Tiburon address to reach them.\n",
            "Do you have a particular quote that I want?\n",
            "We probably do. Just describe the movie and scene and we'll find the quote.\n",
            "Do you have a pic from a particular scene of a movie?\n",
            "See above.\n",
            "What is the instrumental song at the beginning of the Live on Broadway show?\n",
            "It's the intro to the song \"Two Step,\" performed by the Dave Matthews Band. It's from their album Crash, released in 1996. This intro was used for every show in the 2002 tour and the Las Vegas shows until May 2008.\n",
            "What is the classical piece featured at the beginning of the Live at the Met show?\n",
            "It's the overture from Mozart's \"The Marriage of Figaro\".\n",
            "Where can I find videos, DVDs, CDs and other merchandise?\n",
            "Of the items in print, most are available on Amazon.com. Sometimes you can also find out-of-print movies and albums on eBay.\n",
            "I recently saw Robin on Inside the Actor's Studio, but I forgot to record it...\n",
            "The show, and in particular Robin's episode, was released on DVD on September 16, 2008.\n",
            "What was that website he talked about on that show?\n",
            "The website he talked about is called audible.com. The show stopped after 3 years in January 2003.\n",
            "Update: We're re-releasing the interviews on our Audible Interviews page. 1 Interview is being released every 2 weeks, like it was originally done.\n",
            "Why is Robin credited in some projects as Ray D. Tutto, Marty Fromage or Sudy Nim?\n",
            "These are all in-jokes related to Robin's cameo appearances. In The Adventures of Baron Munchausen, Robin's King of the Moon character proclaims himself \"rei di tutto\" (\"king of everything\" in Italian). Robin and friend Bobcat Goldthwait used to perform tag-team standup comedy under the names Marty Fromage and Jack Cheese, so Robin used the Fromage name for his cameo in Goldthwait's Shakes the Clown. And Robin's cameo voice in the animated special A Wish for Wings that Work called for a pseudonym, which inspired the name Sudy Nim.\n",
            "MEPs have revealed they want boys to learn about traditionally ‘female’ activities and should be taught domestic work and care at school.\n",
            "The Brussels politicians said they want to stand against ‘sexist’ education by encouraging children to take an equal interest in all subjects ‘beyond gendered stereotypes’.\n",
            "They hope that girls will take up scientific and technical subjects while boys could take up activities such as cleaning the home.\n",
            "MEPs have revealed they want boys to learn about traditionally ‘female’ activities and should be taught domestic work and care at school\n",
            "Textbooks showing old-fashioned stereotypes about male and female roles would also be thrown out of schools, under the European Parliament’ proposals.\n",
            "The suggestions were made in the report on Empowering Girls Through Education from the Parliament’s FEMM Committee on Women’s Rights and Gender Equality, with MEPs approving the proposals by 408 to 236.\n",
            "The report says it 'encourages girls and boys in the education process to take an equal interest in all subjects, beyond gendered stereotypes, in particular as regards scientific and technical subjects, including boys’ learning about activities regarded as female, in areas such as domestic work and care’.\n",
            "The Brussels politicians said they want to stand against ‘sexist’ education by encouraging children to take an equal interest in all subjects ‘beyond gendered stereotypes’\n",
            "Another point says that schools should be guided ‘to embrace a gender perspective and gender equality, and to ensure the elimination of stereotypes and sexist distortions that textbooks and teaching materials may include in their content.'\n",
            "It added: '[This will then] encourage them also to combat this sexism in literature, film, music, games, media, advertising and other areas that can contribute decisively to changing the attitudes, behaviour and identity of girls and boys’.\n",
            "Portuguese socialist MEP Liliana Rodrigues, who spearheaded the proposals, said: ‘We are still living in an unequal Europe…women continue to be a prime target for discrimination and violence. I believe that school plays a fundamental role in changing this.\n",
            "‘[We want to] create a school culture of gender equality, critically oversee the curricula and educational materials, ensure gender equality with regard to personal and professional decisions and improve the percentage of women in positions of responsibility.’\n",
            "But Leading Labour Eurosceptic MP Kate Hoey told The Daily Express: ‘I have confidence in our nation’s ability to deal with educating our own children. It is time for the EU to stop wasting money on interfering in matters that are none of their business.’\n",
            "This is the text of a lecture from the German Glider Pilot Symposium held November 21, 2000 in Stuttgart as well as Febr. 3rd 2001 in Unterwössen for members of the German National Contest Groupn.\n",
            "Dear Glider pilots – as most of us are.\n",
            "It has been seven years since I founded DG Flugzeugbau and began our first Internet appearance. Via the Internet, I received an e-mail with a lapidary text from the representative of a US competitor who wrote:\n",
            "“Friedel, you are learning a very important thing: Safety does not sell.”\n",
            "Being new to the business, and full of energy, I thought I really wanted to prove them wrong!\n",
            "Yes, and four years later, I am addressing a lecture on this very topic. I have drawn on my personal experience and can’t help but ponder if it is worthwhile for a glider manufacturer to produce safety devices like, for example the automobile industry. They have been doing it for years, it has become second nature.\n",
            "We begin therefore with a little trip into a neighboring industry:\n",
            "No automaker can take the liberty today to not equip their vehicles with the latest safety equipment, following engineering tests. If a Norwegian auto tester performs one completely non-practical test and the new car almost flips over, the whole world laughs at the “moose test” and a new buzzword of the year is born.\n",
            "It wasn’t always like that. In the 1970, all cars came equipped with seatbelts, but almost no one used them. Then the law enabled penalties for those not wearing seatbelts, and the following year there were 1500 fewer deaths! This safety consciousness in the car industry has steadily increased, with fewer accidents despite the rising number of vehicles on the road. Can such a shift of consciousness occur with us?\n",
            "Another example: Everyone knows that driving a motorcycle is dangerous. And each motorcyclist wears a protective helmet and special clothes. In addition modern machines have an aerodynamic design giving wind protection, also the shin guards of today are much better designed to protect more than in former times. A motorcyclist acknowledges the fact that his hobby is dangerous and therefore voluntarily buys very expensive protective clothing. And us? It is nevertheless actually possible to develop mechanisms to increase security in the glider and to market them. There would just have to actually be a demand for it .\n",
            "If we want to examine this topic a little closer, ask yourself these questions:\n",
            "What is this really about?\n",
            "Is it a valid statement?\n",
            "What are the reasons?\n",
            "What can and should we do?\n",
            "First let’s take a glance at some of DG’s safety developments of the past seven years. Some of these will even be new to you:\n",
            "Emergency egress assistance. Many of the visitors to our website have already downloaded the spectacular video-clip of one of our apprentices at the time being thrown out of the cockpit of a DG-808.\n",
            "he idea is actually quite simple. In case of emergency, a pilot can egress quickly and reliably from the cockpit by way of an inflatable air cushion. The system will only function if the canopy has been released – thus during normal flight, it is blocked. Pulling the release will release the seat and shoulder belts, after which the air pillow inflates, lifting the pilot to the edge of the cockpit. Actual “stepping out” is then limited to a lateral rollout by the pilot.\n",
            "Also a parachute jump would have to actually be possible at airfield traffic circuit height – and in still lower heights neither collisions with other gliders nor other reasons for an emergency exit are probable.\n",
            "In the meantime, we could gain knowledge in the inconceivable case that the NOAH system would release unintentionally. Would it break all the pilot’s ribs? What would happen if he remained seated with the belts buckled?\n",
            "Someone in the plant made the seat buckle nonfunctional, and a visitor wanted to try out the NOAH……..\n",
            "So, we are in the fortunate position of being able to interview the “dummy” after the test. He said that he was pressed into the belts no more harshly than when pulling negative G’s in a loop. There is a secondary capillary opening allowing the pillow to deflate within about 30 seconds. Therefore, in the “impossible case” of a malfunction, nothing bad happens at all.\n",
            "When the NOAH system came out 7 years ago, Glaser-Dirks (our predecessor company) received 5 orders, one of them from me. Since then we sold about 20 systems, and with about 45 new single-seaters per year, the NOAH system wasn’t exactly a “sales hit”.\n",
            "By the way, the NOAH system could in itself fill an entire lecture. If you’re interested, please see our website.\n",
            "The Consummate Safety Cockpit\n",
            "The safety conscious surely know the investigations of Martin Sperber of the TUEV Rheinland crash tests with glider cockpits. He had asked the different manufacturers for a test cockpit and my predecessor had at that time been ready to supply one. The results were simply terrible!\n",
            "That is also the reason, why Martin Sperber only showed the high-speed film of the crash to us and does not publicize it. Naturally such pictures can be business damaging.\n",
            "Really?\n",
            "He said it to us and I repeat it here: What occurred with a cockpit of the DG-800 in the case of the impact at 70 km/h, would have occurred also with every other single-seater cockpit offered today. Professor Roeger of Aachen University of Applied Sciences determined similar results: It let a fuselage fall from a rack and strike at a speed of 6 m/sec. That should simulate the impact that occurs with “landing” with a well-designed total rescue system. The fuselage – a quantity production of a competitor – bore the impact.\n",
            "Then Professor Roeger repeated the test with a falling speed of 8 m/sec. ! It is the opinion that with the parachute sizes, the following impact would occur. The result was such that one shows no pictures of it. So what’s 8 m/sec? Only 29 km/hour! The cockpit would never survive a real crash.\n",
            "So, what is to be done? Martin Sperber suggested a set of measures, in order to make a fuselage clearly more crash resistant: Two very strong stringers from the front to the rear transverse force pipe are to direct the collision forces around the pilot – just like a sturdy cage around the passenger space in the automobile.\n",
            "There is additional reinforcements to prevent bursting of the fuselage and an additional frame in the back of the fuselage against indenting. A fuselage so equipped passed the same test and bored itself “only” deeply into the container. Due to these tests we developed a “consistently safe cockpit” and we offer exactly these additional reinforcements as an option.\n",
            "This is the most expensive of our developments in addition, the most important, because a cockpit always breaks open in a crash. It uses up an additional 2 cm of space in the workstation in the shoulder width, because somewhere the strong stringers must be fixed to the workstation. And then some pilots say to me, that it’s already tight without the reinforcement. My standard response then always is: “True, but the fit is closer still in the coffin!” – well, I believe so anyway! Possibly these are the same people squeezing themselves into so-called “competition cockpits” from other manufacturers, which are still smaller. However, perhaps 0.2 points of lift/drag ratio less resistance the result!\n",
            "How many safety cockpits did we sell so far? I fly in in one. Other than that, sales have been non-existent\n",
            "Piggott-Hook\n",
            "The Piggott Hook is now something completely new however one cannot buy it at all: It is included in all our new glider series.\n",
            "In Albuquerque in spring 2000 the well-known Glider Instructor, Derek Piggott from England addressed me and explained the idea:\n",
            "“An inadvertently unlocked air brake can work it’s way open during takeoff and bring the pilots into serious danger even resulting in a crash.”\n",
            "That can be prevented by metal flange on the inside wall of the cockpit with several teeth and an appropriate hook at the linkage. The hook intervenes in a tooth and cannot run any longer automatically to the rear. The pilot needs only to grasp the handle and turn only a little and the hooks is released. That is ingeniously easy – to build and develop at low cost and is a reliable means to eliminate a frequent cause of accidents.\n",
            "It personally annoyed me that we had not come up with the idea!\n",
            "I accelerated the development, after I found myself taking off while on vacation with an unlocked air brake. My glider launched nevertheless, and nothing serious happened.\n",
            "Is it not depressing that Derek Piggott said to me, he brought this suggestion via the different SSA Conventions to all German manufacturers described and nobody so far has taken up the suggestion!?\n",
            "Why not?\n",
            "Why don’t glider pilots demand safety-increasing developments with their gliders?\n",
            "Canopy safety lock pin – Roeger Hook\n",
            "You are surely familiar with the functioning of the Roeger Hook: If the pilot must leave an airplane by parachute, it will generally be falling downward in a completely uncontrollable flight attitude. The wind pressure against the canopy makes it difficult to eject. If however the canopy is to be thrown off, there is a great danger that it will strike the pilot in the side of the head if there is any lateral motion. Only about 50% of all attempted parachute jumps succeed. The other half ends deadly and often is due to the fact that the pilot is temporarily knocked out by a contact with at least the canopy framework and was therefore not able to exit. The Roeger Hook holds the canopy in the back, so that it can open only in front and fly in a high arc over the pilot’s head before rotating away. This is such an elementary safety device that the Roeger Hook is now logical with all newly certified airplanes with forward opening canopies. But what about the many older gliders and those, which are built still with old certifications? Unfortunately there was an accident in the United States, in which exactly the operational sequence described above occurred:\n",
            "From the pilot’s head wounds it could be proven clearly that he had gotten the canopy framework to release and was knocked out by it. The American Federal Office of Aviation inquired therefore with us whether there would be a re-tooling possibility for a Roeger Hook or something similar. And in such a way we developed the canopy safety lock pin and by way of a Technical Note to all owners of older DG gliders we advise them of the optional retrofitting. In order to promote the sales of this important safety device, we established also quite a cheap price – 50 Euro for the complete set. And now guess how many canopy safety lock pins we sold in one year after the publication? Of a possible 1,400 airplanes concerned: 28 canopy safety lock pins were sold within two years – That‘s 2 % of owners. Is it because it‘s not fun?\n",
            "Or: Safety does NOT sell! There are probably many ways to increase the safety of gliders. As a glider manufacturer, however, the development of such mechanisms has not been worthwhile so far. In order to describe it in monetary terms, we spent about 250,000 Euro on development costs of the described mechanisms.\n",
            "Perhaps you work at Airbus Industries and are of the opinion, that this is not very much. But these 250,000 Euro are from my own pocket. I felt a certain responsibility to do what was possible and I am sour now, because it was obviously to a considerable degree futile! What is the situation? 1. Are these safety options perhaps too expensive? Well, this cannot be the case, as proven by the story of the canopy safety lock pin. But Martin Volck had a much better and more general response in glider symposium in Stuttgart with his lecture: “How much is a glider pilot’s life worth?” Many will have read that it delivered the costs of a set of measures added and the estimate that the number of deaths could be reduced by 40%, if all airplanes were provided with these mechanisms. Since we with the Noah and the Piggott hook offer still more, I estimate a survival or an accident avoidance chance of even 50%. The remainder is then mathematics:\n",
            "The number of deaths with gliders over ten years in Germany is as well known as the life span of the airplanes and its number. Hence it follows that each airplane would be involved, on the average each 600 years in a deadly accident. That sounds like a lot, it means that in a thirty-year life span with each 20 airplanes one crash with a fatal outcome occurs. All DG measures cost together about 9,000 Euro. With 19 of the 20 aircraft, an unnecessary investment – however with the twentieth, a 50% probability life-saving. Thus it costs about 360,000 Euro to save a glider pilot’s life (9,000 X 20 / 0.5).\n",
            "We won‘t even talk about the reduction in injuries up to wheelchair results. Dear pilots – there is hardly a life-saving measure, which would be cheaper in the world. Or perhaps: A pilot buys the machine of his dreams and pays all together easily more than 120,000 Euro. However it does not even estimate his own life to be three times as valuable as the value of his toy. Tell me, are we glider pilots collectively crazy? 2. Do we perhaps displace safety questions unconsciously? Now I am not a psychologist, but much speaks for the fact that it is like that. If a pilot thinks about safety questions, he must also admit that gliding it is not harmless – that perhaps the car ride to the airfield is not the most dangerous part. I cannot express it better than the well-known Bruno Gantenbrink in his lecture “Safety comes first”, which you should really re-read on our web page. Obviously many pilots are subject to the internal conflict, over safety questions and – to not want to think of mechanisms, because that confession results in the knowledge of partaking in a dangerous sport. A “cognitive dissonance” for you – a contradiction between own conviction and the reality. Each pilot knows not to take any unnecessary risks. Additionally one “knows” that certain errors simply don’t happen and that accidents only happen to other people. On the other hand he knows that this cannot actually be correct. Even experienced and careful pilots sometimes make “such stupid” mistakes. But can he/she admit that? My friend was a surgeon and chief doctor at an ophthalmologic clinic. By his occupation, he was an extremely meticulous person. And so were also his pre-flight checks. I always said: ” Jens, you are my life insurance. If you checked my airplane, it is completely safe.” And that’s how he flew too. From him I first heard the advice of what to do in an aborted launch, “Think down! Stuff the nose down!” And that’s exactly what happened to him at an altitude of 90 meters, after releasing the launch cable. He did not stuff the nose down, and he probably did not think. When he hung quietly in the air, he rolled and spun, four seconds into a black hole. Why exactly did I tell you this? It is a small psychological experiment:\n",
            "Perhaps you are now feeling a little annoyed, and are thinking, why is Friedel Weber telling us that? What does it have to do with the Lecture? What concerns of ours is his friend? Does he always have to come back to this topic? Or do you feel a little uncomfortable? Nevertheless, we are all alone with our answers. “I do not want to hear this! ” See when we say “cognitive dissonance”?: You know very well that this could happen to you, but you do not want to admit that because it doesn’t fit into your idea of the world order. And such a conflict overcomes all humans simply by us ignoring reality. And in such a way safety consciousness is something to ponder in the distance. 3. Is it perhaps also because of the fact that we do not have a lobby, which worries about safety questions? So, what is to be done? I can point out three solutions to you. Two would not be feasible. One is feasible however will not be successful. And if you are of the opinion that this result is depressing, then consider this: 1. Are we to make all available safety devices simply the standard in each series?\n",
            "That would add approximately 9,000 Euro to the cost of each airplane! And there my personal commitment now unfortunately stops: If we would make this strategic decision alone, our airplanes would be more expensive, by the amount mentioned above, than the competitors, and we could not absorb that. It would save human lives however at the cost of the existence of our company. In order for all manufacturers to offer together these technically and economically feasible safety components, they must be convinced only of the fact that the development is also worthwhile itself.\n",
            "And I do not have the impression that it will be worthwhile itself in foreseeable time. 2. Are we to call after the state and request the Federal Office of Aviation to make these safety components obligatory?\n",
            "I personally would not like to see that either. The state already regulates far too much, and has its hands everywhere, that I would rather leave! 3. Or are we to continue as before and trust in our customers?\n",
            "Now, you must be the one to request the glider manufacturer of your choice to initiate the appropriate developments. Then you would have all safety devices that you want at your request. We manufacturers always make exactly what the market demands, that’s how this tough business works. Therefore, there is only one real solution.\n",
            "The extent of the safety developments in the German glider manufacturing industry rests certainly only with the customer! It‘s all in your hands. Always, happy landings!\n",
            "This article was a lead article taken from the pilots’ magazine “Luftsport”\n",
            "by Holger Back Safety does not sell Friedel Weber (DG-Flugzeugbau) chose this provocative but unmistakable thesis as the subject of the speech he held during a glider pilots’ symposium in Stuttgart (Germany) in November, and also at a meeting of the German national team in Unterwössen.\n",
            "“Safety does not sell” – and he proved this with some facts from the day-to-day life of a glider manufacturer. Whether it’s the comparatively simple fitting of the Röger hook, the rescue system “Noah” or the safety cockpit – the customers haven’t recognized the factory’s efforts. The financial effort of developing and certifying these safety features did not turn out to be profitable for the company. Fitting the Röger hook to older gliders for example only costs the customer 150 Euro and is proven to be a critical safety feature in the case of the pilot having to bale out – but only very few people have bought it so far. And this is not only a problem of one specific glider manufacturer.\n",
            "The investments other glider manufacturers make in safety features are often ignored or, even worse, rejected. Even leading pilots are no exception. For purely aerodynamic reasons some pilots wedge themselves into the narrow cockpit – knowing that an emergency bailout will take just as long as getting in the glider. Or important safety details like for example a bigger undercarriage are rejected for weight reasons. The “leisure pilot” is no exception to this rule either. His main concern is whether the glide angle might decrease by one or two points. Safety however will not be an item on his list of priorities when buying a glider. The interest in rescue systems has clearly decreased in the past two years. A lot of people will argue that the slow development of these systems is the main reason for this. This is probably true as nobody could have foreseen how time-consuming the development and testing of the rescue systems would turn out to be. Therefore it is all the more important to show the manufacturers that pilots are indeed still interested in these systems. We have often accused the manufacturers of a lack of interest in the safety of their gliders. Times have changed, though. Glider manufacturers have done their homework. Now pilots and customers have to do their part and change their attitudes towards safety. Holger Back … and here are some reactions: betreff: Safety classification in gliders received: Sonntag, 13. Juli 2014, 15:56:40 (Sun, 13 Jul 2014 15:56:40 +0200) Dear Sirs, I have been thinking about safety issues in gliding and reading your safety related documents in your web site. Specially the one called “safety does not sell “. On my personal view, safety would be the first criteria for choosing a new glider.\n",
            "However I understand that people might not want to think about that. There is a similar successful experience with energy consumptions in home appliances. People might be driven by price to buy a new washing machine but with the introduction of the Energy Efficiency classification with colours and letters in the EU. People tend to buy the most efficient. A similar system could be created in gliding, a standard classification about safety in all gliders. This could include active and passive safety. Customers might be aware that a high performance glider might not stall as nicely as a beginners glider. It could be made in a simple and transparent way, with ranking of clear things like: Hard landing absorption:\n",
            "Parachute:\n",
            "Ballistic parachute:\n",
            "FLARM:\n",
            "Piggott-Hook:\n",
            "NOAH:\n",
            "Stall warning indicator:\n",
            "Tip stall and general stall friendly:\n",
            "Spin out friendly:\n",
            "Sustainer:\n",
            "Instantaneous Electrical sustainer:\n",
            "GPS/orientation maps:\n",
            "Artificial horizon:\n",
            "Cloud flying instruments:\n",
            "Cabine crash cell (as Formula 1 cells):\n",
            "Side string:\n",
            "Rigging easiness:\n",
            "::::\n",
            "::::\n",
            ":::: The list may go on with many more features… Many or them are optional in the purchase or upgrade of a glider. The inclusion of this features increase the safety classification of the glider and boost its sales. The same glider might have different classification depending on the equipment. This should be a readily available classification for all gliders in the market. Where the person purchasing the glider can easily compare: price, claim performance, claim safety and overall brand prestige, delivery times. I believe the adoption of this would benefit all of us in the future. You said in your web site that safety is in the hands of the customers when we buy a glider. Well, lets create a transparent and easy to use tool to compare the actual gliders with the safety devices on from different manufacturers. It would be great if you could forward this to relevant people. I imagine this idea is in the mind of many people around. This could be a practical way to boost safety in gliding and a way to increase the sales of your company, as I understand DG is in the forefront of safety devices. Kind regards.\n",
            "Dr. Gonzalo Garcia-Atance.\n",
            "PhD. Aeronautical Engineer. Betreff: Safety\n",
            "Datum: Thu, 02 Aug 2001 09:41:00 -0700\n",
            "Von: Pete Williams Dear K.F. I sincerely appreciate your quest for safety devices in sailplanes. You are making a contribution to the sport that may not appear to be bearing fruit now but will in the future. Having been a military (Navy) pilot for 21 years, I can attest to the problems of making pilots aware of safety. Most of the time our safety programs fell on dead ears. Pilots continued to be killed doing dumb things…yours truly almost several times. Yes there were plenty of safety devices installed including a wing fold lever that looked like a wing and actuated in the proper direction. Still one pilot tried to takeoff with the wings folded!! After being an instructor for several years I discovered that I could not convince my students to be safe, so told them they were free to kill themselves if they desired and then told them how to kill themselves. Maybe this worked because some came to me in later years to say thanks. So what to do? My recommendation is to install in all production gliders a strong cockpit including the hard cushion and the Roger hook as standard. These items are basic to surviving a crash and bailing out. The other items can be options. Pilots are no more crazy than car drivers. Both have in common haste, pride and it will never happen to me syndrome. Its normal. Of course you are not responsible to save their lives…THEY are. Keep on keeping on and recognize your time and your employees time plus your money has not been invested in vain. Pete Williams Betreff: Emergency Egress\n",
            "Datum: Fri, 3 Aug 2001 11:07:42 +1000\n",
            "Von: LARCEY, PAT I was interested to see your article on the inflatable cushion to assist emergency egress from the cockpit. When I was test flying RAF V Bombers in the 1960’s, in particular the Victor, the rear crew had assister cushions to assist overcoming G forces and speed the egress in the event it was necessary. Sounds a great idea to me, a consideration, probably, is to have a means of deflating it if it deploys inadvertently. I suppose that although safety might not sell, the lack of safety is an even greater deterrent, so on balance safety must sell ???? Safe flying Patrick Larcey Betreff: Re: My New DG-808B\n",
            "Datum: Wed, 01 Aug 2001 10:07:36 -0500\n",
            "Von: Gary Flandro Dear Friedel: I read your recent article on soaring safety with great interest and considerable concern. You should be recognized and praised for the effort you have made to improve the safety of your gliders. It should be mentioned that you have already made a very significant impact on the soaring community in terms of awareness of key safety issues. I understand your frustration with the customers for not responding more favorably to your obvious hard work in implementing important safety features in the DG sailplane designs. Please understand though that we all very much admire and appreciate your efforts. Hopefully this will pay off for your economically in the long run. I guess that I am one of the offending parties since I chose not to purchase some of the key safety devices you have developed for installation in DG-808B # 236/B150. I must tell you that I have worried about this considerably since placing the order a year ago. I think I must be typical of most buyers in that the economics played a major role in my decision. For many of us a purchase of this magnitude greatly stretches our economic limits. In other words, I had to sacrifice some safety in order to buy the glider. I hope to be able to add the DSI indicator later. I can’t give you good reasons for not purchasing the NOAH system or the special safety cockpit design. In the latter regard, I was impressed that your standard cockpit design is very good in application of Kevlar materials to improve safety. I think many pilots feel that additional crash resistance is only useful up to a certain point. In the event of a heavy impact, one wonders if there is anything that can be done to prevent serious injury. You are to be complimented for your implementation of the Piggott Hook. I hope this important step will soon be recognized by the world soaring community. You should receive an FAI award. With kindest regards,\n",
            "Gary A. Flandro (DG-808B #236)\n",
            "Betreff: Safety does not sell\n",
            "Datum: Thu, 2 Aug 2001 05:46:12 -0400\n",
            "Von: “David Noyes” You may not win the battle to sell every pilot safety devices,\n",
            "but you win the hearts of all pilots and sleep with a good conscience.\n",
            "Betreff: Safety does not sell\n",
            "Datum: Fri, 3 Aug 2001 08:56:52\n",
            "Von: Peter Redshaw Hello Friedel I would like to congratulate you on your article on safety. I believe the biggest problem is that most pilots believe it will not happen to them, in particular those that have the experience and money to be flying new gliders. To be buying expensive new kit means that they should be able to afford the marginal extra cost. On the other hand the vast majority of gliders being flown are second hand, handed down from the pilots with the money to buy new and with the alleged wisdom and experience to know that is right. They obviously do not value safety highly and are setting a bad example to the next generation of pilots. To be fair it is also true that glider manufacturers seem to be more concerned with performance than safety, as you say safety does not sell. The one exception is the barbed canopy wire deflector which sells well in Holland, but why? Has it to do with the Dutch Gliding Association? I have only ever bought one brand new glider and had every possible extra added, except for the wire deflector extra safety features were not on offer. Most of the time I have bought the best second hand machine I could afford related to my ability, they did not have safety features built in. An Oly 2B, Club Libelle, Kestrel 19, Nimbus 3, Super Falke, LS6c, DG-800B (G-BYEC) and for real fun an old Slingsby Capstan T49. The safety features you describe generally cannot be retro-fitted to any of these machines. I have had one accident in 41 years of gliding, which is close enough to your statistics. I was 26 years of age and at the stage of total self confidence. I came in nose first from about 50ft having cart-wheeled over a tree attempting a field landing. It was in the Oly 2B, the entire cockpit became match-wood back to the leading edge of the wings. What saved me was pulling myself into a ball instead of bracing myself. It in effect allowed a crush zone. I don’t remember thinking it out when it happened and I don’t know if I would have the presence of mind to repeat the trick if it happened again. In fact I couldn’t do it in most glass ships as I cannot get my legs back past the instrument panel! I have about 4000hrs gliding, instructed for 26 years, was CFI for 5 years and have 3 diamonds. I get more cautious as I get older so I appreciate your efforts. My greatest concern is having a mid air collision and it is probably true to say I believe (like most pilots) it will be the other persons fault. I therefore now ensure I have a good serviceable parachute. I have been following the development of glider recovery systems and would like to see more of this coupled with better cockpit design and crash tests.\n",
            "I know a crumple zone works and you only have to look at Formula One motor racing to see the benefits of cockpits designed to withstand serious crashes. Our gliding movement is caught in a catch 22 situation, unless new machines have safety features built in then the majority of us with our second hand gliders will be flying without safety features forever. I only see one way to break this circle. Is there any chance of ALL glider manufacturers together with All National Gliding Associations, agreeing a set of safety standards/features that MUST be incorporated into ALL future designs? A long process but it might get there if ALL manufacturers include specific safety designs. Isn’t this what essentially happened on a smaller scale in Holland with the canopy wire deflector? Cheers and keep trying Peter Betreff: Glider Safety\n",
            "Datum: Sun, 5 Aug 2001 23:42:09 +0100\n",
            "Von: Brian Dear Sir, thank you for another excellent article on your web site about the strange situation that safety apparently does not sell. I have only been a glider pilot for about three years. At first the excitement of the sport and the challenge of learning overcame most other thoughts. After going solo the challenge of building my basic competences and gaining the Bronze badge / cross-country endorsement were uppermost in my thoughts. I told friends and family how wonderful it all was. But I found it strange how they thought I was taking a big risk with my new hobby. It is only in the last year that the risks of the sport have become my main concern. At first when I heard of a serious or fatal accident elsewhere, I assumed the people involved must have taken silly risks, or simply been very unlucky, and it couldn’t happen to me. Then I saw a couple of incidents at my airfield that could have been very serious, and was able to talk to the people involved. I realized that (of course) it could happen to me. I found out more about accidents from others at the club and by reading the published reports. Suddenly I realized that gliding was much more dangerous that I had thought. And it seemed that in many cases, the victims of accidents would have been saved if the glider had basic warning equipment like stall warning, airbrake warning on take-off. Or if the machine was easier to escape from when subjected to g-forces, or much more able to survive an impact. With about 8,500 pilots in the UK and about 8 or 9 fatal or serious accidents a year, it looks like you have a one in a thousand chance of being another statistic. Most people would not take part in anything with such a high risk. Last year, after being worried by road safety and the chances of injury in my ten year old VW Golf, I bought a nearly-new car with many modern safety devices like airbags, seat belt pre-tensioners, side impact bars, very good crumple zones, etc. Then when flying in an old glider like the PZL-SZD Junior I really started to feel exposed to physical danger, like I was sitting in a fast-moving cardboard box. This has had a bad effect on me, as I have started making excuses not to fly, and I feel I am in danger of giving up my hobby due to the risks. To overcome this I simply cannot afford a new glider with new safety devices. The club gliders that I rely on will not be replaced soon, and even if they are replaced I doubt if they will include any of the safety features that you provide in DG machines. But like the car industry in the 1970’s, it can only be a matter of time until we demand better safety. It might just take 20 years. I therefore applaud your efforts and wish you every success in making other glider pilots see the reality of the danger of their sport and demand safer aircraft. You are already well known for your stance against the safety complacency that pervades gliding. I am sure that in years to come you will be recognized as one of the most important innovators in the history of gliding. Please do not be discouraged by the poor response you get when marketing a new safety feature. There are many people like me who are increasingly worried about the lack of built-in safety in gliders. We cannot expect training alone to prevent the risks! When I win the lottery, if I am still flying, I will be sure to purchase one of your fine aircraft with all the available safety options you can build in. Many thanks,\n",
            "Brian Rogers Betreff: Re: DG Flugzeugbau – Newsletter No. 34\n",
            "Datum: Mon, 6 Aug 2001 14:22:34 -0400\n",
            "Von: Thomas Knauff Dear Mr. Weber, Regarding your article “Safety Does no Sell.” Before seat belts were required equipment, Ford motor Co offered them as an option. Something less than 10% ordered them even though Ford priced them at no profit. Even after they were required, it took a law to force people to use them, and even now, a very high percentage do not wear them while driving. As for motor cycle helmets, they are not required everywhere, so you will see many people driving motorcycles without a helmet in those areas where the law does not require there use. Safety does not sell, and the result in our case makes flying gliders more dangerous than all other activities. Smart people choose to act dumb. Tom Knauff Betreff: Safety does not sell.\n",
            "Datum: Tue, 7 Aug 2001 12:48:05 +0000 (GMT)\n",
            "Von: Bernt Hustad Hembre Hi. I read your article “Safety does not sell” with great interest. I do agree with you on many of your thoughts, but there is one thing I’d like to comment. Safety does sell, it just takes a long long time.\n",
            "Just remember how many years it took from it was prohibited to drive a car without wearing a seatbelt until people started to put their seatbelts on\n",
            "because it was safer. In the beginning people just took them on to avoid being fined for not wearing them. I’d like to use your article as a base for a discussion on a club-evening in our glider club. I will have my focus on what can be done to increase safety, reduce the chances for damage and reduce our annual repair budget. As a conclusion I will strongly urge you to keep up the good work and continue to find new improvements regarding safety. And articles on the\n",
            "subject are probably the best way of starting discussions in clubs around the world.\n",
            "Bernie\n",
            "Some people think that Hillary Clinton, for all her corruption, is at least smarter than Barack Obama. Others observe her ruthlessness toward political opponents and infer that, unlike Obama, she will be a tough defender of American interests. No: for goodness’ sake, the woman has a four-year history as Secretary of State! If there is one thing we know for certain about Hillary, it is that as president she would preside over an inept and America-destructive foreign policy.\n",
            "And, to put the most charitable construction on her words, the woman is an idiot. Andy McCarthy records, and places in context, Hillary’s first comment on the latest terrorist attack in France:\n",
            "Let’s be clear: Islam is not our adversary. Muslims are peaceful and tolerant people and have nothing whatsoever to do with terrorism.\n",
            "We simply cannot afford another four years of such mind-numbing stupidity in the White House. Donald Trump has many faults as a presidential candidate. You don’t need me to list them for you. But he is not Hillary Clinton, he is not committed to a view of the world’s dangers that is almost literally insane, and he will not give us a third Obama term in either domestic or foreign policy. He also won’t appoint people like Ruth Bader Ginsburg to the Supreme Court. Our next president will be either Donald Trump or Hillary Clinton; we desperately need for it to be Trump. He deserves, and badly needs, our financial support.\n",
            "Vivian Howard sits, her legs crossed, her feet propped on pots and pans under a stainless-steel countertop. “I have a lot of anxiety nowadays. I, for so long, identified myself as a working chef,” she says, her arms whirling as she speaks.\n",
            "“I don’t do that anymore. And I feel uncomfortable with it. I don’t want to be a phony. I don’t want my cooks to see me as a phony. I don’t want to be the person who just shows up with her entourage and her cameras. But all of that is part of what’s happening. And I literally have nightmares at night about it, trying to figure out what it is I am now.\n",
            "“So I don’t know. What am I?”\n",
            "Then she breaks. “I’m Vivian, and I’m a chef,” she says as she bursts out laughing, and slaps the countertop. “I’m a doofus, is what I am. Lord.”\n",
            "“That’s good,” says Cynthia Hill, who’s holding the boom mike. “And sorta like ending it, ‘Am I still a chef? Can we still call it A Chef’s Life?’ ”\n",
            "Vivian looks back into the camera. Take two: “So I really am trying to figure out what I am,” she says.\n",
            "“You want to do that again?” Cynthia asks. “You’re sort of soft-spoken.” She brings the audio level up.\n",
            "Take three: “I want to be able to do it all. … But that just ain’t gonna happen, I don’t think.”\n",
            "Cynthia asks Vivian to be a little more reflective. Take four: “To what degree does what I do every day define who I am? I don’t know. What do I want to do?” Vivian says. “I don’t know, Cynthia, come on! This is not healthy, talking about yourself so much.”\n",
            "Vivian would much rather talk about the time when Scott Avett, who sings the show’s theme song, came to her restaurant, Chef & the Farmer. She wants to talk about how cute he is, and how she’s kinda embarrassed for fangirling over him. So she does several takes about that. It relieves some tension. Afterward, though, Vivian and the crew are spent.\n",
            "“That was good, Vivian,” Cynthia says. “I know that was painful.”\n",
            "Then they stop for lunch.\n",
            "Over tacos, guacamole, and salsa brought from nearby Olvera Street Taqueria, Vivian and Cynthia explain how an episode of A Chef’s Life comes together. First, a camera crew goes out and follows Vivian around Kinston. They visit a farm that she uses as a supplier. Later, an editor cuts the video into scenes. Then Cynthia, the director and producer, takes the scenes; groups them based on a specific ingredient, like sweet corn, turnips, oysters, or moonshine; and assembles them into a crude episode. She figures out what’s missing, and finally it’s up to Vivian to fill in the gaps with “confessionals,” where she looks straight ahead into the camera and narrates. They tried scripts at first, but Vivian sounded too stiff. So Cynthia will prompt Vivian with a topic or idea, and she’ll speak.\n",
            "During a break, someone asks who she’s talking to. “I’m talking to America,” Vivian says, and she and everybody else belly laugh.\n",
            "But seriously. “I never really think about that. I do talk, and then I look at Cynthia for approval.”\n",
            "Today, they have to get through confessionals for the final five episodes of A Chef’s Life’s third season. The kitchen in a condo, which used to be the back corner of a tire warehouse in downtown Kinston, has been turned into a set. Pots and pans are arranged on shelves, where they will appear, out of focus, behind Vivian — noticeable, but not too noticeable. The air conditioner is off. So is the refrigerator. They make too much noise. Sheets taped over the tall plate-glass windows soften the light. There are only a few crew members here, all from North Carolina. The two camera guys drove down this morning from Durham. A makeup woman came over from Deep Gap. Cynthia is simultaneously directing, producing, and wielding a boom mike with such precision that she can use it to knock down flyaway hairs on Vivian’s head. They purposely travel light. “One, because we don’t have any money,” Cynthia says. “And two, because we don’t have any money.”\n",
            "Vivian, now in a different shirt for a different episode, sits down inside a cocoon of reflectors, lights, and blankets around the countertop, which muffle the echoes from the tall ceilings.\n",
            "“I think that we can talk about the beef tongue a little bit,” Cynthia says.\n",
            "“If you had told me nine years ago that I would be drinking beer brewed in Kinston, serving beef tongue tacos at a gallery in downtown,” Vivian says, right into the camera, “I would have laughed in your face.”\n",
            "It doesn’t land with the right punch, so Vivian does a more playful take: “That’s some good tongue. That’s some luscious, fatty tongue.”\n",
            "“I wonder if that’s going to make us PG,” says Cynthia.\n",
            "Next take: “These Kinstonians are loving my tongue.” Laughter follows.\n",
            "“And the art,” says Cynthia.\n",
            "“And the art,” Vivian tacks on to the end of the next take.\n",
            "“Good with me,” Cynthia says, putting down the boom mike.\n",
            "The TV show was Vivian’s idea. She’d been a volunteer at WRAL in Raleigh during college and had interned at CBS Sunday Morning in New York. She originally wanted to be a broadcast journalist, but shifted to advertising and then to food writing, and figured she would need to learn about food by working in kitchens. Over time, cooking, not journalism, became her career.\n",
            "Chef & the Farmer opened nine years ago, in 2006. Anyone who’s watched the beginning of A Chef’s Life knows the story. Vivian and her husband, Ben Knight, were working at restaurants in New York City. Her parents offered to help her open a restaurant, but the catch was that the restaurant would have to be in eastern North Carolina, a place where Vivian grew up, but swore never to return.\n",
            "It’s a little more complex than that. It was really Vivian’s sister and brother-in-law who wanted to have a little niche grocery store with a sandwich shop in it. That idea morphed into the concept for a high-end restaurant in Kinston. Local leaders, thirsty for any kind of new business downtown, helped make a building at the corner of Herritage and Gordon streets more affordable. Vivian’s parents, who had been prominent tobacco farmers in the area, pitched in. They all renovated a century-old boxy brick building, once a mule stable and more recently a print shop, into Chef & the Farmer.\n",
            "The restaurant got off to a slow start. The food wasn’t Southern, and people in Kinston didn’t connect with it. Vivian called some dishes, like smoked goat cheese ravioli with tomato petals, bad versions of the food she was cooking in New York. A year in, she had her breakthrough moment with a breakthrough dish: barbecued chicken with a blueberry eastern-style sauce. Over the next few years, Chef & the Farmer started to turn into a word-of-mouth destination for people around Raleigh, Durham, and Chapel Hill who were looking for an alternative to restaurants there. A 2009 Raleigh News & Observer review brought more interest. Still, Vivian was restless, and turned back to writing as an outlet. She became obsessed with old traditions.\n",
            "Most of them already know the whole backstory. They want to see if real life matches up with what they watch on TV.\n",
            "Some elderly neighbors invited her over to make collard kraut. She wrote a blog post about it. “Nobody read it,” she says.\n",
            "But the idea stuck with her. Ben suggested Vivian call Cynthia Hill, a documentary filmmaker in Durham. Cynthia was between projects, and agreed to come down to Kinston to experiment under one condition: Vivian wouldn’t be merely the narrator for the show. She would be its main character.\n",
            "Vivian explains what happened next over a glass of white wine at Buy Local, an art gallery and wine tasting room a block away from her restaurant. For the pilot episode, cameras followed Vivian and Ben around the restaurant. To a parade. To a farm, where she talked about sweet corn, and how to cook it. The first show ended with the fire that destroyed the restaurant’s kitchen in 2012. The cliffhanger: How would Ben and Vivian rebuild?\n",
            "Cynthia sent Vivian a rough cut of the episode, but Vivian couldn’t bring herself to look at it for weeks. “If you want to do this,” Cynthia told her, “you have to be able to watch it.”\n",
            "At first, nobody else seemed to want to watch the show, either. UNC-TV and other networks originally passed, before South Carolina’s ETV agreed to help distribute it to PBS stations nationwide. The show’s first episode aired in September 2013. And with that, Chef & the Farmer went from a regional destination to a national one. People started to look up Vivian Howard en masse on Google. Who is she? Where is Kinston? How can I get there? Where do I stay? What else can I eat while I’m there?\n",
            "“I think we were really surprised at how much of an impact it had,” Vivian says, “because we didn’t really think anyone was going to watch it.”\n",
            "Here’s what that impact looks like: While Vivian sits in the gallery drinking wine, three sisters are traipsing around town. First they took an Amtrak train to Wilson, then they took a Greyhound bus to Kinston. The sisters — Kathleen Bradford and Denny Galarza from Pennsylvania, and Jeanie Cavanagh from Denver, Colorado — planned to stay for four days. With no car. Kinston has no taxi service. They’ve eaten for two nights at the Boiler Room Oyster Bar, and tonight they’re finishing up dessert at Chef & the Farmer, which is packed with a diverse crowd, young and old. The restaurant’s marketing director caught wind that the sisters were in town, and, in the pouring rain, carted them out to Warren Brothers’s farm in La Grange, which is featured prominently on the TV show. She took them to a local barbecue place, where they learned why people in these parts drink Pepsi and not Coke. At one point, when they passed by the condo, Vivian herself came outside and surprised them.\n",
            "Chef & the Farmer has been a spark, creating an industry nobody ever envisioned for downtown Kinston: tourism.\n",
            "“We’ve seen a lot,” says Kathleen. They’d had no idea what to expect. They brought books and a deck of cards to pass the time. They haven’t touched them.\n",
            "Jeanie says this isn’t a vacation. It’s a bucket-list destination. “I have such respect for Vivian,” she says. “I don’t know her, but it’s obvious from the show that she’s so clear about what she wants to achieve.”\n",
            "It takes planning to eat at Chef & the Farmer now. Reservations book up weeks in advance. As the sisters finish dessert, empty tables fill up again. Plates of Cornish game hens, tomato pies, pork belly skewers, and wood-roasted halibut fly out of the kitchen. Waiters hustle. Before the TV show, people would walk in, and you could see a certain look on their faces. One that said, I drove a long way to be here. This had better be as good as everyone says it is. Now, expectations are even higher. It has to be good, because they put it on television, right? Chef & the Farmer is more than a restaurant now. It’s a voyage. People don’t wander in and discover the menu anymore. Most of them already know the whole backstory. They want to see if real life matches up with what they watch on TV.\n",
            "Vivian sits in the art gallery, contemplating this. “It’s, like, larger than life,” she says of Chef & the Farmer. “It’s like they somehow expect that it’s going to change their lives.”\n",
            "“That’s good,” says Ben Harper, Buy Local’s owner.\n",
            "“Except that it’s just a restaurant,” Vivian says. “If I were to do what I used to do, which is stand at the end of that pass, we would never get a plate of food out. I did it for a while. There were layers of people waiting to come up to talk to me.”\n",
            "That’s part of the reason why Vivian no longer hangs out in her restaurant every night. The show reflects that. If A Chef’s Life doesn’t show Vivian going from table to table, that expectation is no longer there, and people won’t feel as disappointed if they don’t meet her. There are autographed menus in the back for visitors who want a totem to take home. But a selfie, the current currency of closeness to celebrity, is increasingly hard to come by, because Vivian no longer needs to have as active a role in the day-to-day running of her restaurant. She, like most of us, wants to be at home in the evening with her 4-year-old twins, Florence and Theodore.\n",
            "Out on the sidewalk in front of Buy Local,\n",
            "a woman and her kids walk up to the gallery’s windows and peer inside. They’ve been searching for Vivian, who puts down her wine and walks out.\n",
            "“Lila wanted to meet you,” says Lila’s stepmom, Missy Aldridge.\n",
            "“You want to take a picture?” Vivian asks, before crowding between Lila, 11, and her brother Trey, 10.\n",
            "Missy explains that Lila keeps changing her mind about what she wants to do when she grows up. She’s shifting between wanting to be a chef, a baker, or an artist.\n",
            "“You can be all three of those,” Vivian tells her.\n",
            "Missy raises her smartphone. “We watch you on the show all the time,” she tells Vivian. “When we watched y’all, I said ‘I really do know her.’ ”\n",
            "Missy’s visiting from Clayton. “None of this was here!” she says, a bit wide-eyed. “I never thought this one restaurant would have started all this.”\n",
            "After Chef & the Farmer, next came Mother Earth Brewing, which opened downtown in October 2009. The taproom opened five months later. Then, starting in late 2011, a slew of new restaurants opened: Queen Street Deli & Bakery, Irie Eat’s Café. Ginger 108. The Red Room. Sweetiepies Cupcakery. The Boiler Room Oyster Bar (opened by Ben and Vivian. And, most recently, Olvera Street Taqueria. All of those restaurants feed off of Chef & the Farmer, which is only open for dinner. Last year, the O’Neil Hotel opened in a long-dormant five-story bank building, and now offers seven rooms for people who want to be within walking distance of it all. The place is often full, even on weekdays.\n",
            "“I think we were really surprised at how much of an impact it had,” Vivian says, “because we didn’t really think anyone was going to watch it.”\n",
            "Businesses that have been mainstays in downtown Kinston for decades are happily adjusting to the cameras and the people they bring. In one episode, Vivian buys some chicks from Parrott’s General Store, which has been open for 71 years. You can buy live rabbits, too, but tourists can’t figure out a way to take animals home, so the store started selling gifts. Business has been so good that Parrott’s expanded five years ago. “I’m not used to cameras,” says owner Tommy Jones, smiling. “I didn’t run and hide. But I wanted to.”\n",
            "Another time, Vivian interviewed J.C. Reynolds of Reynolds Seafood, which has flounder, speckled trout, and spot on ice in a display case. “It was OK,” J.C. says of the experience, but the response to the show was intense. He had Harper print up T-shirts for out-of-towners who wanted to buy something, but weren’t prepared to drive home with a whole sea mullet.\n",
            "Some people are coming here to live, lured by a growing art and restaurant scene, $300-a-month apartments, and $600-a-month mortgages. Harper, a young, bearded, family man, used to be a T-shirt printer in Carrboro, where he paid high rent on his shop. But after eating at Chef & the Farmer in 2013, he was intrigued by what he saw in town. Last year, he moved his business and his family to Kinston. He bought a storefront on West North Street downtown. He bought a house for a song. And he’s happy. He’s betting on Kinston’s future, as a small town that’s big on food and culture. “There are progressive things happening here that gave me the confidence that this would be an interesting place to be,” he says.\n",
            "Even with all this — the new restaurants, the pioneers, and the buzz — a walk from the O’Neil Hotel at night is a surprisingly solitary affair. Walk inward, toward the Chef & the Farmer down Gordon or North streets, and you pass restaurants that are open, or about to open. There’s a spa. There’s a butcher shop. There are people, dressed like out-of-towners — Hawaiian shirts, big necklaces, and shorts with belts — maybe heading to The Red Room for a drink. Windows in empty storefronts now display framed walls waiting for Sheetrock.\n",
            "Walk in the other direction, down Queen Street, and you see old Kinston, the one still unaffected by the rising tide coming from Vivian and her TV show. There’s the abandoned Paramount theater, a karate studio, a thrift shop, a shoe store, and a used-book seller. This town, built on tobacco farms, wounded by Hurricane Floyd, and left fallow by the loss of lumber and cotton mills, has been steadily losing people since 1990. “It’s a town in transition,” says Adrian King, executive director of Pride of Kinston, a group trying to revitalize old buildings downtown and to get new businesses to move in. Chef & the Farmer has been a spark, creating an industry nobody ever envisioned for downtown Kinston: tourism. “It basically set that end of town on fire,” he says.\n",
            "But despite the show’s success — its third season debuted on PBS in September — the struggle for funding is constant. PBS doesn’t pay production costs; they merely provide distribution. So it’s up to Vivian and Cynthia to go out and find underwriters who will get very limited exposure during each episode (PBS doesn’t allow product placement, so one sponsor, a cookware company, isn’t allowed to have its products featured on A Chef’s Life). Vivian is branching out, with cookbooks and a short series of sponsored online videos for Yahoo.com, where the advertising rules aren’t as strict. The constant hustle is proof that being this kind of a TV star doesn’t bring an automatic paycheck. What it does bring is fame. Which brings tourists. Which brings hope. In Kinston, Vivian Howard has created her own ecosystem, where hungry diners are the raw material for aspiring chefs, entrepreneurs, and artists. Chef & the Farmer remains at the top of the food chain. No matter where you go, you eventually end up there. That’s the power of Vivian’s story. A story well told.\n",
            "“For a long time, it was the Ben and Vivian show in downtown Kinston,” Vivian says, her eyes focused into the camera lens. She’s back at the confessional, trying to provide some context for an episode late in the third season. “But in the last few years, a lot of other people are looking at our downtown as a place to open businesses. I mean, to think that we have this cool gallery. We have several restaurants. It’s very, very exciting.” Vivian nails the first take, but she still looks away from the camera and over to Cynthia for approval.\n",
            "“That’s fine,” Cynthia says, and they move on to the next scene.\n",
            "Veteran Adelaide Strikers batsman Brad Hodge has thrown his hat in the ring to be part of the Pakistan Super League (PSL) in Qatar next February.\n",
            "Hodge, 40, enjoyed a productive stint in the Caribbean Premier League (CPL) this year with Guyana, finishing as the tournament’s seventh-highest runs-scorer, and will again line up for the Strikers in his dual role of batsman and assistant coach in BBL|05.\n",
            "The PSL, set to be staged in the Qatar capital of Doha, has already attracted the likes of West Indian T20 superstars Chris Gayle, Dwayne Bravo and Kieron Pollard, and Hodge has confirmed that he has applied to play in the tournament.\n",
            "Quick Single: BBL|05 ins and outs so far\n",
            "“I’ve put my name down to be a part of it – fingers crossed, you never know what can happen,” he told cricket.com.au. “My name just gets put into a draft, and hopefully someone picks me up and I can contribute.\n",
            "“There’s a lot of good players out there that I assume want to be involved, so we’ll see.\n",
            "“I feel a little bit for the Pakistan cricket side, how they’ve had to play away and they’ve had no home cricket for a long time.\n",
            "“They’re such an exciting team, their supporter base is really good, so I reckon this will be a good tournament to lift their spirits.\n",
            "“It could be a landmark moment where people get back on board the Pakistan cricket cause.”\n",
            "The inaugural season of the PSL is set to include five teams from the provincial capitals – Islamabad, Karachi, Lahore, Peshawar and Quetta – and include 24 matches between February 4-24.\n",
            "Quick Single: Stars name new captain, vice-captain\n",
            "Hodge, who has plied his trade in various domestic Twenty20 competitions around the world and is hoping to again play in next year’s CPL, said he was eagerly anticipating another season of the Big Bash, during which he will turn 41.\n",
            "“I’ve probably got a little bit left in me, which is nice,” he said. “I think if my performances drop off, I’ll know, but that hasn’t happened yet.\n",
            "“The breaks actually really help your motivation. You get excited in the build-up to each tournament, whereas sometimes, when you’ve been in the game for this long and continue training Monday to Friday, you just drift in and out.\n",
            "“I don’t have that anymore in my life – cricket season just comes, I get into the action and away I go.”\n",
            "Hodge broke his arm during last year’s Big Bash season, his first with Adelaide, and missed much of the tournament before returning for the home final at which 52,000 Strikers fans watched their team lose to the Sydney Sixers.\n",
            "Quick Single: Pattinson needs more time: Smith\n",
            "“I sort of feel like a new recruit this year,” he said. “But when I played at the new Adelaide Oval in front of a full crowd, as a Victorian, I had to actually question which venue was better out of the MCG or Adelaide – it was a tough call.\n",
            "“It was quite a spectacle really. They’ve got a really good fan base there now and we played a good, exciting brand of cricket which they loved as well.”\n",
            "Though he was lured to South Australia by former coach Darren Berry, Hodge is looking forward to the prospect of linking up with new mentor Jason Gillespie.\n",
            "“Jason’s done well in the four-day comp over there (with Yorkshire in England) and hopefully he can bring some of that success to the Strikers,” he said.\n",
            "“He’ll have his own ideas and the way he wants to do it, so I’ll fit in and work with Jason, find out the game plan and try to help those young kids that we have as best I possibly can.\n",
            "“That’s the idea of having that experience in the side – you have to try to pass that knowledge on as well as you can.\n",
            "“We saw some really good glimpses of that last year where it paid off. Travis Head went really well, Alex Ross came in and had a few cameos, which was something I’d spoken about with him after my own experiences with Rajasthan Royals.”\n",
            "Nintendo's Wii U may not have gotten off to a hot start, but you shouldn't bet against the platform, according to GameStop president Tony Bartel. Speaking with investors today during an earnings call, Bartel laid out the reasons why he thinks the Wii U can be a hit.\n",
            "\"We've been very consistent every time we're asked about the Wii U,\" he said. \"Really what they needed was two things: they needed strong first-party titles, which we think that they are getting; and they also needed to do a better job of explaining exactly what the connected tablet--as we like to call it--how you actually use that. I think they are going to do a great job on both of those.\"\n",
            "Some major first-party Wii U games on the way include Super Smash Bros. (fall 2014), Hyrule Warriors (September), and a brand new, open-world Legend of Zelda game for 2015.\n",
            "GameStop executive vice president Mike Hogan also chimed in, saying Mario Kart 8's record-setting release in May helped drive Wii U sales substantially. The game has sold nearly 3 million copies worldwide to date. In addition, GameStop management referenced the data released this week from Amazon (via MCV) that showed Wii U game preorders spiked by a major margin after Gamescom last week. \"So we're very excited about that,\" Bartel said.\n",
            "Finally, Bartel teased that Nintendo's upcoming toy line, amiibo, could be a big hit. \"I would also not sell amiibo short, either,\" he said. Amiibo launches this fall with Super Smash Bros. for Wii U, and will also work with Mario Kart 8, Yoshi's Woolly World, Captain Toad: Treasure Tracker, and Mario Party 10. Nintendo will also sell an adapter that will allow the toys to work with the 3DS.\n",
            "GameStop today reported earnings for its latest quarter, and the results were strong across the board. The company posted year-over-year gains for revenue and profit, and also saw hardware and software post positive growth compared to last year.\n",
            "‘This Is Our Signature’: iOS 7\n",
            "No one has questioned whether Jony Ive can lead a hardware design team. Whether he could lead a software design team, however, has been the biggest question facing Apple over the last eight months. In fact, it might be the biggest question facing Apple, ever, because it’s another way of asking whether the company could produce innovative software without you-know-who at the helm of the ship.\n",
            "Today, we have our answer, and it is a resounding yes. Jony Ive can lead a software team.\n",
            "The key, I think, is that his approach, and Apple’s as a whole under its post-Forstall organizational structure, is not to view this as two different things, hardware and software, but rather as a single thing: design.\n",
            "There is a deep intellectual rigor to the design of iOS 7, and it’s hard not to see it as being profoundly informed by Ive’s background in hardware. In hardware, design is limited by physics: weight, density, size, connections, seams. Software doesn’t face those design limits. The old design of iOS 6 took advantage of that lack of limits, to its detriment. In iOS 6, you open a folder on the home screen, and linen is something you see underneath. You pull down Notification Center, and linen is something you see over. It’s both over and under. Hardware doesn’t work like that, but software can, because software can show you anything, conceptual logic be damned.\n",
            "The design of iOS 7 is based on rules. There’s an intricate system at work, a Z-axis of layers organized in a logical way. There is a profound reduction in the use of faux-3D visual effects and textures, but iOS 7 is anything but flat. It is three dimensional not just visually but logically. It uses translucency not to show off, but to provide you with a sense of place. When you pull the new Control Center panel up from the bottom of the screen, its translucency lets you know that you haven’t gone somewhere new, you’re just looking at something over where you were.\n",
            "There’s a sense of place, depth, and spatiality in iOS 7 that makes it feel like hardware. A real thing, not pixels rendered on glass. It’s as though Ive has brought the same design goals that have always informed Apple’s hardware to software. And here, his team isn’t limited by physics. Planes can have zero thickness. But it’s a system, in the truest sense of the word.\n",
            "iOS 7 is not perfect; this new design framework will evolve and improve over time, just like iOS’s original aesthetic did. But it’s a conceptual foundation that corrects all of the excesses of the original iOS aesthetic. It’s radically different but not disorienting. Less flashy, less bling, more subtle, more refined.\n",
            "This is the first product of the post-Jobs Apple. The result shows that in some ways Apple’s software design has gotten better, because it was Jobs (and Forstall) who had a penchant for exuberant textures and gimmickry. Jobs’s taste in hardware was nearly perfect, but his taste in software had a weakness for the saccharine. Wood grain, linen, Rich Corinthian leather, etc. It was all just sugar for the eyes. This is a weakness Jony Ive’s software taste clearly does not suffer.\n",
            "The software is now of a piece with the hardware. Two sides of the same coin. Not hardware design and software design. Just design.\n",
            "Sports betting picks and handicapping information for your Saturday card. Sports betting aid once again brings to you the latest stats , trends, and handicapping situations to help you in all of your sports handicapping ventures. And if sports capping isn’t really your thing no worries we have also included some free sports picks from some of the best known pro sports handicappers and services on the web. We hope this information will aid you in all of your sports predictions today and best of luck !\n",
            "NBA Stats and Trends for Saturday\n",
            "Hot Teams\n",
            "— Thunder won/covered last four games (6-4 AF).\n",
            "— Bulls won four of their last five games (3-8 last 11 HF).\n",
            "— Toronto won ten of its last twelve games (3-1 last four AU).\n",
            "— Clippers won their last three games (7-3 last ten HF).\n",
            "Cold Teams\n",
            "— Hornets lost last three games, by 8-1-28 points (4-6-1 HU).\n",
            "— Miami lost six of last nine games (2-8-1 last 11 HF). Pelicans lost their last four games (8-0 last eight AU).\n",
            "— Suns lost six of their last seven games (1-3 last four AU).\n",
            "— Rockets lost three of last four games (6-2-1 last nine HF).\n",
            "— Kings lost five of their last seven games (3-8 last 11 AU).\n",
            "Series Records\n",
            "— Thunder won their last eight games with Charlotte.\n",
            "— Pelicans won last two games with Miami, after losing previous five.\n",
            "— Bulls won six of last eight games with Phoenix.\n",
            "— Home side won last 12 Toronto-Houston games.\n",
            "— Clippers won ten of last twelve games with Sacramento.\n",
            "Totals\n",
            "— Eight of last nine Charlotte home games stayed under.\n",
            "— Six of last eight New Orleans games went over.\n",
            "— Six of last seven Chicago games stayed under total.\n",
            "— Five of last seven Houston home games went over.\n",
            "— Four of last five Sacramento road games stayed under.\n",
            "Back-to-Backs\n",
            "— Pelicans are 10-2 vs spread if they played night before. Heat is 6-4-2.\n",
            "— Suns are 4-6 vs spread if they played night before; Bulls are 7-6.\n",
            "— Raptors are 8-2 vs spread if they played night before; Rockets are 8-5.\n",
            "— Sacramento is 3-4 vs spread on road if it played night before.\n",
            "NBA > (505) PHOENIX@ (506) CHICAGO | 02/21/2015 – 08:05 PM\n",
            "Play AGAINST CHICAGO using the money line in Home games after a division game\n",
            "The record is 8 Wins and 12 Losses for the last three seasons (-26.3 units)\n",
            "NBA > (503) NEW ORLEANS@ (504) MIAMI | 02/21/2015 – 07:35 PM\n",
            "Play ON NEW ORLEANS using the money line in All games after scoring 85 points or less\n",
            "The record is 15 Wins and 9 Losses for the last three seasons (+18.2 units)\n",
            "NBA > (505) PHOENIX@ (506) CHICAGO | 02/21/2015 – 08:05 PM\n",
            "Play ON PHOENIX using the against the spread in Road games in non-conference games\n",
            "The record is 19 Wins and 5 Losses for the last two seasons (+13.5 units)\n",
            "NBA > (505) PHOENIX@ (506) CHICAGO | 02/21/2015 – 08:05 PM\n",
            "Play UNDER CHICAGO on the total in Home games on Saturday games\n",
            "The record is 6 Overs and 22 Unders for the last three seasons (+15.4 units)\n",
            "NBA > (503) NEW ORLEANS@ (504) MIAMI | 02/21/2015 – 07:35 PM\n",
            "Play OVER NEW ORLEANS on the total in All games when playing against a team with a losing record\n",
            "The record is 61 Overs and 29 Unders for the last three seasons (+29.1 units)\n",
            "NBA > (507) TORONTO@ (508) HOUSTON | 02/21/2015 – 08:05 PM\n",
            "Play OVER TORONTO on the total in Road games versus good offensive teams – scoring 99+ points/game\n",
            "The record is 32 Overs and 12 Unders for the last three seasons (+18.8 units)\n",
            "NBA > (509) SACRAMENTO@ (510) LA CLIPPERS | 02/21/2015 – 10:05 PM\n",
            "Play UNDER SACRAMENTO on the total in All games versus good offensive teams – scoring 99+ points/game – 2nd half of the season\n",
            "The record is 9 Overs and 25 Unders for the last two seasons (+15.1 units)\n",
            "NBA > (509) SACRAMENTO@ (510) LA CLIPPERS | 02/21/2015 – 10:05 PM\n",
            "Play AGAINST SACRAMENTO in the first half in Road games after a non-conference game\n",
            "The record is 6 Wins and 20 Losses for the last two seasons (-16 units)\n",
            "NBA > (501) OKLAHOMA CITY@ (502) CHARLOTTE | 02/21/2015 – 07:05 PM\n",
            "Play UNDER CHARLOTTE on the total in All games on Saturday games\n",
            "The record is 1 Overs and 10 Unders for the this season (+8.9 units)\n",
            "College Basketball Stats and Trends for Saturday\n",
            "CBB > (709) N DAKOTA@ (710) SACRAMENTO ST | 02/21/2015 – 10:00 PM\n",
            "Play ON SACRAMENTO ST using the money line in All games as a favorite\n",
            "The record is 16 Wins and 1 Losses for the last two seasons (+14.6 units)\n",
            "CBB > (551) DAYTON@ (552) DUQUESNE | 02/21/2015 – 02:00 PM\n",
            "Play AGAINST DUQUESNE using the money line in All games in February games\n",
            "The record is 33 Wins and 76 Losses for the since 1992 (-72.05 units)\n",
            "CBB > (613) COLUMBIA@ (614) YALE | 02/21/2015 – 07:00 PM\n",
            "Play ON YALE using the money line in All games versus good defensive teams – allowing <=64 points/game after 15+ games\n",
            "The record is 11 Wins and 4 Losses for the last three seasons (+17 units)\n",
            "CBB > (563) NEVADA@ (564) BOISE ST | 02/21/2015 – 03:00 PM\n",
            "Play AGAINST NEVADA using the money line in All games in February games\n",
            "The record is 5 Wins and 13 Losses for the last three seasons (-20.1 units)\n",
            "CBB > (681) IDAHO ST@ (682) MONTANA ST | 02/21/2015 – 04:30 PM\n",
            "Play AGAINST IDAHO ST using the money line in All games when playing against a team with a losing record\n",
            "The record is 3 Wins and 17 Losses for the last two seasons (-16.75 units)\n",
            "CBB > (657) HAWAII@ (658) CAL DAVIS | 02/21/2015 – 10:00 PM\n",
            "Play ON CAL DAVIS using the money line in All games off a win against a conference rival\n",
            "The record is 8 Wins and 1 Losses for the this season (+10.2 units)\n",
            "CBB > (515) PITTSBURGH@ (516) SYRACUSE | 02/21/2015 – 12:00 PM\n",
            "Play AGAINST PITTSBURGH using the money line in Road games versus good defensive teams – allowing <=64 points/game\n",
            "The record is 5 Wins and 14 Losses for the last three seasons (-20 units)\n",
            "CBB > (607) COLL OF CHARLESTON@ (608) JAMES MADISON | 02/21/2015 – 07:00 PM\n",
            "Play ON JAMES MADISON in the first half in All games when playing against a team with a losing record\n",
            "The record is 19 Wins and 3 Losses for the last two seasons (+15.7 units)\n",
            "CBB > (587) SOUTHERN MISS@ (588) CHARLOTTE | 02/21/2015 – 05:30 PM\n",
            "Play AGAINST CHARLOTTE using the money line in All games when playing with one or less days rest\n",
            "The record is 21 Wins and 27 Losses for the since 1992 (-43.25 units)\n",
            "CBB > (655) SAN DIEGO ST@ (656) SAN JOSE ST | 02/21/2015 – 10:00 PM\n",
            "Play ON SAN DIEGO ST using the money line in All games in all games\n",
            "The record is 37 Wins and 11 Losses for the last two seasons (+25.95 units)\n",
            "CBB > (527) ELON@ (528) DELAWARE | 02/21/2015 – 12:30 PM\n",
            "Play ON DELAWARE using the money line in All games when playing against a team with a losing record\n",
            "The record is 15 Wins and 2 Losses for the last two seasons (+13 units)\n",
            "CBB > (577) CLEMSON@ (578) DUKE | 02/21/2015 – 04:00 PM\n",
            "Play ON CLEMSON using the money line in All games after scoring 60 points or less\n",
            "The record is 8 Wins and 1 Losses for the this season (+10.05 units)\n",
            "CBB > (511) S FLORIDA@ (512) E CAROLINA | 02/21/2015 – 11:00 AM\n",
            "Play ON E CAROLINA using the money line in Home games in February games\n",
            "The record is 8 Wins and 2 Losses for the last three seasons (+13.3 units)\n",
            "CBB > (655) SAN DIEGO ST@ (656) SAN JOSE ST | 02/21/2015 – 10:00 PM\n",
            "Play AGAINST SAN JOSE ST using the against the spread in All games when playing against a team with a winning record after 15 or more games\n",
            "The record is 4 Wins and 22 Losses for the last three seasons (-20.2 units)\n",
            "CBB > (665) COLORADO@ (666) OREGON ST | 02/21/2015 – 11:00 PM\n",
            "Play UNDER COLORADO on the total in Road games on Saturday games\n",
            "The record is 0 Overs and 10 Unders for the last three seasons (+10 units)\n",
            "CBB > (691) MERCER@ (692) SAMFORD | 02/21/2015 – 07:00 PM\n",
            "Play OVER SAMFORD on the total in All games off a loss against a conference rival\n",
            "The record is 15 Overs and 2 Unders for the last two seasons (+12.8 units)\n",
            "CBB > (675) IUPUI@ (676) S DAKOTA | 02/21/2015 – 03:00 PM\n",
            "Play UNDER IUPUI on the total in All games after a conference game\n",
            "The record is 1 Overs and 11 Unders for the this season (+9.9 units)\n",
            "CBB > (649) UCLA@ (650) ARIZONA | 02/21/2015 – 09:00 PM\n",
            "Play UNDER UCLA on the total in Road games when playing against a team with a winning record\n",
            "The record is 1 Overs and 11 Unders for the this season (+9.9 units)\n",
            "CBB > (597) AIR FORCE@ (598) COLORADO ST | 02/21/2015 – 06:00 PM\n",
            "Play UNDER AIR FORCE on the total in Road games after a conference game\n",
            "The record is 2 Overs and 16 Unders for the last three seasons (+13.8 units)\n",
            "CBB > (531) S ILLINOIS@ (532) INDIANA ST | 02/21/2015 – 01:00 PM\n",
            "Play UNDER INDIANA ST on the total in All games when playing against a team with a losing record after 15 or more games\n",
            "The record is 1 Overs and 12 Unders for the last two seasons (+10.9 units)\n",
            "CBB > (707) N COLORADO@ (708) PORTLAND ST | 02/21/2015 – 10:00 PM\n",
            "Play AGAINST N COLORADO in the first half in All games in February games\n",
            "The record is 1 Wins and 12 Losses for the last two seasons (-12.2 units)\n",
            "CBB > (681) IDAHO ST@ (682) MONTANA ST | 02/21/2015 – 04:30 PM\n",
            "Play AGAINST MONTANA ST using the against the spread in All games on Saturday games\n",
            "The record is 3 Wins and 17 Losses for the last two seasons (-15.7 units)\n",
            "CBB > (577) CLEMSON@ (578) DUKE | 02/21/2015 – 04:00 PM\n",
            "Play ON CLEMSON in the first half in Road games when playing against a team with a winning record\n",
            "The record is 8 Wins and 0 Losses for the this season (+8 units)\n",
            "CBB > (671) ST PETERS@ (672) FAIRFIELD | 02/21/2015 – 02:00 PM\n",
            "Play AGAINST FAIRFIELD in the first half in Home games after a conference game\n",
            "The record is 0 Wins and 8 Losses for the this season (-8.8 units)\n",
            "CBB > (527) ELON@ (528) DELAWARE | 02/21/2015 – 12:30 PM\n",
            "Play AGAINST DELAWARE in the first half in Home games against conference opponents\n",
            "The record is 0 Wins and 8 Losses for the this season (-8.8 units)\n",
            "CBB > (521) OKLAHOMA@ (522) TEXAS TECH | 02/21/2015 – 12:00 PM\n",
            "Play UNDER TEXAS TECH on the total in All games versus good defensive teams – allowing <=64 points/game after 15+ games\n",
            "The record is 0 Overs and 8 Unders for the this season (+8 units)\n",
            "CBB > (709) N DAKOTA@ (710) SACRAMENTO ST | 02/21/2015 – 10:00 PM\n",
            "Play OVER N DAKOTA on the total in All games when playing with one or less days rest\n",
            "The record is 8 Overs and 0 Unders for the this season (+8 units)\n",
            "CBB > (633) WRIGHT ST@ (634) VALPARAISO | 02/21/2015 – 08:00 PM\n",
            "Play AGAINST WRIGHT ST using the against the spread in All games versus good defensive teams – allowing <=64 points/game\n",
            "The record is 0 Wins and 8 Losses for the this season (-8.8 units)\n",
            "CBB > (637) UC-SANTA BARBARA@ (638) CS-FULLERTON | 02/21/2015 – 08:00 PM\n",
            "Play AGAINST UC-SANTA BARBARA in the first half in All games versus the first half line in all games\n",
            "The record is 3 Wins and 15 Losses for the this season (-13.5 units)\n",
            "CBB > (565) SAN FRANCISCO@ (566) PEPPERDINE | 02/21/2015 – 04:00 PM\n",
            "Play ON SAN FRANCISCO using the against the spread in Road games in February games\n",
            "The record is 45 Wins and 13 Losses for the since 1992 (+30.7 units)\n",
            "CBB > (619) FORDHAM@ (620) DAVIDSON | 02/21/2015 – 07:00 PM\n",
            "Play ON DAVIDSON in the first half in All games versus the first half line in all games\n",
            "The record is 17 Wins and 4 Losses for the this season (+12.6 units)\n",
            "CBB > (519) GEORGIA TECH@ (520) N CAROLINA | 02/21/2015 – 12:00 PM\n",
            "Play ON GEORGIA TECH in the first half in Road games after a conference game\n",
            "The record is 14 Wins and 2 Losses for the last two seasons (+11.8 units)\n",
            "CBB > (677) IUPU-FT WAYNE@ (678) N DAKOTA ST | 02/21/2015 – 03:00 PM\n",
            "Play ON IUPU-FT WAYNE in the first half in All games in February games\n",
            "The record is 15 Wins and 2 Losses for the last three seasons (+12.8 units)\n",
            "CBB > (655) SAN DIEGO ST@ (656) SAN JOSE ST | 02/21/2015 – 10:00 PM\n",
            "Play AGAINST SAN JOSE ST in the first half in All games versus good defensive teams – allowing <=64 points/game after 15+ games\n",
            "The record is 2 Wins and 15 Losses for the last three seasons (-14.5 units)\n",
            "CBB > (593) SANTA CLARA@ (594) LOYOLA-MARYMOUNT | 02/21/2015 – 06:00 PM\n",
            "Play AGAINST LOYOLA-MARYMOUNT in the first half in Home games off a loss against a conference rival\n",
            "The record is 2 Wins and 15 Losses for the last three seasons (-14.5 units)\n",
            "CBB > (641) MARSHALL@ (642) UAB | 02/21/2015 – 08:00 PM\n",
            "Play AGAINST MARSHALL using the against the spread in Road games on Saturday games\n",
            "The record is 2 Wins and 15 Losses for the last three seasons (-14.5 units)\n",
            "CBB > (571) DREXEL@ (572) NORTHEASTERN | 02/21/2015 – 04:00 PM\n",
            "Play AGAINST NORTHEASTERN using the against the spread in Home games off a win against a conference rival\n",
            "The record is 2 Wins and 15 Losses for the last three seasons (-14.5 units)\n",
            "CBB > (677) IUPU-FT WAYNE@ (678) N DAKOTA ST | 02/21/2015 – 03:00 PM\n",
            "Play ON IUPU-FT WAYNE using the against the spread in All games as an underdog\n",
            "The record is 32 Wins and 9 Losses for the last three seasons (+22.1 units)\n",
            "CBB > (601) VIRGINIA TECH@ (602) NC STATE | 02/21/2015 – 06:00 PM\n",
            "Play AGAINST NC STATE using the against the spread in Home games when playing with 5 or 6 days rest\n",
            "The record is 3 Wins and 18 Losses for the since 1992 (-16.8 units)\n",
            "CBB > (539) BUFFALO@ (540) BOWLING GREEN | 02/21/2015 – 02:00 PM\n",
            "Play ON BOWLING GREEN using the against the spread in All games in all games\n",
            "The record is 16 Wins and 4 Losses for the this season (+11.6 units)\n",
            "CBB > (669) NIAGARA@ (670) RIDER | 02/21/2015 – 02:00 PM\n",
            "Play AGAINST RIDER using the against the spread in Home games when playing against a team with a losing record\n",
            "The record is 0 Wins and 8 Losses for the last two seasons (-8.8 units)\n",
            "CBB > (523) TEXAS A&M@ (524) S CAROLINA | 02/21/2015 – 12:00 PM\n",
            "Play AGAINST S CAROLINA using the against the spread in All games versus good defensive teams – allowing <=64 points/game after 15+ games\n",
            "The record is 3 Wins and 16 Losses for the last three seasons (-14.6 units)\n",
            "CBB > (661) CS-NORTHRIDGE@ (662) UC-IRVINE | 02/21/2015 – 10:00 PM\n",
            "Play AGAINST CS-NORTHRIDGE using the against the spread in All games in all games\n",
            "The record is 5 Wins and 17 Losses for the this season (-13.7 units)\n",
            "CBB > (563) NEVADA@ (564) BOISE ST | 02/21/2015 – 03:00 PM\n",
            "Play ON NEVADA using the teaser in Road games off a loss against a conference rival\n",
            "The record is 41 Wins and 4 Losses for the since 1992 (+36.6 units)\n",
            "CBB > (573) TCU@ (574) KANSAS | 02/21/2015 – 04:00 PM\n",
            "Play ON KANSAS using the teaser in Home games against conference opponents\n",
            "The record is 23 Wins and 1 Losses for the last three seasons (+21.9 units)\n",
            "NHL Stats and Trends for Saturday\n",
            "Hot teams\n",
            "— Islanders won five of their last six games; Washington won seven of last nine.\n",
            "— Predators won six of their last seven games.\n",
            "— Blue Jackets won three of their last four games.\n",
            "— Panthers won last two games, are 5-4 in their last nine.\n",
            "— Carolina won three of its last five road games. Devils won last two games, allowing three goals.\n",
            "— Blues won four of their last five games.\n",
            "— Dallas Stars won four of last six games.\n",
            "— Kings won their last six games, allowing 12 goals.\n",
            "Cold teams\n",
            "— Flyers lost four of their last five games.\n",
            "— Toronto is 4-22-2 in its last 28 games. Jets lost six of their last seven road games.\n",
            "— Montreal lost three of its last five games.\n",
            "— Ottawa is 0-7 in game following its last seven wins.\n",
            "— Ducks lost five of their last seven games. Edmonton lost three of last four.\n",
            "— Penguins lost last three games: 2-1/3-1/2-1.\n",
            "— Red Wings lost three of their last four games.\n",
            "— Lightning is 4-5 in its last nine games. Arizona lost last four games, outscored 16-6.\n",
            "— Sharks lost five of their last seven games.\n",
            "Series records\n",
            "— Islanders lost three of last four visits to Washington.\n",
            "— Predators lost three of last four visits to Philly.\n",
            "— Jets won their last five games with Toronto.\n",
            "— Canadiens won three of last four games with Columbus.\n",
            "— Panthers won three of last four games with Ottawa.\n",
            "— Devils won three of last four games with Carolina.\n",
            "— Ducks won 19 of last 23 games with Edmonton.\n",
            "— Blues are 4-3 in last seven with Pittsburgh, winning 2-1/1-0 in last two.\n",
            "— Red Wings won eight of last ten games with Dallas.\n",
            "— Lightning is 5-4 in its last nine games with Arizona.\n",
            "— LA-SJ game is outdoors at Levi’s Stadium in Santa Clara; Kings won five of last seven in series. .\n",
            "Totals\n",
            "— Seven of last nine Washington home games stayed under.\n",
            "— Nine of last thirteen Nashville road games went over.\n",
            "— Six of last eight Winnipeg road games went over total.\n",
            "— Seven of last eight Columbus road games stayed under.\n",
            "— Six of last eight Ottawa home games went over total.\n",
            "— Six of last seven New Jersey home games stayed under.\n",
            "— Four of last five Edmonton home games stayed under.\n",
            "— Nine of last eleven Penguin road games stayed under.\n",
            "— Eight of last ten Detroit road games stayed under.\n",
            "—\n",
            "— Seven of last ten LA road games stayed under total.\n",
            "Back-to-back\n",
            "— Carolina is 2-7 if it played the night before; New Jersey is 4-8. .\n",
            "— Ducks are 3-2 on road if they played night before; Oilers are 1-6 overall.\n",
            "— NHL home teams are 6-10 if they also played at home the night before.\n",
            "NHL > (3) NASHVILLE@ (4) PHILADELPHIA | 02/21/2015 – 01:05 PM\n",
            "Play ON NASHVILLE using the money line in All games\n",
            "The record is 29 Wins and 6 Losses for the this season (+20.8 units)\n",
            "NHL > (13) ANAHEIM@ (14) EDMONTON | 02/21/2015 – 07:05 PM\n",
            "Play ON ANAHEIM using the money line in Road games on Saturday games\n",
            "The record is 17 Wins and 4 Losses for the last three seasons (+15.1 units)\n",
            "NHL > (13) ANAHEIM@ (14) EDMONTON | 02/21/2015 – 07:05 PM\n",
            "Play ON ANAHEIM using the in Road games on Saturday games\n",
            "The record is 17 Wins and 4 Losses for the last three seasons (+15.1 units)\n",
            "NHL > (19) DETROIT@ (20) DALLAS | 02/21/2015 – 08:05 PM\n",
            "Play AGAINST DALLAS using the in Home games revenging a loss versus opponent\n",
            "The record is 4 Wins and 13 Losses for the this season (-12.9 units)\n",
            "NHL > (19) DETROIT@ (20) DALLAS | 02/21/2015 – 08:05 PM\n",
            "Play AGAINST DALLAS using the money line in Home games revenging a loss versus opponent\n",
            "The record is 4 Wins and 13 Losses for the this season (-12.9 units)\n",
            "NHL > (5) COLUMBUS@ (6) MONTREAL | 02/21/2015 – 07:05 PM\n",
            "Play ON COLUMBUS using the money line in All games revenging a home loss versus opponent\n",
            "The record is 21 Wins and 11 Losses for the last two seasons (+17.05 units)\n",
            "NHL > (5) COLUMBUS@ (6) MONTREAL | 02/21/2015 – 07:05 PM\n",
            "Play ON COLUMBUS using the in All games revenging a home loss versus opponent\n",
            "The record is 21 Wins and 11 Losses for the last two seasons (+17.05 units)\n",
            "NHL > (5) COLUMBUS@ (6) MONTREAL | 02/21/2015 – 07:05 PM\n",
            "Play OVER COLUMBUS on the total in Road games after playing 3 consecutive road games\n",
            "The record is 8 Overs and 0 Unders for the last three seasons (+8.45 units)\n",
            "NHL > (1) NY ISLANDERS@ (2) WASHINGTON | 02/21/2015 – 12:35 PM\n",
            "Play AGAINST WASHINGTON using the money line in All games after a 3 game unbeaten streak\n",
            "The record is 0 Wins and 6 Losses for the this season (-8.3 units)\n",
            "NHL > (1) NY ISLANDERS@ (2) WASHINGTON | 02/21/2015 – 12:35 PM\n",
            "Play AGAINST WASHINGTON using the in All games after a 3 game unbeaten streak\n",
            "The record is 0 Wins and 6 Losses for the this season (-8.3 units)\n",
            "NHL > (7) WINNIPEG@ (8) TORONTO | 02/21/2015 – 07:05 PM\n",
            "Play AGAINST TORONTO using the money line in All games revenging a loss versus opponent\n",
            "The record is 8 Wins and 24 Losses for the this season (-19 units)\n",
            "NHL > (7) WINNIPEG@ (8) TORONTO | 02/21/2015 – 07:05 PM\n",
            "Play AGAINST TORONTO using the in All games revenging a loss versus opponent\n",
            "The record is 8 Wins and 24 Losses for the this season (-19 units)\n",
            "NHL > (11) CAROLINA@ (12) NEW JERSEY | 02/21/2015 – 07:05 PM\n",
            "Play UNDER NEW JERSEY on the total in Home games when playing against a team with a losing record\n",
            "The record is 6 Overs and 21 Unders for the last three seasons (+14.7 units)\n",
            "NHL > (11) CAROLINA@ (12) NEW JERSEY | 02/21/2015 – 07:05 PM\n",
            "Play ON CAROLINA using the money line in All games when playing against a team with a losing record in the second half of the season\n",
            "The record is 6 Wins and 0 Losses for the this season (+7.05 units)\n",
            "NHL > (11) CAROLINA@ (12) NEW JERSEY | 02/21/2015 – 07:05 PM\n",
            "Play ON CAROLINA using the in All games when playing against a team with a losing record in the second half of the season\n",
            "The record is 6 Wins and 0 Losses for the this season (+7.05 units)\n",
            "NHL > (17) PITTSBURGH@ (18) ST LOUIS | 02/21/2015 – 08:05 PM\n",
            "Play UNDER PITTSBURGH on the total in All games after a division game\n",
            "The record is 5 Overs and 15 Unders for the this season (+10.1 units)\n",
            "NHL > (7) WINNIPEG@ (8) TORONTO | 02/21/2015 – 07:05 PM\n",
            "Play OVER WINNIPEG on the total in All games in a road game where where the total is 5.5\n",
            "The record is 12 Overs and 4 Unders for the this season (+9.05 units)\n",
            "NHL > (3) NASHVILLE@ (4) PHILADELPHIA | 02/21/2015 – 01:05 PM\n",
            "Play OVER PHILADELPHIA on the total in All games on Saturday games\n",
            "The record is 26 Overs and 10 Unders for the last three seasons (+16.6 units)\n",
            "NHL > (1) NY ISLANDERS@ (2) WASHINGTON | 02/21/2015 – 12:35 PM\n",
            "Play OVER NY ISLANDERS on the total in All games after scoring 4 goals or more in their previous game\n",
            "The record is 29 Overs and 13 Unders for the last two seasons (+15.3 units)\n",
            "Sports picks from the Pro’s\n",
            "StatFox Super Situations\n",
            "NBA | OKLAHOMA CITY at CHARLOTTE\n",
            "Play On – Any team (CHARLOTTE) revenging a road blowout loss vs opponent of 20 points or more, off an upset loss by 15 points or more as a home favorite\n",
            "29-8 since 1997. ( 78.4% | 20.2 units )\n",
            "1-0 this year. ( 100.0% | 1.0 units )\n",
            "NBA | OKLAHOMA CITY at CHARLOTTE\n",
            "Play On – Home underdogs vs. the 1rst half line (CHARLOTTE) a marginal losing team (40% to 49%) playing a winning team, on Saturday games\n",
            "96-50 since 1997. ( 65.8% | 41.0 units )\n",
            "3-2 this year. ( 60.0% | 0.8 units )\n",
            "StatFox Super Situations\n",
            "CBB | IUPU-FT WAYNE at N DAKOTA ST\n",
            "Play Against – Home favorites of 3.5 to 9.5 points (N DAKOTA ST) after allowing 25 points or less in the first half last game against opponent after 2 straight wins by 15 points or more\n",
            "46-18 over the last 5 seasons. ( 71.9% | 26.2 units )\n",
            "6-5 this year. ( 54.5% | 0.5 units )\n",
            "CBB | GONZAGA at ST MARYS-CA\n",
            "Play Against – Home underdogs of +145 to +350 vs. the money line (ST MARYS-CA) off a home win by 10 points or more, in February games\n",
            "150-38 since 1997. ( 79.8% | 62.5 units )\n",
            "2-3 this year. ( 40.0% | -5.9 units )\n",
            "CBB | CALIFORNIA at STANFORD\n",
            "Play Against – A home team vs. the 1rst half line (STANFORD) after failing to cover the spread in 5 or more consecutive games, a good team (60% to 80%) playing a team with a winning record\n",
            "41-15 over the last 5 seasons. ( 73.2% | 24.5 units )\n",
            "7-5 this year. ( 58.3% | 1.5 units )\n",
            "Basketball Crusher\n",
            "Houston +7.5\n",
            "EZWINNERS\n",
            "All 1st Half Plays\n",
            "3* Pittsburgh +2\n",
            "3* Georgia Tech +6.5\n",
            "3* Texas Tech +5.5\n",
            "3* UMass +5.5\n",
            "3* Oklahoma St -2.5\n",
            "3* Texas -1.5\n",
            "3* Marquette +5.5\n",
            "3* Virginia Tech +7.5\n",
            "3* Georgia +1.5\n",
            "3* UCLA +8.5\n",
            "3* St. Mary’s +3.5\n",
            "3* Colorado +2\n",
            "Stephen Nover\n",
            "3* marshall +10.5\n",
            "2* central mich -12\n",
            "1* austin peay +19\n",
            "MADDUX SPORTS\n",
            "Fresno St +6\n",
            "Niagra +11\n",
            "North Dakota St -4\n",
            "South Alabama +10\n",
            "ARLON SPORTS\n",
            "N Carolina -11.5\n",
            "Kansas St +9\n",
            "Xavier -4.5\n",
            "Geo Wash +5\n",
            "Detroit pk\n",
            "Pacific +4\n",
            "The Rainman\n",
            "3*North Carolina -11′\n",
            "3*Vanderbilt -12\n",
            "3*Old Dominion -1\n",
            "1*Mississippi St. +8 over Arkansas\n",
            "1*Western Michigan +1′ over Toledo\n",
            "1*Arkansas St. +2′ over LA Monroe\n",
            "1*Pacific +4 over Portland\n",
            "1*Virginia Tech +13′ over N.C. St.\n",
            "River City Sharps\n",
            "Utah -7.5\n",
            "Al DeMarco\n",
            "15 dme – Davidson\n",
            "5 dime – Jams Madison\n",
            "BigAL\n",
            "3* Texas -2 (rotation #554)\n",
            "3* Nevada +13.5 (rotation #563)\n",
            "3* N. Dakota St. -4 (rotation #678)\n",
            "Jason Sample\n",
            "2U:\n",
            "Pepperdine -2.5\n",
            "Richmond -1/James Madison -1\n",
            "1U:\n",
            "ODU -1.5\n",
            "Marc Lawrence\n",
            "duquesne\n",
            "st marys cal\n",
            "Gaberial Dupont\n",
            "150 dimes – pepperdine\n",
            "Platinum Plays\n",
            "500K Pac12 Lock/Month – Oregon St Beavers -3½\n",
            "500K College Blowout/Year – Xavier Muskateers -4\n",
            "Premier Pick – Houston Rockets -3\n",
            "Premier Pick – Texas Longhorns -2\n",
            "Hockey Crusher\n",
            "San Jose Sharks +101\n",
            "VERNON CROY\n",
            "Florida Panthers-118\n",
            "Soccer Crusher\n",
            "Kortrijk + Westerlo OVER 2.5\n",
            "Chad Dawson claims that he's processed and easily gotten past last September's devastating loss to Andre Ward at 168 pounds, but the way he talks about the fight makes you wonder if that's really the case, as he prepares for Saturday's HBO main event at 175 against Adonis Stevenson.\n",
            "Dawson (31-2, 17 KO) was stopped in the tenth round, going down in the third, fourth, and 10th frames before telling referee Steve Smoger that he'd had enough. The \"quitting\" loss was criticized somewhat at the time, but others couldn't really blame Dawson, as he'd simply been outclassed and beaten up by a healthier, stronger Ward. The weight drain to get down to 168 seemed pretty obvious.\n",
            "But here's what Dawson said this week about that fight:\n",
            "\"My honest opinion is it was definitely a set up by HBO and other people too. It was set up for me to come in at 168. I would be vulnerable and they could expose it and he would look good at the same time. That's just my opinion on the whole situation. We could've done it at a catch weight where we both would've been comfortable but they wanted it at 168. They wanted me to come to Oakland. And I know HBO has high hopes for Andre Ward. I know they have high hopes for him and they want to make him this big champion but that's water under the bridge for me.\"\n",
            "I don't even want to say he's wrong or crazy, because what the hell do I know? It's true that nobody forced Chad Dawson to go to 168 pounds. He could have passed on the idea and taken a lesser fight, but the money was right for this one. I'm sure most at HBO did expect Ward to win, and those who know boxing largely did believe that dropping down a weight class for the first time in several years ran a very high risk of hurting Dawson physically, and that appeared to be the case.\n",
            "But I don't know if you can call it a \"set-up,\" really. It was a money offer and Dawson took it. In some ways, it sounds like he didn't really think that the weight would be a big deal. He acknowledges now that it was. And again, a whole lot of pundits and fans and even fellow fighters thought it was too much of a risk.\n",
            "The way I look at it, Dawson miscalculated the impact the weight loss would have on him, and the best you can hope for now is that it didn't mentally affect him too terribly moving forward. If it has, Adonis Stevenson is definitely a live dog on Saturday.\n",
            "“It’s obvious to anyone who picks up a newspaper or turns on the news that the nation is in the midst of a crisis,” FBI Acting Director Andrew McCabe said at a July 13 press conference at the U.S. Department of Justice, where he joined Attorney General Jeff Sessions and the heads of the Department of Health and Human Services (HHS) and the Drug Enforcement Agency (DEA) in announcing the charges. “Opioid abuse destroys lives and it devastates families. This week, we arrested once-trusted doctors, pharmacists, and other medical professionals who were corrupted by greed. These people inflicted a special kind of damage.”\n",
            "Additionally, HHS began suspending 295 providers—including doctors, nurses, and pharmacists—so they can no longer participate in federal health programs like Medicare, Medicaid, and TRICARE, a health insurance program for veterans and the military.\n",
            "The takedown targeted schemes that billed the federal programs for medically unnecessary prescription drugs. It also focused on medical professionals who unlawfully distributed opioids and other prescription narcotics, thereby contributing to the opioid epidemic.\n",
            "REUTERS/Brendan McDermid Next semester, 200 lucky Princeton students will have the opportunity to take The Great Recession: Causes, Consequences, and Remedies with professor Paul Krugman.\n",
            "The reading list is stacked (and we can't believe that only 178 students are currently signed up to take a class with a Nobel Prize winner, according to Princeton's office of the registrar).\n",
            "\"The course will begin by reviewing the causes of the recession that began in December 2007. It will concentrate on consumer behavior, financial markets, unemployment, and the housing sector,\" according to the course description. \"The role of public policies in contributing to the economic crisis and in ending the crisis will be explored. The state of the recovery will be assessed and monitored.\"\n",
            "Regular Krugman readers will recognize many of the texts from his bloggings at the New York Times, but it's cool to see them laid out all syllabus style.\n",
            "You can follow along at home all semester. Things get a little more free-form by April, but the midterm is on March 12, kids (via David Dayen):\n",
            "Updated Stats\n",
            "Video Views Per Day: 1,422,483\n",
            "Current Number of Subscribers: 4,634,338\n",
            "Current Number of Views: 818,214,380\n",
            "Estimated Earnings Per Day: $1830\n",
            "Youtube is number three most visited website on the internet. Containing millions of active users, Youtube gets around 4 Billion unique visits daily and that's not it. Youtube started a program called \"youtube partnership\" in 2007. With that program anyone can earn revenue for making original content and sharing it on youtube. So from the year 2007 up till now youtube has approved hundreds and thousands of youtube channels for revenue generation.\n",
            "Fact: Youtube in October 2012 announced that its top 1000 partners are making more than $23,000 per month, on average, just by advertising google ads on their videos. This doesn't include the product promotions that youtube partners can do to make extra few bucks .\n",
            "The best part of youtube is that you don't have to make a million dollar movie to start. All it takes is just a camera, a laptop(or desktop, whatever you prefer), an internet connection and a good idea(which won't cost you any money). There are thousands of examples on youtube of people who are making a living out of youtube and all they did was they started to follow their passion and now they are dealing in some good money.\n",
            "Youtube is open for you all the time, but I would recommend that a good age to find the audience of your kind would be 13-21 years. Most of the Youtube partners who are making handsome money on youtube are above 21, but they started when they were in there teens like, Shane Dawson, Ray William Johnson, Phillip de Franco, Pewdiepie and many more...\n",
            "So its time for you to either sit back, relax and watch videos other people have made, or jump in as a youtube partner and make money, or even living out of youtube.\n",
            "Alumni Legacy Scholarship\n",
            "The school we all know today as Eastern Michigan University has had many names over its history, and so have the students who’ve attended. Normalites, Hurons, and Eagles have gathered over the years to share in the traditions and rich history of EMU. Traditions not only pass through each graduating class, but strengthened through numerous generations – continuing a family legacy. Do you have relatives who have graduated from EMU? If so, you are in luck, in 1970, a scholarship was established to honor the children and grandchildren of EMU alumni.\n",
            "This scholarship helps many students who are currently enrolled at EMU, and those who have already come and gone through the campus know what a great experience EMU can be. The school is always looking for almuni to keep the traditional alive by donating to the Alumni legacy scholarship fund.\n",
            "Eligible recipients can be awarded $1,250 per academic year and the reward is renewable based upon class level, degree program, and successful completion of other eligibility requirements. Awards are renewable for up to four consecutive years. That’s a cool 5k!\n",
            "Application Guidelines\n",
            "So your granddad loves to tell stories of his time here and now you’re interested in the money up for grabs. What do you need to know?\n",
            "Be accepted as a Freshman, Sophmore or transfer student Submit a fully completed application. Provide official transcripts reflecting at least a 3.2 grade point average. Entering freshman, please provide official high school transcripts that include ACT/SAT scores and a cumulative grade point average. If you have already been admitted to EMU, your transcripts will be pulled through the Admissions Office. Entering sophomores and transfer students should provide official college transcripts. A written letter of recommendation. (not from a family member, they’ve already helped enough with this particular scholarship) An essay (500 words or less), describing what it means to you to be an EMU Legacy. Resume outlining work, school, extracurricular activities, and volunteer experience.\n",
            "Additionally, all applicants must be sponsored by a parent or grandparent who is an Eastern Michigan University alum. Demonstration of financial need is not a requirement for this scholarship. The Office for Alumni Relations must receive all applications, complete and ready for review, by the posted deadline of May 1, 2015 at 5 p.m.\n",
            "View Alumni Scholarship Scoring Rubric\n",
            "Renewal Criteria\n",
            "So if you’ve earned it, how do you keep it?\n",
            "In order to remain eligible for the Alumni Association Legacy Scholarship, all recipients must:\n",
            "Earn and maintain a 3.2 grade point average Participate in at least 3 service hours at alumni events and other activities sponsored by the Office for Alumni Relations per semester Enroll as a full-time students for every semester (fall and winter) this scholarship is received. That 12 credit hours.\n",
            "After you apply, winners are announced in July 2015. That’s right around the corner, so get to it! The deadline is May 1st. Follow this link to the application. Good luck!\n",
            "Downwell is a roguelike vertically scrolling shooter platform video game developed by Ojiro \"Moppin\" Fumoto and published by Devolver Digital. The game was released for iOS and Microsoft Windows in October 2015, for Android in January 2016, and for PlayStation 4 and PlayStation Vita in May 2016. A port for the Nintendo Switch was released in January 2019 and was ported by British studio Red Phantom Games, which also developed the PlayStation ports.[1]\n",
            "Plot [ edit ]\n",
            "Downwell centers around a \"curious man\" named Welltaro, who is at the local park one night when he decides to explore the depths of the well nearby. Knowing that monsters are waiting for him inside, he straps on his gunboots and starts his trip downwards, killing his enemies to proceed and collect treasure.[2]\n",
            "Gameplay [ edit ]\n",
            "Downwell 's protagonist shooting a stomping-resistant snail enemy with the \"burst\" ammo upgrade. protagonist shooting a stomping-resistant snail enemy with the \"burst\" ammo upgrade.\n",
            "Downwell's art style and underground level design have been compared to Spelunky and Cave Story.[3] The game features three basic controls, the movement to left and right, and the ability to jump while on a solid surface. When mid-air, the player may press and hold the jump button down to fire from their gunboots.[4] Downwell is completely presented in a palette consisting of three colors, which default to black background and white outlines, with red as a highlight for gems, enemies and other important items. The precise colors are interchangeable through unlockable in-game palettes.[5]\n",
            "The well is procedurally generated,[3] and the player is often confronted with a large number of different types of enemies, most of which can either be shot with the gunboots or stomped by jumping on them, others, however, are resistant to being stomped and can thus only be killed with bullets. While normal enemies only feature a red glare alongside the white outlines, these particular enemies are completely highlighted in red, with a more filled sprite body.[4] The gunboots are automatically reloaded when the player hits a surface, regardless of whether or not it is solid.[2] During the descent, players will find \"timevoids\", which, upon entering, stop everything outside them, as well as the music. These timevoids may lead to caves, which provide the player with a large amount of gems, or with an alternate weapon, which influences speed, delay, ammo usage, ammo count and the shoot pattern of the gunboots, as well as providing either health or a max ammo upgrade.[6] In other cases, the timevoids lead to a shop, where he is welcomed by a Jizō, who is taking the place of a shopkeeper, offering the player three different items in exchange for gems.[7]\n",
            "The well is built up from multiple stages, each of them having three levels and featuring different sets of environmental designs and enemies.[8] At the end of each level, the player is given the choice between three player upgrades, which persist throughout the play session and support the player, additionally to ammo upgrades or shop purchases.[9] Additionally, alternating with color palettes unlocks, the player unlocks different movement styles, which influence his position and movement animation while on solid ground, as well as some of the well's procedural attributes, like frequency and type of timevoids.[5]\n",
            "Development [ edit ]\n",
            "Downwell's development began around March 2014. At that time, Fumoto had graduated in opera singing at the Tokyo University of the Arts, but felt like that was not the way he wanted to go in life.[10] So, at the end of February 2014, Fumoto canceled his studies and went on to make multiple \"game-a-week\" projects, after he read about that idea in Rami Ismail's article on Gamasutra.[11] Downwell was Fumoto's thirteenth project, in which he instantly saw a high potential and decided to continue its development.[7] Much of the game was influenced with Fumoto's obsession with Spelunky, and the initial idea for the game came from him wondering what a mobile phone game with similar gameplay would be like.[12] In early development the gameplay was that of a standard platform game, however once Fumoto came up with the Gunshoe mechanic he rebuilt the game around it.[13] During early development, the game was titled Fall or Well, however, during an indie meet-up in Tokyo, Japan, Fumoto figured that he needed an actual name for his game, and therefore came up with Downwell, and stuck with it.[14] While the game was not very popular at the time, Fumoto started posting animated GIF images of Downwell's gameplay in early development stages on his Japanese Twitter account, which came to the attention of Cara Ellison from The Guardian, as the game was largely different from Japan's usual indie gaming market.[15] On the same day of the article being published, indie publisher Devolver Digital hooked up with Fumoto through a comment on one of his GIF images, which showed the player shooting monsters and crates together with the \"drone\" upgrade, which repeats the actions the player takes.[16] This comment and the following dialog then led to Devolver Digital becoming Downwell's publisher.[17] With constant support from Devolver Digital and Fumoto's continuous posting of GIF images led to the opening of the game's official website and the announcement for the late 2015 release on iOS and Microsoft Windows.[18]\n",
            "Due to Devolver Digital's international influence, the game went to different gaming conventions, one of which was the Independent Games Festival 2015, which was held from 2 March 2015 to 6 March 2015, at which Downwell got to the finalists of the \"Student Showcase Award\".[19] Another important convention was Japan's largest indie games festival, BitSummit 2015, which was held from 11 July 2015 to 12 July 2015, and at which Downwell was nominated for the Vermillion Gate Award by the Grand Jury and scored second behind La-Mulana 2.[20] The game was finally announced for a release on 15 October 2015 and was released for iOS and Microsoft Windows, while Moppin continued working on the Android version. On 12 November 2015, Downwell was nominated for \"Best Mobile/Handheld Game\" for The Game Awards 2015, which took place on 3 December 2015.[21] On 11 December 2015, publisher Devolver Digital teased Downwell for future release on PlayStation Vita by sharing a picture showing the game running on a PlayStation Vita device with tate mode enabled.[22] On 26 January 2016, it was announced that Downwell would be released the following day, respectively, 27 January 2016. On 9 February 2016, an official video by Sony Computer Entertainment revealed that the game would be released for PlayStation 4 and PlayStation Vita sometime in 2016.[23]\n",
            "Fumoto announced in January 2018 that had started working for Nintendo, saying \"I'll do my best\".[24] It is believed that his success with Downwell was a large factor in his hiring.[25] Fumoto previously stated that \"[i]t was super fun developing games as indie\" and that he \"[could not] wait to see what it's like to develop games as part of a bigger team.\"[26] In September 2018, during that month's Nintendo Direct show, it was announced that Red Phantom Games was developing a port of Downwell for the Nintendo Switch.[1]\n",
            "Reception [ edit ]\n",
            "Upon release, Downwell received critical acclaim. On Metacritic, the game currently holds a score of 81/100 for Microsoft Windows,[27] 91/100 for iOS,[28] 80/100 for PlayStation 4,[29] and 85/100 for PlayStation Vita.[30] Steven Hansen from Destructoid praised the game's design concept and giving it a perfect 10/10 by saying that \"[t]hese are the kind of things you learn as you delve deeper and deeper into Downwell's four worlds (three levels each) and they are presented intelligently.\"[31] Another positive review was written by Nadia Oxford from Gamezebo, who gave the game a 5 out of 5 stars ranking and praising Downwell's overall play-style and design, stating that \"Downwell is easily one of the best action games to hit mobile this year. It’s intense, it’s unique, and every game you play goes towards unlocking something new.\"[32] Peter Bathge from German PC gaming magazine PC Games, however, gave the game a 70% score, criticizing that \"[the game] feels somehow weird on the PC. This does not only come from the not always perfect, fixed and predefined controls, but the whole game principle is obviously cut to the mobile market needs.\"[33]\n",
            "Awards [ edit ]\n",
            "Exclusive interview with Alex Youden, an ambitious young engineer from the UK who’s launched the NFire 1 modular 3D printer on Kickstarter.\n",
            "Alex tinkering with the NFire 1\n",
            "Though he’s only 19 years old, Alex Youden has already a remarkable track record in product design. Two years ago, he won the Intermediate Engineering and Technology category of the Young Engineer of Britain awards, developing a new way to practise a golfing technique.\n",
            "Since then, he’s had one simple but ambitious vision; every home should have a 3D printer. He invested his prize money to start NFire Labs, and today he launches the NFire 1 on Kickstarter, a modular 3D printer that promises to be affordable and high-quality. ALL3DP spoke to Alex about the features of the NFire 1 and his plans for the campaign.\n",
            "Q: This is an Elevator pitch for the NFire 1. Tell us what it is and why it’s exciting in 30 seconds or less. GO!\n",
            "The NFire 1 is the worlds first truly modular 3D Printer which can change and adapt to your own personal journey through 3D printing. If you purchase a small 3D printer and then want to print larger objects, currently you would have to buy a new 3D printer. Not any more. If you purchase the standard sized NFire 1 3D printer, all you have to do is upgrade the axis and you can instantly double your build volume! Simply swap out the specific components and you are good to go. Printing in dual colours with the NFire 1 is as easy as printing with a single colour with a simple upgrade! We have focused on making upgrade-ability as easy as possible.\n",
            "Q: We can see it’s a Delta printer. What advantages does this offer over a conventionally designed FDM printer?\n",
            "NFire 1 Modular 3D Printer\n",
            "The NFire 1 is a delta configured 3D printer, which means it overcomes some of the common issues that a standard cartesian FDM 3D printer has. It has increased printing speed due to its low mass print head. The footprint is vastly reduced compared to the print volume. We also think the delta design and the NFire 1 looks a lot nicer than the alternative cartesian 3D printers.\n",
            "Q: The asking price is, frankly, astonishing. How on earth can you offer a 3D printer so cheap to backers without sacrificing quality?\n",
            "Right from the start we wanted the NFire 1 to be local. Most of the printer has been sourced from within a 2 mile radius of our location. Why get something shipped half way around the world when it’s on your doorstep? The NFire 1 is therefore completely designed and manufactured here in the UK! A fact which not many other 3D printers can boast. Bulk pricing is another factor in offering the NFire 1 at such a great price. This is one of the reasons we turned to Kickstarter and crowd funding.\n",
            "Q: And when you say “truly modular”, that’s a big promise! How does this work, exactly?\n",
            "We started from scratch. With years of experience with building, modifying 3D printers we knew what works and what doesn’t. We combined all of the best ideas and the result of this was the NFire 1. With starting from the ground up we were able to create the necessary design changes to make all the parts interchangeable. This is something we are incredibly proud of, especially when the sky is the limit with the amount of upgrades and add-ons you can get for it.\n",
            "Q: The theoretical build volume of 3,375cm^3 or 6,750cm^3 is also impressive. Is it easy to implement? What are the possible applications?\n",
            "Sample prints from the NFire 1\n",
            "We all know bigger is better, right? The possible applications of such a large volume is limited by only your imagination! You are able to print some of the most intricate and detailed prints which are huge compared to others. Peoples minds are blown when these 3D printed objects go from simple key-chains to something which can be held with two hands!\n",
            "Q: The E3D extruder has a lot of fans in the 3D printing community. What makes this component so special, and why did you choose it for the NFire 1?\n",
            "The E3D V6 Lite is a metal hotend which prints to an extremely high standard when tested, no other hotend came close to its performance. With the whole ethos of having high quality the e3D V6 Lite was an obvious choice. With it also being produced in the UK, it tied in with our whole local concept.\n",
            "Q: For the Kickstarter campaign, do you have any stretch goals lined up, in the event you meet your initial targets?\n",
            "We could tell you, but then we’d have to kill you! They will be released during the Kickstarter campaign.\n",
            "Q: Tell us a bit more about yourself. What inspires you?\n",
            "Two years ago I built my very first 3D Printer (A Reprap i2 for anyone out there interested). The first time I saw that hotend move about, buzzing and wiring, with this thin strip of molten plastic flowing out the end… It put a spark in my head. From that moment onwards 3D printing has been this simply mind-blowing concept of having a physical object created out of thin air to me! It’s this wonder and this amazement which I want to give to as many people in this world as possible. The NFire 1 provides a platform to do this. With it being made with high quality components and the fact it is so affordable it opens the door of many people! I firmly believe that in the near future every home will have a 3D printer.\n",
            "Q: What advice would you give other folks using crowd-funding platforms for the first time? (Answering as a Start-Up)\n",
            "Take your time, what ever you do don’t rush into it. The last thing you want to happen is to release something which is untidy and unfinished. Not only will it be hard to fund, but if it does, you are going to have one hell of a time sorting all the issues out with backers pressurising you for their pledge! The NFire 1 launch has been pushed back a few weeks just to make sure everything is in place and ready to go.\n",
            "The C4DI has been incredibly helpful in regards to the progress of NFire Labs. I have never experienced anything like it before, and it really is a place which you have to go to to realise its full potential and what it can offer. For growing start-ups and indeed growing established businesses the C4DI in Hull is the place to go to! I don’t think NFire Labs would be half the company it is now with out the support from them.\n",
            "Q: Finally, what’s the single most useful thing you’ve ever 3D printed?\n",
            "It may not be the most useful, but it certainly is the most powerful. I 3D printed a traffic cone on the NFire 1 and it has turned into a firm favourite around the office. People are lost by its seamless appearance and it’s the same expression which I first had when I saw 3D printing. This amazement and you can see people trying to work out how it was made. It’s this feeling which needs to be had by anyone and everyone!​\n",
            "China to build new parliament for Zimbabwe – free of charge\n",
            "China’s Zhang Ming was quoted as saying the new building was a donation to Zimbabwe.\n",
            "JOHANNESBURG - A Chinese envoy has handed over construction plans for a new parliament building to President Robert Mugabe.\n",
            "State radio is reporting that China plans to build the new parliament just outside Harare at no cost to Zimbabwe.\n",
            "The state ZBC is reporting that a special envoy of Chinese president Xi Jinping handed over the construction plans to President Mugabe yesterday.\n",
            "The new parliament building is planned for Mount Hampden, 18km west of the capital.\n",
            "Apparently the current building is too small for Zimbabwe's large houses of assembly.\n",
            "The envoy, Zhang Ming, was quoted as saying the new building was a donation to Zimbabwe.\n",
            "Mugabe considers the Chinese government a key ally; he's called China Zimbabwe's all-weather friend.\n",
            "But earlier this month, Mugabe criticised some Chinese businessmen based in Zimbabwe for failing to bank their money inside the country.\n",
            "I might have to change the name of this blog to Firetail BBQ!\n",
            "It seems I am barbecuing more than I am brewing these days.\n",
            "But never fear, I whipped up a batch of Bra Beer today (more on that in a few weeks when it is ready to drink)\n",
            "I did however venture back into the world of low and slow cooking on the Weber Q.\n",
            "Today's challenge was Bacon Wrapped Baby Back Ribs (try saying that 5 times quickly!)\n",
            "That's right, wrap your pork ribs in bacon! Baby back ribs are pretty lean, the bacon will render and keep everything nice and moist.\n",
            "Ingredients:\n",
            "A couple racks of Baby Back pork ribs (I got mine from Super Butcher)\n",
            "Your favourite BBQ rub.\n",
            "Bacon\n",
            "Method:\n",
            "Rubbed and ready.\n",
            "The night before your cook\n",
            "Remove the membrane from the back of the ribs.\n",
            "Cover in your favourite BBQ rub.\n",
            "Wrap in foil and place in the fridge.\n",
            "On the day of the cook\n",
            "Take your bacon and create a loose weave over the top of the ribs, mine is very loose as I only had a little bit of bacon left (Oh No!!)\n",
            "Ohhhh Yeah.\n",
            "3 hours before serving clean and prep the Weber\n",
            "Set up your smoke box or pouch with whatever wood you like (I used Mesquite and Hickory)\n",
            "Place a double layer of foil on the grill, top that with a trivet\n",
            "Place a foil pan full of water next to the trivet\n",
            "Get the BBQ to 120ºC (250ºF)\n",
            "Arrange you ribs on the trivet and walk away for 1.5 hours.\n",
            "Here they are looking lovely after 1.5 hours, here I baste liberally in BBQ sauce.\n",
            "I was planning on making my own, but ran out of time and used Master Foods BBQ Rib sauce, which is fantastic.\n",
            "And after 3 hours low and slow, here is the final result.\n",
            "Bones were picked clean, mouths were wiped, requests for more were spoken!\n",
            "Another excellent technique for low and slow cooking on the Weber Q.\n",
            "Cheers\n",
            "In his first public comments since the Los Angeles Lakers announced that he had suffered a torn right rotator cuff, Kobe Bryant went peak Kobe Bryant with his Twitter response:\n",
            "This is what happens when I pass too much! #ShoulderShock thank u all for ur thoughts and prayers #team @DrinkBODYARMOR @Lakers #oneluv — Kobe Bryant (@kobebryant) January 23, 2015\n",
            "That is brilliant. And perfect Kobe – a self-aware joke, an endorsement, hashtags and thanks.\n",
            "Scroll to continue with content Ad\n",
            "More telling? No dour notes, no blacked-out Twitter avatar (as he did during his 2013 rehab from an Achilles tear), no complaints about the struggle being real, nothing for us to chomp down on in wondering if Kobe was game to retire after a third consecutive season was ended by a major injury.\n",
            "Bryant was never going to retire, regardless of the $25 million he would be walking away from in the process, but it wouldn’t be atypical for him to spend the next few months fuming and lamenting his recent streak of bad luck. It would have been just fine for Bryant to be frustrated before the cloud cleared and the summer brought new hope, but we’re not getting any of that from this tweet. Perhaps we’re reading too much into a stupid Twitter message, but it’s still good to see.\n",
            "Kobe Bryant, healthy and on the basketball court again, would be great to see. We’re enjoying 2014-15, but this is one benefit of 2015-16 that can’t get here soon enough.\n",
            "- - - - - - -\n",
            "Story continues\n",
            "Kelly Dwyer is an editor for Ball Don't Lie on Yahoo Sports. Have a tip? Email him at KDonhoops@yahoo.com or follow him on Twitter!\n",
            "Abstract Arbuscular mycorrhizal (AM) fungi are mutualistic symbionts living in the roots of 80% of land plant species, and developing extensive, below-ground extraradical hyphae fundamental for the uptake of soil nutrients and their transfer to host plants. Since AM fungi have a wide host range, they are able to colonize and interconnect contiguous plants by means of hyphae extending from one root system to another. Such hyphae may fuse due to the widespread occurrence of anastomoses, whose formation depends on a highly regulated mechanism of self recognition. Here, we examine evidences of self recognition and non-self incompatibility in hyphal networks formed by AM fungi and discuss recent results showing that the root systems of plants belonging to different species, genera and families may be connected by means of anastomosis formation between extraradical mycorrhizal networks, which can create indefinitely large numbers of belowground fungal linkages within plant communities. Key Words: arbuscular mycorrhizal symbiosis, extraradical mycelium, anastomosis, plant interconnectedness, self recognition, non-self incompatibility, mycorrhizal networks\n",
            "Introduction Most terrestrial plant species establish mutualistic symbioses with arbuscular mycorrhizal (AM) fungi, which develop extensive, belowground extraradical hyphae fundamental for the uptake of nutrients from soil and their transfer to the host plant.1,2 Since AM fungi have a wide host range, they are able to colonize and interconnect plants of different species, genera and families, by means of hyphae extending from one root system to another. Such mycorrhizal networks, first visualized and quantified in vivo by means of two-dimensional experimental systems, spread from colonized roots into the surrounding environment at growth rates ranging from 738 to 1067 mm per day, depending on the host plant, and reach hyphal extent of 10–40 mm per mm of root length.3 Moreover, AM extraradical networks may be interconnected by means of the widespread occurrence of anastomoses, whose formation depends on a highly regulated mechanism of self recognition between compatible hyphae. Successful anastomoses occur between hyphae belonging to the same individual and to different individuals of the same isolate, during the presymbiotic growth of AM fungi.4 By contrast, hyphae of individuals belonging to different genera and species, and even to geographic isolates of the same species, are unable to fuse, and show rejection responses, either before or after contact, revealing AM hyphal ability to discriminate against non-self.5 Extraradical mycorrhizal networks maintain the capacity of self-recognition, evidenced by the high frequency of anastomosis between hyphae originating from the same and different root systems colonized by a single AM fungal isolate.6 Here, we discuss recent advances in the study of self recognition and non-self incompatibility in hyphal networks formed by AM fungal germlings during the presymbiotic stage of their life cycle. We review evidences for the characterization of true anastomoses—i.e., complete fusions of hyphal walls, cytoplasmic flow and migration of nuclei through hyphal bridges—and for the detection of incompatibility responses—i.e., protoplasm retraction from hyphal tips and septum formation in approaching hyphae, even before physical contact—as revealed by time-lapse, video-enhanced and epifluorescence microscopy. Finally, we discuss recent results showing that the root systems of plants belonging to different species, genera and families may become linked by means of anastomosis formation between mycorrhizal networks, which can create indefinitely large numbers of fungal linkages connecting together many plants in a community.\n",
            "Evidence for the Existence of Anastomosis in Presymbiotic Mycelial Networks of AM Fungi Although anastomoses have been extensively studied in vegetative hyphae of Ascomycota and Basidiomycota,7,8 they are believed to be lacking or rare in other fungal phyla.9,10 A few works reported sporadic observations of their occurrence in AM fungi, without giving any quantitative data on the frequency of hyphal fusions in the different isolates or on the cytological events involved.11–14 The first extensive study on anastomosis in AM fungi reported data on fusions of hyphae belonging to the same isolate in different species of the genus Glomus, by using a combination of time-lapse and video-enhanced light microscopy, image analysis, and epifluorescence microscopy.4 Protoplasmic continuity, the characteristic feature of successful hyphal fusions, was evidenced by the complete disappearance of hyphal walls and visualized by histochemical localization of formazan salts in hyphal fusions, after SDH (succinate dehydrogenase activity) staining ( ). Time-course experiments showed that hyphal tips were able to fuse with hyphae growing nearby in about 35 min, and that a bidirectional flow of particles (vacuoles, mitochondria, nuclei, and fat droplets) moved at the speed of 1.8 ± 0.06 µm/s through hyphal bridges formed during anastomosis.4,15 Open in a separate window The established protoplasmic flow was further demonstrated by the detection of nuclei in hyphal bridges, evidenced by DAPI (diamidinophenylindole) staining. Nuclear migration occurred between hyphae belonging to the same germling and to different germlings of the same AM fungal isolate, in three different Glomus species, G. caledonium, G. intraradices, G. mosseae.4 The ability of self compatible hyphae to fuse and exchange nuclei is of critical importance for the maintenance of genetic continuity within AM fungi, which are considered clonal organisms.16 Since they produce multinucleate spores, containing 1,000 to 5,000 nuclei each,17 and have been shown to be multigenomic,18,19 nuclear exchange during anastomosis within the same germling and between different germlings of the same isolate could represent a means for the maintenance of isolate genetic diversity, in the absence of sexual recombination.4,20,21 The frequency of anastomosis formation between contacting hyphae originating from the same germling or from different germlings of the same isolate ranged from 34% to 90%, in G. caledonium and G. intraradices, respectively.4 Similar results were found in other studies carried out on geographic isolates of G. mosseae originating from Europe (France and United Kingdom), USA (Arizona and Indiana) and Middle East (Syria), where anastomosis frequency ranged from 60% in the UK isolate IMA1 to 85% in the Arizona isolate AZ225C.5 Such values were obtained on total hyphal contacts ranging from 91 to 242, which are relatively high numbers, given the inability of AM fungi to grow extensively in the absence of a host plant.22–24 In the experimental data, mycelial length of each germling varied with the different isolates, from 34.5 ± 3.5 mm in the French isolate BEG69 to 119.5 ± 14.4 mm in the UK isolate IMA1. It is interesting to note that anastomosis densities detected in AM fungi, unable to grow saprophytically, ranged from 0.62 ± 0.06 to 1.3 ± 0.23 per cm of hyphal length, values comparable with those reported for the saprophytic fungi Rhizoctonia solani and Gibberella fujikuroi.25–27 Interactions between hyphae belonging to the same germling of AM fungal species of the genera Gigaspora and Scutellospora did never lead to anastomosis formation. In fact, no fusions were found over 220 hyphal contacts in G. rosea and over 460 hyphal contacts in S. castanea.4 These data were confirmed by other works, carried out in in vitro monoxenic cultures on mycelium spreading from Ri T-DNA transformed carrot roots, where no anastomoses were detected among main hyphae (runner hyphae) of Scutellospora reticulata, while only 1% of fusions was found in branching absorbing structures.28 Interestingly, the most important mechanism allowing fungal mycelium to become interconnected was represented by wound healing between broken hyphae, previously described by Gerdemann.29 Further studies, aimed at comparing the different anastomosis ability of two phylogenetically distant AM fungal families, Glomeraceae and Gigasporaceae, confirmed their fundamental diversity in mycelial developmental structure.30\n",
            "Evidence for Non-Self Incompatibility in Presymbiotic Mycelial Networks of AMF When hyphae originating from different species or genera of AM fungi come into contact, no anastomoses are formed.4,13 Different intergeneric and interspecific hyphal pairings yielded zero fusions over large numbers of contacts, ranging from 90 in the pairing G. mosseae-G. caledonium to 140 in G. mosseae-G. rosea and 232 in G. caledonium-G. rosea. Interestingly, hyphal interactions lead to different responses, ranging from no interference—i.e., hyphal intermingling—to the formation of hyphal swellings which become empty and septate after the failure of anastomosis formation. These findings, suggesting that AM fungi can recognize self entities and discriminate self from non-self, opened the way to tests of vegetative compatibility, already used for the identification of genetically different isolates of pathogenic, saprophytic and ectomycorrhizal fungi.8,31–35 Such tests, carried out on geographically different isolates of G. mosseae, showed that hyphal interactions between different isolates do never produce anastomosis, suggesting their genetic isolation. Accordingly, hyphae intermingled without any response in 49–68% of contacts, while developed incompatibility reactions in 32–51% of hyphal contacts, in the different pairings. Incompatibility responses were consistent with those detected in hyphae belonging to different genera and species after physical contact, and were characterized by hyphal swellings, vacuolization, localized wall thickenings, protoplasm withdrawal, retraction septa formation and hyphal lysis ( ),5 and comparable to postfusion incompatibility events reported in other fungi.7,8,36–39 The strong genetic barriers to hyphal fusions exhibited by G. mosseae isolates of different geographic origins could have the function of hindering heterokaryon formation between genetically different mycelia, thus permitting the maintenance of the fittest gene combinations. Moreover, such barriers may prevent the exchange of cytoplasm and the spread of harmful genetic elements.8,40 The major evidence for the existence of a highly regulated system of self recognition and non-self discrimination in AM fungi was represented by the detection of precontact tropism and the formation of hyphal swellings and consecutive retraction septa prior to any physical contact between neighboring hyphae.5 The occurrence of hyphal tropism, previously studied also in other fungal species, Phanerochaete velutina and Stereum spp.,7,36 suggests that specific recognition signals, released by interacting hyphae, are involved in interhyphal attraction and in the regulation of hyphal fusion.32,41 Nevertheless, the nature of the specific compounds acting as signals for self recognition and non-self discrimination in AM fungi remains to be unravelled.\n",
            "Visualization of Intact Mycelial Networks Spreading from Roots Colonized by AMF The most important AM fungal structure for plant nutrition is represented by the extraradical mycelium spreading from mycorrhizal roots into the surrounding soil, which is able to uptake mineral nutrients—N, P, S, Ca, K, Fe, Cu, Zn—and to transfer them to root cells.1,42–44 Mycorrhizal mycelium has been investigated in different experimental studies, based on either destructive extraction from soil or root observation chambers or in vitro systems, which yielded only qualitative data on its structure and growth.45–48 The first visualization of intact AM mycelium extending from mycorrhizal roots into the extraradical environment was obtained by means of a bidimensional model system which utilized two cellulose esters membranes “sandwiched” around the roots of individuals plantlets ( ). After only seven days' growth, a fine network of extramatrical hyphae growing on the membranes was visible to the naked eye, and its length extended from 5169 to 7471 mm (hyphal length), in Thymus vulgaris and Allium porrum, respectively ( ).3 In order to understand the fundamental role played by extraradical mycelium in nutrient uptake and translocation, it is interesting to calculate hyphal length per total root length, which reaches 40.2 mm mm−1 in A. porrum, and the mean growth rate, which ranges from 738 to 1067 mm per day, depending on the host plant. Such data are comparable with the higher values of hyphal densities previously detected by using destructive extraction from soil, which were much variable, ranging from 1.6 to 1420 mm of hyphal length per mm of root.49–52 Open in a separate window Open in a separate window The experimental system deviced to visualize the mycorrhizal mycelium also evidenced that the mechanism allowing the formation of the network was self recognition and hyphal anastomosis. Since AM fungal hyphae showed many branches (8.6–9.7 cm−1) the number of anastomoses per cm of hypha was very high (4.6–5.1), as well as their frequency, 75–78% of hyphal contact ( ). The frequency of anastomosis was higher in extraradical mycelium (symbiotic) than in presymbiotic mycelium of AM fungi or than in self-anastomosing isolates of Rhizoctonia solani.4,5,25 Table 1 Plant Species/Fungal Species Hyphal Density (mm mm−2) No. of Anastomoses Per Hyphal Length (cm) Anastomosis Frequency (%) Ref. Allium porrum/Glomus mosseae 2.7 4.6 75.0 3 Allium porrum/Glomus mosseae 3.5 3.8 59.3 6 Daucus carota/Gigaspora margarita* - 0.0075 9.8 30 Daucus carota/Gigaspora rosea* - 0.012 4.2 30 Daucus carota/Glomus hoi* - 0.057 100 30 Daucus carota/Glomus intraradices* - 0.076 100 30 Daucus carota/Glomus mosseae 3.9 2.5 45.5 6 Daucus carota/Glomus proliferum* - 0.066 100 30 Daucus carota/Scutellospora reticulata* - 0.0079 5.2 30 Gossypium hirsutum/Glomus mosseae 6.8 6.2 53.1 6 Lactuca sativa/Glomus mosseae 2.9 3.0 63.8 6 Petroselinum crispum/Glomus caledonium 3.8 1.5 18.6 § Petroselinum crispum/Glomus intraradices 2.3 5.5 56.9 § Petroselinum crispum/Glomus mosseae 3.5 4.7 62.3 § Prunus cerasifera/Glomus mosseae 2.4 5.1 64.0 3 Solanum melongena/Glomus mosseae 4.1 2.1 47.0 6 Thymus vulgaris/Glomus mosseae 2.1 5.1 78.0 3 Open in a separate window It is important to stress that the viability of the mycorrhizal network was 100% and that all the anastomoses showed protoplasmic continuity and nuclear occurrence in hyphal bridges, confirming the occurrence of nuclear exchange also during fusions between extraradical (symbiotic) hyphae.\n",
            "Visualization of Belowground Interconnections between Plants of Different Species, Genera and Families AM fungi have been reported to be active in mediating nutrient transfer among plants,53–58 mainly through the extensive mycelial networks, which, due to the lack of host specificity, may link the roots of contiguous plant species.57,59,60 Recent studies showed a novel mechanism by which plants may become interconnected, that is hyphal fusions between extraradical hyphae originating from different individual plant root systems of different species, genera and families.6 The bi-dimensional experimental system utilized allowed the visualization and quantification of fusions between mycorrhizal networks spreading from Allium porrum (leek) root systems—after inoculation with the AM symbiont Glomus mosseae—and those originating from Daucus carota (carrot), Gossypium hirsutum (cotton), Lactuca sativa (lettuce), Solanum melongena (eggplant). The use of plants belonging to different species allowed the detection of a host plant effect on the development of extraradical mycelium, since hyphal density in cotton was 6.8 mm mm−2, a value statistically different from those of all the other plant species, which ranged from 2.9 to 4.1 mm mm−2 in lettuce and eggplant, respectively ( ). Cotton was also the species which showed the highest interconnectedeness in the mycorrhizal network: the number of anastomoses per mm of hyphal length was 0.62 compared to values ranging from 0.21 to 0.38 of the other species. The frequency of anastomoses between mycorrhizal networks originating from the different plant species was very high, ranging from 44% in the pairing leek-eggplant to 49% in the pairing leek-cotton, even though lower than that between networks spreading from the same species, leek (62%). The occurrence of true anastomoses was verified by means of SDH and DAPI stainings: formazan salt depositions and nuclei were detected in the middle of hyphal bridges connecting different mycorrhizal networks, whereas no hyphal incompatibility reactions were found in interactions between hyphae connecting different mycorrhizal networks. The high rate of anastomosis formation between extraradical hyphae spreading from the root systems of different plants suggests that plant interconnectedness may be greater than previously thought. Accordingly, due to the wide host range of AM fungi, mycorrhizal mycelium could give rise to an indefinitely large network of hyphae interconnecting contiguous plants, representing a major factor in the distribution of resources in plant communities.56,57,61,62 The bi-dimensional experimental system deviced for visualizing the structure of the mycorrhizal network could be further implemented, to detect and quantify nutrient and carbon transfer in the “soil food web”.63–66\n",
            "Acknowledgements This work was supported by funds from the University of Pisa (Italy) and by C.N.R. (National Research Council, Italy).\n",
            "Footnotes Previously published online as a Plant Signaling & Behavior E-publication: http://www.landesbioscience.com/journals/psb/abstract.php?id=2227\n",
            "Attention! This news was published on the old version of the website. There may be some problems with news display in specific browser versions.\n",
            "Operation \"Dragoon\"\n",
            "“We went down there with 22 men and there were only five of us left at the end. We used to drink ourselves silly every day, out of sheer joy that we were still alive.”\n",
            "Unteroffizier Walter Lang (1./JGr. 200), 23 August 1944\n",
            "War Thunder presents special disсounts:\n",
            "From September 14th, 8am GMT (1am PDT) till September 16th, 8am GMT (1am PDT)\n",
            "Allies: 20% discount on purchasing:\n",
            "P-47D-25, P-47D-28, P-51D-5 Spitifre Mk.Vb, Beaufighter Mk.VIc Germany: 20% discount on purchase and 25% discount on repair costs for: Ju-88A-4, Me.410-A1, Bf.109G-6, Fw.190F-8 Germany gets 30% discount on the XP conversion.\n",
            "The 2nd Invasion of Europe\n",
            "Most remember the D-Day Landings at Normandy. However, few can recall that the original plans called for 2 smain landing in France .Originally planned as Operation Anvil and Overlord. Dragoon was to coincide with the landings at Normandy codenamed “Overlord”. The Attack however was delayed due to slowed than expected progress in the Italian theater of Operations as well as lack of usable landing craft in the sector.\n",
            "On August 15th, 1944 an invasion force of 887 ships, 2,000 aircraft, 173,000 men, and 18,000 vehicles. Set out from ports in Corsica on a path to landing ports in the French Riviera. The Allies chose the Var Coast east of Toulon. Where the German and Vichy French defenders were reinforced, but surprised at the assault nonetheless.\n",
            "Landings Took place with aim at\n",
            "Alpha (Cavalaire-sur-Mer)\n",
            "Delta (Saint-Tropez)\n",
            "Camel (Saint-Raphaël)\n",
            "Prior to the Invasion of Southern France. The German Air Forces in days leading up to the attack had received some much needed re-enforcements to defending units in southern airfields. Germany's Fleigerdivision 2 with it's HQ in Montfin as well as General Blaskowitz's Army Group G were stationed as Static units in the surrounding countryside. Only the 11th Panzer division remained at that time as a full fledged fighting force due to supply issues in the entire Army group prior to the Invasion.\n",
            "The landings on August 15th were aided by ship and air bombardment. As fighting at Camel beach proved to be the most difficult. Especially Camel-red (near Saint-Raphael) where air support from British Hellcats were paramount but nonetheless the landings had to be shifted to other parts of the beach. Repeated airstrikes from German torpedo and bomber planes. Backed by fighter escorts and Heavy fighters rained down upon the ships day and night. This drastic contrast unlike the Normandy invasion. Paints a quite harrowing picture of the ground scene. As the Army fell and retreated, the air war became more and more a desperate race for the German pilots to buy them more time. The German Army group in fact it was noted by General Eisenhower after the fact that had they had know the true status on the ground they would had pushed sooner, faster, with more people than in Normandy.\n",
            "Subsequently though the landings in the south allowed the allies use of the Mediterranean ports in the Riviera. Also opening a spear head that would eventually liberate most of southern France almost effectively dissolving the Vichy government in one fail swoop.\n",
            "23 Jun 1944 Churchill, with misgivings, gave in to pressure from the Americans and sanctioned operation Anvil (the proposed US-French invasion of the south of France). 2 Jul 1944 General Sir Henry Maitland Wilson (Supreme Commander, Mediterranean) received orders from London, England, United Kingdom to organize the invasion of Provence, France. The name of this operation was changed by British Prime Minister Churchill to \"Dragoon\". 20 Jul 1944 French troops began withdrawing from Italy in preparation for the invasion of southern France. 21 Jul 1944 The French Expeditionary Corps and the US VI Corps began to be withdrawn from the front lines in Italy to prepare for the invasion of Southern France. 15 Aug 1944 Operation Dragoon, the invasion of southern France, began. On the same day, Saint-Tropez, Var, France was captured by the 15th Infantry Regiment of 3rd US Infantry Division. 21 Aug 1944 French First Army enveloped Toulon, France. 23 Aug 1944 US troops from the north and French troops from the south met near Bordeaux, France. 24 Aug 1944 Allied forces liberated Cannes in southern France. 26 Aug 1944 French troops captured Tarascon and Avignon, France. 27 Aug 1944 The last German troops in Toulon, France surrendered. 28 Aug 1944 The German garrison in Marseilles, France surrendered to French forces. 29 Aug 1944 French troops captured Montélimar in southern France. 3 Sep 1944 US and French troops reached Lyon, France.\n",
            "All the pictures are taken from the author's personal archive.\n",
            "Discuss the news on the official War Thunder forums\n",
            "About the Author:\n",
            "Clay «FryingTiger» Remy, a Historical Consultant, Alpha tester, and Global Moderator for War Thunder. With over 10 years Military Service in the US Marine Corps active and reserves. Technical/Historical researcher and owner of a company that provides training and licensing of equipment for use in films, documentaries, museums, and other displays. I have been involved in a variety of digital projects stemming from video games stemming from flight Sims like War birds, WWIIOL, to Ship and FPS products on a variety of platforms. I also provide Historic data and quality assurance for films and museums for military related projects.\n",
            "We have a lot of poorly-designed tax policy in this country, and transportation policy is no exception. Under IRS Code Section 132(f), the federal government outlines its \"qualified transportation fringe\" benefit, which, as of 2014, allows for tax deductions of up to $245/month for parking, $130/month for transit, and $20/month for bicycling. For transit, the commuter benefit can cut the costs of a monthly pass by 15 percent or more, which amounts to hundreds of dollars per year for most regular transit users. That kind of cost reduction could have a significant impact on ridership, but, unfortunately, whether you get that discount or not is up to your boss, not you, and most small businesses (and even many large ones) simply don't offer it.\n",
            "Broadening the reach of this program would be hugely beneficial to cities and their residents, saving money for the people who need it most and boosting transit use in the process. I started thinking about how we could accomplish this a while back, and wrote about some of those ideas on my blog over a year ago. If you want a little more detail about how the commuter benefit works in practice, I recommend you check out that previous post.\n",
            "Recently though, as I continued to think about how we might get this benefit into the hands of more people, particularly employees of smaller businesses (which are much less likely to offer these types of fringe benefits — see here, page 9) I started to wonder: if we really want more people to use transit, and we think it's a worthwhile goal to subsidize people's commutes, why go through all the trouble of tax deductions and employer control? Why not make things simple and just subsidize transit passes directly?\n",
            "Photo from Next Stop STL.\n",
            "Currently, if employees want to have their transit passes (or parking) subsidized they're basically reliant on their bosses to care enough about the program to offer it. The benefit does reduce the tax burden on the business as well as the worker, but the bulk of the benefit goes to employees and it's possible that the costs of time and paperwork exceed the benefits for many small employers. It's an unnecessarily complex process that almost seems designed to reduce commuter benefit claims. If the government wants to subsidize commutes it should be prepared to subsidize everyone's commutes, not just those fortunate enough to work for large employers, like Microsoft or GE, that have the resources to offer them.\n",
            "Instead of going to all this trouble we could move to a system where transit pass prices were directly subsidized by the federal government: organizations like the MTA and King County Metro could set their fare prices, then automatically reduce them by a set amount — say 20 percent. Then, at the end of every year, they could determine the cost of the subsidy and submit a bill to the government. In other words, almost exactly what we already do, without all the pointless intermediate steps.\n",
            "Direct subsidies would have a number of benefits over the current system:\n",
            "Everyone would be served by the subsidy regardless of employer, income, or the extent of their knowledge of tax policy. VTPI executive director (and Planetizen blogger) Todd Litman has compiled research indicating that a fare increase of 10% can decrease ridership by 2% to 10%. Insofar as more people would be eligible for discounted passes, transit ridership would be likely to increase. Employers would save time, no longer needing to bother with burdensome and unnecessary tax documentation. Because all transit users would benefit, federal government spending on transit would increase while local jurisdictions saved money (which is already the outcome of commuter tax benefits, just on a smaller scale). Although total federal transit subsidies would increase, per-person costs would decrease because of the reduced time spent on individual tax filings and simplified/consolidated service delivery. More people would buy monthly transit passes, which speeds transit loading and encourages greater reliance on transit: the money's already spent, so the more you use it the more value you get from it. Perhaps most importantly, this would be a far more equitable way to allocate resources. Under the current tax structure, higher-paid workers receive a greater discount because their tax deduction falls under a higher tax bracket. An employee earning $35,000 a year receives a 15% discount, while someone earning ten times that amount receives a 33% discount. It's wrong, and it should change.\n",
            "This would also put more power in the hands of employees. If we wanted to retain the payroll tax deduction for employers (and I'm not really sure why we would, since they wouldn't play a role any longer), it would be up to the employer to reach out to the employee to confirm their participation in the program, not the other way around. If we eliminated the payroll tax deduction for employers, as we probably should under this new system, those savings could help pay for its increased employee-subsidy costs.\n",
            "To those who would argue that the commuter benefit is for getting to and from work, not for all transit trips, my first question would be: Why? Is travel to work the only one of any value, economic or otherwise? I would definitely argue against that notion. Also, since the tax deduction is usually tied to the purchase of a monthly transit pass, not one-off fares, we're already failing to differentiate between work and non-work trips. When I worked for the University of Washington I received a heavily discounted transit pass, and I used it for visiting friends and going to the store as often as I used it for my commute, and I think that's okay. It's especially important that we get people out of their cars during peak commute hours, but it's valuable at all times of day, for all purposes.\n",
            "What do readers think? Any benefits I missed, or unforeseen consequences you think I may have overlooked?\n",
            "“I think we’re in impeachment territory.”\n",
            "Those were not the words of a left-wing political blogger with high hopes, nor a hyperactive Twitter maven spouting conspiracy theories. They came from David Gergen, the typically moderate, mild-mannered CNN analyst and former aide to four presidents, both Republican and Democrat.\n",
            "Gergen appeared on CNN Tuesday night to discuss the controversies that have overwhelmed the Trump administration in the past week.\n",
            "At the top of the list were reports that President Trump had asked then-FBI Director James B. Comey to drop an investigation into Michael Flynn, the former national security adviser. The revelation has fueled claims that Trump might have obstructed justice, which in the past has been treated as an impeachable offense.\n",
            "If true, it could mean devastating consequences for the president, said Gergen, who served under Presidents Richard Nixon and Bill Clinton, both of whom faced the impeachment process.\n",
            "“I was in the Nixon administration, as you know, and I thought after watching the Clinton impeachment I’d never see another one,” Gergen told host Anderson Cooper. “But I think we’re in impeachment territory now for the first time.”\n",
            "Cooper asked: “Really?”\n",
            "“I think that obstruction of justice was the number one charge against Nixon that brought him down,” Gergen responded. “Obstruction of justice was the number one charge against Bill Clinton, which led to his indictment in the House.”\n",
            "In controversy after controversy, Republican lawmakers have defended President Trump's actions. But with his disclosure of highly classified information to Russian diplomats, they've floundered to explain the decision. (Jenny Starrs/The Washington Post)\n",
            "The New York Times first, then The Washington Post and other outlets have reported that Trump had asked Comey in a February meeting not to pursue the Flynn probe and instead go after journalists in leak cases. Comey shared his notes from the conversation with aides, who in turn described the notes to reporters. Legal analysts told The Washington Post Tuesday that a criminal obstruction-of-justice case is possible but would likely require more evidence.\n",
            "The news came within a week of Trump’s dismissal of Comey and a day after reports that Trump had shared classified information with Russia.\n",
            "The White House denied the version of the conversation described by Comey’s aides, telling The Post “the president has never asked Mr. Comey or anyone else to end an investigation.” Trump has separately defended his firing of Comey and his sharing of information with Russian officials.\n",
            "On CNN, Gergen said that “from a lay point of view” it appeared Trump was trying to impede the FBI’s investigation.\n",
            "“He was using his power to do that, and when James Comey didn’t go along with him, when he wasn’t his boy, he fired him, which I think is also relevant to the question of what he was trying to do,” Gergen said. “So from my point of view this is of enormous consequence for his presidency.”\n",
            "“I think if you look at the three bombshells we’ve had,” Gergen added, “what we see is a presidency that’s starting to come apart.”\n",
            "Gergen wasn’t the only moderate voice to suggest Tuesday that a plausible case for Trump’s removal was emerging.\n",
            "Ross Douthat, a conservative columnist for the New York Times and longtime critic of Trump, wrote that while Trump may not be guilty of the “high crimes and misdemeanors” necessary for impeachment, he had shown himself incapable of governing the country. A more appropriate solution, Douthat argued, was to remove Trump under the 25th Amendment, which allows the president’s cabinet to deem him “unable to discharge the powers and duties of his office.”\n",
            "“It is not squishy New York Times conservatives who regard the president as a child, an intellectual void, a hopeless case, a threat to national security,” Douthat wrote. “It is people who are self-selected loyalists, who supported him in the campaign, who daily go to work for him.”\n",
            "The 25th Amendment, ratified in 1967 in the wake of the assassination of John F. Kennedy, sets out both a plan of succession for the presidency and a complicated and daunting means of removing a president deemed incapacitated.\n",
            "The relevant Section 4 reads as follows:\n",
            "Whenever the Vice President and a majority of either the principal officers of the executive departments or of such other body as Congress may by law provide, transmit to the President pro tempore of the Senate and the Speaker of the House of Representatives their written declaration that the President is unable to discharge the powers and duties of his office, the Vice President shall immediately assume the powers and duties of the office as Acting President. Thereafter, when the President transmits to the President pro tempore of the Senate and the Speaker of the House of Representatives his written declaration that no inability exists, he shall resume the powers and duties of his office unless the Vice President and a majority of either the principal officers of the executive department or of such other body as Congress may by law provide, transmit within four days to the President pro tempore of the Senate and the Speaker of the House of Representatives their written declaration that the President is unable to discharge the powers and duties of his office. Thereupon Congress shall decide the issue, assembling within forty-eight hours for that purpose if not in session. If the Congress, within twenty-one days after receipt of the latter written declaration, or, if Congress is not in session, within twenty-one days after Congress is required to assemble, determines by two-thirds vote of both Houses that the President is unable to discharge the powers and duties of his office, the Vice President shall continue to discharge the same as Acting President; otherwise, the President shall resume the powers and duties of his office.\n",
            "1 of 58 Full Screen Autoplay Close Skip Ad × A look at President Trump’s first six months in office View Photos Scenes from the Republican’s beginning months in the White House. Caption Scenes from the Republican’s beginning months in the White House. Jan. 25, 2017 Trump signs an executive order for border security and immigration enforcement improvements at the Department of Homeland Security in Washington. Chip Somodevilla/Pool photo via Bloomberg News Buy Photo Wait 1 second to continue.\n",
            "More from Morning Mix:\n",
            "Yale dean once championed cultural sensitivity. Then she called people ‘white trash’ on Yelp.\n",
            "Sean Spicer cutouts are popping up in bushes worldwide, thanks to this Canadian professor\n",
            "With Comey firing, even more restrained Trump critics let loose\n",
            "There is another house price bubble under way in the Dublin area. Notwithstanding the efforts by the Central Bank to keep mortgage credit under control, some extraordinary prices have been quoted recently for the small parcels of land that become available.\n",
            "There is another house price bubble under way in the Dublin area. Notwithstanding the efforts by the Central Bank to keep mortgage credit under control, some extraordinary prices have been quoted recently for the small parcels of land that become available.\n",
            "A site with planning permission is for sale on Vernon Avenue in Clontarf, on the north side of the capital. This is a nice area, close to the sea and only about four kilometres from the city centre.\n",
            "The site is small, just under an acre, which equates to half a soccer pitch or about seven tennis courts. The permission is for 17 three-bedroom houses, so it will be a bit of a pinch. The guide price quoted for the site is €3.85m, which works out at an average €226,471 in site cost per unit. Given construction costs on smaller sites in the city and the various levies and taxes, if building goes ahead, these homes will need to sell for prices in the €500,000 bracket.\n",
            "There is a three-bedroom semi-detached house available on the property websites currently, also on Vernon Avenue, which runs to 1,313 square feet. It is priced at €640,000, which should help explain the €4m per acre expected for the 17-house site.\n",
            "The Phoenix Park contains 1,752 acres and is worth over €7bn on the per-acre Clontarf valuation. The Government alone must own enough Dublin land, on this basis, to wipe out the national debt.\n",
            "This is not a proposal to carpet the Phoenix Park in three-bedroom houses, it is an illustration of the dysfunctional market that has already re-emerged in the city just a few years after the biggest housing bust in the country's history. There are working farms inside the M50, which is about 8km from the city centre through most of its length.\n",
            "That is to say there is vacant land, not zoned residential, in the inner suburbs of Dublin, while demand pressure is sufficient to push the price for ready-to-go sites to €4m per acre. Huge rolling prairies of land can be found north and west of the ring road, further out, but some of it no more than 10km or 12km from the centre, which the planners have designated for agriculture (or 'amenity', whatever that means).\n",
            "Meanwhile, would-be first-time buyers working in Dublin are eyeing properties in the midlands, 70km and 80km from the city, which are available at prices they can afford, but from which they must contemplate a daily commute of up to two hours through the vacant countryside.\n",
            "On Joe Duffy's radio programme on Friday one young woman explained that she owned a five-bedroom house in Mullingar which she loved, but that the daily commute was a grind. Mullingar is 80km from central Dublin. She reckoned she could sell the Co Westmeath property for about €200,000, which is probably just over the cost of construction. But it is not enough to buy a site in Clontarf.\n",
            "Commuters from the midlands who travel into central Dublin along the M3 motorway pass to the left a district called Pelletstown,, well inside the M50 and convenient to the Phoenix Park and many other amenities. There has been some residential construction around the area in recent years, mainly apartments.\n",
            "The suburban railway line to Maynooth, upgraded at considerable expense in recent times, runs through the area which, like Clontarf, is just 4km from the centre. There are large tracts of undeveloped land around Pelletstown, inside the M50, including some currently devoted to agriculture, a curious obsession for urban planners. Prices in Pelletstown and adjoining districts are below Clontarf levels - it's a nice area too, but not close to the sea. Prices in the €300,000 bracket are nonetheless beyond reach of Dubliners on average incomes, or of the owners of fine five-bedroom properties in distant Mullingar.\n",
            "There is a scheme for 318 new homes in Pelletstown headed for appeal to An Bord Pleanala. The area has been zoned residential since 1999, but existing residents object that the new development will add pressure to inadequate school provision in the area. They also complain that a promised railway station (on an existing line already built and paid for) has not been provided.\n",
            "The merits of their objections are for An Bord Pleanala to adjudicate, but it is lamentable that this squabble continues. The residents may well have a case: public authorities, including the Department of Education and transport providers, can be a part of the problem too.\n",
            "There are local disputes about numerous other residential sites in the Dublin area which actually enjoy residential zoning but where necessary services have not been provided. It is vacuous to point to the availability-on-paper of zoned land if it is impossible as a practical matter to proceed with development. But the deeper problem is the absolute prohibition by the planners of residential development in the outer suburbs of the city. There is enough land between Mullingar and Dublin to build a mega-city. Indeed there is enough land along the outer side of the M50 to meet current and future demands, some of it already close to decent transport links.\n",
            "Two researchers at the London School of Economics, Christian Hilber and Wouter Vermeulen, have just released a paper in The Economic Journal looking at the development of house prices in the United Kingdom in recent decades and the role played by planning restrictions.\n",
            "They conclude that planning restrictions have had a critical cumulative impact and that the areas of the UK with the most unaffordable house prices are the ones with the greatest regulatory interventions. The UK system of planning and zoning, and the system of housing finance, have been copied in this country and the resulting dysfunctionality of the housing market is not a coincidence. Housing costs are not a huge issue in Britain outside London and the southeast. Sounds familiar - there is no great issue in Ireland either, outside Dublin and a few other urban centres.\n",
            "There are two bad answers to the Dublin affordability crisis. The first is to relax the mortgage lending rules, placing both banks and borrowers in the path of temptation. It is not prudent for banks to lend people very large multiples of income for house purchase, nor is it prudent to head back towards 100pc loan-to-value ratios. Remember how well that worked out?\n",
            "The market valuations in Dublin are anyway distorted upwards by the artificial restrictions on supply. Housing is not a reliable form of collateral for long-term secured lending if there is a risk of sensible valuations, and hence negative equity, once a sane housing policy is instituted. Even if banks were permitted by the granite-faced supervisors to loosen mortgage finance, would it be wise for them to do so, or for the borrowers to avail of their imprudence?\n",
            "The second bad answer would be to permit a renewed bout of pay inflation driven by the inability of Dublin-area employees to aspire to home ownership at current prices.\n",
            "The last credit bubble saw government concessions to escalating pay demands, notably in the public service. Why not offer cheaper homes, instead of higher pay and higher debts?\n",
            "Sunday Independent\n",
            "James Martin/CNET\n",
            "Looks like Amazon got its happy holidays after all.\n",
            "The company on Thursday reported a profit of $214 million, or 45 cents a share, on revenue of $29.33 billion, exceeding some analysts' estimates of 17 cents a share on $29.68 billion in revenue.\n",
            "The results show that Amazon learned from the mistakes it made last year when its delivery partners FedEx and UPS ended up with a backlog of packages that caused some shipments to arrive at customers' doorsteps after Christmas. This year, the world's largest online retailer revised its delivery plans and even extended its Christmas delivery cutoff date, getting packages to customers on time, said Scot Wingo, CEO of ChannelAdvisor, which provides analytics and other services to online retailers.\n",
            "For the current quarter, the company said it expects sales to rise to between $20.9 billion and $22.9 billion, growing 6 percent to 16 percent compared with the first quarter last year. It also forecast that operating income would range from a loss of $450 million to a profit of $50 million. Amazon last year reported $146 million in operating income.\n",
            "Last quarter's profit comes after two consecutive quarters of losses, and investors are buying into the results. Shares surged more than 13 percent in after-hours trading. The stock closed at $311.78, up $7.87, or more than 2 percent.\n",
            "\"They did a really good job of extending the holidays and not repeating the debacle of last year,\" Wingo said.\n",
            "Like other retailers, Amazon sees a spike in activity during the season, with big shopping days like Black Friday and, in more recent years, Cyber Monday, driving orders.\n",
            "The holidays also tend to bump up the company's Prime membership, as people take advantage of the program's free, two-day shipping for last-minute shopping. The company in March hiked the price of Prime membership to $99, up from $79.\n",
            "\"When we raised the price of Prime membership last year, we were confident that customers would continue to find it the best bargain in the history of shopping,\" Amazon CEO Jeff Bezos said in a statement. \"The data is in and customers agree -- on a base of tens of millions, worldwide paid membership grew 53 percent last year -- 50 percent in the U.S. and even a bit faster outside the U.S.\n",
            "Amazon did not release specific numbers for Prime. Wingo estimated that Prime membership has grown to roughly 40 million.\n",
            "Prime is a key component of Amazon's strategy. A subscription service that gives members access to e-books, videos and music in addition to two-day shipping, it keeps customer -- and their dollars -- inside Amazon's vast ecosystem. One study of 2014's fourth quarter found that Prime customers spend hundreds more on Amazon than non-Prime shoppers.\n",
            "\"When a customer becomes a Prime member, they do step up their purchases very considerably,\" Chief Financial Officer Tom Szkutak said during a call with journalists following the report. He would not say how much more Prime members spend or whether they're watching more streaming videos than other consumers. Amazon has heavily invested in its Prime Instant Video service, which Szkutak said is driving more consumers to Prime.\n",
            "The real star of Amazon's earnings continues to be its cloud-computing arm, Amazon Web Services. The company said that 1 million people use the service now and that it will start breaking out the service's financials this year, an indication of AWS' growth. Amazon has expanded the service considerably since it launched in 2006. It is a major provider of data storage and computing for several consumer brands, including video-streaming service Netflix, and even government entities like the CIA and NASA.\n",
            "In the past, AWS sales were included in a miscellaneous category for North America sales called \"Other,\" which saw $1.67 billion in sales in the last quarter.\n",
            "Updated, 2:58 p.m. PT: Added more details from earnings call.\n",
            "Updated, 3:17 p.m. PT: Updated stock price.\n",
            "Democratic presidential candidate Bernie Sanders Bernard (Bernie) SandersPush to end U.S. support for Saudi war hits Senate setback Sanders: 'I fully expect' fair treatment by DNC in 2020 after 'not quite even handed' 2016 primary Sanders: 'Damn right' I'll make the large corporations pay 'fair share of taxes' MORE has raised more than $6 million in the 24 hours since polls closed Tuesday night in New Hampshire, the Sanders campaign confirmed late Wednesday.\n",
            "The campaign beat its $6 million fundraising goal and is now urging supporters to donate another $1 million by midnight.\n",
            "ADVERTISEMENT\n",
            "“I’ll be honest — right now, the math looks difficult to raise another $1 million today,” campaign manager Jeff Weaver said in an email sent to supporters, according to The Washington Post. “But I think it’s important for us to try, and not just because there are 14 primaries and caucuses over the next three months.”\n",
            "Sanders crushed rival Hillary Clinton Hillary Diane Rodham ClintonSanders: 'I fully expect' fair treatment by DNC in 2020 after 'not quite even handed' 2016 primary Sanders: 'Damn right' I'll make the large corporations pay 'fair share of taxes' Former Sanders campaign spokesman: Clinton staff are 'biggest a--holes in American politics' MORE in the Granite State. Afterward, he used his victory speech to urge supporters to donate to his campaign.\n",
            "About 18 hours after the polls closed, Sanders had raised more than $5.2 million, with an average campaign contribution of just $34.\n",
            "The Vermont senator has been outraising his rival Hillary Clinton of late. In January, he raised $20 million, besting Clinton's $15 million.\n",
            "Updated at 10:08 p.m.\n",
            "Matthew Stafford tied an NFL record with his seventh fourth-quarter comeback win of the season after the Lions' 16-13 Thanksgiving victory over the Vikings. The overlooked reason, however, for the success of the Cardiac Cats in the final frame has been the reliability of kicker Matt Prater.\n",
            "Detroit's kicker was 2 for 2 on game-tying or game-winning field-goal attempts in the fourth quarter or overtime Thursday, extending his career perfect mark in that metric to 25 of 25.\n",
            "\"I don't think you can ever think anything is automatic, but he's as good as they come in these situations,\" Lions coach Jim Caldwell said after the game.\n",
            "Prater made a 48-yard boot to tie the contest at 13 with 1:45 left. Vikings quarterback Sam Bradford gifted Prater another opportunity to prove his clutch kicking, throwing an interception to Darius Slay to put the Lions in range for another miraculous come-from-behind win.\n",
            "After a Stafford kneel, Prater drilled a kick from 40 yards out through the uprights to give the Lions sole possession of first place in the NFC North.\n",
            "\"I still have a lot of confidence where I don't doubt myself,\" Prater said. \"Basically, I have the same mind set as I would with an extra point in the first quarter. I don't go out and ever expect that I won't make the kick.\"\n",
            "In a season highlighted by the constant struggles of kickers, the Lions have to be relieved that they don't have to fret with their special teams unit with the game on the line.\n",
            "\"Can't say enough about Prater,\" Stafford said. \"The guy, he's as clutch as they get. Really that unit has done a heck of a job for us all season long. I know they'll continue to do a great job for us.\"\n",
            "Researchers have uncovered a major security vulnerability they are calling Heartbleed. It is said to have huge implications for the entire Internet.\n",
            "Here is an overview of the bug and how it works:\n",
            "Yan Zhu of the Electronic Frontier Foundation explained how the bug operates in more detail in her article Why the Web Needs Perfect Forward Secrecy More Than Ever:\n",
            "EFF has long advocated for websites to support HTTPS instead of plain HTTP to encrypt and authenticate data transmitted on the Internet. However, we learned yesterday of a catastrophic bug, nicknamed “Heartbleed,” that has critically threatened the security of some HTTPS sites since 2011. By some estimates, Heartbleed affects 2 out of 3 web servers on the Internet. Heartbleed isn’t a bug in the design of HTTPS itself but rather the result of a simple programming error in a widely-used piece of software called OpenSSL. It allows an attacker who connects to an HTTPS server running a vulnerable version of OpenSSL to access up to 64KB of private memory space. Doing the attack once can easily cause the server to leak cookies, emails, and passwords. Doing the attack repeatedly in a clever way can potentially leak entire encryption keys, such as the private SSL keys used to protect HTTPS traffic. If an attacker has access to a website’s private SSL key, they can run a fake version of the website and/or steal any information that users send, including passwords, private messages, and credit card numbers. Neither users nor website owners can detect this attack as it happens.\n",
            "In case you didn’t catch it, this bug has been around since 2011.\n",
            "CNET.com provided tips on how to protect yourself from the bug:\n",
            "Do not log into accounts from afflicted sites until you’re sure the company has patched the problem. If the company hasn’t been forthcoming — confirming a fix or keeping you up to date with progress — reach out to its customer service teams for information, said John Miller, security research manager for TrustWave, a security and compliance firm. Once you’ve got confirmation of a security patch, change passwords of sensitive accounts like banks and email first. Even if you’ve implemented two-factor authentication — which, in addition to a password asks for another piece of identifying information, like a code that’s been texted to you — changing that password is recommended. Don’t be shy about reaching out to small businesses that have your data to make sure they are secure. While the high-profile companies like Yahoo and Imgur certainly know about the problem, small businesses might not even be aware of it, said TrustWave’s Miller. Be proactive about making sure your information is safe. Keep a close eye on financial statements for the next few days. Because attackers can access a server’s memory for credit card information, it wouldn’t hurt to be on the lookout for unfamiliar charges on your bank statements.\n",
            "CNET also said that a site called LastPass can be used to check websites to see for their Heartbleed patch status.\n",
            "It has been reported that the NSA used Heartbleed for intelligence for years.\n",
            "According to Bloomberg.com, the NSA kept the bug secret in order to exploit its capabilities:\n",
            "Putting the Heartbleed bug in its arsenal, the NSA was able to obtain passwords and other basic data that are the building blocks of the sophisticated hacking operations at the core of its mission, but at a cost. Millions of ordinary users were left vulnerable to attack from other nations’ intelligence arms and criminal hackers. “It flies in the face of the agency’s comments that defense comes first,” said Jason Healey, director of the cyber statecraft initiative at the Atlantic Council and a former Air Force cyber officer. “They are going to be completely shredded by the computer security community for this.” The NSA and other elite intelligence agencies devote millions of dollars to hunt for common software flaws that are critical to stealing data from secure computers. Open-source protocols like OpenSSL, where the flaw was found, are primary targets. While many Internet companies rely on the free code, its integrity depends on a small number of underfunded researchers who devote their energies to the projects. In contrast, the NSA has more than 1,000 experts devoted to ferreting out such flaws using sophisticated analysis techniques, many of them classified. The agency found the Heartbleed glitch shortly after its introduction, according to one of the people familiar with the matter, and it became a basic part of the agency’s toolkit for stealing account passwords and other common tasks.\n",
            "A statement by Michael Sutton, vice president of security research at Zscaler, a San Jose, California-based security firm, sums up the seriousness of the bug and its potential impacts:\n",
            "“We’ve never seen any quite like this. Not only is a huge portion of the Internet impacted, but the damage that can be done, and with relative ease, is immense.”\n",
            "——————-\n",
            "Contributed by Lily Dane of The Daily Sheeple.\n",
            "Lily Dane is a staff writer for The Daily Sheeple. Her goal is to help people to “Wake the Flock Up!”\n",
            "The Chinese government is complaining about Trump’s aggressive stance on North Korea, but its implementation of U.S.-drafted sanctions Monday suggests it is in fact going along with the U.S. effort.\n",
            "The Trump administration secured a major victory this month when China declined to use its veto at the U.N. Security Council and voted for strict sanctions against North Korea’s exports—after it had voiced opposition to using sanctions. The sanctions will slash a third of North Korea’s $3 billion exports.\n",
            "Before the vote, China had indicated they would not support sanctions. U.N. Ambassador Liu Jieyi said that China did not support an “economic blockade,” and scolded the U.S. for being one of two parties that “refuse to move towards what is required by Security Council resolutions.” Days later, China voted for the U.S.-drafted resolution.\n",
            "But in an apparent toughening of their stance, President Xi Jinping warned Trump over the weekend to exercise “restraint” over his language in relation to North Korea.\n",
            "“At present, relevant parties should exercise restraint and avoid words and actions that would escalate tensions on the Korean Peninsula,” Xi said, according to the statement provided by China’s government.\n",
            "However, despite the warning and amid concerns that China would approve the North Korea resolution but not actually follow through with it, China’s Commerce Ministry on Monday announced that China would ban imports of coal, iron, iron ore, lead ore, lead and seafood from North Korea—fulfilling China’s implementation of one of the key pillars of the U.N. resolution.\n",
            "Even with that, the Chinese seemed reluctant, with Chinese foreign minister Wang Yi telling Xinhua state news agency, “Given China’s traditional economic links with the DPRK, it is China that will mainly pay the price for the implementation of (the sanctions).”\n",
            "Meanwhile, the state-controlled Global Times published a furious op-ed criticizing Trump on both North Korea and his reported decision to push for an investigation of China’s trade practices. The op-ed urged China to “turn its passivity around”:\n",
            "The US frequently sends warships to patrol the South China Sea, and now it’s ramping up trade pressure on China. China should turn its passivity around. China will not act as an aggressive provocateur, but we should make Washington realize that China is not the one to be messed around with.\n",
            "However, there was no sign of such a turning around from Beijing as it enforced the sanctions well within the 30-day limit set by the U.S.-drafted resolution.\n",
            "Trump has taken an unusually hard line on China and had been outspoken in his disappointment that China had not worked with the U.S. to clamp down on North Korea.\n",
            "The communist country’s recent backing down on North Korea may have something to do with Trump’s connecting of trade with the crisis in North Korea.\n",
            "The Global Times blasted Trump for doing exactly that, calling the linking of the two “illogical.”\n",
            "Adam Shaw is a Breitbart News politics reporter based in New York. Follow Adam on Twitter: @AdamShawNY\n",
            "A new paper on the Vox Web site by the European economists Rafael Lalive, Simon Luechinger and Armin Schmutzler on the effects of increased rail service makes clever use of natural experiments created by changes in German ownership and regulation. The results aren’t that surprising — more frequent rail service sharply reduces pollution and other costs associated with driving — but it’s good to have this kind of solid work to back our intuitions.\n",
            "And can I say that this is a subject that really deserves a lot more attention? Mea culpa: I haven’t written much for a while on these issues, focusing mainly on the economic crisis, which is on the front burner for the moment. But we know, as surely as we know anything in economics, that there are huge market failures here — that every time an individual chooses to drive during rush hour, he or she is imposing huge costs on other drivers, on people who breathe, and more.\n",
            "Ideally, the answer is to get the incentives right and to charge large fees for driving in congested areas. Short of that, there are huge second-best payoffs to mass transit; if we did the accounting properly, taking all the benefits into account, Amtrak’s Northeast Corridor service (which makes money even without taking this into account) would be seen as a huge social boon, and projects like a proposed rail tunnel under the\n",
            "Hudson River between New York and New Jersey would be total no-brainers.\n",
            "And the thing is that these are externalities that everyone can see. You can deny global warming (and may you be punished in the afterlife for doing so — this kind of denial for petty personal or political reasons is an almost inconceivable sin), but can anyone deny that more drivers means more traffic congestion?\n",
            "Well, maybe I’m understating the power of denial. But still, this is a totally obvious case for government intervention that’s staring us all in the face every time we hit the road.\n",
            "Running the Government Like a Business or a Family\n",
            "I’ve spent a lot of time trying to knock down the bad analogy between governments and individuals, and the line that the government should act like an individual family or business, and cut back on spending when times are tough.\n",
            "The key point is interdependence: your spending is my income, my spending is your income, and if we all try to slash spending at the same time, the result is a depression. Somebody needs to step up and spend when others won’t — and the government can and should be that somebody.\n",
            "That said, the funny thing is that real individuals and businesses don’t behave the way the balanced-budget scolds claim. Businesses often borrow and spend when borrowing is cheap or they see high payoffs to investing; so do families. So the reporter Josh Israel at the commentary website Think Progress, in a recent post titled “14 GOP Congressmen Who Think Government Shouldn’t Borrow Have Big Debts Of Their Own,” is doing good by pointing out how many of those deficit-fearing Congresscritters turn out to have quite large personal debts.\n",
            "Trump Saw A Disturbing Video, Then He Shut Down The CIA’s Covert Syria Program from ZeroHedge\n",
            "While we’ve carefully documented the dynamics in play behind Trump’s decision to end the CIA’s covert Syria program, as well as the corresponding fury this immediately unleashed among the usual hawkish DC policy wonks, new information on what specifically impacted the president’s thinking has emerged.\n",
            "Thomas Joscelyn, a Middle East analyst for the Foundation for Defense of Democracies, explains in the August edition of The Weekly Standard:\n",
            "Earlier this year, President Donald Trump was shown a disturbing video of Syrian rebels beheading a child near the city of Aleppo. It had caused a minor stir in the press as the fighters belonged to the Nour al-Din al-Zenki Movement, a group that had been supported by the CIA as part of its rebel aid program. The footage is haunting. Five bearded men smirk as they surround a boy in the back of a pickup truck. One of them holds the boy’s head with a tight grip on his hair while another mockingly slaps his face. Then, one of them uses a knife to saw the child’s head off and holds it up in the air like a trophy. It is a scene reminiscent of the Islamic State’s snuff videos, except this wasn’t the work of Abu Bakr al-Baghdadi’s men. The murderers were supposed to be the good guys: our allies.\n",
            "Trump pressed his most senior intelligence advisers, asking the basic question of how the CIA could have a relationship with a group that beheads a child and then uploads the video to the internet. He wasn’t satisfied with any of the responses:\n",
            "Trump wanted to know why the United States had backed Zenki if its members are extremists. The issue was discussed at length with senior intelligence officials, and no good answers were forthcoming, according to people familiar with the conversations. After learning more worrisome details about the CIA’s ghost war in Syria—including that U.S.-backed rebels had often fought alongside extremists, among them al Qaeda’s arm in the country—the president decided to end the program altogether.\n",
            "Screenshot of the horrific video of a CIA-backed Syrian group beheading a boy named Abdullah Issa.\n",
            "At the time the beheading video surfaced (July 2016), many in the American public naturally wanted answers, but the story never really picked up much momentum in the media. As Joscelyn describes, it caused nothing more than “a minor stir in the press.” The State Department seemed merely satisfied that the group responsible, Harakat Nour al-Din al-Zenki, claimed to have arrested the men that committed the gruesome crime, though nothing more was known. Absurdly, a US government spokesperson expressed hope that the child-beheading group would “comply with obligations under the law of armed conflict.”\n",
            "The only press agencies that publicly and consistently challenged the State Department at the time were RT News and the Associated Press, yet even these attempts didn’t get picked up beyond the confines of the State Department’s daily briefing. When the AP’s Matt Lee initially questioned spokesman Mark Toner as to whether Zenki would continue to receive any level of US assistance, Toner casually replied “it would give us pause” – which left Lee taken aback.\n",
            "Meanwhile, it wasn’t just the US government which had aided Zenki, but as fighting in Aleppo raged it became a favored group among both the mainstream media and prominent think tank pundits. One of the UK’s major broadcasters (Channel 4) even went so far as to attempt to delete and hide its prior online content which sought to normalize the beheaders as “moderate” and heroic once news of the video got out.\n",
            "Controversial, but @AbuJamajem is largely right: – “In #Syria, U.S. Can Keep Its Hands Clean or Get Things Done”https://t.co/vYzwH0mWXB — Charles Lister (@Charles_Lister) August 22, 2016\n",
            "Among think tankers, Zenki’s most prominent public supporter, frequently presenting the terror group as actually representative of Syria’s “secular” and supposedly democracy-promoting armed opposition (even after the beheading video emerged), was Charles Lister. Lister was finally confronted not by mainstream media, but by AlterNet’s Max Blumenthal at a DC event held by the (largely Gulf funded) Atlantic Council.\n",
            "Only by the time of this January 2017 public forum, and after being persistently questioned, did Lister awkwardly back off his previous enthusiastic promotion of Zenki:\n",
            "We can imagine that Trump saw other things beyond the shocking Zenki beheading video which made him fully realize the utter criminality of the CIA program (Thomas Joscelyn further emphasizes that Trump came to understand the full scope of CIA cooperation with al-Qaeda in Syria).\n",
            "The only question that remains is who in the CIA or Obama-era State Department should be prosecuted first?\n",
            "Sharing is caring!\n",
            "Online gaming may boost school scores but social media is wasted time, study suggests\n",
            "Posted\n",
            "In what could be music to the ears of many parents, teenagers who regularly play online games are more likely to get better school scores, an Australian study suggests.\n",
            "Key points: RMIT research suggests gaming helps boost results in maths, science and reading\n",
            "Online gaming helps develop analytical and problem-solving skills, researchers say\n",
            "Scrolling through Facebook, Instagram or chat sites was shown to hinder academic success\n",
            "Research released from RMIT University has found gaming helps boost results in maths, science and reading.\n",
            "But researchers said scrolling through Facebook, Instagram or chat sites had the reverse effect, by hindering academic success in high school.\n",
            "Associate Professor Alberto Posso from RMIT's School of Economics, Finance and Marketing said online gaming appeared to be a more useful way to spend time online for teenagers, compared with social media.\n",
            "\"Kids that are spending more time on online gaming — for example in a maths test — they're likely to score 17 points above the average, which is about 4 per cent above the average [test score],\" he said.\n",
            "Associate Professor Posso used data from the Program for International Student Assessment (PISA) to analyse the online habits of 12,000 Australian 15 year olds, which he then compared to their academic results.\n",
            "He said the PISA data revealed that online gaming helped young people develop analytical and problem-solving skills.\n",
            "\"Sometimes [players] have to understand some of the principals of chemistry even, so they really have to understand science,\" he said.\n",
            "\"Some psychologists have argued that massive online player games can be beneficial to cognitive development.\"\n",
            "'You're not really going to solve problems using Facebook'\n",
            "Perhaps unsurprisingly, the study found spending hours on social media was mostly wasted time for teenagers, in terms of academic performance.\n",
            "Australian teenagers who used Facebook or chat sites every day scored 20 points worse in maths than students who never used social media, the research said.\n",
            "\"You're not really going to solve problems using Facebook,\" Professor Posso said.\n",
            "\"What's interesting, from an economic perspective, there's a very high opportunity cost of time, where we're spending a lot of time doing something that may not necessarily be associated with performance in school.\n",
            "\"When you play online games you're solving puzzles to move to the next level and that involves using some of the general knowledge and skills in maths, reading and science that you've been taught during the day.\"\n",
            "The research has been published in the International Journal of Communication.\n",
            "Topics: schools, education, games, social-media, australia, melbourne-3000\n",
            "c/o Lauren Billington\n",
            "It’s been three weeks since bleary-eyed Floridians woke up to news that a gunman had killed several people at the Route 91 Harvest music festival in Las Vegas. As the hours rolled on, and the internet did what it does, videos bounced off social networks like Snapchat and Twitter and onto mainstream news channels, warning viewers about the graphic images they were about to see. A click transported you to the moment shots began to ring out from a 32nd-story balcony hundreds of yard from the stage.\n",
            "Details unfolded, and we learned that the shooter — a 64-year-old, millionaire retired real estate investor named Stephen Paddock — stocked a massive arsenal of guns in the Mandalay Bay Hotel room from which he would fire upon the 22,000 people gathered to see country star Jason Aldean. What initially sounded like fireworks were actually bullets, fired from guns outfitted with bump-stocks (which allow semi-automatic weapons to fire more like automatic ones). As victims in the massive crowd were treated for wounds suffered on festival grounds, images of fallen festival attendees, lost Stetsons and crying cowboys made their way to the eyes of a watching world. Eventually, we learned that 59 people died in the terrorist attack media outlets have dubbed “the worst mass shooting in modern American history.”\n",
            "Aldean — who was just in Tampa for a boozy summertime performance at the MidFlorida Credit Union Amphitheatre — took to Instagram to say that the events he witnessed were “beyond horrific.” He sent thoughts and prayers to everyone involved, along with just about everybody else with a Facebook account. Nonprofit pro-gun lobby The National Rifle Association mostly remained silent, though, and pols reminded us that it just wasn’t the right time to talk about gun control.\n",
            "Still, at the center of it all was the truly horrific reminder of the reality that even concerts — where the world goes to be moved by the simple sound of a song — were never truly sacred spaces safe from the twisted fantasies of some asshole with a gun.\n",
            "“I don’t know what to make of it, really. Like most people, I am in shock and a little bit of denial,” yacht-rock singer Michael McDonald told CL in an interview days after the shooting. McDonald, 65, is scheduled to play the Clearwater Jazz Holiday on October 21, and noted that he hunted and collected guns in his younger days. “I was fascinated by guns, but for some reason this NRA lobby prohibits us from even bringing up the subject of why we’re making it easier for mentally impaired people to buy firearms.”\n",
            "Against Me! guitarist James Bowman — who brings his punk band to play two nights of Big Pre-Fest in Ybor City at the end of this month — shelters his 5-year-old son from the news and said events like Vegas do stick in the back of his mind.\n",
            "“You don’t necessarily try to forget about it, but it’s such a hard thing that does hit close to home because of the business we’re in. We all know what happened, and it’s the furthest thing you want to happen on any day,” Bowman, 37, said. “It doesn’t come up in everyday conversation because it is so scary.”\n",
            "But Bowman’s tour rolls on, and so did Bay area concertgoers.\n",
            "Two days after the attack, 15,000 Jack Johnson fans piled into the amphitheatre on the Florida State Fairgrounds. Kelsey and Joe Mitchals, both 27, sat on the lawn and brought their 2-year-old son Rylan and his 8-week-old brother Jackson. There was a little apprehension, but overall the vibe of the show by Johnson — a laid-back pro surfer turned songwriter — was what their family needed. At a Sunday concert by rising R&B star SZA, a sold-out Orpheum crowd was triumphant in the way it sang back every word. Down the street at Crowbar’s Ol’ Dirty Sundays, it was business as usual with a packed concert crowd from a show by electro producer Com Truise mixing in with the B-boys and girls gathered for the late-night hip-hop party.\n",
            "And at Friday’s Band of Horses show at Jannus Live, a most enduring symbol of a collective scene’s unbreakable spirit manifested itself in 8-year-old Legend Billington, who attended the concert with his mom Lauren. She admitted to being nervous about the plethora of dark windows that sit above and around the famed courtyard venue, but made a conscious decision to watch the show from the outer edge of the crowd.\n",
            "“We paid attention to the exits, and I made sure Legend knew all his emergency numbers and his address,” Lauren, 43, told CL, adding that discussing a tragedy like Vegas is a fine line for a kid his age. “I have not had to explain it to him — instead we talk about good, critical thinking in the case of an emergency.”\n",
            "But why would she bring an 8-year-old to a rock show?\n",
            "“For one, he loves the band,” she said, adding that every memory she has is related to a song, lyric band or venue. Billington won’t let anyone take that away from her or her family, and wants her kids to experience as much of the world as possible without being ruled by fear. “There are many risks we take daily. I’m more anxious about taking them to national monuments, Disney World, and sports events than I am about bringing them to smaller local concerts.\n",
            "“I tell my sons all the time that ‘we only get each day once,’” she added before inadvertently explaining Bay area music fans’ resilience in the face of horror:\n",
            "“They are encouraged to be positive risk-takers and to do the things that they love with the people they love.”\n",
            "Taking risks for the love of live music. That’s the simple answer for why we go to concerts, and it’s the simple reason why we’ll never let anything take those experiences away from us\n",
            "Mr Bush has never laid out his plans in this way before\n",
            "In a major policy speech, Mr Bush refused to set an \"artificial deadline\" to withdraw US troops, saying it was \"not a plan for victory\".\n",
            "It comes after the release of the first Iraq strategy document, which rejects widespread calls for a timetable.\n",
            "Mr Bush has come under growing pressure from Democrats on Iraq. Polls give him the lowest approval of his presidency.\n",
            "They also suggest that six out of 10 Americans think the war in Iraq is not worth the cost.\n",
            "As such, this was a speech from a president in deep trouble, says the BBC's Justin Webb in Washington.\n",
            "Harry Reid, the Democratic leader in the Senate, said Mr Bush's speech was \"recycled... tired rhetoric\", and that the president had \"once again missed an opportunity to lay out a real strategy for success in Iraq that will bring our troops safely home\".\n",
            "'Some setbacks'\n",
            "Speaking at the US Naval Academy in Annapolis, Maryland, Mr Bush said there would be violence in Iraq \"for many years\" and that US troops would only be able to withdraw as local forces gained competence.\n",
            "\"These decisions about troop levels will be driven by the conditions on the ground in Iraq and the good judgement of our commanders, not by artificial timetables set by politicians in Washington,\" he said.\n",
            "HAVE YOUR SAY America will not run in the face of car bombers and assassins so long as I am your commander-in-chief\n",
            "President George W Bush\n",
            "Excerpts: Strategy for Victory Send us your comments\n",
            "Mr Bush said victory would come \"when the terrorists and Saddamists can no longer threaten Iraq's democracy, when the Iraqi security forces can provide for the safety of their own citizens, and when Iraq is not a safe-haven for terrorists to plot new attacks on our nation\".\n",
            "This was a partial redefinition of what victory might be, and potentially highly significant, our correspondent says.\n",
            "Mr Bush also openly acknowledged that there had been \"some setbacks in standing up a capable Iraqi security force, and their performance is still uneven in some areas\".\n",
            "But Iraqi forces were regaining control of the country and training programmes had been improved, he said.\n",
            "Withdrawing US troops before they had accomplished their mission would send the wrong message to the insurgents, Mr Bush added.\n",
            "\"America will not run in the face of car bombers and assassins so long as I am your commander-in-chief,\" he said.\n",
            "Shift\n",
            "The new National Strategy for Victory, released hours before Mr Bush's address, defines who the US sees as the enemy in Iraq, listing three groups in declining order of size:\n",
            "\"Rejectionists\" - primarily Sunni Arabs who fared well under Saddam but have lost influence and authority. The US says their resistance will fade if a new democratic government protects minority rights\n",
            "\"Saddamists\" who were active members of the former regime. The US expects their power to wane to the point where Iraqi security forces can defeat them\n",
            "\"Terrorists\" associated with al-Qaeda who want to establish a totalitarian Islamic empire, and who must be killed or captured through counter-terror operations.\n",
            "Our mission in Iraq is to win the war - our troops will return home when that mission is complete\n",
            "Full strategy document (392KB) Most computers will open PDF documents automatically, but you may need to download Adobe Acrobat Reader. Download the reader here\n",
            "\"No war has ever been won on a timetable - and neither will this one,\" the document adds.\n",
            "There are currently more than 150,000 US troops in the country. The White House has said it expects conditions will permit a reduction in US troop numbers next year, after Iraqi parliamentary elections in mid-December.\n",
            "The US is spending about $6bn a month to keep its forces in Iraq. About 2,100 Americans have been killed since the March 2003 invasion.\n",
            "DAVID FARRAR: Providing strategic advice to a group planning a campaign to get rid of MMP.\n",
            "A group of National and Act Party activists is preparing to launch a campaign against MMP.\n",
            "Details of the campaign, to be launched soon in the lead-up to the referendum on MMP on election day in November, have been leaked to the Sunday Star-Times.\n",
            "Among the campaign's key players is Simon Lusk, who played a major role in Don Brash's leadership coup against Act leader Rodney Hide.\n",
            "Jordan Williams, the young Wellington lawyer who accompanied Brash on the day he deposed Hide, is being considered for the role of frontman of the campaign.\n",
            "David Farrar, National's pollster and a well-known right-wing blogger and columnist, is providing strategic advice. Also involved is right-wing blogger Cameron Slater, known as Whale Oil.\n",
            "Whether or not abortion remains a crime in Queensland will likely be decided by who is elected to government on November 25. The procedure is currently only lawful to \"prevent serious danger to the woman's physical or mental health\".\n",
            "Pro-Choice Queensland/Supplied\n",
            "Pro Choice QLD/Facebook\n",
            "\"We need to know how many candidates at the state election are willing to come forward and say what their planned response to the recommendations from the [Queensland Law Reform Commission] will be,\" she told BuzzFeed News. Melinda decided to provide medical terminations to her patients in 2015 when she read about the \"horrible deaths and complications\" that came from backyard abortions in Queensland during the early 20th century. \"The law hasn't changed since 1899 and I was quite outraged and realised that I could choose to provide this to women in need,\" she said. \"Technically any GP in Queensland can provide this service if they want to, but I think the murky legal area is inhibiting a lot of GPs that might be interested in making it part of their policy of reproductive health. \"At the moment it is a bit of a postcode lottery as to whether women can get access to a termination.\"\n",
            "Melinda said she did not want to be named because she worked within the shadows of the law. \"I could be charged and face up to 14 years in jail and my patients could face seven years, but I have a bit of a defence under case law and my patients don't.\" Essential research polling showed in February 76% of Queenslanders believed the state's abortion laws should be changed.\n",
            "Essential Research / Via d3n8a8pro7vhmx.cloudfront.net\n",
            "Most voters polled were less likely to vote for candidates who wanted to keep treating abortion as a crime. More than 100 candidates have signed a pro-choice candidate pledge from independent lobby group Fair Agenda, which is backed by The Human Rights Law Centre, Women’s Legal Service Queensland, White Ribbon and the Queensland Council of Unions. \"One in four women have a termination in their lifetime so this is absolutely an issue for voters and it should be an issue that candidates are willing to stand up on, regardless of their political affiliation,\" Pro-Choice Queensland convenor and former state Liberal senator Sue Boyce said of the pledge.\n",
            "Dan Peled / AAP\n",
            "The candidates committed, if elected, to \"vote to remove abortion from the criminal code, and support laws to ensure all Queenslanders can safely and legally access full reproductive healthcare, without being harassed or intimidated.” Labor member for Murrumba and candidate for the newly-formed electorate of Bancroft, Chris Whiting, has taken the pledge because he believes \"the law is clearly outdated and it needs to be reformed\". \"Whatever people's moral position is, we cannot criminalise the thousands of Queenslanders who undergo this process every year,\" Whiting told BuzzFeed News.\n",
            "\"It is abundantly clear that there are decades worth of case law that circumnavigate what are 19th century statutes.\"\n",
            "Chris Whiting/Facebook\n",
            "Whiting is running against LNP candidate Kara Thomas who is also the director of research, policy and advocacy at anti-abortion lobby group Cherish Life Queensland.\n",
            "Supplied\n",
            "The organisation was responsible for a raft of misleading radio, newspaper and Facebook advertisements in the lead up to the scheduled vote on Pyne's abortion law reform bills earlier this year. Thomas gave evidence for Cherish Life in an inquiry into Pyne's second bill last year, during which she argued the section (s225) of Queensland's criminal code which criminalises women should remain in the law. She argued against abortion in all scenarios, including following a rape. \"Even in cases of rape and incest, does the child pay the death penalty for the actions of the perpetrator, and is a second act on the woman going to make it better?\" Thomas asked the inquiry.\n",
            "Thomas declined an interview with BuzzFeed News.\n",
            "The candidate also argued against safe-access zones — a key part of Pyne's legislation — which would make it illegal for any person seeking to \"intimidate, or impede the access of a person entering or leaving\" an abortion provider (within 50 metres of the clinic). \"There are a number of stories of women who have changed their mind because of sidewalk counsellors,\" Thomas said. \"I am not supportive of violence outside of clinics, but I do not think that it is right to impose a restriction around a building based on a belief that the unborn are not human.\" Thomas joined the most vocal opponent to Pyne's bills, Cleveland MP Mark Robinson, in an interview by anti-abortion rights blogger Dave Pellowe, whose other videos include \"There's no clear definition of what hate speech is\", \"Good news, there is no rape culture\" and \"Are homosexuals born that way?\"\n",
            "Dave Pellowe/YouTube Kara Thomas and Mark Robinson.\n",
            "\"Abortion ... is the intentional killing of an innocent human being,\" she said.\n",
            "\"If the unborn are human, which science says they are, then it is morally wrong according to our laws to unjustly kill a human being.\" She referred to abortion as \"state-sanctioned coercion\" and spruiked the anti-abortion documentary Hush, which showed a purported link between abortion and an increased chance of developing breast cancer. This purported link has been widely and repeatedly rejected by Australia's medical organisations including the Royal College of Obstetricians and Gynaecologists, the Australian Cancer Council, the Breast Cancer Network of Australia, and the Australian Medical Association, as well as American organisations the National Breast and Ovarian Cancer Centre, the National Cancer Institute, and the American Cancer Society. Whiting said his opponent should be \"clear with her constituents about where she is employed and what her stance is\".\n",
            "Pro-Choice QLD/Facebook\n",
            "Chances are you have heard of Roman ‘Amok’ Papsuev. He’s an incredibly talented game industry artist, who has been working hard on a series of incredible images of the Russian folk heroes, recreated in his signature videogame style. This project is called “Tales of Old Rus‘”.\n",
            "His drawings conquered a lot of media websites and became a huge hit on 9GAG. We couldn’t pass this opportunity and managed to talk with Roman about his career, his collaborations with George R. R. Martin, the search for great authentic references and the whole philosophy of the “Old Rus‘” project.\n",
            "Could you please introduce yourself and talk a bit about your work?\n",
            "Well, I’ve been working as an illustrator for quite some time, since the beginning of this century you may say (chuckles). I was making illustrations for books of famous local fantasy authors: Vera Kamsha, Nick Perumov, Dmitry Kazakov.\n",
            "Then, just as a hobby, I began painting portraits of A Song of Ice and Fire characters. Later George R. R. Martin took a notice of me and recommended me as an artist for Fantasy Flight Games (FFG). So I got a job working on a card game based on A Song of Ice and Fire series.\n",
            "It was great getting a direct email from Martin himself and getting his feedback. With his help I was able to fix some of the portraits. All in all there were around 120 of them. Now you can find most in various Game of Thrones related wikis and resources.\n",
            "Eventually Martin and FFG asked me to contribute to the “The Art of Song of Ice and Fire” artbook. The writer was kind enough to give me some exclusive descriptions of the Targaryens and I was happy to create their portraits. Good times.\n",
            "Anyway, I was quite busy, making illustrations for books and fantasy novels. In 2008 I got the “Best Artist” award at Eurocon. Later I got an interesting proposal from local Russian company Astrum-Online. This is how I got into game industry.\n",
            "The thing is that being a freelancer I had a chance to work on various tasks and was lucky enough to collaborate with “13Roentgen” studio, headed by Leonid Sirotin (a well-known Russian game producer, mostly specializing in mobile and social games). He was the one to bring me into game industry, and I’m still incredibly grateful to him. It was interesting working in a completely new field. Right now I’m the Lead Artist in ITT Territory Studio, which is a part of Mail.ru Group.\n",
            "I’ve been able to contribute to a various projects, including such browser games as «Juggernaut», «ThreeKingdoms», «Legend: Legacy of the Dragons». Also I did a lot of art for «Territory-2»: concept art, promo art, you name it. This project was closed, but all the content did not go to waste.\n",
            "Our team switched to mobile games and we’ve managed to use some of the Territory-2 concepts in «Evolution: Battle for Utopia». This game was very successful and even got a custom page in AppStore.\n",
            "I also used to be a writer back in the day and managed to publish 4 novels and a couple of short stories. Thankfully I knew when to stop, because I finally realized I wasn’t good at it. During those days I was studying a lot of mythology, medieval history and other historical sources. So you might say I have a long interest in folk tales and couldn’t resist the temptation to work with our local folklore.\n",
            "How did you come up with this idea of folk heroes reimagining?\n",
            "Being a CG-artist I sometimes get back to the roots and have fun drawing with a pencil. I spent a lot of time working on a sketchbook “MonstaPanopticum”. It was a collection of various monsters, sort of my imagination practice, just a personal project. The sketchbook was done but I still had a lot of ideas. I thought about Russian folk tales and mythology. There’s a ton of very cool characters there and not a lot of video games exploring this amazing setting.\n",
            "My initial idea was to develop the theme and try to look at it at an unusual angle. Then I figured I could try drawing Russian classical characters in a hardcore fantasy-style game setting.\n",
            "It all started as a mind exercise, but slowly grew into something much bigger. I’ve started researching folk tales, read some related books and the whole project became much more interesting. I got an opportunity to make videogame interpretations of the Russian tales. I tried to be not constricted by stereotypes or so called classical vision. You can study the descriptions of some of the images and figure out how much time I’ve spent researching the details.\n",
            "I do not just make up these characters. Everything is in the Russian mythology. I just try to interpret them in my own way, finding some common features, keeping the images in one general style. If it works, we’ll get the feeling of one giant living and breathing fantasy world, populated by these creatures.\n",
            "I got to be honest – I don’t believe that these characters need any particular reimagining. It’s a huge layer of Russian culture and I truly believe you should read these stories to kids: these tales are full of magic, positive characters, happy endings. Then, when your children grow up, they may have a look at my paintings, read the descriptions and learn a lot of new stuff. Because original Russian folk tales are far more complex and brutal than you think. As well as all mythology, as a matter of fact.\n",
            "My images do not destroy the classical visions of these characters. Russia actually has a huge visual heritage and my project is not trying to substitute it. It’s just the way of looking at well-known heroes a bit differently. Yes, I do break stereotypes but I do not mean to change the way we perceive our historical or mythological heritage. I feel that it’s time for us to try and integrate our folk tales in the contemporary life. This is a chance to remind young people in Russia and all around the world about our cultural inheritance, to help them gain a better understanding of the classical characters of Russian folk culture.\n",
            "I believe that since we live in 21st century it’s useful to get rid of some of the stereotypes and patterns we have. Try to imagine our classical Russian bogatyrs standing next to Khorne from Warhammer universe. Just visually try to figure out which character looks stronger? Who would win the imaginary fight? See what I mean?\n",
            "To be fair there was a number of great games which tried to take some of Slavic folk heritage into the world of interactive entertainment. Let’s take Allods Online for example (I didn’t play it). I heard Witcher 3 is also awesome (I didn’t play it either). In my works I tried to put the characters in a harsher, grittier visual reality, giving them my own personal artistic interpretation.\n",
            "Can you shed some light on how does your creative process works? Where do you get all those incredible ideas for details of your character?\n",
            "It was easy to draw the first images. I’ve been creating them almost on the spot, based on the childhood impressions I had. By the way some of these drawings will go through a small modification later, because they really stand out from the general style of the project which is now based on more mature tales and character descriptions I’ve found in different sources.\n",
            "I use a whole bunch of various ways to research the characters for my drawings: Wikipedia, some Mythology encyclopedias, dictionaries, scientific articles and so on. I try to work only with authentic original folk texts. Believe me, it’s much harder than just letting your fantasy go wild.\n",
            "I usually start with reading the original folk tale about the character, then I go to Wiki and start digging: scientific sources, books, websites. After gathering all the information I begin drawing.\n",
            "Details are obviously very important. Having worked in the game industry and knowing its requirements I always try to add as much visual information about the character as possible. It’s the basis for character design. Obviously these guys and girls don’t carry around all that stuff, but these details, these little things help me to tell their story. You can deliver maximum amount of information with minimal effort: just one picture can tell you a lot. All this fantasy equipment, runes, weapons have nothing to do with reality, so people, who don’t play games, can’t figure out why there’s so much gigantic armor plates. I try to explain it all and I even compiled a FAQ for those who are interested in these images. Tastes differ, so there’s bound to be some misunderstandings. I always say – please just try not to take it all too seriously. These are just the artist’s fantasies on the subject.\n",
            "I guess you get this question a lot, but I can’t help myself: what were the main inspirations for developing the style of the characters?\n",
            "Since I’ve been working in game industry, I’m pretty good at understanding all the visual patterns in character design, but I haven’t really played any Warhammer, Allods Online, DOTA 2, The Elder Scrolls, Dark Souls and even World of Warcraft. But I do know how these games look. A lot of people just glance at my images and start naming games I haven’t even heard of. Sometimes they mention movies: “Oh, this dude looks like a Transformer.” Most of them are sorely wrong.\n",
            "If you need to simplify, you might say I’ve used the style of Warhammer, which is very dark, epic, unrealistic and visually shocking. Sometimes I add more cartoonish characters like spirits of the forest or domovoy (Russian house spirit), just to vary the world I’m drawing.\n",
            "As an artist, I think every image must tell some kind of story.\n",
            "A lot of my inspiration comes from my experience. I’ve seen a lot of artbooks, watched tons of movies and learned visual styles of different games. All this baggage is the foundation of my works. I don’t have any visual references next to me when I’m working, just dictionaries, encyclopedias and original texts of Russian folklore. Every visual aspect of the character comes directly from my head.\n",
            "I’ve got two notebooks. One is my personal Sketchbook of Ideas. This is where I develop my characters, work on the images. I share some of these WIP stages in my Instagram (amokrus).\n",
            "When the character is ready I finally draw it in my “Tales of Old Rus’’” Sketchbook, scan it, throw in a bit of Photoshop and publish it in Internet.\n",
            "How will this project develop in the future? Do you plan to make a videogame out of it?\n",
            "Actually, I haven’t really thought about the project’s commercial potential. For now I just want to draw. There are a lot of opportunities to turn this project into a product and I have a lot of offers. There’s definitely going to be an artbook, that I can say for sure. Maybe a board game. There might be a video game (I don’t really care about the genre). Time will tell.\n",
            "You know, the funny part is that this project is unique because it has sparked an incredible interest for Russian folklore all around the world. I did nothing to promote this series of images. I just drew to my heart’s content. Actually my collaboration with George Martin began the same way. I never really struggled to get into the spotlight. I don’t do any PR stuff, I don’t really care for it.\n",
            "We’ll see how this project will go. I’m definitely going to continue my work. It’s just so much fun!\n",
            "***\n",
            "We thank Roman for his time and encourage you to check out his official Facebook page and Instagram.\n",
            "MEETING THE MINISTER\n",
            "\"One of the asymmetries of history,\" wrote Henry Kissinger of Singapore’s patriarch Lee Kuan Yew, \"is the lack of correspondence between the abilities of some leaders and the power of their countries.\" Kissinger’s one time boss, Richard Nixon, was even more flattering. He speculated that, had Lee lived in another time and another place, he might have \"attained the world stature of a Churchill, a Disraeli, or a Gladstone.\" This tag line of a big man on a small stage has been attached to Lee since the 1970s. Today, however, his stage does not look quite so small. Singapore’s per capita GNP is now higher than that of its erstwhile colonizer, Great Britain. It has the world’s busiest port, is the third-largest oil refiner and a major center of global manufacturing and service industries. And this move from poverty to plenty has taken place within one generation. In 1965 Singapore ranked economically with Chile, Argentina and Mexico; today its per capita GNP is four or five times theirs.\n",
            "Lee managed this miraculous transformation in Singapore’s economy while maintaining tight political control over the country; Singapore’s government can best be described as a \"soft\" authoritarian regime, and at times it has not been so soft. He was prime minister of Singapore from its independence in 1959 (it became part of a federation with Malaysia in 1963 but was expelled in 1965) until 1990, when he allowed his deputy to succeed him. He is now \"Senior Minister\" and still commands enormous influence and power in the country. Since his retirement, Lee has embarked on another career of sorts as a world-class pundit, speaking his mind with impolitic frankness. And what is often on his mind is American-style democracy and its perils. He travels often to East Asian capitals from Beijing to Hanoi to Manila dispensing advice on how to achieve economic growth while retaining political stability and control. It is a formula that the governing elites of these countries\n",
            "Former Attorney General Eric Holder has said he is eager to become more involved in resisting President Donald Trump and is also considering a presidential run in 2020.\n",
            "“Up to now, I have been more behind-the-scenes,” Holder told Yahoo News in an interview. “But that’s about to change. I have a certain status as the former attorney general. A certain familiarity as the first African-American attorney general. There’s a justified perception that I’m close to President Obama. So I want to use whatever skills I have, whatever notoriety I have, to be effective in opposing things that are, at the end of the day, just bad for the country.”\n",
            "Holder, who served as Barack Obama’s Attorney General from 2009 to 2015 before being replaced by Loretta Lynch, also said that he had planned to take a back seat role in politics, but Hillary Clinton’s defeat in 2016 election inspired him to re-engage in frontline politics.\n",
            "“I thought, frankly, along with everybody else, that after the election, with Hillary Clinton as president, I could walk off the field,” he continued. “So when she didn’t win, I thought, ‘We’ll have to see how this plays out.’ But it became clear relatively soon — and certainly sooner than I expected — that I had to get back on the field and be in effective opposition.”\n",
            "Since Trump took office, Holder has been involved in legal attempts to torpedo Donald Trump’s immigration agenda. In January, he was hired by Democratic leaders to represent the state of California to maintain its status as a sanctuary city.\n",
            "On Monday, Holder spoke at a public meeting at the Ronald Reagan State Building in Los Angeles, alongside California Senate leader Kevin de Leon, to promote legislation known as the California Values Act, which prevents local police from enforcing immigration law.\n",
            "During the meeting, Holder claimed that the “federal government does not have the ability to force states to do things that are inherently federal in nature,” and that attempts to withhold federal funds for failing to comply with immigration law are unconstitutional.\n",
            "“Now is the time to be more visible,” Holder added. “Now is the time to be heard.”\n",
            "Other Democrats mooted as potential 2020 presidential candidates include Massachusetts Sen. Elizabeth Warren, Vermont Sen. Bernie Sanders, and former Vice President Joe Biden.\n",
            "You can follow Ben Kew on Facebook, on Twitter at @ben_kew, or email him at bkew@breitbart.com\n",
            "John Mayer briefly spoke on Frank Ocean’s new music during a recent Instagram live stream. After amusedly touching on what reggae would sound like in space, he responded to a fan question in the comments: “Favorite Frank Ocean song? ‘Sweet Life,’” he said. “And then he played me this song the other day... I won’t say the title, that's his business, but... He’s doing stuff right now that’s... talk about space reggae, future reggae, that’s Frank Ocean.” Check out a video of the stream below; Mayer speaks on Ocean around the 8:20 mark. Mayer previously played guitar on Channel Orange, while Ocean sang on Mayer’s 2013 album Paradise Valley.\n",
            "Producer Calvin Harris recently announced that his new collaboration with Ocean and Migos, “Slide,” will be released tonight, marking the first piece of new music from Ocean since the release of Blonde.\n",
            "ES Football Newsletter Enter your email address Please enter an email address Email address is invalid Fill out this field Email address is invalid You already have an account. Please log in or register with your social account\n",
            "David Busst has offered Manchester United and England left-back Luke Shaw hope of a return to action before the end of the season.\n",
            "Former Coventry defender Busst suffered a horrific leg break against United at Old Trafford in 1996 after colliding with Denis Irwin and Brian McClair and retired from the game a few months later.\n",
            "Busst needed 26 operations and contracted the MRSA virus but he is confident Shaw, 20, who sustained a double fracture of his right leg in Tuesday night's 2-1 defeat at PSV Eindhoven, will make a full recovery.\n",
            "He told The Sun: \"It wasn't the actual fractures that stopped me from playing, it was the infections.\n",
            "\"But so long as it's a clean break and there is no infection, there shouldn't be a major issue for Luke.\"\n",
            "Manchester United announced on Wednesday that Shaw will remain in Eindhoven for the 'early stages of his recovery' after undergoing surgery in the Netherlands on Tuesday night.\n",
            "Pat Robertson (The 700 Club/screen grab)\n",
            "Pat Robertson on Monday declared that Donald Trump was the “clear winner” of Sunday night’s presidential debate, and the televangelist asserted that the GOP candidate’s admission that he groped women was simply “macho” talk.\n",
            "“Trump was the winner,” Robertson opined on his 700 Club television program. “Basically, the pundits were writing him off.”\n",
            "The TV preacher argued that Trump was like the mythical Phoenix because he had performed well at the debate just days after the leak of a video tape, in which the Republican nominee bragged that he could grab women “by the pussy” without their permission because he was a star.\n",
            "“A guy does something 11 years ago, it was a conversation in Hollywood where he’s trying to look like he’s macho,” Robertson said. “And 11 years after that they surface it from The Washington Post or whatever, bring it out within 30 days or so of the election and this is supposed to be the death blow and everybody writes him off, ‘Okay, he’s dead, now you’ve got to get out of the way and let Mike Pence run the campaign.'”\n",
            "“The Donald says no,” he continued. “He’s like the Phoenix. They think he’s dead, he’s come back. And he came back strong. So, he won that debate.”\n",
            "A scientific poll conducted by CNN following Sunday night’s debate found that 57 percent of people said that Hillary Clinton won, while 34 percent thought Trump came out ahead.\n",
            "Watch the video below from The 700 Club, broadcast Oct. 10, 2016.\n",
            "Mitch McConnell in Washington, D.C., on Wednesday. Yuri Gripas/AFP/Getty Images\n",
            "It became conventional wisdom over the course of the election cycle that Donald Trump was a Frankenstein monster that the Republican Party created through the politics of racial resentment and anti-Obama demonization. This line of reasoning often implied that the Republican establishment was about to receive its comeuppance via a humiliating landslide electoral loss.\n",
            "Well, that didn’t happen. And, as you can see above, chief Senate obstructionist Mitch McConnell—he of the infamous quote about Obama’s defeat being “the single most important thing [Republicans] want to achieve“—is looking mighty smug Wednesday. He has some reason to feel good: He’ll soon get to push through a vote for a conservative Supreme Court justice and to throw the Affordable Care Act in a garbage can.\n",
            "But Trump’s victory speech early Wednesday morning underlined a major McConnell-related irony. A huge part of the opposition plan executed by McConnell, Paul Ryan, and the rest of the Republican Congress involved choking off the parts of President Obama’s agenda that would have helped the working class—the consideration of a public option for health insurance, the expansion of Medicaid, the passage of a stimulus package appropriate to the size of the 2008 recession, and generally the funding of redistributive government spending through higher taxes on top earners. These kinds of potential measures, and even the more limited measures that did pass, were demonized as the socialist agenda of an un-American president. And so even as America’s economy recovered in many ways, its inequalities of wealth and income persisted.\n",
            "The white victims of this process, as we now know, were not happy, and their feeling of economic stagnation was—in addition to the War on Christmas, the suspicion that Barack Obama is African ISIS, and the general feeling that Hillary Clinton is an uppity bitch—one of the things that flipped just enough states to Trump’s side to give him the Electoral College.\n",
            "Look what President-elect Trump described as the first priority of his administration in his victory speech:\n",
            "We are going to fix our inner cities and rebuild our highways, bridges, tunnels, airports, schools, hospitals. We’re going to rebuild our infrastructure, which will become, by the way, second to none. And we will put millions of our people to work as we rebuild it.\n",
            "That’s a mega-stimulus New Deal–style proposal no matter which party’s mouth it comes out of. At the same time, we are all aware that Donald Trump’s policy on free trade is that he is very against it. The “trade” section of Trump’s website is a list of vows to withdraw from agreements and institute tariffs. It’s an aggressive mercantilist promise to use the power of the government to protect American industries that are not competitive internationally, particularly those that are not competitive with China and Mexico.\n",
            "So your Republican president’s agenda is to balance the protection of noncompetitive domestic businesses with a huge increase in public spending (and let’s not even mention the massive public-works project known as the wall with Mexico). It’s not really socialism—Trump’s government is going to take on the cost of propping up big manufacturers without being rewarded via an ownership stake therein—but it’s just as “socialist” as, to take one example, subsidizing the purchase of private health insurance. And it’s unquestionably Big Government.\n",
            "I’m not an economic historian, but I happen to have recently read Tony Judt’s history of 20th-century Europe, Postwar, and Trump’s vision of the American economy sounds a lot like the mixed British economic system that was repudiated by the rise of Margaret Thatcher. (Try to picture a 1970s British car and you get a sense of the pitfalls of overprotection of industry.) Across the pond, of course, Thatcher’s rise was paralleled by the ascendance of a free-market, free-trade-extolling American politician named Ronald Reagan who shared her contempt for the encroachments of the welfare state. Among the politicians elected to Congress in the 1984 Reagan landslide was a young(er) Mitch McConnell, and among the Republicans Reagan inspired was a teenage Paul Ryan, whose brother once told the New York Times that Ryan’s political views were formed by discussing Reagan during family dinners. Now Ryan and McConnell are the architects of a strategy that has ended in the election of someone whose idea of economic policy is the creation of “millions” of government jobs and withdrawal from the Reagan treaty that became NAFTA. (And who, to boot, wants to forge a military and diplomatic alliance with the KGB officer who runs Russia.)\n",
            "History repeats itself as farce, I’ve heard.\n",
            "With apologies to Hamilton Nolan\n",
            "See more Slate coverage of the election.\n",
            "The \"S\" word came up Saturday afternoon following the 6A title game.\n",
            "The \"S\" word I'm talking about is sportsmanship. It's something we seldom salute in what is a winner take all society. It doesn't make the highlights, and it isn't printed on the front page. But perhaps it should be.\n",
            "Pine Bluff head coach Bobby Bolding talked about the good sportsmanship displayed by his Zebras and the Greenwood Bulldogs during Pine Bluff's hard fought 28-21 win. The communities of Greenwood and Pine Bluff are as different as night and day, but on this day all I could see were two well-coached teams giving it their all to win a State Title. The great thing about sports is that it can bring kids together from entirely different backgrounds, and the hope is that they will leave the playing field with a new respect for each other. I hope that happened Saturday.\n",
            "On Friday night, I witnessed a lack of good sportsmanship from Pulaski Academy coach Kevin Kelley.\n",
            "The 5A Title game was the most compelling matchup of Championship weekend: the feel good story of the 2015 season, the McClellan Lions, facing the Mighty Bruins of Pulaski Academy. It was a rematch of a mid-season meeting where PA scored an early knockout. The final in that one was 41-30, but 24 of McClellan's 30 points came against PA's reserves. The Bruins led 35-0 at the half, and 41 to 6 early in third quarter.\n",
            "Friday night's Championship Game was different.\n",
            "McClellan came in with a great plan, and thanks to some friendly bounces and huge plays, they led Pulaski Academy 30 to 29 at halftime. This game wouldn't be decided until late in the 4th quarter.\n",
            "It was the Pulaski Academy defense that came up with the play of the game. Trailing by only five, midway through the fourth quarter, McClellan faced a 4th and two at midfield. The PA defense swarmed the \"Lion King\" Pierre Strong to end McClellan's final threat.\n",
            "The Bruins would add a touchdown to go up 13 at 43-30. That should have been the final score, but it wasn't. What happened next made me cringe.\n",
            "With 12 seconds to go, after using two timeouts on the drive, PA threw for another touchdown to make it 50-30.\n",
            "No one likes a bad winner.\n",
            "It was the final seconds of PA's second straight state title win, and Coach Kelley should have been thinking about the celebration. You'll often hear coaches and former players call the victory formation a team uses when the quarterback kneels down to run out the clock the \"greatest formation in football.\" Apparently, it's gone the way of the punt at PA.\n",
            "Instead of running out the clock and soaking in a second straight title, Coach Kelley decided to use the last seconds to get payback; to punish McClellan. He later went to Facebook to explain why he went for the final Touchdown.\n",
            "I'm not interested in his reasons. Coach Kelley was wrong. And if he needed to justify his decision after the fact on Facebook, he likely knows he was wrong, too. Instead of talking about the tremendous effort by the kids on either sideline or the heroics that turned the game by the overlooked defensive unit instead of PA's offensive juggernaut, we're talking about running up the score.\n",
            "That unnecessary late touchdown has created nothing but bad blood. There's no middle ground on this one; it's PA against the world.\n",
            "The relationship between public and private schools in this state is shaky at best, and stuff like this doesn't help the private schools.\n",
            "Coach Kelley is a great coach who preaches \"we, not me\" to his players. If he were thinking that way late Friday night, he could have saved everyone a lot of grief.\n",
            "Writer Hired For Ron Howard And Brian Grazer's 1984 By Eric Eisenberg Random Article Blend\n",
            "1984 was first published in 1949 and tells the story of a dystopian future where a man named Winston Smith rebels against the evil, ever-watching Big Brother. The book has gotten adapted multiple times, most notably in 1956 (Michael Anderson directing, Edmond O'Brien starring) and 1984 (Michael Radford directing, John Hurt starring).\n",
            "Deadline confirms that Fairey will serve as an executive producer on the project along with Grazer and Howard. No production start or release date is mentioned for the film. Back in March a strange story surfaced online. It was reported that Brian Grazer and Ron Howard would be making a new adaptation of George Orwell's brilliant novel 1984 inspired by renowned street artist Shepard Fairey. At the time the project was in the earliest stages of development and had yet to find a writer or director. The production staff is still working on the latter, but now they've wrangled the former. Deadline reports that Noah Oppenheim has been hired to create the newest adaptation of 1984. Oppenheim, who was previously known as a television producer on shows like Hardball With Chris Matthews, Today and The Buried Life, he has made the switch over to screenwriting and already has a number of other high-profile projects on his plate. As previously reported the scribe is working on the WarGames remake, the redo of Daniel Espinosa's Snabba Cash, and The Secret Life of Houdini (the Harry Houdini movie that Gary Ross will direct).1984 was first published in 1949 and tells the story of a dystopian future where a man named Winston Smith rebels against the evil, ever-watching Big Brother. The book has gotten adapted multiple times, most notably in 1956 (Michael Anderson directing, Edmond O'Brien starring) and 1984 (Michael Radford directing, John Hurt starring).Deadline confirms that Fairey will serve as an executive producer on the project along with Grazer and Howard. No production start or release date is mentioned for the film. Blended From Around The Web Facebook\n",
            "Back to top\n",
            "Sofia addresses the flack she’s received for Gloria.\n",
            "Ever since she began portraying Gloria Pritchett on Modern Family, Sofia Vergara has received tons of flack for portraying a stereotype on TV. But Vergara has no problem with that and even says that it’s because of her that Latinas with strong accents can be on TV.\n",
            "Speaking with Hola! USA, she said,\n",
            "“Gloria’s character is inspired by my mom and my aunt. They are both Latin women who grew up in Colombia, like me. They love color, prints, and shoes.”\n",
            "She added,\n",
            "“It upsets me when Latinos complain about Gloria. I am grateful for the opportunity because the gringos have let me in with this strong accent I have. Eight years ago nobody had an accent like this on television.”\n",
            "Vergara has been nominated four times for a Golden Globe for the role so clearly somebody likes what she’s doing!\n",
            "Since starting on Modern Family, Sofia has landed an endorsement deal with CoverGirl, started her own K-Mart line, and her own underwear line.\n",
            "Modern Family airs on ABC Wednesdays at 9 PM.\n",
            "Photo: Tinseltown / Shutterstock.com\n",
            "1 SHARES Facebook Twitter Google Whatsapp Pinterest Print Mail Flipboard\n",
            "At a White House event, President Trump declared himself no longer a civilian as he seems to believe that being president also makes him a member of the military.\n",
            "Video:\n",
            "According to the official White House transcript, as provided to PoliticusUSA, Trump said, “We’ve had a lot of victories, but we haven’t had a victory on healthcare. We’re disappointed. I am very disappointed because, again, even as a civilian, for seven years I’ve been hearing about healthcare, and I’ve been hearing about repeal and replace. And Obamacare is a total disaster. Some states had over a 200 percent increase — a 200 percent increase — in their premiums, and their deductibles are through the roof. It’s an absolute disaster.”\n",
            "Donald Trump is still a civilian. Being president does make him a member of the military. The role of commander in chief is a title, not an official military position. Trump is still very much a civilian, and subject to civilian laws. Trump is slipping fast, and it is time for the American people to take notice of the fact that they are being governed by someone who might not be all there.\n",
            "If you’re ready to read more from the unbossed and unbought Politicus team, sign up for our newsletter here! Email address: Leave this field empty if you're human:\n",
            "More than 60 dogs seized in Studio City, and none were yet up for adoption. Marin Austin reports for the NBC4 News at 7 on Saturday, May 7, 2016. (Published Saturday, May 7, 2016)\n",
            "The owner of a pet adoption center in Studio city is facing neglect charges after more than 60 dogs were seized from her home Friday night, officials said.\n",
            "Animal Control officers took the dogs of a variety of breeds and sizes from the home of Rachel Kennedy, owner of Lucky Puppy Rescue & Retail at 11734 Ventura Boulevard, according to Brenda Barnette, general manager of Los Angeles Animal Services.\n",
            "Officers said they took 65 dogs, many of them with medical conditions, for humane reasons. Los Angeles County code allows only three dogs per residence, officials said.\n",
            "\"Every single one of them are my babies,\" said Kennedy, who claims she took the dogs home to live with her because many of them are too sick and cannot be legally adopted by the public.\n",
            "In a Facebook post, the pet adoption center said all of the dogs that were taken were \"elderly, blind, deaf, cancer, and mommies left to die at the shelter with their babies.\"\n",
            "Kennedy claims she spends about $200,000 a year to rescue and care for sick dogs who would otherwise be euthanized by the city. \"There's something wrong in this system,\" said Kennedy. \"Us as rescuers, we do this from the heart and they took my heart away.\"\n",
            "Kennedy faces charges of illegal kennel and animal neglect, according to Barnette. Each dog was being examined and a health record was being created.\n",
            "While Kennedy says she has a permit for most of the dogs to live at Lucky Puppy Rescue & Retails, she knew she was breaking the law when she took them home. She said will visit the animals who are now at the Los Angeles City East Valley Animal Shelter.\n",
            "Barnette said the seized dogs are in custody and unavailable for adoption, foster or transfer to a private adoption or rescue group.\n",
            "Correction: A previous version of this article incorrectly stated the dogs were seized at Lucky Puppy Rescue & Retail. The dogs were taken from Rachel Kennedy's home in Studio City. The article has been updated.\n",
            "This article is over 2 years old\n",
            "Results of annual report card based on data collected before bleaching killed a fifth of the reef’s coral, suggesting next year’s results will be even worse\n",
            "Great Barrier Reef scores D for health for fifth year in a row\n",
            "The Great Barrier Reef has been given a D on a report card for its overall health by the federal and Queensland governments for the fifth year in a row.\n",
            "The results of the annual report card were based on data collected before this year’s climate change-induced bleaching event that killed about a fifth of the reef’s coral, suggesting next year’s results will be even worse.\n",
            "The report card measures the progress towards water pollution targets, as well as the overall health of the Great Barrier Reef’s ecosystems, including coral and seagrass.\n",
            "The Great Barrier Reef: a catastrophe laid bare Read more\n",
            "Combining all the measurements, the reef was rated D or “poor”.\n",
            "Many of the individual assessments of progress towards water-quality targets showed very slow progress.\n",
            "The government has a target to reduce inorganic nitrogen flowing into the Great Barrier Reef – mostly from sugar-cane fertilisation – by 50%. The report card rated progress towards that target with an E or “very poor”, finding it had only been reduced by 18.1%.\n",
            "Progress towards improving sugar-cane farming and grazing practices in Great Barrier Reef catchments was rated “poor”, as was progress towards sediment and pesticide targets.\n",
            "Some improvement was seen in coral cover in “inshore reefs” – those closest to the coastline – with their score moving from a D to a C.\n",
            "But the coral cover in the most northerly sections – Cape York and the Wet Tropics – were given a score of D.\n",
            "Those northern sections, particularly Cape York, were the hardest hit by the 2016 bleaching event, which occurred after the data used for this report card was collected.\n",
            "Initial estimates show that 22% of the coral along the whole Great Barrier Reef was killed in the event, with 85% of that mortality occurring in the northern tip, north of Lizard Island.\n",
            "The results follow a Water Science Taskforce report from the Queensland government in August finding $8.2bn was needed to meet water-quality targets the government has committed to, in order to avoid having the reef included on the world heritage in-danger list.\n",
            "The new report card said “hundreds of millions” were being spent on projects to reduce pollution.\n",
            "The government will need to report to the UN’s world-heritage committee in December on implementation of its plan to meet those targets, including on whether the plan has been adequately funded.\n",
            "Imogen Zethoven from the Australian Marine Conservation Society described the report as “startling”.\n",
            "Great Barrier Reef: Unesco pushes for tree-clearing controls Read more\n",
            "“Coral bleaching, crown-of-thorn outbreaks and the ongoing poor condition of inshore reefs – these are the visible signs of a reef in crisis and the need for urgent action on water quality,” she said.\n",
            "“The reef can’t survive and thrive without clean water. We must see accelerated action by the Queensland government to move ahead with legislative caps on water quality, supported by the federal government,” Zethoven said.\n",
            "Sean Hoobin form WWF Australia said the results were “not a good look” ahead of the government’s reporting to the UN.\n",
            "“The continuing poor scores are further evidence that the current programs and spending on reef pollution fall far short of what’s required. At this rate Australia simply won’t meet the targets committed to in Reef 2050,” he said.\n",
            "Hoobin pointed out that almost $13bn is being spent to save the Murray-Darling basin.\n",
            "“The reef is the jewel in the crown of Australia’s tourism industry and deserves a rescue package similar to the Murray-Darling basin,” Hoobin said.\n",
            "The U.S. Air Force unveiled on Friday the first image of its new long-range B-21 bomber developed by Northrop Grumman Corp.\n",
            "U.S. Air Force Secretary Deborah James revealed the drawing of the aircraft at the Air Force Association’s annual Air Warfare Symposium, Reuters reported.\n",
            "The Brief Newsletter Sign up to receive the top stories you need to know right now. View Sample Sign Up Now\n",
            "The B-21 will be designed to launch from the U.S. and strike any target around the world and will eventually replace the Air Force B-52 bombers, according to CNN.\n",
            "Northrop won an estimated $80 billion contract in October to build 100 new bombers, and Boeing Co. said on Friday it would forgo further protests to the contract.\n",
            "The Air Force has said it will release more information about the warplane in March. The aircraft still faces hurdles in Congress, where questions have been raised about funding the program.\n",
            "Write to Katie Reilly at Katie.Reilly@time.com.\n",
            "University of Florida athletic director Jeremy Foley will contact Philadelphia Eagles coach Chip Kelly this week to gauge his interest in coaching the Gators, CBS Philly's Marc Farzetta reports.\n",
            "Florida is seeking to replace Will Muschamp, who was dismissed on Nov. 16 after going 28-21 in four seasons. He was allowed to finish the regular season but will not coach the team in a bowl game.\n",
            "\"Foley does not plan to officially interview Kelly yet, but will simply gauge Kelly’s interest and make his first pitch,\" a source told Farzetta.\n",
            "• Nebraska fires head coach Bo Pelini​\n",
            "Kelly is currently in his second season in the NFL after serving as the head coach at Oregon from 2009 to 2012. Kelly had a 46-7 record with the Ducks and qualified for a BCS bowl in each of his four seasons.\n",
            "The Eagles are currently in first place in the NFC East at 9-3. In Kelly's first year with the team, Philadelphia won the NFC East but lost in the first round of the NFL postseason.\n",
            "Yahoo's Pat Forde reported Sunday that Colorado State's Jim McElwain is a \"leading candidate\" to take over the Gators. Florida was also reported to be interested in Ole Miss head coach Hugh Freeze, but Florida denied that report.\n",
            "- Dan Gartland\n",
            "Lust, Lies And Empire: The Fishy Tale Behind Eating Fish On Friday\n",
            "Enlarge this image toggle caption Adam Cole/NPR Adam Cole/NPR\n",
            "It sounds like the plot of a Dan Brown thriller: A powerful medieval pope makes a secret pact to prop up the fishing industry that ultimately alters global economics. The result: Millions of Catholics around the world end up eating fish on Fridays as part of a religious observance.\n",
            "This \"realpolitik\" explanation of why Catholics eat fish on Friday has circulated for so long, many people grew up believing it as fact. Some, myself included, even learned it in Catholic school. It's a humdinger of a tale — the kind conspiracy theorists can really sink their teeth into. But is it true?\n",
            "\"Many people have searched the Vatican archives on this, but they have found nothing,\" says Brian Fagan, a professor emeritus of archaeology at the University of California, Santa Barbara, whose book, Fish On Friday, explores the impact of this practice on Western culture.\n",
            "The real economic story behind fish on Fridays turns out to be much better.\n",
            "Let's start with a quick lesson in theology: According to Christian teaching, Jesus died on a Friday, and his death redeemed a sinful world. People have written of fasting on Friday to commemorate this sacrifice as early as the first century.\n",
            "Technically, it's the flesh of warmblooded animals that's off limits — an animal \"that, in a sense, sacrificed its life for us, if you will,\" explains Michael Foley, an associate professor at Baylor University and author of Why Do Catholics Eat Fish On Friday?\n",
            "Fish are coldblooded, so they're considered fair game. \"If you were inclined to eat a reptile on Friday,\" Foley tells The Salt, \"you could do that, too.\"\n",
            "Alas, Christendom never really developed a hankering for snake. But fish — well, they'd been associated with sacred holidays even in pre-Christian times. And as the number of meatless days piled up on the medieval Christian calendar — not just Fridays but Wednesdays and Saturdays, Advent and Lent, and other holy days — the hunger for fish grew. Indeed, fish fasting days became central to the growth of the global fishing industry. But not because of a pope and his secret pact.\n",
            "At first, says Fagan, Christians' religious appetite was largely met with herring, a fish that was plentiful but dry and tasteless when smoked or salted. And preservation was a must in medieval times: There was no good way for fresh fish to reach the devout masses. Eventually, cod became all the rage — it tasted better when cured and it lasted longer, too.\n",
            "The Vikings were ace at preserving cod — they \"used dried and salted cod as a form of beef jerky on their ocean passages,\" Fagan says. And the route the Vikings took at the end of the first millennium — Greenland, Iceland, Newfoundland — matches up with the natural range of the Atlantic cod.\n",
            "It's possible that others may have followed the cod trail to Canada before Columbus sailed the ocean blue. Clues suggest that English fishermen from Bristol may have made the voyage by around 1480 but kept mum on the location lest the competition rush in. By some accounts, both Columbus and John Cabot had heard of these adventures when they set off on their own epic journeys west.\n",
            "\"Why do people go over the horizon?\" Fagan says. \"In the case of the North Atlantic after the Norse ... they went looking for cod\" to satiate the demands of the faithful.\n",
            "So that's the empire part of our saga. Funny enough, while the pope story is a fish tale, an official leader of a church did make fish fasting the law for purely practical reasons. For that story — and the lust our headline promised — we turn to a monarch known for his carnal cravings: Henry VIII.\n",
            "By the time Henry ascended the throne in 1509, fish dominated the menu for a good part of the year. As one 15th century English schoolboy lamented in his notebook: \"Though wyll not beleve how werey I am off fysshe, and how moch I desir to that flesch were cum in ageyn.\"\n",
            "But after Henry became smitten with Anne Boleyn, English fish-eating took a nosedive.\n",
            "You see, Henry was desperate with desire for Anne — but Anne wanted a wedding ring. The problem was, Henry already had a wife, Catherine of Aragon, and the pope refused to annul that decades' long marriage. So Henry broke off from the Roman Catholic Church, declared himself the head of the Church of England and divorced Catherine so he could marry Anne.\n",
            "Suddenly, eating fish became political. Fish was seen as a \" 'popish flesh' that lost favour as fast as Anglicism took root,\" as Kate Colquhoun recounts in her book Taste: The Story of Britain Through Its Cooking.\n",
            "Fishermen were hurting. So much so that when Henry's young son, Edward VI, took over in 1547, fast days were reinstated by law — \"for worldly and civil policy, to spare flesh, and use fish, for the benefit of the commonwealth, where many be fishers, and use the trade of living.\"\n",
            "In fact, fish fasting remained surprisingly influential in global economics well into the 20th century.\n",
            "As one economic analysis noted, U.S. fish prices plummeted soon after Pope Paul VI loosened fasting rules in the 1960s. The Friday meat ban, by the way, still applies to the 40 days of the Lenten fast.\n",
            "A few years before the Vatican relaxed the rules, Lou Groen, an enterprising McDonald's franchise owner in a largely Catholic part of Cincinnati, found himself struggling to sell burgers on Fridays. His solution? The Filet-O-Fish.\n",
            "While not exactly the miracle of loaves and fishes, Groen's little battered sandwich has fed millions around the world.\n",
            "Breaking News Emails Get breaking news alerts and special reports. The news and stories that matter, delivered weekday mornings.\n",
            "Dec. 26, 2017, 8:28 PM GMT / Updated Dec. 27, 2017, 5:10 PM GMT By Ali Vitali\n",
            "Lady Luck's moment in Virginia's political spotlight will have to wait.\n",
            "The Virginia State Board of Elections will postpone its Wednesday random-drawing tiebreaker, granting a request from Democrat Shelly Simonds less than 24-hours before the unusual political event.\n",
            "\"We’ve been informed that the SBE is postponing their meeting,\" a board of elections spokeswoman told NBC News Tuesday, adding that more details will be passed along \"as soon as we get them.\"\n",
            "Republican David Yancey and Democrat Shelly Simonds attend a \"take your legislator to school day\" on Nov. 28 at Heritage High School in Newport News, Virginia. Julia Rendleman / The Washington Post via Getty Images\n",
            "The Board of Elections was set to meet Wednesday morning in Richmond to determine the winner of the hotly-contested House of Delegates race between Simonds and Republican incumbent David Yancey by randomly drawing the winner's name, most likely out of a bowl.\n",
            "Simonds' lawyers, Jonathan Berkon and Ezra Reese, told reporters in a conference call Tuesday that \"there isn't any hurry\" and a delay would allow for further review of the tie determination in the contest, 11,608 to 11,608.\n",
            "Simonds' camp has filed motions asking a court to declare her the winner and suspend a judges' panel declaration of a tie last week, arguing that they should not have reviewed the discarded ballot that allowed the race to become tied up. The judges ruled that the previously tossed ballot should have been counted for Yancey, a decision that erased Simonds' one-point victory and equalized the result.\n",
            "Matt Abell, Lead SBE Election Administration Analyst, holds the crystal bowl from which the political party slips were drawn at the June 26th board meeting. Virginia Dept. of Elections\n",
            "That ballot in question had both candidates' bubbles filled in but Simonds' bubble had a slash mark through it and the judges interpreted that as meaning the voter didn't want to vote for the Democrat.\n",
            "Simonds had earlier been declared the apparent winner in the race for the 94th District seat, a victory that would have flipped the red seat blue and created a 50-50 tie in the Virginia House.\n",
            "If the drawing does ever take place, under one of the possible lottery scenarios, the state Board of Election would put each candidate's name in an old film canister, place the canisters into a glass bowl, shake it up, and then pick the canister with the winner's name inside. It remained unclear who would have the honor of selecting the winner.\n",
            "The drawing would have been live-streamed on Facebook starting around 10 a.m., ET.\n",
            "Should Simonds end up eventually taking the seat, it would mark the first time in two decades that Virginia's House was evenly divided and could lead to an unusual power-sharing arrangement in the legislature.\n",
            "The 94th district isn't only Virginia district with its election results hanging in the balance.\n",
            "Democrats are currently challenging the results out of Fredericksburg after about 100 residents in a split precinct were given the wrong ballot on Election Day in a House of Delegates contest. That case is set to go to court early next year and could further impact the balance of power in the legislature.\n",
            "Students from GHumble High School discussing sexual abuse assembly -- (KHOU screen grab)\n",
            "Officials at a Texas high school pulled the plug midway through a girls-only assembly on sexual abuse when the speaker snapped at several girls who weren’t paying attention and said she wouldn’t feel bad if they were raped, reports KHOU.\n",
            "A guest speaker was brought in from a local domestic violence and sexual assault charity to speak to 9th and 10th grade girls at Humble High School about healthy relationships and domestic violence. However, when some girls in the back on the room began chatting, the talk on self-esteem and protecting oneself took an ugly turn when the counselor snapped at them.\n",
            "“She said when she moves the cover from over your face and they start swabbing and combing the hair — she was explaining the rape kit — she said she would not feel bad for us. She said she would tell us, ‘Oh I told you this was going to happen to you,'” explained sophomore Chantranise Lane.\n",
            "According to multiple girls in the room, teens became upset with some of them crying, including a few who are rape victims.\n",
            "The school district confirmed that part of the talk had to do with what girls wear and what they post on social media, but the teens said they should be speaking with the boys and telling them not to rape.\n",
            "“They shouldn’t be telling students that just because you’re posed a certain way, you’re going to get assaulted. That shouldn’t happen. You need to teach the boys not to do that,” said Zaria Rogan,\n",
            "“If somebody comes at me and I tell them ‘No, you stop what you’re doing,’ that is a no. So if boys are not being taught this and they’re being taught that if we portray ourselves like this then they can do what they want to us, then that’s never going to change.” added Emily Nelson\n",
            "“I feel like they were degrading us making us bad for being a female,” Lane explained. “They’re making us feel bad for loving ourselves or trying to accept ourselves.”\n",
            "According to the school, the assembly was abruptly stopped and another planned for later was canceled before issuing a statement saying they never wanted the girls to feel uncomfortable about themselves.\n",
            "The group that supplied the speaker said that they never had an issue before and weren’t sure what happened, but were looking into it.\n",
            "Watch the video from KHOU below:\n",
            "HEMPSTEAD, N.Y. (CBSNewYork) — Summertime heat doesn’t only attract people to barbecues and the beach, it attracts pesky, party crashing flies as well.\n",
            "One in particular — the green-fly — is out in force this year.\n",
            "As CBS2’s Andrea Grymes reported, one Long Island town is using an innovative approach to trap the seasonal pest in its breeding grounds.\n",
            "Normally, the last thing Hempstead Town Supervisor Anthony Santino would like a handful of is green head flies, or more commonly green flies.\n",
            "The voracious summertime biters are no threat to anyone because they’re dead, after having been trapped inside a rather innocuous looking wooden box.\n",
            "“Once they enter the trap they are unable to escape, and then they dehydrate,” Santino (R) said.\n",
            "Hempstead Town has set up dozens of the green-fly boxes in their breeding grounds in the swampy areas of the Lido preserve.\n",
            "The flies can enter the boxes from below, but are then trapped by the covering mesh screen.\n",
            "Every summer, the female green-flies lay millions of eggs in the marsh, but to do so they need to feast on humans.\n",
            "“They literally need a blood meal to produce more offspring,” Santino said.\n",
            "Rob Humphreys knows it all too well since he likes to run through the preserve daily.\n",
            "“You don’t know you’re getting bitten until you feel the pain. You look down, and there’s a nice little hole, blood coming out, and it swells up and it itches like crazy. I hate it,” he said.\n",
            "The green flies will head anywhere they can find a human host, whether at the beach or poolside.\n",
            "“In July and August, there’s no wind, and on a hot day and they’re all over the place,” Joseph Chiodi said.\n",
            "Town managers said there is minimal cost to taxpayers since the wooden fly boxes can be built in-house a cost of $100 each.\n",
            "Local gardener Kathryn Heneghan said she loves that there are no pesticides involved.\n",
            "“I prefer that over any chemical treatment because I worry about my kids,” she said.\n",
            "Town leaders said with sixty fly boxes, they can capture up to 150,000 green flies a day who will never get the chance to bug you.\n",
            "It’s an approach that may be tried elsewhere, because green flies swarm coastal areas from New Jersey to New England.\n",
            "With nine weeks left until the general election, most signs point to a victory for Hillary Clinton. In head-to-head national polling she has been ahead of Donald Trump for much of the past year, and in most recent surveys she has retained the lead. At the state level, too, Clinton holds the advantage: over the summer she moved ahead of Trump in many key battleground states, greatly complicating his path to accumulate two hundred and seventy votes in the Electoral College.\n",
            "Of course, polls aren’t infallible—we relearned that lesson in the recent Brexit referendum. But in the past eight U.S. elections, the candidate who was leading on Labor Day went on to become President. At the online bookmakers, where real money is wagered, Clinton remains the strong favorite. On Tuesday the polls-based forecasting model maintained by the Times Upshot team estimated the probability of her winning at eighty-four per cent. FiveThirtyEight’s “polls-only” model put the probability of a Clinton victory at 68.5 per cent.\n",
            "Why, then, is there so much anxiety among Clinton supporters? One reason is a new CNN poll, released on Monday, which generated headlines saying Trump had taken the lead. Among respondents the pollsters deemed “likely voters” that was true: the poll showed Trump at forty-four per cent, and Clinton at forty-two per cent. But among the broader pool of registered voters, Clinton was still ahead, forty-four per cent to forty-one per cent.\n",
            "Essentially, the pollsters screened out some of Clinton’s supporters because they didn’t adjudge them likely to turn out on November 8th. There’s nothing unusual or untoward in that. As elections approach, many polling organizations switch their focus from registered voters to likely voters. But all such screens are somewhat arbitrary, because each polling organization has its own criteria for screening out unlikely voters.\n",
            "When inspecting the trend in a given poll, it is also better to compare like with like. In this case, we can still look at registered voters: among those voters, Clinton’s lead in the CNN poll has shrunk from eight points a month ago to three points now. That finding is in line with other recent polls. On August 10th the Real Clear Politics poll average, which combines the results of numerous surveys, showed Clinton leading Trump 47.8 per cent to 39.9 per cent, a gap of almost eight percentage points. By Tuesday morning the gap had narrowed to 3.3 percentage points. (Clinton: 46.2 per cent; Trump: 42.9 per cent.)\n",
            "Just as there are many ways to decide who is a likely voter, numerous methods can be used to construct poll averages. The key decisions are which polls to include and how to weigh them. The Huffington Post’s poll average, which is somewhat different than the Real Clear Politics average, indicates that Clinton is still leading by more than five percentage points—48.1 per cent to 42.5 per cent. But it, too, indicates that the race has narrowed over the past month.\n",
            "Why has it narrowed? Part of the explanation may be that Clinton’s recent slippage reflects a predictable correction to the polling gains she enjoyed after a successful Democratic Convention and Trump’s self-defeating attacks on the Khan family. As memories of Conventions fade, the bounce that candidates get from them often subsides, partially or wholly.\n",
            "In this case, it could be argued that the polls have largely reverted to where they were before the Conventions. On July 11th, a week before the gavel came down in Cleveland, the R.C.P. poll average showed Clinton leading Trump by 4.5 percentage points—not far from where the gap is today. Looking at the numbers this way, the key fact about modern American politics is that the country is divided pretty evenly, with this year being no exception. We should always have expected a close race.\n",
            "A more pointed explanation for the recent polling trends acknowledges the possible effect of the recent barrage of negative press about the Clinton Foundation and Clinton’s private e-mail server. In an ABC News/Washington Post poll, which was carried out on August 24th through 28th, fifty-six per cent of respondents said they had an unfavorable opinion of Clinton. That was a jump of six points compared with a poll taken at the start of August. The Post’s Aaron Blake noted that Clinton’s favorable/unfavorable numbers were the worst she “has had in her quarter-century of national public life.”\n",
            "Monday’s CNN poll confirmed that Clinton has a serious image problem. Asked to choose the most honest and trustworthy candidate, just thirty-five per cent of respondents picked her, and fifty per cent chose her opponent. Given Trump’s long record of bankrupting companies, stiffing suppliers, exaggerating his net worth, and running a sham university that charged high fees to low-income people, this was a remarkable (and depressing) finding.\n",
            "But it needs putting in perspective. To come out on top in November, Clinton doesn’t need to transform herself into a beloved leader. She just needs to defeat Trump, who, by most measures, is even more unpopular than she is. Here again, the poll averages provide more reliable information than individual surveys. According to the Huffington Post’s poll average, Trump’s net favorability rating—that is, his favorable rating minus his unfavorable rating—is minus nineteen. Clinton’s figure is minus 14.6.\n",
            "Ultimately, of course, the race will come down not to polls but to the Electoral College and the outcome of voting in the battleground states. For these purposes, I’ll put eleven states in the battleground category: Colorado, Florida, Iowa, Michigan, Nevada, New Hampshire, North Carolina, Ohio, Pennsylvania, Virginia, and Wisconsin. According to the Real Clear Politics polling database, Trump is narrowly ahead in only two of these states: Iowa and Missouri. Clinton is up by five points or more in six: Colorado, Michigan, New Hampshire, Pennsylvania, Virginia, and Wisconsin. In the three remaining states—Florida, Ohio, and North Carolina—Clinton holds narrow leads.\n",
            "Mimicking the national trends, polls published last week showed the race tightening in Pennsylvania, Virginia, and Wisconsin. But Clinton was still ahead of Trump in these surveys, and her unfavorability ratings were better than his. In Pennsylvania, for instance, which is shaping up as a must-win state for the Republicans, a poll from Franklin & Marshall College showed Clinton leading Trump by seven percentage points: forty-seven per cent to forty per cent. Clinton’s net favorability rating was minus sixteen, which is pretty bad. But Trump’s net favorability rating was minus twenty-one.\n",
            "It should be noted that numbers like these have seldom, if ever, been seen before in a U.S. Presidential election. To many Americans, the election has come down to a choice between the unpalatable and the unthinkable. But in this strange and dystopian contest, Clinton retains a distinct advantage. In the weeks ahead, which will see three Presidential debates and, almost certainly, more surprises, we will find out if she can maintain it all the way to the finish line.\n",
            "WASHINGTON, DC — A view of Nationals Park from the Hampton Inn’s Top of the Yards rooftop bar on April 1, 2016, while the Nationals were playing the Minnesota Twins. (Photo by Fritz Hahn/The Washington Post)\n",
            "Rain is inevitable on Thursday as the latest in a conveyor of cold fronts charges through the region. But I am cautiously optimistic the Nationals will be able play their home opener.\n",
            "[Should the MLB schedule early-season games to avoid wintry weather?]\n",
            "The bulk of the rain should fall through early afternoon Thursday\n",
            "Models are in a good agreement that a slug of rain, possibly heavy for a time, will come through during the morning to early afternoon hours.\n",
            "Simulated radar high resolution NAM model 11 a.m. Thursday. (WeatherBell.com)\n",
            "Models *currently* don’t show much rain mid-afternoon into the evening\n",
            "After the morning-early afternoon rain passes, models are in a good agreement that any additional rainfall is light and inconsequential. The European and GFS models simulate less than 0.1 inches of rain between 2 and 8 p.m. Thursday.\n",
            "European model\n",
            "Rainfall forecast from European model 2 p.m. to 8 p.m. Thursday. (StormVistaWxModels.com)\n",
            "GFS model\n",
            "Rainfall forecast from GFS model 2 p.m. to 8 p.m. Thursday. (StormVistaWxModels.com)\n",
            "In other words, from mid-afternoon Thursday into the early evening, any rain showers should be widely scattered, short-lived, and probably won’t amount to much. Skies may even brighten.\n",
            "Could the timing of the rain shift between now and Thursday? Yes. Could showers be heavier and last longer than these models suggest? Sure.\n",
            "I wouldn’t say the game is in the clear at this point.\n",
            "What will temperatures and wind be like?\n",
            "The culprit for the rain is a strong cold front coming into the area, but temperatures probably won’t fall markedly until overnight Thursday, some time after the game.\n",
            "Temperatures during the game should starting off around 60 degrees and gradually fall back into the mid-50s by the late innings. If the sun manages to come out at all Thursday afternoon, it could be slightly warmer than this.\n",
            "European model forecast temperature at 5 p.m. Thursday. (StormVistaWxModels.com)\n",
            "Winds will be steady out of the west, at around 10-20 mph, but nothing too extreme the way it looks now.\n",
            "This forecast is low confidence, so keep an eye on our blog for updates over the next two days.\n",
            "“E-cigarettes can cause cancer”; “Vaping ‘no better’ than smoking”: headlines last week challenged the idea that electronic cigarettes are safer than conventional cigarettes, after findings emerged that their vapour damaged and killed human cells.\n",
            "Jessica Wang-Rodriguez, a head and neck cancer specialist at the University of California at San Diego, and her team found that cells lining human organs sustained up to twice the DNA damage seen in unexposed cells. They were also five to 10 times more likely to wither and die than unexposed cells even if the vapour contained no nicotine, the addictive ingredient in conventional and most electronic cigarettes.\n",
            "“Without the nicotine, the damage is slightly less, but still statistically significant compared with control cells,” says Wang-Rodriguez, who led the research.\n",
            "Advertisement\n",
            "Although the study garnered headlines around the world, researchers contacted by New Scientist have criticised it for its inability to properly compare the damage caused by smoke from conventional and electronic cigarettes.\n",
            "Relative harm\n",
            "“The relative harm compared to real smoking is the critical point here, since the majority of vapers use e-cigarettes to cut down or quit smoking,” says Marcus Munafò of the University of Bristol, UK. “That direct comparison is largely missing.”\n",
            "To assess what vaping does to human tissue, the researchers exposed cells to vapour from two brands of e-cigarettes every three days for between one to 8 weeks. However, with cigarette smoke they were only able to expose the cells for 24 hours before all the cells died.\n",
            "Because the cells were able to survive for far longer when exposed to vapour rather than smoke, the main outcome of the study is the opposite of what the media has reported: that cigarette smoke is far more toxic than e-cigarette aerosol, says Konstantinos Farsalinos of the University of Patras in Greece.\n",
            "“The comparisons were based on unequal treatments, without equivalent exposures for equivalent periods of time,” says John Britton, a toxicologist at the University of Nottingham, UK. Even if the time periods had been equal, the results would not necessarily have reflected real-life hazards, he says. The dose of vapour the cells received was equivalent to that from vaping for hours on end, a much higher dose than someone would typically get.\n",
            "More realistic would have been to compare samples of cells taken from the airways of people who use either e-cigarettes or real cigarettes, says Britton.\n",
            "Toxins from flavourings?\n",
            "One puzzle the results raise is why cells appeared to be damaged even by nicotine-free vapour. One possibility is that other toxins are created when flavourings are exposed to heat.\n",
            "“E-cigarette vapour is known to contain a range of toxins which include impurities in the e-cigarette liquids and toxins generated when solutions are heated to generate vapour,” says Britton. “Some are carcinogenic, so it’s likely some long-term users of e-cigarettes will experience adverse effects on their health, and the authors are correct to point out that these products should not be considered risk-free,” he says. But if smokers can’t give up completely, e-cigarettes are safer than smoking, he says.\n",
            "“Those of us reviewing the evidence are saying that when compared with tobacco smoking, e-cigarettes are a safer option, and I don’t think this new research detracts from that advice,” says Linda Bauld of the University of Stirling, UK.\n",
            "Wang-Rodriguez, however, urges vapers to be cautious. “They shouldn’t assume it’s a safe alternative to smoking,” she says. “We don’t really know all the harmful effects of vaping at this point, so I’d encourage users of both e-cigarettes and regular cigarettes to understand the consequences and stop using.”\n",
            "Journal reference: Oral Oncology, DOI: 10.1016/j.oraloncology.2015.10.018\n",
            "(Image credit: Sefa Karacan/Anadolu Agency/Getty)\n",
            "'Lion King' director Rob Minkoff is producing the animated feature inspired by Brooks' comedy classic 'Blazing Saddles.'\n",
            "Michael Cera, Samuel L. Jackson and Michelle Yeoh have joined the voice cast of Blazing Samurai, an animated feature inspired by Mel Brooks' comedy classic Blazing Saddles.\n",
            "Rob Minkoff, director of The Lion King and Stuart Little, will produce the animated comedy together with Yair Landau and Susan Purcell. Landau's Mass Animation shingle is producing with Huayi Brothers Media Corporation, Flying Tigers Entertainment and GFM Films, which is handling international sales at AFM.\n",
            "George Takei, Gabriel Iglesias, Djimon Hounsou, Aasif Mandvi, Sandra Tsing Loh and former Spice Girl Mel B. are also among the voice cast.\n",
            "Open Road Films has picked up U.S. rights to Blazing Samurai and plans a domestic bow on August 4, 2017. Sony Pictures International Releasing has picked up several territories and Huayi Brothers is distributing the film in Greater China. GFM Films is selling worldwide rights to buyers at AFM.\n",
            "Cera will play Hank, a dog who dreams of becoming a great warrior and saving the town of Kakamucho from becoming the litter box of a nefarious feline warlord. Jackson will voice Jimbo, a once-great cat samurai who has been hitting the catnip too hard for too long. He takes Hank under his wing and teaches him the way of the samurai. Brooks will voice Shogun, a misguided but benevolent leader.\n",
            "Helmers Mark Koetsier (Kung Fu Panda) and Chris Bailey (Alvin and the Chipmunks) will direct from a screenplay by Ed Stone and Nate Hopper.\n",
            "“There is no business like Shogun business,” quipped Brooks. The comedy legend will attend AFM Nov. 4 to showcase footage from Blazing Samurai to buyers, along with Minkoff and Landau.\n",
            "“Throughout the recording sessions from Mel Brooks to Mel B, this cast has not only met the humor, but raised the level of comedy and heart in this unique homage to Blazing Saddles,” said Landau. “Ed and Nate delivered a first-rate script in Blazing Samurai, a story about cats and dogs getting along that will appeal to everyone whether they’ve seen the original comedy classic or not.”\n",
            "Brooks is executive producing Blazing Samurai along with Reginald Hudlin and Pietro Ventani.\n",
            "It has been a major recruiting weekend for Oklahoma with the 2016 class, but the Sooners have added one more piece to their 2015 puzzle.\n",
            "Junior college wide receiver Jarvis Baxter confirmed Friday night he is going to be a walk-on at OU with hopes of earning a scholarship either during the season or next semester.\n",
            "Baxter, who is 5-foot-11, was ranked No. 43 in the Scout juco 100 last season and had signed with South Florida. He was released from his letter of intent with USF two weeks ago when it became clear he was not going to qualify academically.\n",
            "Baxter, however, will make OU’s requirements and said he is going to report to Norman on Tuesday with everybody else.\n",
            "Baxter said USF only allows someone to use nine credit hours in the summer to meet the required GPA. Baxter took more than nine hours but those extra hours couldn’t count for USF but do count for OU. OU allows for more than nine hours in the summer, which is what Baxter needed to reach the required GPA.\n",
            "Baxter was a star at Trinity Valley where he played against fellow OU juco transfer Dede Westbrook. That’s where it all started. Once Baxter talked with Westbrook, Baxter called offensive coordinator Lincoln Riley about joining OU.\n",
            "“I talked with him, and he said they have a walk-on spot at wide receiver open,” Baxter said. “I talked to my family about it and decided this would be the best thing for me to do.\n",
            "“I’m going to have a chance to earn it during the season. Pretty sure I’ll end up with that scholarship because nobody is going to work harder than me.”\n",
            "Baxter visited OU initially for the 2011 OU-Texas A&M game. He remembers it well and said OU has been his dream school ever since.\n",
            "“I got an autograph from Corey Nelson,” Baxter said. “I talked with Jamell Fleming. I wanted to get his gloves, but he said he couldn’t do that.”\n",
            "In the last week Baxter has started to receive full scholarship offers from other schools, but in the end, decided OU was the place to be.\n",
            "“Because of the institution,” Baxter said. “The kind of school OU is. It’s a great community, and I can get a great education.”\n",
            "Baxter said he is a 3-for-2 kid with three years to play two seasons’ worth of eligibility.\n",
            "Almost 400 people have died in clashes between security forces and Rohingya Muslims in Burma, the country's military commander has said.\n",
            "The numbers, posted on the military's official Facebook page, are a sharp increase on the previously reported toll of just over 100. The statement said all but 29 of the 399 dead were insurgents, whom it described as terrorists.\n",
            "The statement said there had been 90 armed clashes including an initial 30 attacks by insurgents on 25 August, making the combat more extensive than previously announced.\n",
            "We’ll tell you what’s true. You can form your own view. From 15p €0.18 $0.18 $0.27 a day, more exclusives, analysis and extras.\n",
            "Advocates for the Rohingya, an oppressed Muslim minority in overwhelmingly Buddhist Burma, say hundreds of Rohingya civilians have been killed by security forces.\n",
            "According to the UN, some 38,000 have fled into neighbouring Bangladesh.\n",
            "It comes after Chris Lewa, director of the Arakan Project pressure group, told ABC: \"So far reports—I think quite credible—mention about 130 people including women and children killed.\n",
            "\"That happened on Sunday when suddenly security forces cordoned [off] the whole area, together with Rakhine villagers. It seems like this has been a major massacre in Rathedaung.\"\n",
            "The latest violence follows an attack by Rohingya insurgents on police posts in the remote region, prompting a huge military crackdown.\n",
            "The insurgent group that claimed responsibility for last week's attacks, the Arakan Rohingya Salvation Army, said it acted to protect Rohingya communities.\n",
            "Shape Created with Sketch. Rohingya mothers face persecution Show all 10 left Created with Sketch. right Created with Sketch. Shape Created with Sketch. Rohingya mothers face persecution 1/10 Ramida Begum holds her 10-day-old daughter in their shelter in Kutupalang, an unregistered refugee camp in Cox's Bazar, Bangladesh. 'The military caught my husband and burnt our house down a week before I left Myanmar. Since then I don't know whether my husband is dead or alive' Reuters 2/10 Minara Begum sits inside the shelter, cradling her one-month-old son Ayub. Minara fled to Bangladesh from Nasha Phuru village in Myanmar with her husband and mother-in-law. 'My child doesn't get enough breast milk as I don't eat enough nutritious food. I have to buy milk powder from local market though it's not very good for my son' Reuters 3/10 Amina, pictured with her 16-day-old daughter Sumaiyin, is in a refugee camp Balukhali that neighbours Ramida and Minara's. 'One and a half months ago the military came to our village and kept firing their guns. I ran away with my neighbours to save our lives. You see us alive here only because the God was so kind. They caught my uncle and my younger brother and we don't know whether they are dead or alive' Reuters 4/10 Fatema sits beside her one-day-old daughter Aasma in Kutupalang. Fatema fled to Bangladesh from Jambuinna village in Myanmar two months ago after her house was burnt down by the military. She crossed Naf River by boat during the night. 'Our situation is better than many other refugees as my husband Mohammad Alom works here as a day labourer. Many of the new refugees have no work here, so they have to rely on relief' Reuters 5/10 Jamalida cradles her two-month-old daughter Shahida. Jamalida came to Bangladesh with her husband from Nasha Phuru village in Myanmar Reuters 6/10 Rehana Begum lays her one-day-old daughter in front of her inside their tarpaulin shelter. Rehana fled her village of Jambuinna in Myanmar three months ago. 'We were in our home and suddenly the military came to our village and started shooting. When we heard the sound of gun shots we immediately went to our relatives. We walked for four hours without any food and water to reach the border at 1 a.m. We paid 25,000 Myanmar kyat (£14) to a broker to cross.' Intercepted by Bangladesh border guards, Rehana's family narrowly escaped being sent home. 'They wanted to send us back, but then we heard gunshots from the Myanmar side and the guards released us, saying, \"Stay in Bangladesh and save your lives\"' Reuters 7/10 Noor Begum sits next to her one-day-old daughter Sumaiya as she stares into the camera. Noor came to the camp one-and-a-half months ago from Nagpura village with her husband Jahangir Reuters 8/10 Rajuma Begum observes her one-month-old son Raihan. 'I fled to Bangladesh because of fear, because I needed to save my children. I was pregnant and suffering from fever while crossing the border. I also have an 11-month-old boy, so it was very difficult to reach the border from our village Wabek in Myanmar. I had to rest frequently. After six hours of horrible walking we finally reached the border at 2am and crossed after paying a broker' Reuters 9/10 Eighteen-year-old Asmot Ara rests her newly born daughter on her lap. Asmot said she came to the camp one month ago with neighbours from Nagpura village. In Myanmar her father-in-law was killed and their home burnt down by the Myanmar military Reuters 10/10 As Marijaan holds her 25-day-old daughter Noor Habi, her son peers over her shoulder. Marijaan fled to Bangladesh from Khyeri Prang village in Myanmar one month ago after her house was burnt down by the Myanmar military. 'I reached the border at night and crossed by the boat. I paid the boatman to cross the Naf River' Reuters 1/10 Ramida Begum holds her 10-day-old daughter in their shelter in Kutupalang, an unregistered refugee camp in Cox's Bazar, Bangladesh. 'The military caught my husband and burnt our house down a week before I left Myanmar. Since then I don't know whether my husband is dead or alive' Reuters 2/10 Minara Begum sits inside the shelter, cradling her one-month-old son Ayub. Minara fled to Bangladesh from Nasha Phuru village in Myanmar with her husband and mother-in-law. 'My child doesn't get enough breast milk as I don't eat enough nutritious food. I have to buy milk powder from local market though it's not very good for my son' Reuters 3/10 Amina, pictured with her 16-day-old daughter Sumaiyin, is in a refugee camp Balukhali that neighbours Ramida and Minara's. 'One and a half months ago the military came to our village and kept firing their guns. I ran away with my neighbours to save our lives. You see us alive here only because the God was so kind. They caught my uncle and my younger brother and we don't know whether they are dead or alive' Reuters 4/10 Fatema sits beside her one-day-old daughter Aasma in Kutupalang. Fatema fled to Bangladesh from Jambuinna village in Myanmar two months ago after her house was burnt down by the military. She crossed Naf River by boat during the night. 'Our situation is better than many other refugees as my husband Mohammad Alom works here as a day labourer. Many of the new refugees have no work here, so they have to rely on relief' Reuters 5/10 Jamalida cradles her two-month-old daughter Shahida. Jamalida came to Bangladesh with her husband from Nasha Phuru village in Myanmar Reuters 6/10 Rehana Begum lays her one-day-old daughter in front of her inside their tarpaulin shelter. Rehana fled her village of Jambuinna in Myanmar three months ago. 'We were in our home and suddenly the military came to our village and started shooting. When we heard the sound of gun shots we immediately went to our relatives. We walked for four hours without any food and water to reach the border at 1 a.m. We paid 25,000 Myanmar kyat (£14) to a broker to cross.' Intercepted by Bangladesh border guards, Rehana's family narrowly escaped being sent home. 'They wanted to send us back, but then we heard gunshots from the Myanmar side and the guards released us, saying, \"Stay in Bangladesh and save your lives\"' Reuters 7/10 Noor Begum sits next to her one-day-old daughter Sumaiya as she stares into the camera. Noor came to the camp one-and-a-half months ago from Nagpura village with her husband Jahangir Reuters 8/10 Rajuma Begum observes her one-month-old son Raihan. 'I fled to Bangladesh because of fear, because I needed to save my children. I was pregnant and suffering from fever while crossing the border. I also have an 11-month-old boy, so it was very difficult to reach the border from our village Wabek in Myanmar. I had to rest frequently. After six hours of horrible walking we finally reached the border at 2am and crossed after paying a broker' Reuters 9/10 Eighteen-year-old Asmot Ara rests her newly born daughter on her lap. Asmot said she came to the camp one month ago with neighbours from Nagpura village. In Myanmar her father-in-law was killed and their home burnt down by the Myanmar military Reuters 10/10 As Marijaan holds her 25-day-old daughter Noor Habi, her son peers over her shoulder. Marijaan fled to Bangladesh from Khyeri Prang village in Myanmar one month ago after her house was burnt down by the Myanmar military. 'I reached the border at night and crossed by the boat. I paid the boatman to cross the Naf River' Reuters\n",
            "Burma's leader, Aung San Suu Kyi has said the \"terrorist\" attacks were \"a calculated attempt to undermine the efforts of those seeking to build peace and harmony in Rakhine state\".\n",
            "The Burmese government has repeatedly denied claims the Rohingya are facing genocide. It previously brushed away evidence of human rights violations as fake news and \"propaganda\".\n",
            "Bangladeshi border guards have tried to keep out the fleeing Rohingya, but thousands could be seen on Friday making their way across muddy rice fields.\n",
            "Young people helped carry the elderly, some on makeshift stretchers, and children carried newborns.\n",
            "Some, carrying bundles of clothes, cooking utensils and small solar panels, said they had walked at least three days to get to the border.\n",
            "Sham Shu Hoque, 34, crossed the border with 17 family members. He said he left his village of Ngan Chaung on 25 August after it was attacked by Burmese security forces who shot at the villagers. He said troops also used rocket-propelled grenades, and helicopters fired some sort of incendiary device.\n",
            "Five people were killed in front of his house, he said. His family survived the attack but was told by the soldiers to leave. They took a week to reach Bangladesh, hiding in villages along the way, he said.\n",
            "Most of Burma's estimated 1 million Rohingya live in northern Rakhine state. They face severe persecution, with the government refusing to recognize them as a legitimate native ethnic minority, leaving them without citizenship and basic rights.\n",
            "Additional reporting by agencies\n",
            "We’ll tell you what’s true. You can form your own view.\n",
            "At The Independent, no one tells us what to write. That’s why, in an era of political lies and Brexit bias, more readers are turning to an independent source. Subscribe from just 15p a day for extra exclusives, events and ebooks – all with no ads.\n",
            "Subscribe now\n",
            "Ahead of England's World Cup qualifier against Poland, former Juventus player and Polish legend Zbigniew Boniek decided it was the perfect time to show he can still take shots better than Andy Carroll can.\n",
            "From the Telegraph:\n",
            "Scroll to continue with content Ad\n",
            "\"My [eight-year-old] grandson Mateo moves better on the pitch than Andy Carroll,\" said Boniek. \"Mateo is very good at golf and tennis. He has better co-ordination.\"\n",
            "Well, OK then. Either Boniek is trying to get Liverpool to buy his grandson for £35 million or he really doesn't think very much of Andy Carroll. Maybe this is just his roundabout way of suggesting that Carroll take up golf and tennis?\n",
            "Whatever the case may be, Boniek has an impressive mustache and Andy Carroll has no mustache at all. Advantage: Boniek.\n",
            "The orientation of a diagram on the page of a textbook may seem inconsequential, but it can have a significant impact on a reader's ability to comprehend the information as presented, according to a team of researchers at UC Santa Barbara, Vanderbilt University, and Western Carolina University. Their findings appear in a recent issue of the journal Bioscience.\n",
            "Focusing on variously formatted cladograms –– also known as phylogenetic trees –– the researchers found that two diagrams may contain the same information, but they aren't necessarily equivalent in terms of how the information is interpreted. \"In Western culture, we read from left to right, so we naturally come to a diagram with that behavior,\" said Andrew T. Stull, a researcher in the Department of Psychological & Brain Sciences at UCSB and an author of the paper. \"The important point in this research, however, is that how efficiently a student comprehends the information presented in the phylogenetic tree depends on how the tree is angled.\"\n",
            "As it turns out, when a diagonal tree extends from tips on the left to the root on the right, and the trunk angles downward to the right-hand side, the information is more easily accessible. \"The way we interrogate the tree is first culturally based –– left to right –– and the strong diagonal line tends to make us flow one way or another,\" said Stull. \"But that combined effect influences the accuracy, or how we're able to use the tree effectively.\"\n",
            "However, most textbooks depict the diagonal cladogram in the upward orientation, Stull noted. \"Many artists draw the diagram in an inefficient and potentially confusing way,\" he said. \"Artists have a tendency to draw it at the upward angle, not realizing they'd communicate the information better if they angled it downward.\"\n",
            "The researchers used a phylogenetic tree for their research because it is very important for a process called tree thinking. \"It's the idea that from an evolutionary perspective, there is a distinctive relationship between taxa,\" said Stull. \"It's not just that things line up together on a tree, but you can infer certain biological, physiological, and pharmaceutical commonalities that might be relevant. There are a lot of things you can do in knowing how all of life is organized, and each organism's relation to everything else.\"\n",
            "Drawing them in tree form, Stull continued, should help teach students the relationships between organisms, and to anticipate the valuable information those relationships can provide.\n",
            "The researchers used eye-tracking technology to carry out their research. They showed test subjects one tree, and then another, and asked them to determine whether or not they were the same. \"In order to answer the question, they had to interpret the two images,\" Stull explained. \"Then we took all the eye positions. What we found is that when people studied the tree with the upward diagonal trunk, they were less accurate than when the tree followed the downward diagonal.\"\n",
            "Why the directional angle makes a difference may have to do with how organisms represented by the individual branches relate to their closest common ancestor and to those with a more distant common ancestor. \"With upward angled trunks, it may be because they [students] are thinking of it from the root up,\" Stull said. \"But it's more efficient to think of it from the branches down. So, from an artistic perspective, it makes more sense to build it that way. With that orientation, the user doesn't have to deconstruct it in order to access the information.\"\n",
            "Stull's co-authors include Laura R. Novick, associate professor of psychology and human development at Vanderbilt University; and Kefyn M. Catley, a professor of evolutionary biology at Western Carolina University.\n",
            "[RETURN TO TOP]\n",
            "† Top image: Although images in textbooks generally represent phylogenetic trees with trunks angling up and to the right, research shows that students have better comprehension when the trunks angle down to the right.\n",
            "BioScience\n",
            "“Now the repute of thy might endures for a space; straightaway again shall age, or edge of the sword, part thee from thy strength, or the embrace of fire, or the surge of the flood, or the grip of the blade, or the flight of the spear, or hateful old age, or the gleam of eyes shall pass away and be darkened; on a sudden it shall come to pass that death shall vanquish thee, noble warrior.” (Beowulf, xxvi)\n",
            "“Teach us to number our days aright, that we may gain a heart of wisdom.” (Psalm 90:12)\n",
            "Why tell a life story? Why recount a person’s deeds? There are times, of course, when we tell the stories of infamous deeds, and these we recount to inspire a warning. But most often we tell stories to inspire greatness. Epic deeds paint grand pictures for our emulation, and the goal of these mighty deeds, planted in our hearts and minds through the stories we absorb, is to bear fruit in our lives. Soak in anemic, empty stories and the fruit in your life will be anemic and empty; saturate yourself in stories with rich and nutritious stuff—even stories which you don’t fully understand—and the fruit in your life will be rich and nutritious. Such a story is Beowulf—no mere epic of swords, golden rings, and monsters, but a powerful, richly nutritious tale for the mythic soul of man, written to inspire us to be better men ourselves.\n",
            "But there is a twist. Where we might expect a story about how to be better men to focus on life, Beowulf is a story about death; it is a tale not of living well, but of dying well. And this makes good sense, because in the ethical economy of Beowulf’s world how you live is closely—nay, intimately—intertwined with how you die. The measure of the man is determined by how well he faces death. Here the ethics of the ancient world are at odds with our modern one, because death is a subject we are particularly at pains to ignore. Thus, when we turn and apply the examining light of ancient literature to our own lives, the results are both stark and uncomfortable.\n",
            "Beowulf’s tale is both short and simple. An evil creature, Grendel, is terrorizing the subjects of King Hrothgar. Beowulf arrives to challenge the beast in a mighty contest. He waits at night for the fell creature to arrive, then slaughters it and wins fame for himself and his king. But the deed is not yet done—soon thereafter the mother of Grendel comes to wreak more evil, but Beowulf chases her down, takes her life and sets the people free from terror, earning gold and fame in the process. This, however, is not where Beowulf’s story ends. Years later Beowulf has become a mighty king in his own right, when a dragon, awoken by the greed of men, begins to terrorize his people. Alone, and knowing he will die, Beowulf pursues and eventually kills the dragon, losing his life in the process.\n",
            "How does the ethic of dying well run throughout this story? There are three currencies in the world of Beowulf: gold, fame, and your life. Mighty (that is, noble and good) men perform mighty deeds (they wager their lives) in order to earn gold and fame. We are tempted to think that accumulation is the goal of these wagers—long life, much gold, and great fame. But Beowulf’s poet wants us to know the grave danger embedded in each of these: namely, to think that these currencies are ends in themselves, to forget that death comes to all. For what does a man take with him when he dies? Does his fame go with him to the grave? Does he carry his gold with him? No. And so the man who fails to use these currencies rightly is an unjust man. The man who forgets the approach of death, and lives in cowardice merely to preserve his property, is in the ethic of Beowulf doomed.\n",
            "“Now the repute of thy might endures for a space,” admonishes King Hrothgar, because “on a sudden it shall come to pass that death shall vanquish thee, noble warrior.” Death sits at the door—do not trust in your wealth or fame! He may as well have quoted Moses: “Teach us to number our days aright, that we may gain a heart of wisdom.” Under this ethic, when death is foremost in our minds, our relationship to the material subjects of our lives is revealed: what you do with your gold, and what you do with your reputation, and what you do with your life, become paramount concerns. These things cannot be kept: they must therefore be used. Noble men spend their gold, their fame, and their lives wisely. Cowardly men do not.\n",
            "From within this, the story of the Beowulf’s contest with the dragon becomes the high point of this struggle. The dragon is an image of greed—it hoards its gold and does not share it. And in this terrible image we ought to see ourselves; we are tempted, even now, to keep and hoard our gold; to be deceived by the allure of wealth into thinking that the more we have, the better off we will be; that the measure of a man is in the accumulation of his possessions. Beowulf preaches the opposite ethic. It is not in possessing, but in giving, that a man is revealed. And hence the dragon must be destroyed.\n",
            "Beowulf knows that taking this task will not earn him earthly fame—a contrast to his struggle with Grendel. There he stood to win gold and fame through that mighty deed. With the dragon, however, there will be neither wealth nor fame. There is only the deed. And here the character of Beowulf is proved once for all: is he a mercenary, we ask, out only for gain? By no means! “Then for the first time,” the poet observes, “he had to show his strength without Fate allotting him fame in battle” (Beowulf, xxxv). An action undertaken without the promise of earthly reward—an action, that is, of self-sacrifice—is thus the most noble of all.\n",
            "Beowulf takes eleven companions with him to fight the dragon, and here the parallels to the Passion of Christ should not be overlooked: Jesus, of course, had twelve disciples, but one (Judas) abandoned the ranks before his passion. Ten of Beowulf’s companions abandon him in cowardice; one, Wiglaf, remains to fight at his master’s side. Ten of Jesus’ remaining disciples also abandoned him—but John alone remained. Thus, as Jesus goes on to fight the dragon of human sin alone, so Christlike Beowulf advances on the dragon of greed alone—a final, brave act to display the grand selflessness of true manhood.\n",
            "Consequently, faithful Wiglaf becomes our stand-in. He is our way to enter into the story of Beowulf. He models for us how we are to respond to the tales of mighty, selfless deeds—that is, with mighty, selfless deeds of our own. Will we be the loyal servants, or the cowardly earls? The poet has no qualms identifying which he thinks is the right path, and Wiglaf declares that:\n",
            "God knows that, as for me, I had much rather the flame should embrace my body with my gold-giver. It does not seem fitting to me, that we should bear shields back to our dwelling, if we cannot first fell the foe, guard the life of the prince of the Weders. I know well that, from his former deeds, he deserves not to suffer affliction alone among the warriors of the Geats, to fall in fight; sword and helmet, corslet and shirt of mail shall be shared by us both. (Beowulf, xxxvi)\n",
            "But of those who ran, he only says this: “Death is better for all earls than a shameful life” (Beowulf, xxxix).\n",
            "How you spend your wealth, how you spend your fame, and how you spend your very life are, according to the ethics of Beowulf, the factors that determine the ultimate value of your life. It is the knowledge of death that determines your choices and actions in the present. Keep your death in mind, and you will make right choices about the currencies you possess. This is a critical voice we continually need to hear—especially in an era which praises what Beowulf’s poet would surely see as the cowardly determination to preserve life, rather than the righteous goal to spend your life-currency justly. To Beowulf, a good death is better than a long life in cowardice. This is a reminder we desperately need, for in this nothing has changed: as with Beowulf, death comes to us all. It is an engagement none of us can avoid. And when death arrives your wealth, your reputation, and (of course) your life cannot go with you. What you have not spent will be accounted to you as waste. Therefore learn to spend your life, your wealth, and your reputation rightly, in the now. Make a study of selflessness and right living. Learn from the ancients how to be a man. Read Beowulf.\n",
            "Pulling a rabbit out of a hat, or tricks of a similar ilk, may have dazzled generations of children.\n",
            "But the future of live magic could be under threat, thanks to the rise of the YouTube magician.\n",
            "The rising popularity of online videos is leading to a loss of key skills which could damage the very existence of live magic in years to come, a leading Magic Circle member has warned.\n",
            "Jamie Raven, who belongs to the prestigious inner magic circle, said amateur magicians are increasingly learning their trade behind the camera, perfecting one trick on film to post for \"likes\".\n",
            "But while YouTube and social media may spark an interest in magic, he warned essential skills such as interacting with real people to control or misdirect their attention are being lost.\n",
            "If the current obsession for online profile continues, he said, in generations to come \"there will be no magic shows, there will be no live interaction\".\n",
            "Simon Rex and Ashley Tisdale star in the latest and worst addition to the “Scary Movie” franchise. (Photo by Quantrell D.Colbert)\n",
            "“Scary Movie 5” must be some kind of psychological experiment. Perhaps the filmmakers sat in a lab, rubbing their temples while wondering aloud how awful they could make a movie and still score at the box office.\n",
            "It’s the only reasonable explanation for this lazy, boring, vile and tragically unfunny attempt at a horror-film spoof that is sure to kill brain cells and may signal the impending apocalypse. It’s the kind of movie that leaves audience members partially lobotomized, exiting the theater and muttering absently to themselves, “How on earth did this movie get made?”\n",
            "The answer: cheaply. Each of the increasingly terrible “Scary Movie” installments has enjoyed an impressive return on investment, so it’s only logical that producers want to keep making them, quite literally, ad nauseam. The cost-cutting is evident, between the not-so-special effects and the amateur editing errors, including a casual relationship between dialogue and moving mouths.\n",
            "Thirteen years after the first “Scary Movie,” this new chapter follows the same general template, with one exception: This incarnation kicks off with Charlie Sheen and Lindsay Lohan, playing themselves, preparing to make a sex tape.\n",
            "From there, the story skips to couple Dan (Simon Rex) and Jody (Ashley Tisdale), who inherit a trio of youngsters after the demise of the kids’ dad (Sheen). What follows shouldn’t be confused with a plot; it’s more a series of snippets in which the characters borrow familiar story lines from other movies, including “Mama,” “Inception,” “Black Swan” and “Rise of the Planet of the Apes.”\n",
            "But “Scary Movie” doesn’t really put any kind of twist on these familiar plots, so much as present them in some brainless context. But stupid doesn’t necessarily equate to funny, and that goes for Heather Locklear, dressed as a pregnant ballerina whose water breaks all over a male dancer’s face as he lifts her above his head.\n",
            "The movie is so appalling that even a film fan who guffawed her way through “The Aristocrats” would feel nothing but a deep emptiness as the end credits begin to roll, wondering if one solid joke was too much to ask from a movie that bills itself as comedy.\n",
            "But no, there’s nothing worthy of even a small smirk. Instead, the movie serves up violence against man, woman, child, doll and ape, not to mention sexual situations involving a German shepherd, a hairy, overweight nanny, Santa Claus, an automatic swimming pool cleaner, a pony and a microwave. A knock-knock joke told by a 6-year-old would be funnier than this endless stream of flatulence jokes, feces-infused food items and gratuitous vomiting.\n",
            "For some reason, watching this movie kept conjuring up the voice of Smokey Bear intoning “Only YOU can prevent forest fires.” It must be a sign from beyond that we — the film-going public — have the power to end this. As long as people keep watching these inept films, the “Scary Movie” franchise will thrive and reproduce. And that is a truly terrifying prospect.\n",
            "No stars\n",
            "PG-13. At area theaters. Contains crude language, brief nudity, sexual situations, allusions to drug use and gory, if obviously fake, violence. 85 minutes.\n",
            "New Delhi (CNN) — Air India, a debt-burdened, state-run carrier, is trying to shed some extra flab.\n",
            "The airline has asked 125 of its flight attendants to lose a few pounds or get ready for an airport job.\n",
            "l e v a r t\n",
            "\"It is an opportunity for them to bring themselves back to the (required) fitness level. If they cannot because of any medical reasons, they will be offered ground duties,\" Air India spokesman G.P. Rao told CNN Tuesday.\n",
            "The decision follows fitness guidelines laid out by India's civil aviation regulator.\n",
            "Last year, the regulator mandated a body mass index (BMI) of 18-25 for male cabin crew members and 18-22 for female cabin crew members. Men with a BMI of 25-29.99, and women with a BMI of 22-27, were classified as overweight.\n",
            "'Safety issue'\n",
            "Some say the index, which is determined by a person's height-to-weight ratio, is not always an accurate indicator of someone's health and body fat percentage.\n",
            "But the airline insists the move to manage its employees' physique is not about appearances.\n",
            "\"It's a safety issue,\" Rao said. \"The crew has to be fit to be able to carry out their inflight duties, including emergencies.\"\n",
            "Rao, however, didn't say how much time the shortlisted 125-odd crew would have to slim down.\n",
            "He added this is not the first time Air India has advised its flight attendants to stay \"fit.\"\n",
            "l e v a r t\n",
            "\"This is an ongoing process. We have been doing this exercise for quite some time,\" he said.\n",
            "Air India currently employs more than 3,000 cabin crew members. According to a Times of India report , a \"large number\" of the employees refused to undergo medical examinations for their BMI as ordered by the company back in 2013. Instead, they asked the airline to first pay for gym memberships before conducting any lab tests, the report said.\n",
            "The NHL won’t play in an outdoor game in California this year. But you can still get your AHL hockey fix on the West Coast.\n",
            "The league announced it will play an outdoor game at Raley Field (a baseball stadium) in West Sacramento, Calif. The contest will include the Bakersfield Condors and the Stockton Heat. The Condors are the Edmonton Oilers’ newly affiliated AHL team. The Heat are with the Flames. The game will take place on Dec. 18.\n",
            "In case you were wondering, the average high in Sacramento in December is 54 degrees. The average low is 38 degrees. Perfect weather for hockey, eh?\n",
            "Scroll to continue with content Ad\n",
            "[Yahoo Sports Fantasy Hockey: Sign up and join a league today!]\n",
            "The event will just be one part of the “Biggest Show on Snow” which is a six-week “holiday extravaganza” at Raley Field.\n",
            "This will be the eighth outdoor game in AHL history according to the release. The last one was at Comerica Park in Detroit on Dec. 30, 2013 as part of the Winter Classic.\n",
            "Also, there will apparently be something called a “Monster Ice Slide” which sounds amazing. This reminds me of the brown snow slide at the Columbus All-Star Game, which we here at Puck Daddy may or may not have tested.\n",
            "- - - - - - -\n",
            "Josh Cooper is an editor for Puck Daddy on Yahoo Sports. Have a tip? Email him at puckdaddyblog@yahoo.com or follow him on Twitter!\n",
            "MORE FROM YAHOO SPORTS\n",
            "Introduction\n",
            "Specifications\n",
            "Lian Li DK-02X CASE TYPE: Mid-Tower MATERIAL: Aluminum & Tempered Glas WEIGHT: 45kg SLOTS: System 1: 2\n",
            "System 2: 8 DRIVE BAYS: 1x External Slim ODD x1 or Internal 2.5\" HDD x1\"\n",
            "System 1: 8x Internal 2.5/3.5\"\n",
            "System 2: 9x Internal 2.5/3.5\" MOTHERBOARD\n",
            "FORM FACTORS: System 1: Mini-ITX\n",
            "System 2: Micro-ATX, ATX. XL-ATX, E-ATX, HPTX DIMENSIONS (WxHxD): 1250 x 805/835 x 600 mm\n",
            "Keyboard tray: 1150 x 50 x 240 mm FRONT DOOR/COVER: N/A FRONT FANS: 120mm x 4 REAR FANS: 120mm x 2 (pre-installed) TOP FANS: N/A BOTTOM FANS: N/A SIDE FANS: 120mm Fan x 3 on each side (included) I/O for each System: 3x USB 3.0\n",
            "1x Headphone\n",
            "1x Microphone FAN/LED Controller: N/A Compatibility System 1: CPU Cooler: 175mm\n",
            "GPU: 300mm\n",
            "PSU: 160mm Compatibility System 1: CPU Cooler: 175mm\n",
            "GPU: 400mm\n",
            "PSU: 400mm\n",
            "I would like to thank Lian-Li for supplying the review sample.Lian Li is known for making some very elaborate aluminum cases, and the company has also offered actual work desks made of the material in the past. But those were large, not very fashionable, and geared toward those who wanted something fancy in their office. The DK range of enclosures come in different sizes and offers a cleaner, simpler look that works in an office, or as a reception desk in a more public surrounding. While we have reviewed fairly traditional Lian Li enclosures in the past, the DK-02X represents an extreme departure into what could be done while still offering some viable functionality.The \"X\" in the \"DK-02X\", like all of Lian Li's cases signifies that the chassis is anodized black inside and out. As of now the DK-02X and the DK-02 are the same case. This might change if Lian Li comes out with a silver version (which would be named DK-02A), but there are no plans for this at this time.\n",
            "\"Food stamps\" arrive in Britain next month, when tens of thousands of vulnerable people will be issued with food vouchers in lieu of money to tide them over short-term financial crises.\n",
            "Rather than, as now, offering a cash loan, most councils will from April offer new applicants who qualify for emergency assistance a one-off voucher redeemable for goods such as food and nappies.\n",
            "Many of the 150 local authorities in England running welfare schemes have confirmed that they will issue the vouchers in the form of payment cards, which will be blocked or monitored to prevent the holder using them for alcohol, cigarettes or gambling.\n",
            "Several plan to issue charity food parcels to people applying for crisis help, and are preparing to give cash grants to food banks to enable them to take on full-time staff and increase opening hours.\n",
            "Each authority has drawn up eligibility rules, setting out who will qualify for crisis help and the conditions under which it will be given. One plans to make emergency help conditional on good behaviour.\n",
            "The shift to in-kind and voluntary assistance follows the decision last year to abolish the government-run social fund and to replace it with more than 150 welfare assistance schemes, operated by English local authorities and the Welsh and Scottish governments.\n",
            "The social fund – known as the \"backstop\" of the welfare system – typically offered small loans of about £50, repayable against future benefits, to help vulnerable individuals who faced short-term crises as a result of having cash stolen or benefits delayed.\n",
            "A separate set of cash grants, typically worth about £1,000, was made to people with a disability, ex-prisoners and victims of domestic violence, to enable them to buy or replace items that would help them live independently, such as beds, clothing and kitchen utensils.\n",
            "Although social fund spending represents a relatively tiny chunk of the social security bill, there is concern that the new arrangements will for the first time build into mainstream welfare provision the distribution of food voluntarily donated by the public, schools and businesses.\n",
            "Lady Lister, a Labour peer and poverty expert, said the shift from cash loans to in-kind help would leave the most vulnerable people \"high and dry\".\n",
            "\"The social fund was a safety net under the safety net,\" Lister said. \"I do not call putting money into food banks a safety net.\"\n",
            "Some fear the use of in-kind vouchers will repeat the shortcomings of cashless payment cards, issued to asylum seekers. Critics said these cards left users unable to buy essential non-food items, and made them more likely to turn to risky or criminal ways of obtaining cash.\n",
            "One welfare charity worker said: \"There's a lot of naivety. The social fund is big, and meets a whole range of needs. There's going to be an awful lot of people that will need to tap into its successor.\"\n",
            "But councils say huge reductions, in some cases cuts of up to a third, in the amount allocated to support people in hardship have left them with no option but to offer vouchers, refer applicants to food banks and secondhand furniture projects, and to drastically tighten eligibility. The government spent £230m on the social fund in 2009-10 but has allocated £178m to local authorities for 2013-14.\n",
            "Inquiries by the Guardian found that:\n",
            "• Conservative-run Hampshire council plans to invest a big chunk of its welfare fund allocation in charities and food banks. Over time, it hopes to stop offering food vouchers as part of a shift towards \"reducing the entitlement culture\".\n",
            "• Labour-run Manchester city council will offer successful applicants low-interest loans of up to £200 a year, with a credit union, rather than food vouchers. It says in future years grants for furniture and cooking utensils will be offered on condition that recipients sign up to \"expected behaviours and actions\".\n",
            "• Bristol city council's crisis fund restricts emergency payments to food, heating, nappies and toiletries. It says the cards \"should not be used for cigarettes, alcohol or entertainment\", and if misuse occurs it will seek repayment.\n",
            "• Labour-controlled Darlington council plans to invest £58,000 in a church food bank, including £30,000 to enable the charity to take on a full-time worker.\n",
            "From April, thousands of applicants who now have access to crisis help will be turned down under the schemes. Many councils plan to refer the expected rising numbers of unsuccessful applicants to soup kitchens and other charities.\n",
            "Alison Garnham, chief executive of the Child Poverty Action Group, said: \"Local authorities have been given a difficult task, to deliver support on a reduced budget at a time of rising need.\n",
            "\"But we are seriously concerned that some authorities will not be providing any access to cash to families to meet their essential needs, and may be offering support in a way that serves to stigmatise those who need it.\"\n",
            "Others have warned that people who are turned down for crisis help will turn to crime, begging or loan sharks. Almost all authorities are bracing themselves for an expected rise in demand for crisis support from April, when the bulk of the benefit reforms, aimed at saving £18bn, are introduced. Among these is the so-called bedroom tax.\n",
            "There is also nervousness that any glitches in universal credit, from October, will see an increase in poorer households seeking help from welfare schemes.\n",
            "The government, and some charities, have argued that the existing system of crisis loans was abused by people – often young men – who did not use the loans for genuine emergencies. They argue the new system will discourage dependency, more efficiently directing scarce resources at the people who most need them.\n",
            "The Guardian also found that:\n",
            "• The cost of administering each of the 150-plus new welfare assistance schemes is typically equivalent to around 20% of the value of the entire local fund. Several authorities, including the Welsh government, have outsourced the running of the voucher schemes to private contractors.\n",
            "• Local authorities are worried that the new patchwork of welfare assistance systems will lead to a postcode lottery, with vulnerable people moving to apply for crisis help in more \"generous\" boroughs.\n",
            "• There are concerns that some welfare systems will not be ready by 1 April. The Furniture Re-use Network said a survey showed two-thirds of its members believed the new system would not be in place in time. There are concerns that, despite huge growth in the numbers of food banks in the past two years, many parts of the country will have little charity food assistance capacity.\n",
            "As the Braves prepare to enter the Division Series, I want to return to two controversial incidents at the end of their regular season, when they embroiled themselves in two separate incidents when a batter admired his home run for far too long. First it was Jose Fernandez, the inspiring and amazing Rookie of the Year candidate, hitting his first home run in the majors; then it was Carlos Gomez, taking revenge for what he perceived to have been an intentional hit by pitch three months earlier. In both cases, Brian McCann got rather peeved. (He also got memed.)\n",
            "So, here’s what happened, as explained by Jason Turbow on his Baseball Codes blog. (I quote him every time I write about unwritten rules, because he breaks everything down from the perspective of unwritten rules violations.)\n",
            "September 11:\n",
            "Bottom of the sixth: Fernandez blasts a nearly 400-foot drive off Braves left-hander Mike Minor for his first career homer, flips his bat away and—ostensibly in response to Gattis—stands to admire it. This is not an innocent would-be slugger in awe of his own unexpected power; the move is intended to disrespect the Braves, who take it precisely that way.\n",
            "As Fernandez crossed the plate, Brian McCann got in his face and yelled at him.\n",
            "September 25:\n",
            "Carlos Gomez, the game’s second batter, homered against Paul Maholm Wednesday, then lingered in the batter’s box. Once he began to trot, his churn rate increased with every step; he shouted with increasing fervor at first baseman Freddie Freeman and Maholm even before reaching third.\n",
            "Watching this, McCann decided to unload a few of his own notions on Gomez, and made sure that his message could not be ignored. The catcher planted himself about 15 feet up the third base line, completely blocking Gomez’s path to the plate. The runner would not pass without first getting an earful.\n",
            "As it turned out, he would not pass at all. McCann shouted him down without ceding the baseline, players from both teams stormed the field, Reed Johnson landed a punch to Gomez’s noggin, and the ensuing scrum carried everybody to the backstop. Gomez was ejected shortly thereafter, and left the field without ever touching the plate.\n",
            "I’m always interested in unwritten rules and the seemingly irrational ways that baseball players enforce them. In this case, Brian McCann and the Braves took a fairly uncontroversial one: you’re not supposed to spend too long admiring your own home runs — “pimping” them — because that is disrespectful to the other team. But the Braves’ angry reactions caused them to lose both moral arguments in the court of public opinion.\n",
            "Jonah Keri called the Fernandez brouhaha “just another case of baseball players taking themselves and their ridiculous unwritten rules way too seriously,” and the Gomez incident led to multiple articles where a yelling Brian McCann was photoshopped into other historical events, as Mike Bates writes, “to tell them to tone it down and cut the fun short.” Emma Span pithily summed up the Braves in a playoff flowchart: “The slightest perceived slight will be met with a benches-clearing brawl.”\n",
            "That said, both of the players accosted by McCann apologized. Fernandez said, “I feel like I don’t deserve to be here, because this isn’t high school. This is a professional game. I made a mistake. I’m going to learn from it… I embarrassed a lot of people. It’s just not right for the game. For sure I can promise 120 percent that that will never ever happen again. I won’t show anybody up like that.”\n",
            "And Gomez tweeted this:\n",
            "The way I carried myself on the field is unacceptable, I should have done better to control myself and set a good example. — Carlos Gomez (@C_Gomez27) September 26, 2013\n",
            "So there appears to be a disconnect: there is a popular viewpoint that the Braves and McCann are in the wrong, while Fernandez and Gomez appear to believe that they were in the wrong. The easy answer could be because they were all in the wrong and that two wrongs don’t make a right. But it is curious to note that Gomez got suspended for his actions, while McCann only got fined. Here is what is likelier: while people outside of baseball think all of these unwritten rules are ridiculous, people in baseball agreed with the Braves.\n",
            "In baseball, what Fernandez and Gomez did was unambiguously wrong. You don’t show up the other team, ever, for any reason. Meanwhile, while McCann’s actions may have been over the top, his intentions were laudable: he was sticking up for his teammates, which in baseball is unambiguously good. Standing in the baseline may have caused a bench-clearing scene, but Brian McCann didn’t do it for the fans, he did it to stand up for Maholm. Defending your buddy’s honor may be old-fashioned and dumb, but that’s what baseball players are.\n",
            "As Michael Wex once wrote, referring to the Yiddish practice of referring to one’s wife as “tsiherste,” meaning “Do you hear me?”:\n",
            "OTTAWA – Canada should support the United States in whatever action it decides to take in Syria, former Liberal prime minister Paul Martin says.\n",
            "In an interview with Global News, Martin said Canada should be working the phones to convince other allies to support the United States “and the actions they will take.”\n",
            "“The American president [Barack Obama] drew a red line and said if chemical weapons were used, that he and others would act. That red line has been crossed,” said Martin.\n",
            "Martin, who in 2005 helped implement the “responsibility to protect” doctrine for UN member states to protect citizens whose governments aren’t guarding them from atrocities, says it clearly applies in today’s Syria situation.\n",
            "“The purpose of responsibility to protect is to say that other countries have a responsibility to act when a country oppresses its own people, and there can be fewer, greater acts of oppression, than when you use a weapon of mass destruction against your people,” he said.\n",
            "Western governments believe the Syrian regime launched a chemical attack against its own people, reportedly killing 355 and injuring 3,600 – a claim Syria’s UN ambassador denies.\n",
            "On Wednesday, Foreign Affairs Minister John Baird met with George Sabra, president of the opposition group Syrian National Council.\n",
            "Following the meeting in Montreal, Baird said Canada does not have the weapons to contribute to the types of strikes being referred to in news reports – such as by armed drones and cruise missiles.\n",
            "But he said Canada is discussing with allies about how to deal with the Assad regime and will continue to co-operate with them.\n",
            "Baird added the government has pledged $42.8 million in humanitarian assistance to go towards emergency food assistance, shelter and sanitation.\n",
            "Liberal leader Justin Trudeau said Wednesday Parliament should be recalled to discuss what role Canada should play in Syria, but he has previously expressed reservations about military intervention.\n",
            "NDP leader Tom Mulcair also said Parliament should be reconvened, and he called it a “tragedy” that Canada does not have a seat on the Security Council.\n",
            "“It’s a tragedy that Canada’s voice won’t be heard because we were never able to get a seat at the UN,” he said.\n",
            "The Conservatives have said it is “premature” to discuss recalling Parliament at this time.\n",
            "Martin said while it’s preferable to get the blessing of the UN Security Council, it’s clear that Russia will not endorse action in Syria.\n",
            "He said the actions made during the Kosovo War in 1999, in which the security council refused to act but NATO intervened, “were exactly the right ones.”\n",
            "“It’s preferable to obviously get the endorsement by the UN Security Council. It’s been very clear by the actions of the Russians that we’re not going to get it in this particular case, and I think the precedent that NATO and Kosovo (set) a number of years ago is the one that we should follow,” he said.\n",
            "Martin said the real justification for responsibility to protect is endorsement by the allies. He said the U.K. and France have shown support, but the Germans are not as clear.\n",
            "“In other words if the United States were to act, because they’re the ones who’ve got the capacity, the missiles to act, and they’ve got the vessels close by, then what the United States will require is the support of their allies,” he said.\n",
            "“Canada has got to play a role in bringing the allies on side.”\n",
            "– with files from the Canadian Press\n",
            "DETROIT -- The Detroit Red Wings have recalled forward Cory Emmerton from the Grand Rapids Griffins. He will make his NHL debut today against the Chicago Blackhawks at Joe Louis Arena (2 p.m., FSD).\n",
            "Center Valtteri Filppula is out with the flu.\n",
            "Emmerton, 22, was Detroit's first choice (41st overall) in the 2006 entry draft. He ranks among Griffins leaders with 17 assists (third) and 23 points (tied for fifth) in 33 games, despite missing 12 games due to injury in December.\n",
            "Filppula should be ready for the next game, Wednesday at home against New Jersey. For now, he's added to a long list of idled Red Wings: Mike Modano (lacerated wrist), Pavel Datsyuk (broken hand), Danny Cleary (broken ankle), Brad Stuart (broken jaw), Chris Osgood (hernia surgery) and Tomas Holmstrom (broken hand).\n",
            "Duck feathers covered a major road in Cambridgeshire, bringing traffic to a standstill, after a fire in a lorry.\n",
            "A ruptured diesel tank caused the fire in the lorry, which was carrying the duck feathers on the A14 near Hemingford Grey.\n",
            "The westbound carriageway of the road was shut after being covered in the white feathers and a rolling roadblock was in place eastbound.\n",
            "Diversions are in place but motorists are advised to avoid the area.\n",
            "Eddie Theaker, from Cambridgeshire Fire and Rescue Service, said: \"This was a potentially dangerous incident as the lorry's diesel tank had ruptured and the fire was extensive.\n",
            "\"Although the fire is out, the westbound carriageway has a lot of debris still on it caused by the feathers and all agencies are working hard to ensure that it is cleared as soon as possible.\"\n",
            "Saudi Arabia has accused the Syrian government of waging \"genocide\" against rebels and criticised Iran and Hezbollah for backing and arming the regime.\n",
            "Speaking at a news conference with US Secretary of State John Kerry in Jeddah, Saudi Foreign Minister Prince Saud al-Faisal Saud has said Syria is facing a \"double-edged attack\".\n",
            "\"(It) is facing a massive flow of weapons to aid and abet that invasion and that genocide. This must end.\" he said.\n",
            "Prince Saud also attacked Iranian involvement in the war-torn country and described Tehran's backing foreign militias as \"the most dangerous development\".\n",
            "He also repeated Saudi Arabia's call for the rebels to be armed.\n",
            "Saudi Arabia, a Sunni state which views Shia Iran as its arch-rival, has increased aid to Syrian rebels in recent months, supplying anti-aircraft missiles among other weapons.\n",
            "At a meeting in the Qataris capital, Doha, on Saturday, ministers from 11 nations in the \"Friends of Syria\" group agreed \"to provide urgently all the necessary material and equipment to the opposition on the ground\".\n",
            "Syrian regime called its neighbours to stop arming rebels battling to overthrow Bashar al-Assad and said a decision by Western and Arab countries on Saturday to arm those rebels would prolong the crisis and deepen the bloodshed.\n",
            "More than 93,000 people have been killed in Syria since peaceful protests erupted in March 2011. Assad's violent response helped provoke what is now a civil war that has driven nearly 1.7 million refugees into neighbouring countries.\n",
            "Kevin Gates was found guilty to battery in Polk County, Florida, on Wednesday (Oct. 26) after he was seen kicking a female fan during a concert in the city of Lakeland last year.\n",
            "The judge sentenced Gates to 180 days in Polk County jail, with credit for any time served, and fines and court costs, WFLA reports. He was convicted by a jury of six women after one day of testimony.\n",
            "Miranda Dixon, 18, admitted to having tugged on Gates' pants during the concert at Rumor's Nite Club on Aug. 28, 2015. She said during the trail, \"I was trying to get his attention for my friend.\"\n",
            "Dixon's friend Teremal Redding testified as well, saying she had asked Dixon to grab Gates to get his attention \"because he is a famous rapper.\" She said that the entire front row had been grabbing at Gates and that the first time Dixon grabbed him, he kept performing, but the second time he got violent, kicking her with force.\n",
            "Gates' defense attorney Jose Baez cross-examined Redding about Dixon's injuries, pointing out that Redding had stayed for the rest of the show after her friend left following the incident.\n",
            "In a previous hearing, Gates admitted to kicking a fan during the show and used the Florida's \"Stand Your Ground\" law in attempt to have the charges dropped.\n",
            "\"I kicked them, I wanted them off me,\" he said during the previous hearing.\n",
            "Rumor's Nightclub head of security Joe Hailey testified during the trial that Gates' security team had not made any special requests for additional security or any barricade between the stage and the crowd for the concert. He said there were about 20 security guards working that night and that there was room for Gates to back up on stage if he did not want to be touched.\n",
            "Thanks to its adorable chubby cheeks and silly dashing gait, a wild European hamster has stolen the show at the 2015 Comedy Wildlife Photography Awards or CWPA. The humorous photo titled \"Rush Hour\" nabbed top honors in the inaugural competition.\n",
            "Distinguished for its lighthearted focus on the giggle-inducing, serendipitous side of wildlife photography, the photo contest aims to highlight technically excellent images that showcase \"the funny side of the majestic creatures we love to photograph and protect.\"\n",
            "CWPA founder Paul Joynson-Hicks was inspired to establish the contest through his own experience as a wildlife photographer. As he explains on the competition's website, \"I often enter wildlife photography competitions (so far with very little success!), but I love seeing the funny pictures. Strangely enough, they are harder to come by than you might think.\"\n",
            "Julian Rad, the Austrian photographer responsible for the winning hamster image, was awarded with an impressive array of prizes, including a trophy, a Nikon D750 camera and a seven-day guided photo safari trip to Tanzania. Meanwhile, the photographers responsible for the silver and bronze award-winning images (below) were awarded some enviable Nikon camera equipment.\n",
            "The ACLU has intervened in the case of a lesbian high school student in Alabama whose principal has forbid her from attending prom with her girlfriend:\n",
            "\"Cynthia Stewart, a 17-year-old junior at Tharptown High School in\n",
            "northern Alabama, is a member of her school’s prom planning committee,\n",
            "had personally raised over $200 for the prom, and created the theme her\n",
            "classmates had chosen for the dance. She is also an out lesbian. When Cynthia approached her principal to ask if she could bring her\n",
            "girlfriend with her to the prom, he said no. He also made Cynthia\n",
            "remove a sticker she was wearing that said, 'I am a lesbian,' telling\n",
            "her, “'You don't have that much freedom of speech at school.' Cynthia’s\n",
            "aunt and guardian, Kathy Baker, then appealed the principal’s decision\n",
            "to the school board. But the board let the decision to bar Cynthia\n",
            "from bringing her girlfriend to the prom stand.\"\n",
            "The school has apparently threatened to cancel the prom for everyone should Stewart bring her girlfriend.\n",
            "UPDATE: School reconsidering request!\n",
            "A few days ago I wrote about the issue of women in ministry. While I don’t think I have ever hidden my views on the topic (I married a female colleague, after all), I also have never written about it on the various blogs I have maintained over the last few years. And maybe recently, I didn’t see it as my place to comment on women in ministry. I am still not sure… I don’t see it as my place to comment on anyone’s “right” or “place” to be a pastor. If anything, I think it is my place to talk about my experience of being a Lutheran pastor or a millennial pastor or a Canadian pastor. It is also to my place to talk about being a male pastor.\n",
            "So let’s talk about that.\n",
            "Being a male pastor is kind of like Louis C.K.’s description of “Being White”. (Warning: The video contains offensive language).\n",
            "Like Louis C.K. says, male pastors aren’t better. But being a male pastor is clearly better.\n",
            "Like all the advantages of being white and male in North America, there are advantages when it comes to being an ordained pastor. Here are some of the obvious ones:\n",
            "No one ever defines my ministry by my gender. No one says, “wow a male pastor or a man in ministry, good for you.” I always get to be just a pastor. I don’t have to constantly live with a qualifier in front of “pastor”, and I am not forced to bear someone’s inappropriate shock that I am my gender and I am a pastor. People expect me to be direct and tell them what I think. They want me to lead them somewhere. I am rarely challenged or expected to defend or make a case for my ideas. I don’t have to apologize for having strong opinions or constantly defend my ideas. People think twice about fighting with me. I always have a leg up in conflict, bullies find it harder to push my buttons because I have fewer to push. I am never automatically second class because of my gender, so conflict is on equal terms or tipped in my favour. I don’t have to suffer being called “boy” or “son” as way of dismissing my point of view, and I am not accused of being divisive if I disagree with something or anything. People are used to pastors of my gender. There are no congregations that are unsure of male candidates for ministry, no parishioners who think it is alright to say something like, “I will never be buried by a man.” I don’t have to endure questions about whether I will take paternity leave, or what will happen when I have kids. People almost never assume that I have a particular gift for ministry before they know me. They don’t automatically think that my gender is suited to particular areas of ministry like preaching or administration. No one assumes that I am not good at pastoral care or being nurturing. People don’t say that I have the gift of speaking with a voice that men can relate to. I don’t have to worry about my safety. I don’t think twice about being alone in the church or if I am safe on my own. If a man asks to meet with me one on one, I don’t have to question my physical safety or his motives. Men don’t try to share the peace with me by hugging me (or grabbing my ass). No one assumes that I am the church secretary or the pastor’s spouse. I am never told, “You don’t look like a pastor or you are took young to be a pastor” even thought I am built like a football player and at times have had long hair and a beard like a hell’s angel. And I have a tattoo. And I am 30 (two decades younger than the average age of pastors in our denomination). Churches are built for men. Pulpits, altars, pastor chairs, vestments are all designed my size and body type in mind. I don’t look ridiculous because the standard garb of my profession is made for my gender, and I don’t look like a cross dresser in a clergy shirt. All the pronouns are for my gender. God is a he. Jesus is a he. Pastors are almost always referred to as he or him or his. I don’t have to correct people because they never use the wrong pronoun to refer to me. Being male is the norm in the church. I didn’t have to take classes in seminary about men’s issues, there is no post-modern male theology, male pastors where never brought in to speak about being male pastors as if it was special or odd or a novelty. I could join the Old Boys Club if I wanted to. Leadership in the church is still overwhelmingly male, and there are no glass ceilings for male pastors in the church. No one pretends it is, “all in good fun” to make sexist jokes about my gender, and none of my colleagues treats me like I am second class because of my gender. I don’t have to walk on egg shells in ecumenical situations. I don’t have to justify my position and call to my conservative colleagues, because they all have male pastors in their denominations. I am not an oddity or the token male at ministerial events.\n",
            "All the advantages of being a male pastor are only advantages because women suffer the opposite. So many of my colleagues have to contend with these annoyances, insults, and frustrations each day because they are the reality of life in the church. This fact makes me very angry. I pray for the day when these will not be male-pastor advantages, but the reality for all pastors, regardless of gender.\n",
            "*** Special thanks to my wife, Courtenay, for helping me write this post, since she knows much more about the struggles women in ministry face than I do. You can follow her on twitter @ReedmanParker ***\n",
            "Read a Christmas Post here:\n",
            "I am at War with Christmas\n",
            "See some more posts:\n",
            "Putting My Jesus Feminism to the Test\n",
            "10 More Reasons Why Male Pastors are Better,\n",
            "So what do you think? Are these true? Are there more advantages to being a male pastor? Share in the comments.\n",
            "Follow me on Twitter: @ParkerErik\n",
            "Advertisements\n",
            "Intake of fish oil reduces the risk of CHD and CHD deaths. Marine n-3 fatty acids (FA) are susceptible to oxidation, but to our knowledge, the health effects of intake of oxidised fish oil have not previously been investigated in human subjects. The aim of the present study was to investigate markers of oxidative stress, lipid peroxidation and inflammation, and the level of plasma n-3 FA after intake of oxidised fish oil. In a double-blinded randomised controlled study, healthy subjects (aged 18-50 years, n 54) were assigned into one of three groups receiving capsules containing either 8 g/d of fish oil (1.6 g/d EPA+DHA; n 17), 8 g/d of oxidised fish oil (1.6 g/d EPA+DHA; n 18) or 8 g/d of high-oleic sunflower oil (n 19). Fasting blood and morning spot urine samples were collected at weeks 0, 3 and 7. No significant changes between the different groups were observed with regard to urinary 8-iso-PGF2α; plasma levels of 4-hydroxy-2-hexenal, 4-hydroxy-2-nonenal and α-tocopherol; serum high sensitive C-reactive protein; or activity of antioxidant enzymes in erythrocytes. A significant increase in plasma level of EPA+DHA was observed in both fish oil groups, but no significant difference was observed between the fish oil groups. No changes in a variety of in vivo markers of oxidative stress, lipid peroxidation or inflammation were observed after daily intake of oxidised fish oil for 3 or 7 weeks, indicating that intake of oxidised fish oil may not have unfavourable short-term effects in healthy human subjects.\n",
            "TRIAL REGISTRATION:\n",
            "ClinicalTrials.gov NCT01034423.\n",
            "[PSA] Removed Games Cards Becoming \"Not Marketable\"\n",
            "Community items for \"banned\" games are now unmarketable (self.Steam)\n",
            "submitted 22 hours ago by Tasty_Salamanders\n",
            "About 2 days ago community items (trading cards, emoticons, backgrounds) for games removed from Steam by Valve, for things such as fake reviews, were made unmarketable. All listed items on the market were cancelled and returned to the inventories to their owners, the market pages for the items still exist but they are now useless.\n",
            "The items still remain tradable.\n",
            "Games removed by the devs themselves are uneffected. i.e. Football Manager 2013 items are still marketable while something like Lemurzin items are not.\n",
            "[steam-tracker.com]\n",
            "Looks like Valve is finally cracking down on the profit loophole for some games. Games that have been removed from Steam are now seeing their cards, emoticons, boosters, and backgrounds labeled as \"Not Marketable\".According to official postings on r/Steam , only games removed from the store due to violations will be affected.The following games's items have been confirmed to be delisted from the Steam Community Market. While not marketable, items associated with these games remain tradable.\n",
            "Matt Finn is your reigning OHL Defenseman of the month.\n",
            "With Finn, the points never quite tell the whole story, but for the sake of a ‘once upon a time,’ the captain of the Storm has registered 18 points in 18 games to start the year, putting him fourth in OHL scoring among defensemen. Add to this the news that Finn has been named to Team Canada in the upcoming Subway Super Series, and that he’s led Guelph to the best record in the OHL to start the season, and it’s been a great start for the 19-year old defenseman.\n",
            "Finn continues to play in all situations with the Storm and at the OHL level he excels at everything. His skating is strong, his shot is heavy, and he makes smart plays with and without the puck. The only real weakness in his game, if there is one, is that he doesn’t play a particularly physical game. Finn is a legitimate candidate for this year’s World Junior team and he looks like he’ll be more than ready to make an impact with the Marlies this time next year.\n",
            "Connor Brown is still leading the OHL scoring race and Verhaeghe, despite a somewhat less productive two weeks (though he did have a hattrick…), remains in 7th.\n",
            "2013 first round pick Frederik Gauthier picked up 3 points in 2 games this weekend and is now registering a point per game after a slow statistical start to the season. I haven’t seen him play yet this year so I’ll defer to a good twitter-follow Tobias Drundridge: “my only complaint is that he doesn’t seem to have any confidence in his shot. Always dishing. Other than that I like what I see.”\n",
            "Have a good Remembrance Day, everyone.\n",
            "Maple Leafs Prospect Statistics - 2013-14 Season\n",
            "Last Update: Tuesday, May 27.\n",
            "CHICAGO, Ill. (Friday, Dec. 13, 2013) – The Chicago Fire Soccer Club announced Friday that the club has acquired defender/midfielder Lovel Palmer from 2013 MLS Cup finalists Real Salt Lake in exchange for allocation money. Per MLS and team policy, terms of the deal were not disclosed.\n",
            "“We’re pleased to welcome Lovel to the Fire,” said Fire Head Coach and Director of Soccer Frank Yallop. “He is a versatile, experienced player who will be instrumental in building a deep squad to compete in 2014.”\n",
            "Palmer joins the Fire after one season with Real Salt Lake. Selected in Stage 2 of the MLS Re-Entry Draft on Dec. 14, 2012, Palmer made 17 appearances and tallied one assist in regular season play, and made three post-season appearances as RSL reached the 2013 MLS Cup.\n",
            "The Mandeville, Jamaica product began his professional career with Jamaican First Division side Harbour View FC in 1999, appearing in 143 matches. Palmer joined MLS in March 2010 when he was signed by the Houston Dynamo. In one-and-a-half seasons with the Dynamo, Palmer scored three goals and tallied one assist across 45 appearances. On July 21, 2011, Palmer was traded to the Portland Timbers where he would appear 34 times and tally one assist.\n",
            "A Jamaican youth international, Palmer made his senior national team debut in 2005 and has since earned 24 caps.\n",
            "Britain's current account deficit has ballooned to its highest ever level, official statistics showed yesterday, raising fresh concerns about the sustainability of the UK’s recovery.\n",
            "The balance of payments gap shot up to £27bn in the three months to September, equal to 6 per cent of GDP. The current account deficit for the third quarter of 2013 was also revised up to 6 per cent. Both figures represent the biggest quarterly deficits registered since modern records began in 1955.\n",
            "The previous record was set in the third quarter of 1988, during the Lawson boom, when the current account deficit, according to the latest figures, hit 5.1 per cent of GDP. Revisions to the GDP level since the Office for National Statistics’ previous balance of payments report revealed that the current account deficit in the 2013 calendar year was the highest annual deficit on record, at 4.5 per cent of GDP, beating the 4.4 per cent seen in 1989.\n",
            "We’ll tell you what’s true. You can form your own view. From 15p €0.18 $0.18 $0.27 a day, more exclusives, analysis and extras.\n",
            "Analysts warned that the deficit made the UK vulnerable to a sudden reversal of global capital flows. “If the UK falls out of favour with international investors for any reason, the economy might be forced to rebalance the hard way, with a combination of currency depreciation to make exports more competitive and lower domestic demand to suck in fewer imports,” said Simon Ward of HSBC.\n",
            "The main cause of the balance of payments deterioration was slippage in the income account to minus £12.6bn, with a decline in the revenue earned by British firms on their overseas investments and an increase in the UK profits of overseas firms. The trade deficit for goods and services actually narrowed slightly over the quarter to £9bn.\n",
            "In a further concerning sign, the ONS doubled its estimate for the fall in business investment in the third quarter from a 0.7 per cent to a 1.4 per cent decline. The annual rate of growth collapsed from 10.7 per cent to 5.2 per cent.\n",
            "The ONS also confirmed that net trade and business investment dragged down growth in the three months, with almost all of the 0.7 per cent GDP growth coming from household consumption. The household savings rate in the quarter declined to 7 per cent, indicating people are financing their additional spending by saving less of their disposable income.\n",
            "The ONS also made some sizeable downward revisions to GDP growth over the past year, taking the annual rate of growth in the third quarter from 3 per cent to 2.6 per cent. Those revisions will make it hard for the economy to grow in line with the Office for Budget Responsibility’s forecast this month for 3 per cent growth in 2014. The economy would need to grow by 2.2 per cent in the final quarter alone to hit that target. “The latest set of national accounts leave the UK’s economic recovery looking more fragile than it seemed before,” said Samuel Tombs of Capital Economics.\n",
            "In the US, by contrast, growth in the third quarter was revised up to its strongest rate in 11 years. GDP rose 5 per cent on an annualised basis, up from the 3.9 per cent previously estimated. That news sent the benchmark Dow Jones index up above 18,000 for the first time ever, while the dollar also strengthened. “After four years of rocky recovery, the US economy is now hitting its stride and growth should remain good next year, with lower gasoline prices a big plus for consumers,” said Gus Faucher of PNC Financial Services.\n",
            "Signs emerged yesterday of a further slowdown in the UK housing market. The British Bankers’ Association reported that mortgage approvals last month were down 20 per cent on November 2013, from 45,594 to 36,717.\n",
            "We’ll tell you what’s true. You can form your own view.\n",
            "At The Independent, no one tells us what to write. That’s why, in an era of political lies and Brexit bias, more readers are turning to an independent source. Subscribe from just 15p a day for extra exclusives, events and ebooks – all with no ads.\n",
            "Subscribe now\n",
            "The Pirate Bay dropped the dime on an anti-piracy agency it claims illegally copied system files to create a \"fraudulent parody site.\"\n",
            "The Pirate Bay, as you may already know, is the world's largest file sharing site, and as its name suggest it has a certain propensity toward, shall we say, content that is not necessarily respectful of the copyright laws of the land. But while it doesn't have much respect for copyright, it has even less respect for those who disrespect copyright in the name of copyright. The Pirate Bay alleges that Finland's Copyright Information and Anti-Piracy Center unlawfully used some of its materials to create a parody site - piraattilahti.fi, complete with a sinking galleon - and so it called the police.\n",
            "\"While The Pirate Bay may have a positive view on copying, it will not stand by and watch copyright enforcing organizations disrespect copyright,\" The Pirate Bay said in a press release. \"It's funny that we have to teach the copyright lobby the meaning of the law. The fact that they wrote it doesn't mean that they are above it.\"\n",
            "The Pirate Bay claimed the CPIAC \"is not new to balancing on the edge of what's right and wrong,\" noting that the group instigated a police raid in 2012 against a nine-year-old girl who had downloaded music from the internet, which resulted in the confiscation of her Winnie the Pooh laptop.\n",
            "\"CIAPC is like an ugly high school bully without friends,\" the statement said. \"It's time to take a stand. Cyber bullying is a serious matter to us all.\"\n",
            "The Pirate Bay also pledged that any financial award it wins as a result of this case will go toward replacing the girl's laptop.\n",
            "Source: The Pirate Bay\n",
            "Kotaku East East is your slice of Asian internet culture, bringing you the latest talking points from Japan, Korea, China and beyond. Tune in every morning from 4am to 8am.\n",
            "A few weeks back, I received an email from a reader who was blind in one eye and wanted to know how the Oculus Rift would work for him. I thought it was an interesting question but honestly didn't know the answer, so I put it to the back of my mind. Then this last weekend I went in for Lasik surgery. It didn't go well.\n",
            "Simply put, the surgery was a failure and while the doctor is hopeful that I will make a full recovery in the coming months, I have been effectively blind in my right eye for the past few days. So in an attempt to turn something horrible into something useful to others, I decided to see what Oculus Rift is like for those with only one functioning eye.\n",
            "I started simple with a little tech demo called GirlMirrorLook. Basically, you are a girl in a small villa surrounded by beautiful scenery. There is also a mirror.\n",
            "Now, in recent weeks, I had spent more than a little time with the Rift, but I was still surprised when I put the Rift on in my one-eyed state; everything still looked like it was in 3D. I still felt like I could reach out and touch the objects I was seeing—they just seemed a little flatter somehow. The objects didn’t quite pop out of the frame like they had before. After all, how could they? Stereoscopic 3D is impossible with just one eye.\n",
            "Advertisement\n",
            "However, due to the screen completely enveloping my peripheral vision, I was still visually immersed in the game. And since when I moved my head the camera moved identically in my Rift-vision, my brain seemed happy to interpret it as what I was really seeing—regardless of whether objects had the 3D “pop” or not.\n",
            "Advertisement\n",
            "I tried out another few tech demos—a room full of Hatsune Miku knock-offs, a Rift version of Tetris—and they seemed to be a comparably immersive experience to what I had had when playing the Rift with both eyes.\n",
            "So it wasn't until I tried playing Proton Pulse Rift, the first real game I ever played on the Rift, that the lack of Stereoscopic 3D became an issue. Proton Pulse is an Arkanoid/Breakout-style game where you look down a long vector graphics corridor and block a bouncing ball with a semi-transparent paddle controlled by your head movements. Without the Stereoscopic 3D, it was much harder to judge how far the ball was from me at any given time. Like in the real world, I had lost my depth perception.\n",
            "Advertisement\n",
            "Lastly, I decided to try out something a bit more flashy and fast-paced in the form of Half Life 2: Episode 2. I only played through the first ten minutes or so, but there was one major issue I noticed when compared to my several hour stint last week with Half Life 2: Episode 1: the motion blur is much, much worse. Perhaps it is just that having two eyes lets you filter out the blur better, but every time I made a big, quick head or camera movement, my vision became little more than a mass of color. Despite this, however, the game was still quite playable.\n",
            "Advertisement\n",
            "In the end, I was really surprised at how well the Oculus Rift worked with only one eye. The immersion factor was still strong—I still felt like I was in the game and not in my living room. The motion blur and lack of depth perception were downsides to be sure, but everything was still perfectly playable.\n",
            "And on one level, at least, playing one-eyed was definitely superior. I never felt even the least bit queasy.\n",
            "Kotaku East is your slice of Asian internet culture, bringing you the latest talking points from Japan, Korea, China and beyond. Tune in every morning from 4am to 8am.\n",
            "Advertisement\n",
            "This Is Kotaku East As more and more publications are dialing down their Japan coverage, Kotaku had this crazy idea:… Read more Read\n",
            "Cognitive liberty is the concept that an individual has absolute sovereignty over their state of consciousness as long as it does not infringe on the rights of another. This includes the use of meditation, prayer, and psychoactive drugs, as well as the right to not be force-fed any psychoactive drug against one’s will.\n",
            "Currently, cognitive liberty is not a very much respected philosophy in American politics. Most psychoactive drugs like cannabis, psilocybin mushrooms, LSD, and mescaline are considered illegal to possess and consume. Similarly, we see young children all throughout the nation being force-fed certain psychoactive drugs (the “good kinds”) in name of “normal thinking” and “normal behavior.”\n",
            "As a libertarian, I feel cognitive liberty is a necessary component to any free society. I have written numerous times about my disdain for the War On Drugs and specifically the failure that is marijuana prohibition.\n",
            "I am also against our tendency as a society to administer psychoactive drugs so carelessly and with so little respect for the free choice of young people and the mentally ill. It is an awful thing to offer a drug to any person without appropriately informing them on what the drug is supposed to do or how it might make them feel. We should also explain to children very clearly that they have a choice whether or not to continue taking the drug if they don’t like its effects. It often happens where a child’s personal interest is put secondary or even overlooked completely.\n",
            "Let it be known that I do think there are cases where someone needs to be given a drug against their will (maybe if they are unconscious or completely delusional and incoherent). But these situations are limited, and I think it is safe to say that society and government has overstepped its boundaries on more than a couple fronts when it comes to this issue.\n",
            "Who really has the authority to tell a conscious and thinking being what they can and cannot put into their body or how they should experience reality? I don’t consider it justified for any free society to draw such arbitrary distinctions between what is “good” or “bad” for an individual if that individual is exercising their own rational free choice. People have different values and interests in life – this is the same dimension of diversity that characterizes all of nature and what has made evolution possible.\n",
            "The act of consuming drugs is a victimless crime, and it does not justify people getting locked up in prison or having a criminal record that inhibits them from ever getting a good job or building a bright future. Drugs have been present in every society known to man and it is time we respect their place in the structure of our humanity. It is one thing to advise others against the use of certain drugs, and it is another to try to banish their existence or ignore it entirely.\n",
            "I am perfectly comfortable acknowledging the fact that some drugs have negative consequences while still promoting the freedom for others to use these same drugs. For one thing, negative consequences are a part of all decisions we make – it does not mean we sacrifice our freedom to make those decisions. To borrow from something I wrote in a recent article about government spending,\n",
            "“Just because half of American marriages end in a divorce doesn’t mean we want the government to make decisions on who we should marry. Life is filled with mistakes; it is how we learn, and it is a part of freedom.”\n",
            "If you really want to help those who are dependent on drugs – or if you really want to make a long lasting change in any individual’s behavior – then you need to appeal to that person’s reason. You can not rule a rationally thinking person by force. And yes, even a drug addict has his or her own mode of rational thought in accordance with their own values and interests.\n",
            "When people value something strong enough, they find a way to go against government restrictions in order to satisfy that want. This is why prohibition always leads to a black market for goods that society finds valuable.\n",
            "When will people recognize that others value these drugs? When will we learn to tolerate these differences? If we really want to make a positive change than we need to re-think freedom and re-think the way we influence others if we want to continue living in a free society. If we don’t accept the notion of cognitive liberty, if we sacrifice those fundamental principles of self-ownership and freedom of thought, then in what ways are we really free anymore?\n",
            "To learn more please visit The Center For Cognitive Liberty & Ethics, which includes notable members like visionary artist Alex Grey, libertarian psychiatrist Thomas Szasz, and psychedelic researcher Ralph Metzner (who used to work with Timothy Leary).\n",
            "Three people from Blount County are facing felony reckless endangerment charges after deputies say they forced a minor to inject methamphetamine.\n",
            "Deputies say 29-year-old Samuel Thomas Hill, 32-year-old Michael Eugene Hill and 18-year-old Felicia Faith Hannah lured a 17-year-old girl in the bedroom of Samuel Hill’s home at Butterfly Gap Loop Road on Tuesday, held her down by her arms and injected her with methamphetamine.\n",
            "- Advertisement -\n",
            "Investigators say the girl was injected with meth against her will. She later had bruising on her arms and tested positive for methamphetamine at a hospital.\n",
            "Samuel and Michael Hill are being held on a $25,000 bond and have a court date on April 8th. A bond has not been set for Hannah at this time.\n",
            "The incident is still under investigation.\n",
            "Christopher Garza said he knows he shouldn't have been driving his muscle car at 158 mph early Friday on the Indiana Toll Road near Gary — he just wanted to help his buddy clear his mind by showing him how fast his new ride could go.\n",
            "When the 30-year-old from Chicago passed an Indiana State Trooper squad car, he decided to slow down and own up, he said.\n",
            "\"Obviously, what I did was very dumb,\" Garza said Friday afternoon.\n",
            "The trooper was parked in his squad car at mile marker 13, three miles east of the Cline Avenue exit, when at 1:31 a.m. he saw a black 2016 Dodge Challenger Hellcat traveling \"at a high rate of speed,\" the release states.\n",
            "The trooper clocked the Challenger at 158 mph in a \"well-posted 70 mph zone,\" and clocked the car a second time at 151 mph, according to police. The car \"rapidly decreased its speed\" when it passed the police car, according to the release, and the trooper pulled Garza over.\n",
            "Garza told police he knew he was doing 160 mph and wanted to show his friend \"what his 707-horsepower engine could do\" and \"thought the Indiana Toll Road would be the safest place to do this since it was empty,\" the release states.\n",
            "\"It was not my intention for us to get busted,\" Garza said.\n",
            "Garza's friend, an injured service member, had recently returned home, he said. Because the two like to work on cars and trucks together, he decided to take him for a ride in the Hellcat to distract him from his troubles, he said.\n",
            "\"Really, I was just kind of getting my buddy's mind off of reality for a little bit,\" Garza said.\n",
            "They've taken cars to tracks before, where Garza said he has gotten the Hellcat \"up to 184,\" 16 shy of the speedometer's 200 mph max.\n",
            "Indiana State Police An Indiana State Police trooper took a picture of the speed clock before arresting Chicagoan Christopher Garza for reckless driving. An Indiana State Police trooper took a picture of the speed clock before arresting Chicagoan Christopher Garza for reckless driving. (Indiana State Police)\n",
            "\"As Ricky Bobby once said, I just want to go fast,\" Garza joked, referencing the protagonist in the comedy \"Talladega Nights.\"\n",
            "But he also said, \"I'm pro-law enforcement. I'm pro-military. And I'm pro-not breaking the law.\"\n",
            "\"The moral of the story is I know the risk I take every time I decide to speed,\" Garza said. \"This was the lesser of the consequences to suffer going such a high speed. I'm kind of fortunate that I learned my lesson through jail.\"\n",
            "When asked whether state troopers have witnessed drivers traveling at higher speeds, state police spokeswoman Sgt. Ann Wojas said in an email that \"it's right up there.\" While police \"do see over 100 mph,\" it's \"not usually in the 150s,\" Wojas said.\n",
            "Lake County sheriff's Deputy Chief Dan Murchek said his officers have encountered speeders driving over 100 mph, although not recently. The speed that Garza was driving at was \"an extremely excessive speed,\" and he noted \"it's dangerous to do that\" for public safety and the officers involved.\n",
            "Garza was arrested and taken to Lake County Jail, where he was bailed out Friday, and the Hellcat was towed. Going forward, Garza knows he'll have to appear in court.\n",
            "Still, Garza said his buddy \"had a great time last night and that's all that matters to (him).\"\n",
            "\"I'll deal with the consequences,\" Garza said.\n",
            "rejacobs@post-trib.com\n",
            "Twitter @ruthyjacobs\n",
            "Introduction of Ramses II\n",
            "Ramses II was the third ruler in the 19th Dynasty (also known as the New Kingdom Period) and was born in 1303 BC. This pharaoh is many times referred to as Ramses the Great. Some alternative spellings for this Ancient Egyptian pharaoh are Rameses and Ramesses. Ramses II is also believed to be the pharaoh mentioned in the biblical story of Moses which gives him additional importance in history.\n",
            "Ramses II’s Family\n",
            "His father, Seti I, appointed Ramses II Prince Regent at the age of fourteen. After the death of his father, Ramses II then became pharaoh and ruled Egypt for many decades. Ramses II took the throne in his very late teens or early twenties. It is believed that Ramses II ruled from 1279 BC to 1213 BC for a total of 66 years. Ramses II had many wives and fathered about 100 children during his lifetime; although, the actual number is not known. At the time of his death, Ramses II was over 90 years of age and due to his longevity outlived many of his offspring, wives, and family members. Living this long was almost unheard of for people of that time. During his lifetime, Ramses II celebrated an unprecedented 14 Sed Festivals. These Sed Festivals were held after 30 years of a pharaoh’s reign and then every three years thereafter. The Sed Festivals were meant to celebrate the continued success of a pharaoh and also to rejuvenate a king’s strength.\n",
            "Ramses II’s father was Seti I and his mother was Queen Tuya. Ramses II’s chief wife and consort was Nefertari although he had many other wives as well. Two other known wives are Isetnofret and Maathorneferure. It is believed that 12 sons died during Ramses II’s lifetime. Upon Ramses II’s death, he was succeeded by his thirteenth son Merenptah (aka Merneptah). Merneptah was thought to be between 55 or 60 years old when he actually took over the throne. Merneptah went on to rule for about ten years.\n",
            "Ramses II’s Mummy\n",
            "Due to the great discovery of Ramses II’s mummified body, Egyptologists and scientists have since been able to piece together some great information about this pharaoh. For example, upon analysis of Ramses II’s remains, the pharaoh is thought to have had red hair. Because this was not a prominent characteristic in Ancient Egypt, it most probably helped set him apart from other citizens of that time. In addition to standing out from the general population, persons with red hair were also seen as followers of the God Seth. Regarding other characteristics, Ramses II’s mummy further revealed that he had a hook nose with a strong jaw line and was about 5 feet 7 inches tall. It was also revealed that he was ridden with arthritis.\n",
            "In addition to finding Ramses II’s mummy and learning more about this pharaoh’s attributes, information regarding the movement of his corpse was revealed through a linen cloth found wrapped around the mummified body. The linen cloth contained hieroglyphic pictures and provided details of how this move was accomplished in order to protect the pharaoh’s body from looters. The recordings revealed that Ramses II had been originally buried in the tomb KV7 (Valley of the Kings), but then the body was rewrapped and transferred to a holding area; the tomb of Queen Inhapy. It appears that within 72 hours, the body was again moved. This time it was moved to the tomb of the High Priest Pinudjem II. Today, Ramses II’s mummy can be found at Cairo’s Egyptian Museum.\n",
            "More study of this great pharaoh reveals that toward the end of Ramses II’s reign, he developed serious health problems; although, these can be mostly attributed to old age. It appears that Ramses II walked with a hunched back due to his arthritis, had a hardening of the arteries, and had severe dental problems (abscessed tooth issues caused by bacterial infections).\n",
            "Before Ramses II came to power, two thousand years of Pharaohs had come to rule before him, and the pyramids had been standing for at least a thousand years. By the time Ramses II died, he had accomplished many things. He was known as warrior, great king, family man, and venerated as a god. He had acquired several wives, many offspring, and had made the country wealthy by collecting riches and supplies from other empires. Due to his love of architecture, he had also created many memorials throughout Egypt. Some of the memorials completed were especially created for his first queen, Nefertari. Upon his death,\n",
            "at least nine pharaohs are known to have taken the name Ramses; however, none came close to leaving the legacy that Ramses II had left for Egypt and the world except for one – Ramses III. As history continues to reveal more about this great pharaoh, his success most likely came as a result of being the master of propaganda and politics.\n",
            "Says Warren Buffett has publicly said his secretary \"should not be paying a higher tax rate\" than him.\n",
            "During the second presidential debate, Hillary Clinton argued against tax rules that allow the ultra-rich to pay lower rates than the middle class. She said she would change them as president, a move that has become known as the \"Buffett rule.\"\n",
            "\"Warren Buffett is the one who's gone out and said somebody like him should not be paying a lower tax rate than his secretary,\" Clinton said.\n",
            "Warren Buffett is the CEO of Berkshire Hathaway Inc. Forbes estimates him to be worth $64.3 billion and one of the richest men in the world. He is also a Hillary Clinton supporter and has sparred with Donald Trump over Trump’s refusal to release his tax returns.\n",
            "Urging major tax changes, Buffett has repeatedly said he pays a lower tax rate than his secretary and other employees.\n",
            "In 2007, Buffett told NBC Nightly News that he pays a smaller tax rate than multiple employees in his office.\n",
            "In 2011, Buffett wrote an op-ed in the New York Times called \"Stop Coddling the Super-Rich.\" In the article, Buffett said that his taxes amounted to \"only 17.4 percent of my taxable income — and that’s actually a lower percentage than was paid by any of the other 20 people in our office.\"\n",
            "In 2013, he told CNBC that while his tax rate rose 8 to 9 points more that year, \"The differential between me and the rest of the office, not just my secretary but the rest of the office, was greater than that. It'll be closer, but I'll probably be the lowest-paying taxpayer in the office.\"\n",
            "A quick phone call to Buffett’s office confirmed this fact-check. Buffett’s assistant Debbie Bosanek -- the secretary Buffett mentions -- confirmed her boss pays a lower tax rate than she does.\n",
            "So, how can this be?\n",
            "Buffett’s op-ed says his tax rate is lower than his employees’ because money made off investments is taxed at a lower rate than wage income\n",
            "\"If you make money with money, as some of my super-rich friends do, your percentage may be a bit lower than mine. But if you earn money from a job, your percentage will surely exceed mine — most likely by a lot,\" Buffett said.\n",
            "Personal income is taxed based on how much a person makes each year. In 2016, the brackets range from 10 percent for those making $9,275 or less, to a top marginal rate of 39.6 percent for those making $415,051 or more. Meanwhile, the tax rate on long-term capital gains -- the profits from investments -- is 15 percent. However, if the taxpayer's taxable income is greater than $415,051, their long term capital gains are taxed at 20 percent.\n",
            "Here are the current rates for a single taxpayer.\n",
            "Taxable Income Tax Rate $0 to $9,275 10% $9,276 to $37,650 $927.50 plus 15% of the amount over $9,275 $37,651 to $91,150 $5,183.75 plus 25% of the amount over $37,650 $91,151 to $190,150 $18,558.75 plus 28% of the amount over $91,150 $190,151 to $413,350 $46,278.75 plus 33% of the amount over $190,150 $413,351 to $415,050 $119,934.75 plus 35% of the amount over $413,350 $415,051 or more $120,529.75 plus 39.6% of the amount over $415,050\n",
            "On Oct. 10, 2016, Buffett released details about his personal tax data in a statement titled, \"Some Tax Facts for Donald Trump.\" He says he paid $1.85 million in federal taxes in 2015, and his adjusted gross income was $11.6 million. That means he paid 15.9 percent in effective federal income tax.\n",
            "Lawrence Zelenak, a Duke University law professor, said in an email most taxpayers with incomes in the low six figures have a higher rate than 16 percent.\n",
            "\"That's true even if you consider only federal income tax. Add in payroll tax and Buffet's rate becomes even lower compared with his secretary,\" he said.\n",
            "Payroll taxes are separate from income taxes, and are deducted by your employer before you get your paycheck. These taxes pay for Medicare and Social Security. The employee tax rate for Social Security is typically 6.2 percent of your gross income, up to $113,700, and 1.45 percent on all income for Medicare.\n",
            "These two rates are not progressive or applied to all income, so as income increases, they make up a smaller percentage of the total income.\n",
            "Since Buffett makes the majority of his money from investments, his wage income -- and therefore his payroll taxes -- are low. But those who work for a living, especially those who make higher than average salaries, get taxed at higher rates. We don’t know what salary Buffett’s secretary earns, but it’s likely much higher than the median pay for secretaries. (Remember that Buffett is one of the richest men in the world.)\n",
            "A study from the nonpartisan The Tax Policy Center says that people making between $100,000 and $200,000 pay an average 19.6 percent in taxes, including payroll and income taxes.\n",
            "Our ruling\n",
            "Clinton said Buffett claimed to pay a smaller tax rate than his secretary. Buffett’s secretary confirmed she pays a higher rate than her boss. Buffett said he pays about 16 percent in income taxes, while any employee making between $100,000 and $200,000 typically pays about 20 percent in taxes. We rate this claim True.\n",
            "The driver who left a woman trapped inside a burning, crashed car on the Gowanus Expressway wasn’t heartless — he was a hero, the man’s brother told The Post.\n",
            "“He did not just run away from the scene. He lost his phone in the car [and was] unable to call the ambulance,” Waheed Ahmad, 21, said about his brother Saeed Ahmad, 23.\n",
            "Ahmad crashed a 2007 Infiniti G35 sedan into a concrete barrier Friday morning and hailed a cab while his friend Harleen Grewal, 25, sat burning inside the passenger’s seat, police said.\n",
            "“He tried to get her out. That’s how his hands and his legs and his neck got burned. He couldn’t get her out. The fire got too crazy. It just burned so quick,” said Ahmad’s distraught brother.\n",
            "Saeed Ahmad “was in pain” and since “the ambulance wasn’t coming” he asked the taxi to take him to Maimonides Hospital in Borough Park, his brother said.\n",
            "Police tracked Ahmad down to the hospital where he was charged with criminally negligent homicide, leaving the scene of an accident, aggravated unlicensed operation of a vehicle and speeding.\n",
            "His family was told by cops that they couldn’t visit or speak to Ahmad until after his arraignment, the family told The Post at their Brooklyn home.\n",
            "The hospital contacted Ahmad’s friend listed as his emergency contact who spoke to him before police arrived, the family said.\n",
            "“Everything is chaos right now. We are shocked. It’s horrifying for the girl and her family,” said Waheed.\n",
            "He added, “He’s emotionally distraught. Every time they ask him about what happened, he’s crying and screaming. His friend burned alive.”\n",
            "Ahmad is being treated for burns to his extremities at the Staten Island University Hospital.\n",
            "Global Payments, a third-party payments processor at the center of a Visa and MasterCard security breach, reiterated Monday morning that while customer data may be at risk, the data breach has been \"contained to the best of our ability.\"\n",
            "Overall, 1.5 million accounts may have been affected.\n",
            "Global Payments CEO Paul Garcia said that the \"diligent work\" may take some time, but it will complete the ongoing investigation and identify any changes that need to be implemented.\n",
            "Garcia said the company will get its record of compliance back with Visa and MasterCard \"as soon as possible.\" Executives were upbeat about Global Payments' ability to regain its record of compliance with credit card associations.\n",
            "The company also said it wasn't aware of any fraudulent transactions taking place.\n",
            "Separately, Global Payments reported third quarter earnings of $57.9 million, or 73 cents a share, on revenue of $533.5 million, up 17 percent from a year ago. Non-GAAP earnings in the third quarter were 83 cents a share. Wall Street was looking for earnings of 84 cents a share.\n",
            "Global Payments projected 2012 revenue to be $2.15 billion to $2.2 billion. The company expects non-GAAP earnings of $3.50 a share to $3.58 a share. GAAP earnings were $3.10 a share to $3.18 a share.\n",
            "Charges related to the breach weren't disclosed because the investigation is ongoing.\n",
            "Approximately three weeks ago, the breach was discovered. Within hours, law enforcement had been contacted. Garcia described how the company \"jumped on this instantly\" and said that only a \"handful of servers\" were affected.\n",
            "Here's what happened and when:\n",
            "On Friday, it was first reported that Global Payments suffered a security breach, where as many 50,000 cardholders may have had their information exposed.\n",
            "Global Payments processes card payments between merchants and banks, sitting in the \"middle-ground\" directing where payment data should go.\n",
            "Brian Krebs, who first reported the breach, initially warned that 10 million cards may be compromised. On Sunday, Global Payments revised down Krebs' figure as it confirmed as many as 1.5 million Visa and MasterCard accounts may have been compromised by the security breach.\n",
            "While card numbers may have been downloaded from its systems, no other personal data -- such as names, addresses, or Social Security numbers -- were accessed.\n",
            "Both Visa and MasterCard confirmed there was no breach to its own systems.\n",
            "Visa and MasterCard both sent out non-public alerts to banks to warn of the breach, which was thought to have occurred between January 21 and February 25, as Global Payments informed law enforcement and brought in an independent data security organization to inspect any damage.\n",
            "Visa, as a result of the breach, removed Global Payments from its list of approved service providers, but invited it to reapply once it submits evidence to show its security is \"in compliance with Visa's standards.\"\n",
            "MasterCard said it had not followed Visa's move, but was awaiting the result of an independent forensic investigation before it made any decision.\n",
            "The Associated Press reported that a technical problem affected the Visa network for 45 minutes on Sunday evening, which resulted in users unable to use their credit and debit cards. Visa confirmed this was not as a result of the recent security breach.\n",
            "While the reputation of Visa and MasterCard stands in jeopardy, Global Payments lies in ruins. But Jefferies analyst Jason Kupferberg said that the processor can weather the storm.\n",
            "The processor has $300 million to $400 million in unrestricted cash, which could pay for the damage left by the breach, compared to figures by the 2009 Heartland data breach, in which 130 million accounts ran compromised. Analysts weighed in almost immediately after the breach with their opinions.\n",
            "This story originally posted as \"Global Payments: Data breach is contained\" on ZDNet.\n",
            "Texans' attitudes shifting along with U.S. on legalizing pot\n",
            "Gov. Rick Perry’s objections to a law designed to halt prison rape appear to be about his aversion to federal involvement. Gov. Rick Perry’s objections to a law designed to halt prison rape appear to be about his aversion to federal involvement. Photo: Justin Hayworth, FRE Photo: Justin Hayworth, FRE Image 1 of / 25 Caption Close Texans' attitudes shifting along with U.S. on legalizing pot 1 / 25 Back to Gallery\n",
            "SAN ANTONIO — Whether it's for economic, medical or personal liberty reasons, more and more Texans are taking the position that marijuana should be legal.\n",
            "About half of Texans — 49 percent — support legalizing small amounts of marijuana for recreational use, and 77 percent support legalizing medical marijuana, according to a University of Texas/Texas Tribune poll released this week. Nationally, 58 percent of Americans say pot should be legalized, according to an October Gallup poll.\n",
            "“Texas, like the rest of the country, is coming to the realization that we can no longer afford to imprison thousands of our citizens for consuming a substance much less harmful than alcohol and much less addictive than cigarettes,” said Gerry Goldstein, a nationally renowned defense attorney based in San Antonio who has represented drug cartel kingpins and journalist Hunter S. Thompson, among others.\n",
            "About 73,611 adults in Texas were arrested last year for marijuana possession, according to Department of Public Safety data, accounting for 59 percent of all drug possession arrests in the state.\n",
            "Goldstein said the state spends more than $50,000 a year to house each prisoner.\n",
            "“We could be sending these folks to Harvard,” he said.\n",
            "Mike Helle, president of the San Antonio Police Officers Association, said his organization has “so many different pressing issues right now that legalizing marijuana is not a priority,” but that if police did not have to worry about Class B misdemeanors for pot possession, “you could free up thousands, if not millions, of dollars in manpower alone.”\n",
            "“You want to talk about freeing up resources,” Helle said. “But as police officers, we enforce the laws, not create them.”\n",
            "In Texas, an offender with less than 2 ounces of marijuana can be sentenced to up to 180 days in jail and a fine of up to $2,000. An offender with more than 5 pounds faces up to two years in jail.\n",
            "Ana Yañez-Correa, executive director of the Texas Criminal Justice Coalition, a policy research group advocating for criminal justice reform, said she isn't surprised at the poll results.\n",
            "“People in general understand arresting someone for the use of marijuana is more likely to waste taxpayer dollars and law enforcement's time than to deter use,” Yañez-Correa said. “That's why you see libertarians, members of the tea party and Democrats all saying the same thing on this issue.”\n",
            "At an international conference last month, Gov. Rick Perry touted Texas drug courts and forms of decriminalization, or lessening of criminal penalties, for marijuana use, calling it an economic issue.\n",
            "Perry made clear he does not support legalization, but said he supports individual states' rights to legalize the drug and predicted Texas would not do so anytime soon.\n",
            "The governor's comments added fuel to an already burning issue across the country and state. Washington and Colorado became the first states to legalize marijuana for recreational use for adults last year, and more than 20 states have legalized pot for medicinal purposes.\n",
            "State Sen. Wendy Davis, a Fort Worth Democrat running to replace Perry as governor, has said she supports medical marijuana. Attorney General Greg Abbott, the likely Republican candidate for governor, has said he opposes decriminalization or legalization and that Texas' current laws — outlawing any use of marijuana — should be better enforced.\n",
            "“Legalizing drugs would encourage drug use, which affects every sector of society, straining our economy, our health care and criminal justice systems, and endangering the lives of future generations,” a spokesman for Abbott said last month.\n",
            "The Texas poll, which surveyed 1,200 registered voters, found only 23 percent of respondents said marijuana should remain illegal in all cases in the state. About 49 percent said marijuana should be legal for any use, including recreational, either in small quantities (32 percent) or any amount (17 percent).\n",
            "“How can we as a society deny people who need marijuana for medical treatment?” Yañez-Correa said.\n",
            "Marijuana is prescribed for a wide range of medical issues in states that allow medicinal use.\n",
            "For example, it is prescribed to glaucoma patients, people undergoing chemotherapy for cancer to boost their appetite, insomnia, pain relief, seizures and muscle spasms.\n",
            "The University of Texas/Texas Tribune poll methodology has been criticized by other polling outfits and some media outlets because it is an Internet opt-in survey.\n",
            "In 2011, the American Association for Public Opinion labeled the methodology as unreliable because it is not completely random.\n",
            "The poll was conducted Feb. 7-17 and the margin of error is 2.83 percentage points.\n",
            "“Hundreds of thousands of people around the world have died from an overdose of aspirin,” Goldstein said. “Find me a physician who has ever admitted a patient for an overdose of marijuana.”\n",
            "kparker@express-news.net\n",
            "Twitter: @KoltenParker\n",
            "The House vote on the GOP’s ObamaCare repeal bill is down to the wire, with dozens of Republicans waffling as “undecideds.” What’s the holdup? Ninety-six percent of people who have to buy their own insurance stand to benefit from this bill, which will likely drive down premiums by double digits.\n",
            "The remaining 4 percent — those with pre-existing conditions — will be protected by a federal fund to subsidize their insurance costs. They won’t get priced out of the market, because the fund will pay the lion’s share of their premiums.\n",
            "But some Republicans are running scared. Although the bill solves two problems — lowering premiums and protecting people with pre-existing conditions — these fence-sitters are worried about something else: getting re-elected.\n",
            "As a member of the New York delegation put it, the issue is “optics.” They’re cowed by the media’s false reports that the GOP is abandoning people with pre-existing conditions.\n",
            "In fact, no one wants to do that. There is a consensus that people with pre-existing conditions should be able to get insurance. The issue is who pays the hefty price tag.\n",
            "ObamaCare forced healthy buyers in the individual market to foot the entire bill. That’s why their premiums have doubled since the law went into effect.\n",
            "The new House bill sets up a fairer way: a $130 billion pot of federal money to pay for people with pre-existing conditions. The entire nation chips in, not just people stuck in the individual market.\n",
            "see also ObamaCare replacement bill is on the verge of dying WASHINGTON — House Republican leaders frantically trying to convince moderate...\n",
            "Under ObamaCare, the healthy and the chronically ill paid the same premiums. It’s called community pricing. Healthy people would never reach their sky-high deductibles.\n",
            "Instead the premiums extorted from them would be used to cover huge medical bills for the chronically ill, who consume 10 times as much medical care.\n",
            "In fact, Aetna CEO Mark Bertolini reports that less than 5 percent of ObamaCare enrollees consume over half of the health care. Most healthy people saw that being charged the same as these sick people was fundamentally unfair and refused to sign up.\n",
            "ObamaCare’s community pricing was the single biggest reason premiums have doubled since 2013, according to actuarial consultants at Milliman.\n",
            "The GOP bill offers a solution. States can choose to get a waiver from ObamaCare’s community-pricing rule, so that insurers can sell to healthy people at a far lower cost. States that get the waiver should see double-digit premium decreases for the healthy almost immediately.\n",
            "Naysayers claim the federal fund to subsidize people with pre-existing conditions won’t be adequate. Nonsense.\n",
            "How many people will need help? Not as many as Democrats claim.\n",
            "Before ObamaCare, 250,000 people a year with pre-existing conditions were denied coverage for health reasons by major insurers. Most of them got help through state high-risk programs, but these no longer exist.\n",
            "In 2010, the ACA established a temporary program for people not being helped by the state high-risk programs. About 115,000 enrolled there.\n",
            "Adding the two figures together, count on 365,000 people to need help paying for their premiums because of their medical histories. To be safe, call it 400,000.\n",
            "Based on the $32,000 per person the ACA’s temporary program spent insuring people with pre-existing conditions, the federal fund will need $12.8 billion a year. So the $13 billion a year the GOP bill provides is likely adequate.\n",
            "New York ruined its individual insurance market two decades ago by imposing community pricing, which drove out healthy buyers. Don’t count on the state Legislature here to wise up, get a waiver and offer low prices to most buyers.\n",
            "But several states — Alaska, Minnesota, Idaho and Oklahoma among them — have already acted, without waiting for Congress. They used state funds to help cover the sickest people, and relieve pressure on healthy premium payers. Alaska averted a 40 percent premium hike that way last year.\n",
            "Let’s see, the funding is adequate, and the approach works. Spineless politicians whining about “optics” should look in the mirror. What they’re really missing is backbone.\n",
            "Betsy McCaughey is a senior fellow at the London Center for Policy Research and author of “Beating Obamacare.”\n",
            "FORMER Shetland–based computer hacktivist Jake Davis was freed from a young offender institution on Tuesday, however the 20 year old has been ordered to restrict his use of the internet.\n",
            "Davis was sentenced to two years in the Feltham Young Offenders Institute last month for hacking into a wide range of websites, including Sony and the Serious Organised Crime Agency (SOCA) as part of the LulzSec online collective.\n",
            "However 21 months spent on an electronic tag were taken into account and he was released after serving just 37 days.\n",
            "The court has said he is once again allowed to use the internet, but he must not contact anyone connected to the Anonymous hacktivist collective of which he was a part.\n",
            "He is also banned from creating any encrypted files, securely wiping any data or deleting his internet history.\n",
            "Davis was the confident and cocky public voice of the Anonymous splinter group LulzSec that enjoyed making fun of organisations such as The Sun and the right wing Westboro Baptist Church.\n",
            "He was arrested in Lerwick two years ago when police swooped on his Hoofields chalet where he had recently moved from Yell, the island where he grew up with his mother.\n",
            "The final message posted by his online alter ego Topiary was: “You cannot arrest an idea.”\n",
            "At the weekend Davis returned to Twitter, where one of many humourous tweets said: “You can arrest an idea, you can imprison an idea, you can warp an idea, you can break an idea, but you still can’t lick your own elbow.”\n",
            "He is now based in the London borough of Islington where he is working on a number of projects with contemporary art group Artangel.\n",
            "He is also talking about co-writing a film about the internet, he describes as “a drama/sattire” (sic).\n",
            "Aside from that, he tweets: “654 days on curfew and 37 days in Feltham. Up next: another 365 days on license (parole) and 1825 days of intense monitoring. Free though!”\n",
            "He also wants to publish a “nerdy prison diary online once it’s been vetted by twenty million lawyers…”\n",
            "Just before he was sentenced, Davis gave this exclusive interview to the BBC.\n",
            "Senate Democrats emboldened by the GOP’s failure to repeal and replace ­ObamaCare are increasingly coming out against President Trump’s Supreme Court nominee, narrowing Neil Gorsuch’s path to confirmation.\n",
            "Sen. Bill Nelson Clarence (Bill) William Nelson2020 party politics in Puerto Rico There is no winning without Latinos as part of your coalition Dem 2020 candidates court Puerto Rico as long nomination contest looms MORE (D-Fla.) on Monday announced he would vote against ending debate on Gorsuch’s nomination.\n",
            "Nelson, one of several Democrats to announce their opposition on Monday, was a significant blow to Gorsuch because he represents a state won by President Trump and faces reelection in 2018.\n",
            "He’s also one of three Democrats remaining in the Senate who voted to end debate on Justice Samuel Alito’s nomination in 2006.\n",
            "ADVERTISEMENT\n",
            "“I will vote no on the motion to invoke cloture and, if that succeeds, I will vote no on his confirmation,” Nelson said Monday.\n",
            "Gorsuch’s prospects for ending a filibuster got a boost later on Monday when Sen. Joe Manchin Joseph (Joe) ManchinTrump claims Democrats ‘don’t mind executing babies after birth’ after blocked abortion bill Democrats block abortion bill in Senate The Hill's Morning Report - A pivotal week for Trump MORE (D-W.Va.) told a reporter for NBC that he would vote to end debate on the nomination. Manchin said he had not yet decided whether he will back Gorsuch’s nomination.\n",
            "Still, Nelson’s decision suggests Democrats are seeing better political prospects in opposing Trump and Gorsuch than in backing him and risking the ire of the left.\n",
            "In a similar sign, Sen. Patrick Leahy Patrick Joseph LeahySenate plots to avoid fall shutdown brawl Booker wins 2020 endorsement of every New Jersey Democrat in Congress The Hill's Morning Report - Can Bernie recapture 2016 magic? MORE (D-Vt.), a former chairman of the Judiciary Committee, appeared to walk back comments that hinted he might oppose a filibuster against Gorsuch.\n",
            "After telling a Vermont news outlet that he was “not inclined to filibuster,” Leahy tweeted that Gorsuch would be filibustered unless he “provides REAL answers to written Qs & senators have ample time for review & debate.”\n",
            "Democrats are under enormous pressure from liberal groups to oppose Gorsuch after Republicans last year blocked Merrick Garland, President Obama’s nominee to replace the late Justice Antonin Scalia.\n",
            "Republicans need to find eight Democrats to vote to end debate to break a filibuster against Gorsuch.\n",
            "Doing so would prevent them from having to use the nuclear option — voting to change the Senate’s rules to prohibit a filibuster against the Supreme Court nominee.\n",
            "Senate Majority Leader Mitch McConnell Addison (Mitch) Mitchell McConnellHouse to push back at Trump on border Democrats block abortion bill in Senate Overnight Energy: Climate protesters storm McConnell’s office | Center-right group says Green New Deal could cost trillion | Dire warnings from new climate studies MORE (R-Ky.) has signaled his willingness to take this step if Democrats block Gorsuch, who emerged relatively unscathed from last week’s confirmation hearings and is seen by Republicans as well-qualified for the court.\n",
            "A path remains for the GOP to get to 60.\n",
            "All 52 Republican senators will back ending debate on Gorsuch.\n",
            "And a number of Democrats have not said how they will vote.\n",
            "They include senators who are up for reelections in states won by Trump.\n",
            "And while Trump’s victory over Hillary Clinton Hillary Diane Rodham ClintonSanders: 'I fully expect' fair treatment by DNC in 2020 after 'not quite even handed' 2016 primary Sanders: 'Damn right' I'll make the large corporations pay 'fair share of taxes' Former Sanders campaign spokesman: Clinton staff are 'biggest a--holes in American politics' MORE in Florida was close, he dominated the Democrat in North Dakota, Indiana, Montana and Missouri as well as West Virginia.\n",
            "Those states are home to five of 2018’s most vulnerable Democratic incumbents: Sens. Manchin, Heidi Heitkamp Mary (Heidi) Kathryn HeitkampOvernight Energy: Trump taps ex-oil lobbyist Bernhardt to lead Interior | Bernhardt slams Obama officials for agency's ethics issues | Head of major green group steps down Trump picks ex-oil lobbyist David Bernhardt for Interior secretary On The Money: Shutdown Day 27 | Trump fires back at Pelosi by canceling her foreign travel | Dems blast 'petty' move | Trump also cancels delegation to Davos | House votes to disapprove of Trump lifting Russia sanction MORE (N.D.), Claire McCaskill Claire Conner McCaskillPoll: 33% of Kentucky voters approve of McConnell McCaskill: Lindsey Graham 'has lost his mind' Trey Gowdy joins Fox News as a contributor MORE (Mo.), Jon Tester Jonathan (Jon) TesterOvernight Energy: Trump ends talks with California on car emissions | Dems face tough vote on Green New Deal | Climate PAC backing Inslee in possible 2020 run Dems face tough vote on Green New Deal How the border deal came together MORE (Mont.) and Joe Donnelly Joseph (Joe) Simon DonnellyOvernight Energy: Trump taps ex-oil lobbyist Bernhardt to lead Interior | Bernhardt slams Obama officials for agency's ethics issues | Head of major green group steps down Trump picks ex-oil lobbyist David Bernhardt for Interior secretary EPA's Wheeler faces grilling over rule rollbacks MORE (Ind.). Four of the five have kept a tight lip on whether they’ll back Gorsuch.\n",
            "The two other Democrats who backed Alito are Sen. Tom Carper Thomas (Tom) Richard CarperDems slam EPA plan for fighting drinking water contaminants EPA to announce PFAS chemical regulation plans by end of year Overnight Energy: Zinke joins Trump-tied lobbying firm | Senators highlight threat from invasive species | Top Republican calls for Green New Deal vote in House MORE (Del.), who says he will oppose Gorsuch, and Maria Cantwell Maria Elaine CantwellThis week: Congress, Trump set for showdown on emergency declaration Senate reignites blue slip war over Trump court picks Senate votes to extend key funding mechanism for parks MORE (Wash.). Cantwell’s spokesman told the Seattle Times that she’s undecided and will meet with Gorsuch this week.\n",
            "Republicans pounced on Nelson’s decision, pledging that the Supreme Court will be fodder for the 2018 Senate battle.\n",
            "“Senator Bill Nelson proved to Floridians today that he no longer shares their values and instead is more politically aligned with the liberal elite of Washington,” said Katie Martin, a spokeswoman for the National Republican Senatorial Committee.\n",
            "But Nelson is not the only Democrat up for reelection in 2018 who opposes Gorsuch.\n",
            "Sens. Sherrod Brown Sherrod Campbell BrownWorse than nothing's been done since the massive Equifax hack Dems face internal battle over budget On The Money: Dems set Tuesday vote on Trump's emergency declaration | Most Republicans expected to back Trump | Senate plots to avoid fall shutdown drama | Powell heading before Congress MORE (Ohio), Bob Casey Robert (Bob) Patrick CaseyTrump claims Democrats ‘don’t mind executing babies after birth’ after blocked abortion bill Democrats block abortion bill in Senate GOP wants to pit Ocasio-Cortez against Democrats in the Senate MORE Jr. (Pa.) and Tammy Baldwin Tammy Suzanne BaldwinKlobuchar, O'Rourke visit Wisconsin as 2020 race heats up Dems offer smaller step toward ‘Medicare for all' Overnight Health Care — Sponsored by America's 340B Hospitals — Powerful House committee turns to drug pricing | Utah governor defies voters on Medicaid expansion | Dems want answers on controversial new opioid MORE (Wis.) have also all come out against Trump’s nominee, a signal they believe they are better off siding with their party’s base.\n",
            "Another Democratic vote to watch is Sen. Michael Bennet Michael Farrand BennetDemocratic donors stuck in shopping phase of primary Overnight Health Care — Sponsored by America's 340B Hospitals — CDC blames e-cigs for rise in youth tobacco use | FDA cracks down on dietary supplements | More drug pricing hearings on tap The Hill's Morning Report - Presented by the American Academy of HIV Medicine - Next 24 hours critical for stalled funding talks MORE, who represents Gorsuch’s home state of Colorado. He’s facing sustained pressure to back Gorsuch, whom he helped introduce at the committee last week.\n",
            "If Republicans flip Bennet and the five most vulnerable red-state 2018 Senate Democrats, they’d just need two more to confirm Gorsuch.\n",
            "Other targets include Leahy and Sens. Jeanne Shaheen Cynthia (Jeanne) Jeanne ShaheenCongress must step up to protect Medicare home health care Dems slam EPA plan for fighting drinking water contaminants Bipartisan Senators reintroduce legislation to slap new sanctions on Russia MORE (D-N.H.) and Angus King Angus Stanley KingHillicon Valley: Senators urge Trump to bar Huawei products from electric grid | Ex-security officials condemn Trump emergency declaration | New malicious cyber tool found | Facebook faces questions on treatment of moderators Key senators say administration should ban Huawei tech in US electric grid Addressing repair backlog at national parks can give Congress a big win MORE (I-Maine).\n",
            "King, who is up for reelection, hasn’t announced a decision on Gorsuch’s nomination and immediately distanced himself from a filibuster.\n",
            "Shaheen isn’t up for reelection but is from a politically purple state. A spokeswoman told The Hill on Monday that she is undecided and is “currently reviewing hearing transcripts.”\n",
            "Gorsuch’s nomination is expected to come to the Senate floor next week, after Democrats on the Judiciary Committee delayed an initial vote on his nomination until Monday. Under the committee’s rules, any one senator can request that a nomination be held over the first time it appears on the agenda.\n",
            "Republicans want to clear Gorsuch’s nomination before they leave for two weeks and have warned they could delay the Easter recess to confirm him.\n",
            "And McConnell again threatened to go nuclear on Monday.\n",
            "“This much is clear: If our Democratic colleagues choose to hold up this nominee, then they’re acknowledging that they’ll go to any length to block any Supreme Court nominee of a Republican president,” he said.\n",
            "Following news of Demonoid's takedown, the hacktivist group Anonymous attacked the Ukrainian government on Tuesday with its usual technique: a Distributed Denial of Service (DDoS) attack.\n",
            "The collective targeted and took down the National Television and Radio Broadcasting Council of Ukraine (nrada.gov.ua), the Ukrainian Agency for Copyright and Related Rights (uacrr.kiev.ua), and the Ukrainian Anti-Piracy Association (apo.kiev.ua).\n",
            "All the sites appear to be fully operational again at the time of writing.\n",
            "The initiative, dubbed OpDemonoid, was explained by an Anonymous-released video -- seen above -- which announced the plan on Tuesday.\n",
            "In a previous statement to the public, Anonymous reminded the Ukrainian government that they had faced its wrath before. Here's an excerpt:\n",
            "Haven't you, Ukraine, learned anything from the Anonymous Collective? You were attacked once, and yet feel the need to keep censoring us, your people, and every day hard working citizens? Ukrainian government, You should have expected us.\n",
            "Last month, a massive Distributed Denial of Service (DDoS) attack brought Demonoid to its knees. Last week, the server was turned off completely and the site led to a dead end . Then it came back to life, and started redirecting to random sites full of advertisements . Eventually this stopped and both demonoid.me and demonoid.ph crumbled again.\n",
            "Then we learned that the situation was much worse. Demonoid was busted by Ukrainian authorities who had a talk with ColoCall, the largest datacenter in Ukraine. Mexico initiated a criminal investigation into Demonoid's owners .\n",
            "Anonymous is apparently interested in punishing Ukraine right now; it's not yet clear if Mexico will be next.\n",
            "See also:\n",
            "Today, HM is proud to premiere Emery’s newest single, “The Less You Say,” as part of the band’s inventive plan to release their entire new album, You Were Never Alone, via podcast. All in all, there are 12 tracks on the band’s upcoming full-length album, due out May 19 from the label they own, BC Music. Emery band members Matt Carter and Toby Morrell are in the midst of a new podcast launch called Break It Down (iTunes), dedicated to the stories behind the songs, with the first 12 episodes the premieres of every track on the band’s sixth studio release.\n",
            "Carter and Morrell both agree on one thing: This is the best song on the album. Carter spoke with HM about their favorite track.\n",
            "You might not believe me, but I am so thankful to have people enjoying this podcast. I was nervous about trying something this different and am overwhelmed by the response. What’s more, this song is actually my favorite on the entire album. Toby says it is his as well. It feels like an old school me song and something we would have tried to write in 2000. It has the perfect blend of Toby’s and Devin’s vocals, and it has some guitar parts that just plain tickle me to listen to and play. I hope you enjoy it at least partly as much as I do.\n",
            "As the members of Emery wrote, recorded and toured, some of them remained committed to their call-it-like-it-is Bad Christian podcast (and accompanying brand), which had skyrocketed out of the gate after its launch in 2013. They were able to begin BC Music with the infrastructure they created with Bad Christian, also dipping their toes into publishing and remaining committed to a cutting edge approach to the music industry.\n",
            "Bats and moths have been engaged in acoustic warfare for more than 60 million y. Yet almost half of moth species lack bat-detecting ears and still face intense bat predation. We hypothesized that the long tails of one group of seemingly defenseless moths, saturniids, are an anti-bat strategy designed to divert bat attacks. Using high-speed infrared videography, we show that the spinning hindwing tails of luna moths lure echolocating bat attacks to these nonessential appendages in over half of bat–moth interactions. Further we show that long hindwing tails have independently evolved multiple times in saturniid moths. This finding expands our knowledge of antipredator deflection strategies, the limitations of bat sonar, and the extent of a long-standing evolutionary arms race.\n",
            "Abstract\n",
            "Adaptations to divert the attacks of visually guided predators have evolved repeatedly in animals. Using high-speed infrared videography, we show that luna moths (Actias luna) generate an acoustic diversion with spinning hindwing tails to deflect echolocating bat attacks away from their body and toward these nonessential appendages. We pit luna moths against big brown bats (Eptesicus fuscus) and demonstrate a survival advantage of ∼47% for moths with tails versus those that had their tails removed. The benefit of hindwing tails is equivalent to the advantage conferred to moths by bat-detecting ears. Moth tails lured bat attacks to these wing regions during 55% of interactions between bats and intact luna moths. We analyzed flight kinematics of moths with and without hindwing tails and suggest that tails have a minimal role in flight performance. Using a robust phylogeny, we find that long spatulate tails have independently evolved four times in saturniid moths, further supporting the selective advantage of this anti-bat strategy. Diversionary tactics are perhaps more common than appreciated in predator–prey interactions. Our finding suggests that focusing on the sensory ecologies of key predators will reveal such countermeasures in prey.\n",
            "–\n",
            "Tension between the international community and Iran is mounting. With the International Atomic Energy Agency (IAEA) ready to issue a report on Tehran’s attempts of developing nuclear weapons, with recent claims that scientists from Russia, Pakistan, and North Korea have all played an important role in helping Iran come close to full nuclear capacity, and perhaps most crucially, with Israel warning of the possibility of launching a pre-emptive attack, every move now made in the international arena is crucial. While the US and UK weigh up their military options, we must ask: should the UK really get involved?\n",
            "The IAEA report on Iran’s nuclear developments is scheduled to be released to the 15 members of the UN Security Council on Wednesday. However, as details of the report begin to emerge from leaked information in today’s Washington Post, the gravity of the situation is becoming clearer. Namely, the details demonstrate Iran is at a particularly advanced stages of designing a nuclear explosive.\n",
            "The Telegraph’s Alex Spillius describes this well in his recent article on the IAEA report. He emphasizes that Iran is currently designing a nuclear explosive device small enough to fit in a warhead that involves an R265 generator. The R265 generator has been described in the Washington Post as ‘a hemispherical aluminum shell with an intricate array of high explosives that detonate with split-second precision. These charges compress a small sphere of enriched uranium or plutonium to trigger a nuclear chain reaction.’ Additionally, although tests have not yet been carried out, the IAEA states’ satellite images have confirmed a site in the Qom region that is supposedly being used to house a completed a steel container.\n",
            "Iran’s build up of nuclear technology has been aided by foreign assistance. Vyacheslav Danilenko, a former Soviet atomic scientist, has recently been named as having played a key role in Iran’s nuclear development. Not only is he said to have given lectures, but he has also contributed substantial technical information to Iran’s nuclear developers. However, nuclear scientists from Pakistan and North Korea are also said to have supplied formulas and assistance as well.\n",
            "Most importantly, the way that the international community now plays its cards is absolutely fundamental. Although particular countries such as France and China have condemned Iran’s so called ‘obsession’ with nuclear development, anger at perceived USA hypocrisy only seems to be spurring Iran on even more. In fact, Saeed Jalili, Iran’s nuclear negotiator, has claimed that the USA is waging terrorism against the country.\n",
            "However, tensions with Israel are also starting to mount. Shimon Peres, the Israeli President, has warned of the possibility of a pre-emptive attack. If this materialises, it seems highly unlikely that the US and its allies would not interfere. Therefore, the role of the UK in this matter is substantially more important than the media is portraying it to be. Namely, if the US seeks UK support in targeting Iran’s nuclear sites, enormous decisions will have to be made.\n",
            "The UK is not really sitting in the sidelines. It has already stated it is upping its contingency plan to join any potential offensives. Although the UK might be giving the USA a reassuring pat on the shoulder, if crunch time really comes and the UK becomes heavily involved, then we will all have to step back and reassess.\n",
            "It has to be said that despite concern of these recent developments, it is pertinent that other issues are addressed; focus cannot be lost. The UK needs to handle its current involvement in Libya and continue playing an active role in bringing solutions to the table regarding the EU crisis. Furthermore, it needs to ensure domestic happiness before it pursues what could potentially become an extremely regrettable war. To reiterate, The UK should not start any more unnecessary and preventable wars but should focus on its agenda that is already full of issues far from easy to tackle.\n",
            "Advertisements\n",
            "Gay journalist Chadwick Moore – who recently came out as a conservative – spoke at Portland State University in a speech that drew protests and prompted Moore to boldly engage demonstrators who heckled him.\n",
            "The speech, “The Joys of Being an Infidel: Challenging Orthodoxy and Standing Up for Free Speech in America,” drew roughly 60 students and community members, including about a dozen student protesters.\n",
            "They held signs declaring “No sympathy for alt-right trash” and “Destroy your local fascist,” and at times disrupted the speech with verbal outbursts. Moore responded in sometimes feisty rebuttals as the two sides clashed.\n",
            "Moore entered the national spotlight after coming out as a conservative in an op-ed in the New York Post in February that detailed the intense backlash and hatred he received from his once beloved and supportive gay community for writing a feature on Milo Yiannopoulos for Out magazine.\n",
            "“If you dare to question liberal stances or make an effort toward understanding why conservatives think the way they do, you are a traitor,” Moore wrote in his coming out piece. “It can seem like liberals are actually against free speech if it fails to conform with the way they think. And I don’t want to be a part of that club anymore.”\n",
            "Now, as an emerging defender of free speech, he finds himself a target.\n",
            "‘The Joys of Being an Infidel’\n",
            "In opening his Portland State speech on April 28, he alluded to its title with an Islamic greeting: “As-Salaam-Alaikum,” he said. “That’s how they say it in France.”\n",
            "The event was organized by Freethinkers of PSU, a nonpartisan classical liberal and humanist student group.\n",
            "Blake Horner, one of the leaders in Freethinkers of PSU, said that some protest was expected given that dozens of flyers promoting the event had been vandalized or torn down during the preceding week.\n",
            "“It seems that many people at PSU were motivated to halt public knowledge of this event,” Horner said. “We were also confronted by someone who was determined to intimidate us.”\n",
            "On the day of the speech, messages plastered on the group’s display case called Moore a “fascist defender.”\n",
            "Moore reserved strong criticism for PSU’s Queer Resource Center at the beginning of his speech. He pointed out what he perceived as the center’s political bias for refusing Freethinkers’ request to place a flyer in its space while socialist promotional material is displayed on its windows.\n",
            "“Here I am, a public gay person who was working for the two largest gay magazines in the world as their top investigative journalist, and they can’t put that up there because they don’t like my politics,” Moore said. “Maybe the Queer Resource Center should rebrand itself as something less misleading.”\n",
            "Moore suggested the center call itself the “Ministry of Propaganda” or the “I’m with Her Memorial Museum and Gift Shop,” referring to Hillary Clinton’s failed 2016 presidential bid. The audience burst out in laughter.\n",
            ".@Chadwick_Moore christens the PSU Queer Resource Center the “I’m With Her Memorial Museum & Gift Shop.” Full video: https://t.co/vqhlAw0GBn pic.twitter.com/0medDT34P7 — Andy C. Ngo (@MrAndyNgo) May 7, 2017\n",
            "Moore later read from a “power and privilege” training document he received from a PSU student. He criticized the training material, which defined white people, heterosexuals and English-speaking people, among other groups, as “agents of oppression” due to their privilege.\n",
            "After addressing the training material’s arguments point-by-point with counter facts and statistics, Moore ripped up the document.\n",
            "“Anyone who gets this in a future class, this is what you have to do to it,” he said. “Sign up for a new class.”\n",
            "‘We can punch you too’\n",
            "Protesters began to heckle and disrupt Moore further in his speech as he continued to ridicule social justice activism and the political far-left.\n",
            "“Can you not wait until the Q&A and be polite?” Moore responded as the interruptions continued. “Why don’t you shut up and have respect for your fellow students?”\n",
            "Later, an audience member called out the rude behavior of some protesters. “Stop being homophobic, let the gay man talk!” he shouted. “You’re stifling gay speech.”\n",
            "Moore carried on with his lecture but about midway through another student yelled at him from the middle of the room.\n",
            "“I am black, I am disabled, I’m a woman,” she shouted. After a back-and-forth, Moore invited her to speak during the Q&A. The student stormed out of the room and pounded on the window with her fists.\n",
            "“Girl, there’s still time, we can punch you too,” a student shouted to Moore after he mocked the disrupters’ low energy. “Sorry, not a threat,” she said after the audience gasped. Some students in the audience recognized her as a candidate for student government.\n",
            "1 in 5 gay Americans are conservative\n",
            "Moore closed his speech by reading part of a letter he received by a gay man who thanked him for “coming out” in his New York Post op-ed.\n",
            "“This touched me so much and I cried a little because I was thinking about how much the gay community has meant to me my entire life,” Moore said.\n",
            "Citing a Gallup survey that estimates 1 in 5 gay Americans are conservative, Moore shifted his ire to queer resource centers across the country.\n",
            "“If you decide to shun a huge percentage of your community simply because they might not agree with your political views … you’re denying people a chance to true happiness of living authentically,” he said.\n",
            "During the Q&A, audience members used the opportunity to express support, criticism or gratitude for Moore’s partisan views.\n",
            "“I was one of those people who wrote you a message when you came out,” said a young woman in the audience. “I want to personally thank you for being as loud as you are because you’re speaking up for people like me.”\n",
            "Later on, one of the protesters who earlier held a “Black Lives Matter” sign asked Moore about his views on racial matters.\n",
            "“You talk about how you feel like you don’t have free speech in some places,” she said. “Are you also fighting then for the free speech of black gay Americans?”\n",
            "Puzzled by the question, Moore asked her to clarify.\n",
            "“Knowing people who side with the right-side … they tend to be racist,” she said.\n",
            "Moore stated that he supports free speech full stop.\n",
            "“Why would I not want black people to have a voice?” he asked. “I want everyone to have a voice. More speech is more speech.”\n",
            "Freethinkers\n",
            "After answering questions for about 40 minutes, Moore thanked the audience and some of the protesters for voicing their dissent in a respectful manner.\n",
            "Several attendees expressed their gratitude to event organizers for hosting the event.\n",
            "“I was impressed by Chadwick standing up to these bullies and speaking his mind,” said Mykle Curton, a self-identified leftist who graduated from PSU in 2013. “Just because I disagree with him on politics doesn’t mean I can’t like and support him. I agree with him about his rejection of identity politics. They argue that you can lump people into groups and generalize their experiences and beliefs.”\n",
            "Marko Balogh, a student leader of the Freethinkers, expressed concern that the event was too politically polarizing and didn’t further the mission of the organization.\n",
            "“While I think free speech—including the freedom to offend—is an absolutely vital component of an intellectually healthy society, I don’t think the excessively combative demeanor of the speaker was helpful,” he said. “If we are going to reduce political polarization and make our society better for everyone, we have to approach politics from a charitable and well-meaning mindset.”\n",
            "Balogh said he hopes future events organized by Freethinkers would encourage conversations in which “all sides of the issues are considered wholeheartedly.”\n",
            "Editor’s note: Andy Ngo was involved in organizing this event.\n",
            "Like The College Fix on Facebook / Follow us on Twitter\n",
            "PHOTO CREDITS: Main, Twitter screenshot; Top, Collin Berend; Bottom, Andy Ngo\n",
            "The Matt Damon film also had the biggest debut weekend for the month of October.\n",
            "The Martian earned a massive $12.1 million in South Korea over the weekend, reaping almost half of the box-office revenue in the country over the period.\n",
            "The fact that Friday was a national holiday helped boost figures, which marked the biggest opening of all time for Fox in the country.\n",
            "The Matt Damon film, distributed by 20th Century Fox Korea, grabbed 48.3 percent of the three-day weekend market share, according to the Korean Film Council. This is also the biggest opening score for the month of October in the Asian country, whose population watches more films per year on average, more than four, than any other country in the world.\n",
            "Hollywood films set in outer space have generally done well. The Martian also broke opening-day records previously held by Avatar and Interstellar, drawing a total of over 1.53 million admissions. Local industry observers in the country of a population of 50 million primarily measure a film's performance by theater attendance.\n",
            "The successful debut of The Martian pushed The Intern down to No. 2. The Anne Hathaway-Robert De Niro comedy accounted for 15.3 percent of the box-office market share this weekend. Handled by Warner Bros. Korea, the film has so far earned a cumulative $15.8 million.\n",
            "Local film The Advocate: A Missing Body debuted in third place as it reaped 13.8 percent of box-office revenue. The CJ Entertainment title has grossed about $4 million overall. Starring A Hard Day actor Lee Sun-kyun in the lead role, the crime actioner is about an ace advocate who is about to win a missing-body homicide case. Things take an unexpected turn, however, when his defendant, the primary suspect, suddenly confesses to the murder, and the lawyer suddenly finds himself having to defend his pride and stand up for justice.\n",
            "Following the successful debuts of The Martian and The Advocate, homegrown period drama The Throne dropped two spots to finish in fourth place this weekend. The costume drama, which is Korea's entry for the foreign-language Oscar race, took 7.2 percent of sales that added to its gross total of over $4.1 million. The film set in the Joseon Kingdom (1392-1910) crossed 6 million admissions, making it the eighth best film of the year. Showbox/Mediaplex distributes the film, which was chosen as the opener for the Hawaii International Film Festival that kicks off on Nov. 12.\n",
            "The Accidental Detective, a second CJ Entertainment title in the top ranks, finished at No. 5. This was two spots down from the previous week as the film accounted for 6.3 percent of the box-office total. The crime comedy's cumulative box-office revenue comes down to a little less than $17 million.\n",
            "It pays to have friends in high places.\n",
            "Al Sharpton gave himself a 71 percent raise last year after his National Action Network group drew a record $6.9 million in donations — as the controversial cleric’s association with Mayor de Blasio and President Obama lent him a newfound air of legitimacy.\n",
            "De Blasio’s election gave Sharpton a seat at City Hall, as the mayor treated him as an adviser and presented him at a press event next to Police Commissioner Bill Bratton after the death of Eric Garner.\n",
            "Also in 2014, Obama addressed NAN’s annual convention, bringing along five of his Cabinet members.\n",
            "The Harlem-based nonprofit collected $2 million more in 2014 than the year before, according to the latest financial records available.\n",
            "Sharpton’s pay increased from $241,545 in 2013 to $412,644, including a bonus of $64,400, tax filings obtained by The Post show.\n",
            "In April, four months after his pal de Blasio took office, some 5,000 people attended NAN’s 2014 conference headlined by Obama and attended by the mayor, Gov. Cuomo and then-US Attorney General Eric Holder. It was NAN’s biggest fund-raiser of the year.\n",
            "In October, a few months after Sharpton’s summer of police protests, he pulled in a reported $1 million at his 60th-birthday bash. Pols and dignitaries rushed from the annual Al Smith Dinner hosted by Timothy Cardinal Dolan that same night to the NAN fund-raiser at the Four Seasons restaurant.\n",
            "NAN does not need to disclose its donors, but a program distributed at his birthday parties had full-page ads from companies that gave money. AT&T, real-estate developer Forest City Ratner, Walmart, McDonald’s, Verizon and GE Asset Management were all included.\n",
            "Although Sony’s then-co-chairwoman, Amy Pascal, met with Sharpton in 2014 after a hacker revealed racially insensitive e-mails, the reverend told The Post that Sony did not give NAN money.\n",
            "The windfall allowed the organization to finish 2014 in the black. Yet NAN, and Sharpton, continued their deadbeat ways.\n",
            "NAN didn’t pony up its last installment on the $780,145 it owed the IRS for unpaid payroll taxes until this Oct. 22, the organization’s financial statements show. The nonprofit had racked up unpaid payroll taxes since at least 2003, according to public records.\n",
            "Sharpton, meanwhile, still has outstanding tax liens of $3.4 million, including money he owes personally to both New York state and the IRS, and taxes owed by his businesses, according to public records. He paid off one federal lien, for $931,398, in April.\n",
            "Sharpton has maintained that he owes much less money than records reflect and that he has been chipping away at the bills through payment plans.\n",
            "He also insists he didn’t really get a raise last year.\n",
            "“I’m glad that NAN has resolved all of our past tax debts three years earlier than our agreement with the IRS and paid part of the compensation owed to me for several years I did not receive a salary,” Sharpton said in a statement.\n",
            "Tax records show NAN paid him $4,860 in 2006 and nothing in 2007 and 2008.\n",
            "But a spokesman for the organization offered a different explanation, saying the salary hike was repayment of loans Sharpton previously made to NAN.\n",
            "Former IRS official Marcus Owens, an expert on nonprofit law, said the explanation seemed odd.\n",
            "“To structure the payoff of a loan through a salary transfer doesn’t seem really plausible because no one would want to pay income tax on their own money coming back to them,” Owens said. “It seems like that characterization was developed after the fact for some other purpose.”\n",
            "I decided to add my name to a petition by, as of this writing, 81 MIT faculty, calling on MIT to divest its endowment from fossil fuel companies. (My co-signatories include Noam Chomsky, so I guess there’s something we agree about!) There’s also a wider petition signed by nearly 3500 MIT students, faculty, and staff, mirroring similar petitions all over the world.\n",
            "When the organizers asked me for a brief statement about why I signed, I sent them the following:\n",
            "Signing this petition wasn’t an obvious choice for me, since I’m sensitive to the charge that divestment petitions are just meaningless sanctimony, a way for activists to feel morally pure without either making serious sacrifices or engaging the real complexities of an issue. In the end, though, that kind of meta-level judgment can’t absolve us of the need to consider each petition on its merits: if we think of a previous crisis for civilization (say, in the late 1930s), then it seems obvious that even symbolic divestment gestures were better than nothing. What made up my mind was reading the arguments pro and con, and seeing that the organizers of this petition had a clear-eyed understanding of what they were trying to accomplish and why: they know that divestment can’t directly drive down oil companies’ stock prices, but it can powerfully signal to the world a scientific consensus that, if global catastrophe is to be averted, most of the known fossil-fuel reserves need to be left in the ground, and that current valuations of oil, gas, and coal companies fail to reflect that reality.\n",
            "For some recent prognoses of the climate situation, see (for example) this or this from Vox. My own sense is that the threat has been systematically understated even by environmentalists, because of the human impulse to shoehorn all news into a hopeful narrative (“but there’s still time! if we just buy locally-grown produce, everything can be OK!”). Logically, there’s an obvious tension between the statements:\n",
            "(a) there was already an urgent need to act decades ago, and\n",
            "(b) having failed to act then, we can still feasibly avert a disaster now.\n",
            "And indeed, (b) appears false to me. We’re probably well into the era where, regardless of what we do or don’t do, some of us will live to see a climate dramatically different from the one in which human civilization developed for the past 10,000 years, at least as different as the last Ice Ages were.\n",
            "And yet that fact still doesn’t relieve us of moral responsibility. We can buy more time to prepare, hoping for technological advances in the interim; we can try to bend the curve of CO 2 concentration away from the worst futures and toward the merely terrible ones. Alas, even those steps will require political will that’s unprecedented outside of major wars. For the capitalist free market (which I’m a big fan of) to work its magic, actual costs first need to get reflected in prices—which probably means massively taxing fossil fuels, to the point where it’s generally cheaper to leave them in the ground and switch to alternatives. (Lest anyone call me a doctrinaire treehugger, I also support way less regulation of the nuclear industry, to drive down the cost of building the hundreds of new nuclear plants that we’ll probably need.)\n",
            "These realities have a counterintuitive practical implication that I wish both sides understood better. Namely, if you share my desperation and terror about this crisis, the urgent desire to do something, then limiting your personal carbon footprint should be very far from your main concern. Like, it’s great if you can bike to work, and you should keep it up (fresh air and exercise and all). But I’d say the anti-environmentalists are right that such voluntary steps are luxuries of the privileged, and will accordingly never add up to a hill of beans. Let me go further: even to conceptualize this problem in terms of personal virtue and blame seems to me like a tragic mistake, one on which the environmentalists and their opponents colluded. Given the choice, I’d much rather that the readers of this blog flew to all the faraway conferences they wanted, drove gas-guzzling minivans, ate steaks every night, and had ten kids, but then also took some steps that made serious political action to leave most remaining fossil fuels in the ground even ε more likely, ε closer to the middle of our Overton window. I signed the MIT divestment petition because it seemed to me like such a step, admittedly with an emphasis on the ε.\n",
            "We've been hearing a lot about Volantis lately, but what about the other supposed Nexus device - Shamu? Since we originally broke the story back in July (with the Information affirming Shamu's existence soon after) things have been relatively quiet, with only a benchmark test here or there popping up with alleged specs that seemed to point to a smaller device.\n",
            "Today, however, 9to5Google has divulged specs and details about the device in which the outlet seems fairly confident. 9to5 says it can corroborate the 5.9\" screen size, and notes that \"the device we've learned about\" actually carries the following specs, some of which clash with earlier reports:\n",
            "5.92\" QHD display (498PPI)\n",
            "3200mAh battery\n",
            "13MP camera and 2MP front shooter\n",
            "Snapdragon 805 processor\n",
            "3GB RAM\n",
            "Worth noting is that 9to5's information aligns almost perfectly with information we received earlier this year, but they also sound very close to a device we've seen under the name Quark, which may in fact be the basis for Shamu.\n",
            "We can also fill in some of the gaps in 9to5's information. For instance, Shamu (as we noted in our original coverage) is expected to carry a fingerprint sensor on its back, replacing Moto's signature dimple/logo décor. Additionally, 9to5 notes that Shamu doesn't appear to be running a 64 bit version of Android, and indeed the information we saw earlier this year indicated that the device would launch on a 32 bit version of Android L (while Volantis will carry a 64 bit version). Our information at the time also suggested that Shamu would have wireless charging capabilities (and compatibility with Moto's new Turbo Charger), but as with any pre-release information, we can't be totally certain just yet.\n",
            "9to5 also notes that the form factor has only been slightly modified from the Moto X as far as they can tell - the volume rocker and power button are said to be positioned more toward the center of the device to allow for easier access.\n",
            "Finally, 9to5 doesn't rule out the possibility of multiple Nexus phones. While we can't corroborate the existence of a smaller Nexus phone, there has certainly been evidence floating around to suggest a 5.2\" device. Shamu, according to 9to5's information is still aiming for a November release, with an announcement at some point in October. Hit the link below for the full story, and 9to5's own mockup of the device.\n",
            "Source: 9to5Google\n",
            "“Thinking about Niebuhr’s Serenity Prayer.” Photo illustration by Slate. Photo by Cheriss May/NurPhoto via Getty Images.\n",
            "James Comey has quietly been on Twitter since 2014, but since that time, the former FBI director had only tweeted once—and it was only after Gizmodo blew up his spot. Then last week, as though suddenly possessed, he started tweeting, posting five times in six days, a fairly rapid rate for someone whose previous output was a single Will Ferrell joke. Still using the name Reinhold Niebuhr, for the theologian he wrote his college thesis on (and still not bothering to change the default profile picture), Comey decided to allow us a peek into his post-FBI, country-spanning, decidedly Under the Tuscan Sun–like journey of self-reflection, which has taken him from Gettysburg, Pennsylvania, to Iowa. Yes, Iowa—the very place you go to kick the tires of a presidential run. (Comey’s wife is from Iowa, but c’mon, we all have “relatives from Iowa.”) Whether Comey is trying to lay that particular groundwork or simply feels inspired to connect with regular Americans (who can one day buy his book, which is conveniently forthcoming) is anybody’s guess. For now, all we have to go on is the tweets themselves. Let’s review them, with the forensic attentiveness Comey would no doubt demand, one by one.\n",
            "We begin at the beginning, back in March, with Comey’s response to the Gizmodo investigation by Ashley Feinberg:\n",
            "It’s a macro of Will Ferrell in Anchorman, and it feels a lot like Dad trying to pretend he’s in the know with what the cool kids are doing—the outdated movie reference (Daad), the way he capitalized the F in Fbijobs.gov (Daaad), the laissez-faire “I don’t even care that you found my secret Twitter” attitude (Daaaaaaaaaaaad). Not cool, Dad. Grade: D\n",
            "Beautiful fall day at West Point. Lone kayaker on the Hudson. pic.twitter.com/gcJ730VD7p — James Comey (@Comey) October 18, 2017\n",
            "Reinie Niebs was quiet until this past Wednesday, when he was so moved by nature and the view at West Point that he decided to fire up the ol’ Twitter and to post what is essentially an out-of-place Instagram. It’s still the work of an old who doesn’t understand the platform or how most people use it, but instead of repeating the embarrassing try-hardery of the Anchorman tweet, it’s endearing. He loves his country and his country’s historic military academy, he loves rivers, and he finds beauty in images of men striking out on their own. Which, reminder, he does too from time to time. Lordy! Grade: B\n",
            "Little Round Top, Gettysburg. Good place to think about leadership and values. pic.twitter.com/o1cKBXrLCl — James Comey (@Comey) October 19, 2017\n",
            "This photo is of a bunch of rocks, but not a very aesthetically pleasing pile of rocks. Comey still gets points for being at Gettysburg, the Mecca of late-middle-age patriots. He loses points for “Good place to think about leadership and values,” which is a bit too transparent. You’re not supposed to tell people you’re thinking about leadership and values, Jim, you’re supposed to imply it with your constant self-seriousness. Anyway, what, pray tell, would all that thinking about leadership and values yield? “Ah! Here I am at Gettysburg. Now I will reflect on leadership as well as values. Both … are good?” Grade: C\n",
            "Good to be back in Iowa. pic.twitter.com/TGJHOhQ9KF — James Comey (@Comey) October 21, 2017\n",
            "First of all, this is just a nice picture: nice sky, nice sunset. There’s a “Where’s Waldo?” aspect of it, because you have to find the former FBI director hiding among the corn, and that’s always satisfying. (But then you imagine Comey asking his wife to go take a picture of him hidden in the corn, and it’s back to being weird.) The text itself is an understated gem: Yes, it is good to be back in Iowa. Iowa is real America. By the transitive property, James Comey is real America? It seems like Comey wouldn’t be mad if that was your takeaway. Exactly. Grade: A-\n",
            "Watching migrating white pelicans in Iowa thinking about Niebuhr’s Serenity Prayer. pic.twitter.com/TCAU4gs0Jw — James Comey (@Comey) October 22, 2017\n",
            "Why is this guy so obsessed with Niebuhr! Confidential to James Comey: Most of us still have no idea who that is. And, please, show don’t tell: A picture of birds in a V formation, subtly reminding us that Comey believes in following procedure and the chain of command, is much better than stating, piously, that you’re thinking about a prayer that happens to be your favorite theologian’s claim to fame. Grade: B-\n",
            "Goodbye Iowa. On the road home. Gotta get back to writing. Will try to tweet in useful ways. pic.twitter.com/DCbu3Yvqt3 — James Comey (@Comey) October 23, 2017\n",
            "Another one presumably taken by Comey’s long-suffering wife: “Hon, could you get one of me in the street here?” That he wanted a shot of him looking plaintively into the Iowa distance is understandable, though it’s somewhat undercut by his giantlike stature. Still, he’s learned: He doesn’t tell us that he’s thinking about leadership and values, but clearly that’s what he wants people to read into this photo, even if one gets lost trying to envision the scale like the tweet is a math problem on the SAT: “If he’s 6 foot 8, then how wide is the road??” Hiding in plain sight amid the strange composition, Comey finally mentions what he’s really been up to with this slow-form tweetstorm: “Gotta get back to writing.” I think you forgot the #ad hashtag there, buddy. Then (note the hapless two spaces between sentences), “Will try to tweet in useful ways.” What could he possibly mean by “useful” here? Can we expect a tweetstorm with instructions on how to dismantle the government? Trenchant commentary on the Russia investigation? So far, his tweets are the opposite of useful. But in their entertaining bizarreness, they offer a small insight into the mind of a guy who wants to say more than he’s been able to, and, we hope, one day soon will get a chance to. Instant follow. Grade: B+\n",
            "A horrifying new dashcam video shows at least one white member of the Aiken, S.C. Police Department search inside a black man’s rectum for the apparent crime of being a passenger in a car with paper tags, which, it bears noting, is not a crime in the state of South Carolina. Driving a car with paper tags isn’t illegal either, as long as they aren’t expired.\n",
            "The Washington Post reports the car belonged to a woman named Lakeya Hicks, who had the paper tags because she had recently purchased the vehicle. Hicks’ tags weren’t expired, and a check on her license came up clean. But the cops didn’t let her go.\n",
            "Instead, Officer Chris Medlin asked the passenger, Elijah Pontoon, for identification. Pontoon, who has a criminal record, was ordered out of the car and handcuffed. In the video, Medlin explains his legal rationale: “Because of your history, I’ve got a dog coming in here. Gonna walk a dog around the car.” Also: “You gonna pay for this one, boy.”\n",
            "The k9 arrives but find no contraband. The cops search the car but find no contraband. This evidence does not lead them to conclude there was no contraband. This evidence apparently leads them to decide to search Pontoon’s asshole.\n",
            "Audio from the video suggests all they find was a hemorrhoid. At the same time, according to a federal lawsuit filed on their behalf, a female officer was searching Hicks, exposing the woman’s breasts to the other three male officers and anyone else who happened to be on the road.\n",
            "Again, no contraband was found.\n",
            "Later, a dejected Medlin can be heard on the radio speaking to a superior after the fruitless search.\n",
            "“We search the car. There ain’t nothing in the car... And on a search of him, up in his crotch by the butt, I felt something hard. I lifted his pants and pulled the back of his underwear down and I didn’t see anything but I didn’t get all the way up in there to get no vertical up shot. I just pulled his underwear back, but I didn’t see nothing. But it felt, he said it was a hemorrhoid. It ain’t no... it was a rock. It was a rock of crack. It’s gotta be a rock. He’s got it up in his butt.”\n",
            "Medlin sadly concludes, “But there ain’t no way to justify. He said, ‘I got nothing here. That’s a long time ago. I ain’t doing nothing.’ He said it’s a hemorrhoid. I got nothing else to go on. Nothing. Yeah we’re gonna have to cut him loose here.”\n",
            "Who needs probable cause to perform an invasive search? Apparently not Medlin.\n",
            "Get the biggest Celtic stories by email Subscribe Thank you for subscribing We have more newsletters Show me See our privacy notice Could not subscribe, try again later Invalid Email\n",
            "Celtic are sending highly rated teenager Kris Ajer on loan to Kilmarnock for the rest of the season.\n",
            "The Norwegian midfielder signed a four-year deal with the Hoops last February after impressing on trial.\n",
            "Ajer, 18, has yet to start for Celtic under Brendan Rodgers but has always been considered a player for the future.\n",
            "(Image: Ken McKay)\n",
            "Now he will be given to Killie gaffer Lee Clark for the next four months as his education in the Scottish game moves up a gear.\n",
            "Rodgers said: “Kris will go to Kilmarnock. There were a couple of options but it’s good for him to play there.\n",
            "“He has settled in here so he can go and play there for the rest of the season where we can keep a close eye on him and take him back.”\n",
            "(Image: SNS Group)\n",
            "Kris Commons has returned after an emergency loan at Hibs, who are keen to keep him at least until the end of the season.\n",
            "And Kieran Tierney could make his comeback from injury against Albion Rovers in the Cup on Sunday, although new signing Kouassi Eboue won’t play.\n",
            "He added: “Kieran’s back. I wouldn’t be reluctant to play him, even on the astro.\n",
            "“Kouassi won’t feature. His visa has just been completed and he’ll be back but not by then.”\n",
            "There's something creepily totalitarian about Orange Coast College's handling of the now nationally known incident involving 19-year-old Caleb O'Neil, who was suspended from the college for two semesters for secretly videotaping one of his instructors engaging in an anti-Trump rant. The suspension was bad enough, but the college also insisted that O'Neil apologize to the professor and hand in an apologetic essay about his transgressions. Fortunately, after a groundswell of public support for the student—and academic freedom generally—the college backed down and rescinded the suspension and other sanctions.\n",
            "For those who have missed the Register articles, human sexuality instructor Olga Perez Stable Cox was caught on video telling her class that Donald Trump's presidential victory was \"an act of terrorism.\" The most frightening thing, she said, \"is that the people who are leading the assault are among us.\" O'Neil eventually posted the video after being unsatisfied with the college's response. It went viral. As someone who campaigned for Trump, O'Neil had reason to fear the instructor's views could have repercussions on his grades. After all, he's one of those \"among us.\"\n",
            "O'Neil's supporters, including some local Republican leaders, blasted the professor for using her teaching position to shame students. The union representing faculty there criticized O'Neil for not engaging in an \"open dialogue.\" The school administration said it would investigate the complaint filed against the teacher, but the only obvious repercussions so far are the harshly punitive sanctions it tried to impose against the student.\n",
            "The college's letter to O'Neil, published on a website, understandably drew a backlash. It said that O'Neil's essay should, among other things, discuss his \"thoughts and analyses on the impact of the video going 'viral' and the ensuing damage to Orange Coast College students, faculty and staff.\" So he was going to be required to write an essay—and the school was telling him the basic view he had to express in it.\n",
            "Cox says she has received angry emails and messages and now feels \"paranoid\" and like she's been \"attacked by a mob of people all across the country.\" Any threats and intimidation are wrong, of course. But this whole situation could have been avoided had the college administration acted in an even-handed manner in the beginning. We don't want a world where college professors are afraid to speak forthrightly to their classes, but students are at least owed an apology when subjected to an inappropriate rant.\n",
            "O'Neil was being disciplined for violating the school's prohibition on unauthorized recording, which is a picayune point, given that this is a taxpayer-funded school. The First Amendment should still apply there. The punishment was outsized compared to the transgression. And even such a punishment would have been more tolerable if one could have any confidence the college treated with any seriousness Cox's in-classroom transgression.\n",
            "This scene has sparked anger because it confirms the worst fears conservatives have about liberal intolerance in academia. In a sane world, a few apologies would suffice. \"Hey, I was distraught about the election and was unfair in my comments.\" \"OK, I was upset by your remarks and believed that a video was my only recourse.\" The administration would step in and everyone would sing \"Kumbaya.\" Instead, lawyers got involved, and there was even a threatened recall of college board members. It's the result of a college that decided to dig in its heels rather than pursue fairness. The punitive response suggests the problem at Orange Coast goes a lot deeper than one professor's lack of a filter.\n",
            "Ironically, Coast Colleges, the community college district that includes Orange Coast, has an entire Office of Equity, Inclusion and Compliance. One document posted on its website details the evils of \"microaggressions\"—\"common, subtle messages that communicate racial indignations.\" Such indignations typically are \"unconscious in nature,\" but they \"have an integral influence on students' perceptions of campus climates as hostile, alienating and isolating.\"\n",
            "The controversy isn't racial, although Cox reportedly referred to white supremacy and evoked the Civil War. But if a college system is so worried about unconscious, little aggressions that create a hostile and alienating classroom environment, shouldn't it also be worried about instructors who make loud, conscious and aggressive statements that are alienating to students with a minority political viewpoint? The hypocrisy here is almost too rich for words.\n",
            "I admire O'Neil's willingness to fight back. It seems likely he's not the only student to endure such things, so this is a worthy fight. But perhaps he should find a school that values his temerity. Americans—and academics, in particular—need a healthy reminder of why free expression is the foundation of a free society. And why petty, unjust bureaucrats remain its enemy.\n",
            "Help FIF Make the Album of your Dreams!\n",
            "We need your support now more than ever. The inspiration and energy to create an amazing record is there but this time we will be working without the support of a label. This gives us incredible artistic freedom, but it also means we need to raise our own financing. Our hope is to release an album in 2013, ten years after our initial final show.\n",
            "We are shooting for the financial goal of $30,000.Your pledges will be used to pay for demoing new material, recording, mixing, mastering, and associated travel expenses.Keep in mind, this budget covers making the album only. Every dollar we raise over $30,000 will be used for such things as promotion of the album once it's released because we think it would be great to have this album reach as many people as possible! Additional funds could also enable us to pla"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbSf-qj1Fko"
      },
      "source": [
        "!cat /content/drive/MyDrive/Adaptive_pretrain/bio.txt /content/drive/MyDrive/Adaptive_pretrain/med.txt > /content/drive/MyDrive/Adaptive_pretrain/biomed.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITfMgolh5SG9"
      },
      "source": [
        "!sed -i '/^ *$/d' /content/drive/MyDrive/Adaptive_pretrain/biomed.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGsxwOmt5YLq"
      },
      "source": [
        "!cp /content/drive/MyDrive/Adaptive_pretrain/cs.txt /content/drive/MyDrive/Adaptive_pretrain/cs_0830.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SR7FSFB5srC"
      },
      "source": [
        "!sed -i '/^ *$/d' /content/drive/MyDrive/Adaptive_pretrain/cs_0830.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uOxrfIL6Z2K",
        "outputId": "f6d2107e-6647-4600-d33d-61ad744eebcf"
      },
      "source": [
        "!wc -l /content/drive/MyDrive/Adaptive_pretrain/cs_0830.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "106646 /content/drive/MyDrive/Adaptive_pretrain/cs_0830.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0NUj0TO6g_T"
      },
      "source": [
        "max_len = 106646\n",
        "train_len = int(max_len * 0.8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06zojwgz6u-j",
        "outputId": "192475d8-be43-4f2f-ed78-d64d33da72d0"
      },
      "source": [
        "train_len\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "85316"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0aWQi0h6y3y",
        "outputId": "29453da8-dbbd-4fe0-8fa5-c63f7a342ff5"
      },
      "source": [
        "max_len - train_len\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21330"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tj2Vq2nOl_W",
        "outputId": "ed90c98f-17d8-473b-f817-8a5ae6270cd3"
      },
      "source": [
        "del max_len\n",
        "del train_len\n",
        "del fill_mask\n",
        "\n",
        "import gc\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "454"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl__utU25w7h"
      },
      "source": [
        "## Fine tune\n",
        "## CS\n",
        "\n",
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" #@param [\"roberta\", \"bert\"]\n",
        "MODEL_DIR = \"/content/models/roberta\" #@param {type: \"string\"}\n",
        "OUTPUT_DIR = \"/content/models/roberta/output\" #@param {type: \"string\"}\n",
        "TRAIN_PATH = \"/content/data/train01.txt\" #@param {type: \"string\"}\n",
        "EVAL_PATH = \"/content/data/dev01.txt\" #@param {type: \"string\"}\n",
        "\n",
        "# TRAIN_SIZE = 85316 #@param {type:\"integer\"}\n",
        "# VAL_SIZE = 21330 #@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOMlfaKf628A"
      },
      "source": [
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/Adaptive_pretrain/cs_0830.txt) > data/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/Adaptive_pretrain/cs_0830.txt) > data/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nunvdI3g7REz"
      },
      "source": [
        "train_params = {\n",
        "    \"output_dir\": OUTPUT_DIR,\n",
        "    \"model_type\": MODEL_TYPE,\n",
        "    \"config_name\": MODEL_DIR,\n",
        "    \"tokenizer_name\": MODEL_DIR,\n",
        "    \"train_path\": TRAIN_PATH,\n",
        "    \"eval_path\": EVAL_PATH,\n",
        "    \"do_eval\": \"--do_eval\",\n",
        "    \"evaluate_during_training\": \"\",\n",
        "    \"line_by_line\": \"--line_by_line\",\n",
        "    \"should_continue\": \"\",\n",
        "    \"model_name_or_path\": \"\",\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7YW0YVgRlfK",
        "outputId": "6dd508fb-7dca-469b-a939-448f7de93441"
      },
      "source": [
        "    !{cmd.format(**train_params)}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "09/03/2021 01:29:49 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "09/03/2021 01:29:49 - INFO - __main__ -   Training new model from scratch\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:582: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 01:29:59 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/dev01.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path=None, model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name='/content/models/roberta', train_data_file='/content/data/train01.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 01:29:59 - INFO - __main__ -   Creating features from dataset file at /content/data/train01.txt\n",
            "tcmalloc: large alloc 2026496000 bytes == 0x555cb53f4000 @  0x7fbd19dec1e7 0x555b834d7a18 0x555b834eee06 0x555b834eeae8 0x555b834fc8bf 0x555b834ed5eb 0x555b8357ab93 0x555b834a5ea9 0x555b834a5da0 0x555b8351a2f9 0x555b83514c35 0x555b834a7dd1 0x555b834a7280 0x555b834a9208 0x555b83586141 0x555b836249f1 0x555b834a5ea9 0x555b83597c0d 0x555b8351a0d8 0x555b83514c35 0x555b834a7fec 0x555b834e8bc9 0x555b834e5ac4 0x555b834a68a9 0x555b8351ab0a 0x555b83514c35 0x555b834a773a 0x555b8351693b 0x555b83515235 0x555b834a773a 0x555b83515b0e\n",
            "tcmalloc: large alloc 4052983808 bytes == 0x555d2e092000 @  0x7fbd19dec1e7 0x555b834d7a18 0x555b834eed65 0x555b834eeae8 0x555b834fc8bf 0x555b834ed5eb 0x555b8357ab93 0x555b834a5ea9 0x555b834a5da0 0x555b8351a2f9 0x555b83514c35 0x555b834a7dd1 0x555b834a7280 0x555b834a9208 0x555b83586141 0x555b836249f1 0x555b834a5ea9 0x555b83597c0d 0x555b8351a0d8 0x555b83514c35 0x555b834a7fec 0x555b834e8bc9 0x555b834e5ac4 0x555b834a68a9 0x555b8351ab0a 0x555b83514c35 0x555b834a773a 0x555b8351693b 0x555b83515235 0x555b834a773a 0x555b83515b0e\n",
            "tcmalloc: large alloc 4038033408 bytes == 0x555e1f9cc000 @  0x7fbd19dec1e7 0x555b834d8488 0x555b835861a7 0x555b836249f1 0x555b834a5ea9 0x555b83597c0d 0x555b8351a0d8 0x555b83514c35 0x555b834a7fec 0x555b834e8bc9 0x555b834e5ac4 0x555b834a68a9 0x555b8351ab0a 0x555b83514c35 0x555b834a773a 0x555b8351693b 0x555b83515235 0x555b834a773a 0x555b83515b0e 0x555b83514c35 0x555b83514933 0x555b835de402 0x555b835de77d 0x555b835de626 0x555b835b6313 0x555b835b5fbc 0x7fbd18bd6bf7 0x555b835b5e9a\n",
            "tcmalloc: large alloc 4038033408 bytes == 0x555cb53f4000 @  0x7fbd19dec1e7 0x555b834d7a18 0x555b834eed65 0x555b8350724c 0x555b835862a6 0x555b836249f1 0x555b834a5ea9 0x555b83597c0d 0x555b8351a0d8 0x555b83514c35 0x555b834a7fec 0x555b834e8bc9 0x555b834e5ac4 0x555b834a68a9 0x555b8351ab0a 0x555b83514c35 0x555b834a773a 0x555b8351693b 0x555b83515235 0x555b834a773a 0x555b83515b0e 0x555b83514c35 0x555b83514933 0x555b835de402 0x555b835de77d 0x555b835de626 0x555b835b6313 0x555b835b5fbc 0x7fbd18bd6bf7 0x555b835b5e9a\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8prpd2ZRqux"
      },
      "source": [
        "## DAPT\n",
        "## 1. News data\n",
        "\n",
        "!cp /content/articles1.csv.zip /content/drive/MyDrive/RoBERTa_domain_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JejXavsyawPk",
        "outputId": "19195f2c-8e7b-44d4-807c-2ee5795e60c0"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 02:09:40--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Sports_and_Outdoors_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68275834 (65M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Sports_and_Outdoors_5.json.gz’\n",
            "\n",
            "reviews_Sports_and_ 100%[===================>]  65.11M  3.30MB/s    in 9.8s    \n",
            "\n",
            "2021-09-03 02:09:50 (6.61 MB/s) - ‘reviews_Sports_and_Outdoors_5.json.gz’ saved [68275834/68275834]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1bz7amBazSi",
        "outputId": "2170164f-c216-4148-97d3-b44cb825bce8"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 02:12:38--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Movies_and_TV_5.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791322468 (755M) [application/octet-stream]\n",
            "Saving to: ‘Movies_and_TV_5.json.gz’\n",
            "\n",
            "Movies_and_TV_5.jso 100%[===================>] 754.66M  93.1MB/s    in 8.3s    \n",
            "\n",
            "2021-09-03 02:12:46 (91.3 MB/s) - ‘Movies_and_TV_5.json.gz’ saved [791322468/791322468]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkHe2c7sbejR",
        "outputId": "3cc50c1b-d6cd-492b-b6dc-2d418bff0070"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Pet_Supplies_5.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 02:13:12--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Pet_Supplies_5.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 306131006 (292M) [application/octet-stream]\n",
            "Saving to: ‘Pet_Supplies_5.json.gz’\n",
            "\n",
            "Pet_Supplies_5.json 100%[===================>] 291.95M  90.4MB/s    in 3.4s    \n",
            "\n",
            "2021-09-03 02:13:16 (85.4 MB/s) - ‘Pet_Supplies_5.json.gz’ saved [306131006/306131006]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9tbGGoWbm_5",
        "outputId": "d5db0808-ed1a-45d8-ced0-a93464576b7e"
      },
      "source": [
        "!wget http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 02:13:33--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Home_and_Kitchen_5.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 138126598 (132M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Home_and_Kitchen_5.json.gz’\n",
            "\n",
            "reviews_Home_and_Ki 100%[===================>] 131.73M  2.01MB/s    in 61s     \n",
            "\n",
            "2021-09-03 02:14:33 (2.17 MB/s) - ‘reviews_Home_and_Kitchen_5.json.gz’ saved [138126598/138126598]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPEzJ-dZbr-h",
        "outputId": "4381fd79-3562-4eeb-d52b-9bed2d9e0d17"
      },
      "source": [
        "!wget http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Cell_Phones_and_Accessories_5.json.gz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-03 02:16:14--  http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Cell_Phones_and_Accessories_5.json.gz\n",
            "Resolving deepyeti.ucsd.edu (deepyeti.ucsd.edu)... 169.228.63.50\n",
            "Connecting to deepyeti.ucsd.edu (deepyeti.ucsd.edu)|169.228.63.50|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169071325 (161M) [application/octet-stream]\n",
            "Saving to: ‘Cell_Phones_and_Accessories_5.json.gz’\n",
            "\n",
            "Cell_Phones_and_Acc 100%[===================>] 161.24M  86.6MB/s    in 1.9s    \n",
            "\n",
            "2021-09-03 02:16:16 (86.6 MB/s) - ‘Cell_Phones_and_Accessories_5.json.gz’ saved [169071325/169071325]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTIfMtvocTc9",
        "outputId": "1a8144c2-4249-45ed-a8e8-f1c315ff5129"
      },
      "source": [
        "import os\n",
        "\n",
        "os.listdir('/content/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'Pet_Supplies_5.json.gz',\n",
              " 'articles1.csv.zip',\n",
              " 'Movies_and_TV_5.json.gz',\n",
              " 'reviews_Home_and_Kitchen_5.json.gz',\n",
              " 'Cell_Phones_and_Accessories_5.json.gz',\n",
              " 'drive',\n",
              " 'reviews_Sports_and_Outdoors_5.json.gz',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYRjCwgrcd2X"
      },
      "source": [
        "jsongzs = [x for x in os.listdir('/content/') if x[-8:] == '.json.gz']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Vi_Zl-pcsAi",
        "outputId": "77a18b5d-165e-49b9-c87c-da10ee1886fb"
      },
      "source": [
        "jsongzs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Pet_Supplies_5.json.gz',\n",
              " 'Movies_and_TV_5.json.gz',\n",
              " 'reviews_Home_and_Kitchen_5.json.gz',\n",
              " 'Cell_Phones_and_Accessories_5.json.gz',\n",
              " 'reviews_Sports_and_Outdoors_5.json.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpEujlGBctVC"
      },
      "source": [
        "import pandas as pd\n",
        "import gzip\n",
        "\n",
        "def parse(path):\n",
        "  g = gzip.open(path, 'rb')\n",
        "  for l in g:\n",
        "    yield json.loads(l)\n",
        "\n",
        "def getDF(path):\n",
        "  i = 0\n",
        "  df = {}\n",
        "  for d in parse(path):\n",
        "    df[i] = d\n",
        "    i += 1\n",
        "  return pd.DataFrame.from_dict(df, orient='index')\n",
        "\n",
        "df = getDF(jsongzs[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87JHRn4yc-kN",
        "outputId": "bfb30729-f5d8-4709-db89-2edc0b7cd1d4"
      },
      "source": [
        "len(df['reviewText'].values)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "296337"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eSMdBpQdDwe"
      },
      "source": [
        "review = []\n",
        "for i in range(len(jsongzs)):\n",
        "    df = getDF(jsongzs[i])\n",
        "    review.append(df['reviewText'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksEdRTyOdEnl",
        "outputId": "cd24ba7b-b735-4453-ffed-f7632da96d27"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0l5qNgHxjZI",
        "outputId": "aeeb717a-1af6-4746-b5a3-dda6380ea1d0"
      },
      "source": [
        "len(review[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2098325"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_Yj335ayODq",
        "outputId": "e9990e0f-71c1-4e67-e72f-4efddb8641e7"
      },
      "source": [
        "len(review[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "389"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "tgIu9xThyVlO",
        "outputId": "aba8ccba-fd8d-415c-f730-08a6bba4040d"
      },
      "source": [
        "review[1][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"So sorry I didn't purchase this years ago when it first came out!!  This is very good and entertaining!  We absolutely loved it and anticipate seeing it repeatedly.  We actually wore out the cassette years back, so we also purchased this same product on cd.  Best purchase we made out of all!  Would purchase on dvd if we could find one.\""
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n01QpLZsycFI",
        "outputId": "87f1d8ea-9a89-42c5-9ded-0d881f0102f4"
      },
      "source": [
        "review[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"I purchased this cd for my Pocket Parrot. It has both a woman and a man speaking. Pet store recommends to buy a female voice Only on bird cds because birds respond to female voice better. Con: there's some real stupid things said on this CD. Prefer female voice because pet shop recommends female voice. Pros: it's cheap. I will review again in 6 months see if my Polly picks up few words.\",\n",
              "       \"Maybe it's just my Amazon parrot, but she's not picking up very quickly or very well.\",\n",
              "       'I bought this to help me teach my blue quaker named Booger to talk and it sure has helped. I put it in the CD player and let it play all day while I am at work. It might drive Booger nuts, but he sure has learned a lot of words.',\n",
              "       ...,\n",
              "       'My destroyer French Bulldog was not able to destroy the squeaker within the usual 2 minutes! I was surprised and happy. Love the toy and would recommend it.',\n",
              "       \"This is one of my dog's favorite toys, but all three quickly lost their squeaking ability. He still plays with them and tries to make them squeak, but no sound comes out. I would be tempted to purchase more if the price were lower - I'm forced to view them as more of a disposable item than a durable, long-lasting toy and therefore the high expense is a big deterrent. My advice to the manufacturer: find a way to make the squeaker more durable, or lower the cost of the overall item. My advice to other customers: the price has increased by $7 since I purchased the small set of three, so use a price tracker (like camelcamelcamel) to avoid peak pricing, and treat the toy as one which only comes out during special occasions in order to make it last longer.\",\n",
              "       \"Best toy we've purchased for our new puppy. Easy for her to squeek (unlike other toys we purchased) and the shape slows the rolling a little bit so they don't roll away too fast. We're missing a couple of them, (under something no doubt) so I'll be buying more soon.\"],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P00h2nILywx-"
      },
      "source": [
        "import numpy as np\n",
        "flat_review = np.array(review, dtype=object).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5LyBYcCzGTJ",
        "outputId": "f105aec5-c00b-43dc-a8a9-392f2af04348"
      },
      "source": [
        "len(flat_review[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1128437"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPOaUB1b1R_X",
        "outputId": "60e75e98-2590-425c-f883-c81decaf6aa4"
      },
      "source": [
        "len(flat_review[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "389"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "Kz91ZSHA1biO",
        "outputId": "13d5035a-8429-44ef-874a-54c77f2ceefd"
      },
      "source": [
        "flat_review[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I purchased this cd for my Pocket Parrot. It has both a woman and a man speaking. Pet store recommends to buy a female voice Only on bird cds because birds respond to female voice better. Con: there's some real stupid things said on this CD. Prefer female voice because pet shop recommends female voice. Pros: it's cheap. I will review again in 6 months see if my Polly picks up few words.\""
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8sJftoX1f8y",
        "outputId": "e76196f9-1cd3-4849-849f-162aa15f0b44"
      },
      "source": [
        "sent_tokenize(flat_review[0][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I purchased this cd for my Pocket Parrot.',\n",
              " 'It has both a woman and a man speaking.',\n",
              " 'Pet store recommends to buy a female voice Only on bird cds because birds respond to female voice better.',\n",
              " \"Con: there's some real stupid things said on this CD.\",\n",
              " 'Prefer female voice because pet shop recommends female voice.',\n",
              " \"Pros: it's cheap.\",\n",
              " 'I will review again in 6 months see if my Polly picks up few words.']"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMRfz60WgL3D",
        "outputId": "6f5df96d-441b-4dd5-9c5e-13a5f8e192be"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sent_review = []\n",
        "for i in range(len(flat_review)):\n",
        "    for j in range(len(flat_review[i])):\n",
        "        try:\n",
        "            sent_review.extend(sent_tokenize(flat_review[i][j]))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(len(sent_review))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33997335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpusKUBgxb4Y"
      },
      "source": [
        "with open('/content/drive/MyDrive/RoBERTa_domain_data/review.txt', 'w') as f:\n",
        "    for txt in sent_review:\n",
        "        f.write(\"%s\\n\" % txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v4fIQxz2zpT",
        "outputId": "1dcd3439-aec4-42b8-9233-e80fb4c9692a"
      },
      "source": [
        "!wc -w /content/drive/MyDrive/RoBERTa_domain_data/review.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "515381387 /content/drive/MyDrive/RoBERTa_domain_data/review.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWILDnda7H-0"
      },
      "source": [
        "!mv /content/Cell_Phones_and_Accessories_5.json.gz /content/drive/MyDrive/RoBERTa_domain_data/\n",
        "!mv /content/Movies_and_TV_5.json.gz /content/drive/MyDrive/RoBERTa_domain_data/\n",
        "!mv /content/Pet_Supplies_5.json.gz /content/drive/MyDrive/RoBERTa_domain_data/\n",
        "!mv /content/reviews_Home_and_Kitchen_5.json.gz /content/drive/MyDrive/RoBERTa_domain_data/\n",
        "!mv /content/reviews_Sports_and_Outdoors_5.json.gz /content/drive/MyDrive/RoBERTa_domain_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TbviXbO8LTi",
        "outputId": "75b084e1-5d22-4554-879d-770b323f384f"
      },
      "source": [
        "!unzip /content/articles1.csv.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/articles1.csv.zip\n",
            "  inflating: articles1.csv           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXtXhIFh8UDC"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/articles1.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "id": "1UT7JVxB8fZL",
        "outputId": "ee65b4ca-44b8-4c38-f04c-bdf7025cd5ca"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>publication</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>year</th>\n",
              "      <th>month</th>\n",
              "      <th>url</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>17283</td>\n",
              "      <td>House Republicans Fret About Winning Their Hea...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Carl Hulse</td>\n",
              "      <td>2016-12-31</td>\n",
              "      <td>2016.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>17284</td>\n",
              "      <td>Rift Between Officers and Residents as Killing...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Benjamin Mueller and Al Baker</td>\n",
              "      <td>2017-06-19</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>After the bullet shells get counted, the blood...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>17285</td>\n",
              "      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Margalit Fox</td>\n",
              "      <td>2017-01-06</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>17286</td>\n",
              "      <td>Among Deaths in 2016, a Heavy Toll in Pop Musi...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>William McDonald</td>\n",
              "      <td>2017-04-10</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>17287</td>\n",
              "      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n",
              "      <td>New York Times</td>\n",
              "      <td>Choe Sang-Hun</td>\n",
              "      <td>2017-01-02</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0     id  ... url                                            content\n",
              "0           0  17283  ... NaN  WASHINGTON  —   Congressional Republicans have...\n",
              "1           1  17284  ... NaN  After the bullet shells get counted, the blood...\n",
              "2           2  17285  ... NaN  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
              "3           3  17286  ... NaN  Death may be the great equalizer, but it isn’t...\n",
              "4           4  17287  ... NaN  SEOUL, South Korea  —   North Korea’s leader, ...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3U-zPqk8isM"
      },
      "source": [
        "contents = df['content'].values\n",
        "\n",
        "news = []\n",
        "for txt in contents:\n",
        "    news.extend(sent_tokenize(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1-aGPFI_EBO",
        "outputId": "06c5b653-aba6-4979-d642-6f604d83b714"
      },
      "source": [
        "len(contents[0].split(\" \"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "920"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiB2koTu9H3P",
        "outputId": "05d25ac6-cfea-4967-8418-5a4c8f003268"
      },
      "source": [
        "len(news)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1520613"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51KCfLEy9nGW",
        "outputId": "5d72cd32-e6c8-4b32-8c9d-93e0bc98a5f5"
      },
      "source": [
        "!unzip /content/articles2.csv.zip\n",
        "!unzip /content/articles3.csv.zip\n",
        "\n",
        "df2 = pd.read_csv('/content/articles2.csv')\n",
        "df3 = pd.read_csv('/content/articles3.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/articles2.csv.zip\n",
            "  inflating: articles2.csv           \n",
            "Archive:  /content/articles3.csv.zip\n",
            "  inflating: articles3.csv           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVTDQylM-E9N"
      },
      "source": [
        "contents = df['content'].values\n",
        "\n",
        "for txt in contents:\n",
        "    news.extend(sent_tokenize(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fl0z3QH69-E2"
      },
      "source": [
        "contents = df['content'].values\n",
        "\n",
        "for txt in contents:\n",
        "    news.extend(sent_tokenize(txt))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbY63IEj-rmT",
        "outputId": "9e476b42-c258-436b-ea8d-2839afe6ef60"
      },
      "source": [
        "len(news)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4561839"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ywKBa_Oi-6tp",
        "outputId": "b6ba6af5-aa46-4bcc-b023-02d1b3f8b9b9"
      },
      "source": [
        "news[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WASHINGTON  —   Congressional Republicans have a new fear when it comes to their    health care lawsuit against the Obama administration: They might win.'"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmXJhkzp844s"
      },
      "source": [
        "with open('/content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt', 'w') as f:\n",
        "    for txt in news:\n",
        "        f.write(\"%s\\n\" % txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zt7OKS70--Fg",
        "outputId": "38b7e5e8-57c8-4ff3-ef82-bc4e6199042b"
      },
      "source": [
        "!wc -w /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96913761 /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fol4WeFa_UnC",
        "outputId": "ddd7c26e-f285-4ba6-8495-8bf74323badf"
      },
      "source": [
        "!wc -w /content/drive/MyDrive/Adaptive_pretrain/cs.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16405408 /content/drive/MyDrive/Adaptive_pretrain/cs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zON7-4RU_gfX"
      },
      "source": [
        "cs = []\n",
        "with open('/content/drive/MyDrive/Adaptive_pretrain/cs.txt', 'r') as f:\n",
        "    for txt in f.readlines():\n",
        "        cs.extend(sent_tokenize(txt))\n",
        "biomed = []\n",
        "with open('/content/drive/MyDrive/Adaptive_pretrain/biomed.txt', 'r') as f:\n",
        "    for txt in f.readlines():\n",
        "        biomed.extend(sent_tokenize(txt))\n",
        "\n",
        "with open('/content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt', 'w') as f:\n",
        "    for txt in cs:\n",
        "        f.write(\"%s\\n\" % txt)\n",
        "\n",
        "with open('/content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt', 'w') as f:\n",
        "    for txt in biomed:\n",
        "        f.write(\"%s\\n\" % txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwESjMffAbzQ",
        "outputId": "35ac6dd8-741a-4dd3-a656-4fd2bf658c62"
      },
      "source": [
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt\n",
        "!wc -l /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "905793 /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt\n",
            "712923 /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt\n",
            "35265731 /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt\n",
            "4561839 /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf815wGaCARx",
        "outputId": "8c865829-5e02-4063-f06c-11410a236a55"
      },
      "source": [
        "712923 * 0.8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "570338.4"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR7BIfFbCHPx",
        "outputId": "06ae08bb-7532-407d-b398-0406a51e92e7"
      },
      "source": [
        "712923 - 570338"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142585"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhnsGuF1BTty"
      },
      "source": [
        "!mkdir -p /content/data/biomed\n",
        "!mkdir -p /content/data/cs\n",
        "!mkdir -p /content/data/review\n",
        "!mkdir -p /content/data/news\n",
        "\n",
        "TRAIN_SIZE =  570338#@param {type:\"integer\"}\n",
        "VAL_SIZE =  142585#@param {type: \"integer\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ady5xu7sCPbq"
      },
      "source": [
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt) > /content/data/cs/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_cs.txt) > /content/data/cs/dev.txt\n",
        "\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt) > /content/data/biomed/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_biomed.txt) > /content/data/biomed/dev.txt\n",
        "\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt) > /content/data/review/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_review.txt) > /content/data/review/dev.txt\n",
        "\n",
        "!(head -n $TRAIN_SIZE /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt) > /content/data/news/train.txt\n",
        "!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p /content/drive/MyDrive/RoBERTa_domain_data/domain_news.txt) > /content/data/news/dev.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqa09P7DG5Dd"
      },
      "source": [
        "!rm -r /content/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIPIaf3eG2uc"
      },
      "source": [
        "import json\n",
        "\n",
        "config = {\n",
        "    \"architectures\": [\n",
        "\t\t\"RobertaForMaskedLM\"\n",
        "\t],\n",
        "\t\"attention_probs_dropout_prob\": 0.1,\n",
        "\t\"hidden_act\": \"gelu\",\n",
        "\t\"hidden_dropout_prob\": 0.1,\n",
        "\t\"hidden_size\": 768,\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"intermediate_size\": 3072,\n",
        "\t\"layer_norm_eps\": 1e-05,\n",
        "\t\"max_position_embeddings\": 514,\n",
        "\t\"model_type\": \"roberta\",\n",
        "\t\"num_attention_heads\": 12,\n",
        "\t\"num_hidden_layers\": 12,\n",
        "\t\"type_vocab_size\": 1,\n",
        "\t\"vocab_size\": 50265\n",
        "}\n",
        "\n",
        "# with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "#     json.dump(config, fp)\n",
        "\n",
        "tokenizer_config = {\"max_len\": 512}\n",
        "\n",
        "# with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "#     json.dump(tokenizer_config, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bz8GjcRrHIa3"
      },
      "source": [
        "# Command line\n",
        "cmd = \"\"\"python run_lm.py \\\n",
        "    --output_dir {output_dir} \\\n",
        "    --model_type {model_type} \\\n",
        "    --mlm \\\n",
        "    --config_name {config_name} \\\n",
        "    --tokenizer_name {tokenizer_name} \\\n",
        "    {line_by_line} \\\n",
        "    {should_continue} \\\n",
        "    {model_name_or_path} \\\n",
        "    --train_data_file {train_path} \\\n",
        "    --eval_data_file {eval_path} \\\n",
        "    --num_train_epochs {num_train_epochs} \\\n",
        "    --do_train \\\n",
        "    {do_eval} \\\n",
        "    {evaluate_during_training} \\\n",
        "    --overwrite_output_dir \\\n",
        "    --block_size 512 \\\n",
        "    --max_step 25 \\\n",
        "    --warmup_steps 10 \\\n",
        "    --learning_rate 5e-5 \\\n",
        "    --per_gpu_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 4 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --adam_epsilon 1e-6 \\\n",
        "    --max_grad_norm 100.0 \\\n",
        "    --save_total_limit 10 \\\n",
        "    --save_steps 10 \\\n",
        "    --logging_steps 2 \\\n",
        "    --seed 42 \\\n",
        "    |& tee -a /content/drive/MyDrive/Adaptive_pretrain/domain_output.txt\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XczwE9U_DJVJ",
        "outputId": "1b776111-ea3f-4a8b-f7ff-fa822968fcc3"
      },
      "source": [
        "# Train RoBERTa domain\n",
        "\n",
        "import os\n",
        "# from distutils.dir_util import copy_tree\n",
        "domains = ['cs', 'biomed', 'review', 'news']\n",
        "base = '/content/data/'\n",
        "for d in domains:\n",
        "    print(\"__________START \" + f\"{d}__________\")\n",
        "    train_file = f\"{d}/train.txt\"\n",
        "    dev_file = f\"{d}/dev.txt\"\n",
        "    # COPY model\n",
        "    !cp -R /content/drive/MyDrive/Adaptive_pretrain/sentenced/roberta_models_base9/models/ /content/\n",
        "    !cp -R /content/drive/MyDrive/Adaptive_pretrain/sentenced/roberta_models_base9/runs/ /content/\n",
        "    !cp /content/drive/MyDrive/Adaptive_pretrain/tokenizer.json /content/models/roberta/\n",
        "\n",
        "    with open(\"/content/models/roberta/config.json\", 'w') as fp:\n",
        "        json.dump(config, fp)\n",
        "\n",
        "    with open(\"/content/models/roberta/tokenizer_config.json\", 'w') as fp:\n",
        "        json.dump(tokenizer_config, fp)\n",
        "    \n",
        "    # Model paths\n",
        "    MODEL_TYPE = \"roberta\"\n",
        "    MODEL_DIR = \"/content/models/roberta\"\n",
        "    OUTPUT_DIR = \"/content/models/roberta/output_base\"\n",
        "    TRAIN_PATH = \"/content/data/\"+train_file\n",
        "    EVAL_PATH = \"/content/data/\"+dev_file\n",
        "\n",
        "    train_params = {\n",
        "        \"output_dir\": OUTPUT_DIR,\n",
        "        \"model_type\": MODEL_TYPE,\n",
        "        \"config_name\": MODEL_DIR,\n",
        "        \"tokenizer_name\": MODEL_DIR,\n",
        "        \"train_path\": TRAIN_PATH,\n",
        "        \"eval_path\": EVAL_PATH,\n",
        "        \"num_train_epochs\": 1.0,\n",
        "        \"do_eval\": \"--do_eval\",\n",
        "        \"evaluate_during_training\": \"\",\n",
        "        \"line_by_line\": \"--line_by_line\",\n",
        "        \"should_continue\": \"--should_continue\",\n",
        "        \"model_name_or_path\": \"\",\n",
        "    }\n",
        "\n",
        "    # if num_corpus != 0:\n",
        "    #     train_params['should_continue'] = \"--should_continue\"\n",
        "    \n",
        "    !{cmd.format(**train_params)}\n",
        "\n",
        "    save_dir = f'/content/drive/MyDrive/Adaptive_pretrain/roberta_models_domain_{d}/'\n",
        "    if not os.path.isdir(save_dir):\n",
        "        os.mkdir(save_dir)\n",
        "    !cp -R /content/models \"$save_dir\"\n",
        "    !cp -R /content/runs \"$save_dir\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__________START cs__________\n",
            "09/03/2021 05:44:51 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 05:44:59 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/cs/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path='/content/models/roberta/output_base/checkpoint-20', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=True, tokenizer_name='/content/models/roberta', train_data_file='/content/data/cs/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 05:44:59 - INFO - __main__ -   Creating features from dataset file at /content/data/cs/train.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 05:45:26 - INFO - __main__ -   ***** Running training *****\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Num examples = 570338\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Num Epochs = 1\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Total optimization steps = 25\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Continuing training from epoch 0\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Continuing training from global step 20\n",
            "09/03/2021 05:45:26 - INFO - __main__ -     Will skip the first 20 steps in the first epoch\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/142585 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 21/142585 [00:00<40:35, 58.54it/s]\u001b[A\n",
            "Iteration:   0%|          | 27/142585 [00:00<1:14:32, 31.88it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0%|          | 31/142585 [00:01<1:34:40, 25.10it/s]\u001b[A\n",
            "Iteration:   0%|          | 34/142585 [00:01<1:47:32, 22.09it/s]\u001b[A\n",
            "Iteration:   0%|          | 37/142585 [00:01<1:55:58, 20.48it/s]\u001b[A\n",
            "Iteration:   0%|          | 39/142585 [00:01<1:57:00, 20.30it/s]\u001b[A\n",
            "Iteration:   0%|          | 41/142585 [00:01<1:48:17, 21.94it/s]\n",
            "Epoch:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "09/03/2021 05:45:27 - INFO - __main__ -    global_step = 26, average loss = 2.3395097989302416\n",
            "09/03/2021 05:45:27 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 05:45:30 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 05:45:31 - INFO - __main__ -   Creating features from dataset file at /content/data/cs/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 05:45:38 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 05:45:38 - INFO - __main__ -     Num examples = 142585\n",
            "09/03/2021 05:45:38 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35647/35647 [13:15<00:00, 44.80it/s]\n",
            "09/03/2021 05:58:54 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 05:58:54 - INFO - __main__ -     perplexity = tensor(22870.7676)\n",
            "__________START biomed__________\n",
            "09/03/2021 06:00:20 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 06:00:28 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/biomed/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path='/content/models/roberta/output_base/checkpoint-20', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=True, tokenizer_name='/content/models/roberta', train_data_file='/content/data/biomed/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 06:00:28 - INFO - __main__ -   Creating features from dataset file at /content/data/biomed/train.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 06:00:55 - INFO - __main__ -   ***** Running training *****\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Num examples = 570338\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Num Epochs = 1\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Total optimization steps = 25\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Continuing training from epoch 0\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Continuing training from global step 20\n",
            "09/03/2021 06:00:55 - INFO - __main__ -     Will skip the first 20 steps in the first epoch\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/142585 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 21/142585 [00:00<41:43, 56.95it/s]\u001b[A\n",
            "Iteration:   0%|          | 27/142585 [00:00<1:24:18, 28.18it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0%|          | 31/142585 [00:01<1:38:55, 24.02it/s]\u001b[A\n",
            "Iteration:   0%|          | 34/142585 [00:01<1:58:21, 20.07it/s]\u001b[A\n",
            "Iteration:   0%|          | 36/142585 [00:01<2:12:03, 17.99it/s]\u001b[A\n",
            "Iteration:   0%|          | 38/142585 [00:01<2:16:24, 17.42it/s]\u001b[A\n",
            "Iteration:   0%|          | 40/142585 [00:01<2:28:59, 15.95it/s]\u001b[A\n",
            "Iteration:   0%|          | 42/142585 [00:02<2:00:57, 19.64it/s]\n",
            "Epoch:   0%|          | 0/1 [00:02<?, ?it/s]\n",
            "09/03/2021 06:00:57 - INFO - __main__ -    global_step = 26, average loss = 2.332837673333975\n",
            "09/03/2021 06:00:57 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 06:01:00 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 06:01:01 - INFO - __main__ -   Creating features from dataset file at /content/data/biomed/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 06:01:08 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 06:01:08 - INFO - __main__ -     Num examples = 142585\n",
            "09/03/2021 06:01:08 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35647/35647 [13:48<00:00, 43.02it/s]\n",
            "09/03/2021 06:14:57 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 06:14:57 - INFO - __main__ -     perplexity = tensor(24105.8066)\n",
            "__________START review__________\n",
            "09/03/2021 06:16:21 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 06:16:33 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/review/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path='/content/models/roberta/output_base/checkpoint-20', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=True, tokenizer_name='/content/models/roberta', train_data_file='/content/data/review/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 06:16:33 - INFO - __main__ -   Creating features from dataset file at /content/data/review/train.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 06:16:52 - INFO - __main__ -   ***** Running training *****\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Num examples = 568106\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Num Epochs = 1\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Total optimization steps = 25\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Continuing training from epoch 0\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Continuing training from global step 20\n",
            "09/03/2021 06:16:52 - INFO - __main__ -     Will skip the first 20 steps in the first epoch\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/142027 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 21/142027 [00:00<13:22, 176.96it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0%|          | 39/142027 [00:01<1:31:51, 25.76it/s]\n",
            "Epoch:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "09/03/2021 06:16:54 - INFO - __main__ -    global_step = 26, average loss = 2.294904988545638\n",
            "09/03/2021 06:16:54 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 06:16:56 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 06:16:58 - INFO - __main__ -   Creating features from dataset file at /content/data/review/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 06:17:02 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 06:17:02 - INFO - __main__ -     Num examples = 142086\n",
            "09/03/2021 06:17:02 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35522/35522 [09:01<00:00, 65.65it/s]\n",
            "09/03/2021 06:26:03 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 06:26:03 - INFO - __main__ -     perplexity = tensor(21509.7598)\n",
            "__________START news__________\n",
            "09/03/2021 06:27:30 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:592: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "09/03/2021 06:27:42 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-06, block_size=512, cache_dir=None, config_name='/content/models/roberta', device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/data/news/dev.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=4, learning_rate=5e-05, line_by_line=True, local_rank=-1, logging_steps=2, max_grad_norm=100.0, max_steps=25, mlm=True, mlm_probability=0.15, model_name_or_path='/content/models/roberta/output_base/checkpoint-20', model_type='roberta', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/models/roberta/output_base', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=10, save_total_limit=10, seed=42, server_ip='', server_port='', should_continue=True, tokenizer_name='/content/models/roberta', train_data_file='/content/data/news/train.txt', warmup_steps=10, weight_decay=0.01)\n",
            "09/03/2021 06:27:42 - INFO - __main__ -   Creating features from dataset file at /content/data/news/train.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 06:28:08 - INFO - __main__ -   ***** Running training *****\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Num examples = 570339\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Num Epochs = 1\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Gradient Accumulation steps = 4\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Total optimization steps = 25\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Continuing training from checkpoint, will skip to saved global_step\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Continuing training from epoch 0\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Continuing training from global step 20\n",
            "09/03/2021 06:28:08 - INFO - __main__ -     Will skip the first 20 steps in the first epoch\n",
            "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0%|          | 0/142585 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0%|          | 21/142585 [00:00<31:27, 75.54it/s]\u001b[A/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "\n",
            "Iteration:   0%|          | 29/142585 [00:00<1:23:11, 28.56it/s]\u001b[A\n",
            "Iteration:   0%|          | 33/142585 [00:01<1:41:47, 23.34it/s]\u001b[A\n",
            "Iteration:   0%|          | 36/142585 [00:01<1:55:22, 20.59it/s]\u001b[A\n",
            "Iteration:   0%|          | 39/142585 [00:01<2:00:36, 19.70it/s]\u001b[A\n",
            "Iteration:   0%|          | 42/142585 [00:01<1:48:45, 21.84it/s]\n",
            "Epoch:   0%|          | 0/1 [00:01<?, ?it/s]\n",
            "09/03/2021 06:28:10 - INFO - __main__ -    global_step = 26, average loss = 2.280958606646611\n",
            "09/03/2021 06:28:10 - INFO - __main__ -   Saving model checkpoint to /content/models/roberta/output_base\n",
            "09/03/2021 06:28:13 - INFO - __main__ -   Evaluate the following checkpoints: ['/content/models/roberta/output_base']\n",
            "09/03/2021 06:28:14 - INFO - __main__ -   Creating features from dataset file at /content/data/news/dev.txt\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "09/03/2021 06:28:20 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "09/03/2021 06:28:20 - INFO - __main__ -     Num examples = 142594\n",
            "09/03/2021 06:28:20 - INFO - __main__ -     Batch size = 4\n",
            "Evaluating: 100%|██████████| 35649/35649 [12:36<00:00, 47.15it/s]\n",
            "09/03/2021 06:40:57 - INFO - __main__ -   ***** Eval results  *****\n",
            "09/03/2021 06:40:57 - INFO - __main__ -     perplexity = tensor(20322.3887)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ll5F7HZAIhZQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}