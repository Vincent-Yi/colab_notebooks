{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Remote_connection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qxiP06dDrjaI",
        "SSdz5NWyrs9p",
        "dpcTl2pHKFWn"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "882a368f044c4b76a5f01051be6de3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9e3acf5ac884d078ccefba0887752f4",
              "IPY_MODEL_b1e8f1c48c4d46acabbc6125ea8a55f5",
              "IPY_MODEL_134480709fae407c8fb9047fc0f3ac70"
            ],
            "layout": "IPY_MODEL_3f04e0496cbd4a6eb659b2f8b98080a5"
          }
        },
        "b9e3acf5ac884d078ccefba0887752f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb3b40323a424016ba3f17439f7fdb5b",
            "placeholder": "​",
            "style": "IPY_MODEL_be78cf9df2d34a639de5581692043e76",
            "value": "Downloading: 100%"
          }
        },
        "b1e8f1c48c4d46acabbc6125ea8a55f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b26ae0761a0c4317b4078e6bd86a70f3",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef8116fc3a944570bfe1efb04530c93a",
            "value": 481
          }
        },
        "134480709fae407c8fb9047fc0f3ac70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49b7a4301ecb434fa59ea1fe1b8f2a03",
            "placeholder": "​",
            "style": "IPY_MODEL_60e962122ecb47bd9029210bd4ba7b8d",
            "value": " 481/481 [00:00&lt;00:00, 14.9kB/s]"
          }
        },
        "3f04e0496cbd4a6eb659b2f8b98080a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb3b40323a424016ba3f17439f7fdb5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be78cf9df2d34a639de5581692043e76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b26ae0761a0c4317b4078e6bd86a70f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef8116fc3a944570bfe1efb04530c93a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49b7a4301ecb434fa59ea1fe1b8f2a03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60e962122ecb47bd9029210bd4ba7b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17a0e19138194a91af74f5c75b7f4ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43cd3768c7f546e8b89d523ced35b0ce",
              "IPY_MODEL_fb6a79f65159421a9672c227332feb9a",
              "IPY_MODEL_f9014986b8ac489fa2d1a814255b8508"
            ],
            "layout": "IPY_MODEL_b8916d23d35f47a1b87f6d7d3946eba4"
          }
        },
        "43cd3768c7f546e8b89d523ced35b0ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b932f926ee49a6a98e51f132ad31b4",
            "placeholder": "​",
            "style": "IPY_MODEL_8a50441194b74c468b637c70e2fafe9b",
            "value": "Downloading: 100%"
          }
        },
        "fb6a79f65159421a9672c227332feb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c0ec22e19eb42469f6c70b18721fccb",
            "max": 501200538,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25ef59644e21465896ecdb7a81ddff32",
            "value": 501200538
          }
        },
        "f9014986b8ac489fa2d1a814255b8508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dc3710a3d49469985c2dfd7a111bbd2",
            "placeholder": "​",
            "style": "IPY_MODEL_6e0ef521bae2492b91a14d8fa813c351",
            "value": " 501M/501M [00:13&lt;00:00, 41.5MB/s]"
          }
        },
        "b8916d23d35f47a1b87f6d7d3946eba4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b932f926ee49a6a98e51f132ad31b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a50441194b74c468b637c70e2fafe9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c0ec22e19eb42469f6c70b18721fccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ef59644e21465896ecdb7a81ddff32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7dc3710a3d49469985c2dfd7a111bbd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e0ef521bae2492b91a14d8fa813c351": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b660c0f2f141430bb9f11c7d33f3c7c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14b805938d0c45c1a3bdd0cd7ef04319",
              "IPY_MODEL_6122db1a902c4015be491c3205631cc4",
              "IPY_MODEL_22277833e0db4e67b143d25833aff8c3"
            ],
            "layout": "IPY_MODEL_fdf9173b370546468dd71bb78dbaca2c"
          }
        },
        "14b805938d0c45c1a3bdd0cd7ef04319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f362c331fcb4a8a9928c6a8022bf6ad",
            "placeholder": "​",
            "style": "IPY_MODEL_85bd5999b9594f3cbca1f35f243b8c3f",
            "value": "Downloading: 100%"
          }
        },
        "6122db1a902c4015be491c3205631cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed69199f4b7d403ca87d97c012714e82",
            "max": 760289,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e55012c4b86f458eb7befff8d8ad60d0",
            "value": 760289
          }
        },
        "22277833e0db4e67b143d25833aff8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5daee2157134dd682422777b5de8af3",
            "placeholder": "​",
            "style": "IPY_MODEL_ba3404899150404fa10d65997e3be72d",
            "value": " 742k/742k [00:00&lt;00:00, 3.77MB/s]"
          }
        },
        "fdf9173b370546468dd71bb78dbaca2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f362c331fcb4a8a9928c6a8022bf6ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85bd5999b9594f3cbca1f35f243b8c3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed69199f4b7d403ca87d97c012714e82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e55012c4b86f458eb7befff8d8ad60d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5daee2157134dd682422777b5de8af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba3404899150404fa10d65997e3be72d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27766d1aa5194364bba282c2457fc122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd65aeea4552409381c294cb0944e801",
              "IPY_MODEL_09bc2185ea2d46bf86251b2b0da0fe6a",
              "IPY_MODEL_e436d277fb6645baae2c250e53797e61"
            ],
            "layout": "IPY_MODEL_db9a306341664f7c9b235c57869bc525"
          }
        },
        "bd65aeea4552409381c294cb0944e801": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_208276e991b049b887786420e85c354f",
            "placeholder": "​",
            "style": "IPY_MODEL_fb8b416ccb2a4548ae40545d796870db",
            "value": "Downloading: 100%"
          }
        },
        "09bc2185ea2d46bf86251b2b0da0fe6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_574d370e2c564b1b93456d411d52c11d",
            "max": 1312669,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2225b66a5b6349b6b4eee4fc4a6ef41c",
            "value": 1312669
          }
        },
        "e436d277fb6645baae2c250e53797e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62386326714a4c14a1bb0e477f28f8df",
            "placeholder": "​",
            "style": "IPY_MODEL_ba3e96b2e598429f955983fd95502df1",
            "value": " 1.25M/1.25M [00:00&lt;00:00, 3.04MB/s]"
          }
        },
        "db9a306341664f7c9b235c57869bc525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "208276e991b049b887786420e85c354f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb8b416ccb2a4548ae40545d796870db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "574d370e2c564b1b93456d411d52c11d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2225b66a5b6349b6b4eee4fc4a6ef41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62386326714a4c14a1bb0e477f28f8df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba3e96b2e598429f955983fd95502df1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b2dcd8b4c144da99ed5c086e1c9bd0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df822d7f7d0041e78ef01da89f9c93fb",
              "IPY_MODEL_b52457ffda764e31be5249350180dd38",
              "IPY_MODEL_6023e710c87a4f3ba8b03113db24f39a"
            ],
            "layout": "IPY_MODEL_006b937d51754c6eabf95d723ba7da52"
          }
        },
        "df822d7f7d0041e78ef01da89f9c93fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e357ebdc16b24f92906ff412c9d6d36d",
            "placeholder": "​",
            "style": "IPY_MODEL_1b7d1ac0feb747cdad0b759e69accd3a",
            "value": "Downloading: 100%"
          }
        },
        "b52457ffda764e31be5249350180dd38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6957b0c8ece478baab6f6b90467d7e6",
            "max": 710,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_192aee0856c94ca8913bd011a560a6fb",
            "value": 710
          }
        },
        "6023e710c87a4f3ba8b03113db24f39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5039c028b177416a99642fbe3981d752",
            "placeholder": "​",
            "style": "IPY_MODEL_e61dc9ddb43c4e2ca9a650078492ee9a",
            "value": " 710/710 [00:00&lt;00:00, 20.0kB/s]"
          }
        },
        "006b937d51754c6eabf95d723ba7da52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e357ebdc16b24f92906ff412c9d6d36d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b7d1ac0feb747cdad0b759e69accd3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6957b0c8ece478baab6f6b90467d7e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192aee0856c94ca8913bd011a560a6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5039c028b177416a99642fbe3981d752": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e61dc9ddb43c4e2ca9a650078492ee9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d16bee6af022404d8c02e4f0987ccb11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ead7f91407ef422abdf89cfee9640890",
              "IPY_MODEL_84e73d14c0b44fb1a2193407a99245e8",
              "IPY_MODEL_005a6a768671465490bd50f019b2d94b"
            ],
            "layout": "IPY_MODEL_61ffc44c76e547ddadf6a8f8b2741464"
          }
        },
        "ead7f91407ef422abdf89cfee9640890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4349c04e1dce41058daf326e4e18d6d9",
            "placeholder": "​",
            "style": "IPY_MODEL_b27e79f0f1034190a890e8ee9651bf4a",
            "value": "Downloading: 100%"
          }
        },
        "84e73d14c0b44fb1a2193407a99245e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d4f989223674af59048e3d7ab6a8757",
            "max": 892728632,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_192e5466651d4c0f92a307c2aa6dd456",
            "value": 892728632
          }
        },
        "005a6a768671465490bd50f019b2d94b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23ef7feef4f84aabac5efaa1fa998dc1",
            "placeholder": "​",
            "style": "IPY_MODEL_44dbbba842df4e07ae1b125a0d3a2e8a",
            "value": " 851M/851M [00:23&lt;00:00, 38.7MB/s]"
          }
        },
        "61ffc44c76e547ddadf6a8f8b2741464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4349c04e1dce41058daf326e4e18d6d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b27e79f0f1034190a890e8ee9651bf4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d4f989223674af59048e3d7ab6a8757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192e5466651d4c0f92a307c2aa6dd456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23ef7feef4f84aabac5efaa1fa998dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44dbbba842df4e07ae1b125a0d3a2e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80oBS6qDROE7",
        "outputId": "c8e8111d-3a20-4f5e-944a-7d4996d031fa"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get upgrade -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Connecting to security.u\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (91.189.91.38)] [Connecting to cloud.r-pr\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [3 InRelease 18.5 kB/88.7 kB 21%] [Connecting to security.ubuntu.com (91.189\r0% [2 InRelease gpgv 242 kB] [3 InRelease 21.4 kB/88.7 kB 24%] [Connecting to s\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 242 kB] [3 InRelease 30.1 kB/88.7 kB 34%] [Connecting to s\r0% [2 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.38)]\r                                                                               \rHit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r0% [2 InRelease gpgv 242 kB] [6 InRelease 9,842 B/74.6 kB 13%] [Waiting for hea\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\r                                                                               \rGet:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,809 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [926 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [659 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,213 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,821 kB]\n",
            "Get:19 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [69.5 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [626 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,434 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,385 kB]\n",
            "Fetched 13.2 MB in 3s (4,207 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Calculating upgrade... Done\n",
            "The following packages have been kept back:\n",
            "  libcudnn8 libcudnn8-dev libnccl-dev libnccl2 linux-headers-generic\n",
            "The following packages will be upgraded:\n",
            "  base-files binutils binutils-common binutils-x86-64-linux-gnu\n",
            "  distro-info-data gnupg2 gzip libaudit-common libaudit1 libbinutils libc-bin\n",
            "  libcublas-dev libcublas10 libcudnn7 libcudnn7-dev libgnutls30 libhogweed4\n",
            "  libldap-2.4-2 libldap-common liblz4-1 libnettle6 libp11-kit0 libpam-modules\n",
            "  libpam-modules-bin libpam-runtime libpam0g libsasl2-2 libsasl2-modules-db\n",
            "  libseccomp2 libzstd1 linux-libc-dev openssl r-cran-cpp11 r-cran-data.table\n",
            "  r-cran-desc r-cran-diffobj r-cran-digest r-cran-hms r-cran-knitr\n",
            "  r-cran-lifecycle r-cran-lubridate r-cran-mgcv r-cran-mime r-cran-pillar\n",
            "  r-cran-pkgload r-cran-rcmdcheck r-cran-readr r-cran-remotes r-cran-rvest\n",
            "  r-cran-stringi r-cran-systemfonts r-cran-testthat r-cran-tibble r-cran-tidyr\n",
            "  r-cran-tinytex r-cran-usethis tar ubuntu-keyring\n",
            "58 upgraded, 0 newly installed, 0 to remove and 5 not upgraded.\n",
            "Need to get 464 MB of archives.\n",
            "After this operation, 70.3 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-cpp11 all 0.4.0-1cran1.1804.0 [215 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 base-files amd64 10.1ubuntu2.11 [60.4 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas10 10.2.3.254-1 [43.1 MB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ r-cran-mgcv 1.8-38-1.1804.0 [2,971 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 gzip amd64 1.6-5ubuntu1.1 [89.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 tar amd64 1.29b-2ubuntu0.2 [234 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libc-bin amd64 2.27-3ubuntu1.4 [643 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit-common all 1:2.8.2-1ubuntu1.1 [4,068 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaudit1 amd64 1:2.8.2-1ubuntu1.1 [38.7 kB]\n",
            "Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-data.table amd64 1.14.2-1cran1.1804.0 [1,825 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam0g amd64 1.1.8-3.6ubuntu2.18.04.3 [55.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam-modules-bin amd64 1.1.8-3.6ubuntu2.18.04.3 [40.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam-modules amd64 1.1.8-3.6ubuntu2.18.04.3 [252 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 liblz4-1 amd64 0.0~r131-2ubuntu3.1 [48.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libpam-runtime all 1.1.8-3.6ubuntu2.18.04.3 [37.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libzstd1 amd64 1.3.3+dfsg-2ubuntu1.2 [189 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libnettle6 amd64 3.4.1-0ubuntu0.18.04.1 [111 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libhogweed4 amd64 3.4.1-0ubuntu0.18.04.1 [140 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libp11-kit0 amd64 0.23.9-2ubuntu0.1 [187 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgnutls30 amd64 3.5.18-1ubuntu1.5 [646 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libseccomp2 amd64 2.5.1-1ubuntu1~18.04.1 [43.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 ubuntu-keyring all 2018.09.18.1~18.04.2 [22.3 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 distro-info-data all 0.37ubuntu0.13 [4,656 B]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 openssl amd64 1.1.1-1ubuntu2.1~18.04.13 [614 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-x86-64-linux-gnu amd64 2.30-21ubuntu1~18.04.5 [1,839 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils-common amd64 2.30-21ubuntu1~18.04.5 [197 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 binutils amd64 2.30-21ubuntu1~18.04.5 [3,388 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libbinutils amd64 2.30-21ubuntu1~18.04.5 [489 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-modules-db amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [15.0 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libsasl2-2 amd64 2.1.27~101-g0780600+dfsg-3ubuntu2.3 [49.2 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-common all 2.4.45+dfsg-1ubuntu1.10 [15.8 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libldap-2.4-2 amd64 2.4.45+dfsg-1ubuntu1.10 [154 kB]\n",
            "Get:33 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-desc all 1.4.0-1cran1.1804.0 [502 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 linux-libc-dev amd64 4.15.0-161.169 [985 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 gnupg2 all 2.2.4-1ubuntu1.4 [5,292 B]\n",
            "Get:36 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-diffobj amd64 0.3.5-1cran1.1804.0 [949 kB]\n",
            "Get:37 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-digest amd64 0.6.28-1cran1.1804.0 [190 kB]\n",
            "Get:38 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-lifecycle all 1.0.1-1cran1.1804.0 [96.7 kB]\n",
            "Get:39 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-hms all 1.1.1-1cran1.1804.0 [98.6 kB]\n",
            "Get:40 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-knitr all 1.36-1cran1.1804.0 [1,211 kB]\n",
            "Get:41 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-lubridate amd64 1.8.0-1cran1.1804.0 [996 kB]\n",
            "Get:42 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-mime amd64 0.12-1cran1.1804.0 [34.4 kB]\n",
            "Get:43 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-pillar all 1.6.3-1cran1.1804.0 [926 kB]\n",
            "Get:44 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcublas-dev 10.2.3.254-1 [42.4 MB]\n",
            "Get:45 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-pkgload all 1.2.3-1cran1.1804.0 [153 kB]\n",
            "Get:46 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-rcmdcheck all 1.4.0-1cran1.1804.0 [169 kB]\n",
            "Get:47 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-tibble amd64 3.1.5-1cran1.1804.0 [700 kB]\n",
            "Get:48 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-readr amd64 2.0.2-1cran1.1804.0 [807 kB]\n",
            "Get:49 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-remotes all 2.4.1-1cran1.1804.0 [388 kB]\n",
            "Get:50 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-rvest all 1.0.2-1cran1.1804.0 [192 kB]\n",
            "Get:51 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-stringi amd64 1.7.5-1cran1.1804.0 [805 kB]\n",
            "Get:52 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-systemfonts amd64 1.0.3-1cran1.1804.0 [245 kB]\n",
            "Get:53 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-testthat amd64 3.1.0-1cran1.1804.0 [1,445 kB]\n",
            "Get:54 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7-dev 7.6.5.32-1+cuda10.2 [165 MB]\n",
            "Get:55 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-tidyr amd64 1.1.4-1cran1.1804.0 [779 kB]\n",
            "Get:56 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-tinytex all 0.34-1cran1.1804.0 [123 kB]\n",
            "Get:57 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 r-cran-usethis all 2.1.0-1cran1.1804.0 [743 kB]\n",
            "Get:58 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  libcudnn7 7.6.5.32-1+cuda10.2 [189 MB]\n",
            "Fetched 464 MB in 7s (64.0 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../base-files_10.1ubuntu2.11_amd64.deb ...\n",
            "Unpacking base-files (10.1ubuntu2.11) over (10.1ubuntu2.10) ...\n",
            "Setting up base-files (10.1ubuntu2.11) ...\n",
            "Installing new version of config file /etc/issue ...\n",
            "Installing new version of config file /etc/issue.net ...\n",
            "Installing new version of config file /etc/lsb-release ...\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../gzip_1.6-5ubuntu1.1_amd64.deb ...\n",
            "Unpacking gzip (1.6-5ubuntu1.1) over (1.6-5ubuntu1) ...\n",
            "Setting up gzip (1.6-5ubuntu1.1) ...\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../tar_1.29b-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking tar (1.29b-2ubuntu0.2) over (1.29b-2ubuntu0.1) ...\n",
            "Setting up tar (1.29b-2ubuntu0.2) ...\n",
            "update-alternatives: warning: forcing reinstallation of alternative /usr/sbin/rmt-tar because link group rmt is broken\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-bin_2.27-3ubuntu1.4_amd64.deb ...\n",
            "Unpacking libc-bin (2.27-3ubuntu1.4) over (2.27-3ubuntu1.3) ...\n",
            "Setting up libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../libaudit-common_1%3a2.8.2-1ubuntu1.1_all.deb ...\n",
            "Unpacking libaudit-common (1:2.8.2-1ubuntu1.1) over (1:2.8.2-1ubuntu1) ...\n",
            "Setting up libaudit-common (1:2.8.2-1ubuntu1.1) ...\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../libaudit1_1%3a2.8.2-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking libaudit1:amd64 (1:2.8.2-1ubuntu1.1) over (1:2.8.2-1ubuntu1) ...\n",
            "Setting up libaudit1:amd64 (1:2.8.2-1ubuntu1.1) ...\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam0g_1.1.8-3.6ubuntu2.18.04.3_amd64.deb ...\n",
            "Unpacking libpam0g:amd64 (1.1.8-3.6ubuntu2.18.04.3) over (1.1.8-3.6ubuntu2.18.04.2) ...\n",
            "Setting up libpam0g:amd64 (1.1.8-3.6ubuntu2.18.04.3) ...\n",
            "(Reading database ... 155047 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules-bin_1.1.8-3.6ubuntu2.18.04.3_amd64.deb ...\n",
            "Unpacking libpam-modules-bin (1.1.8-3.6ubuntu2.18.04.3) over (1.1.8-3.6ubuntu2.18.04.2) ...\n",
            "Setting up libpam-modules-bin (1.1.8-3.6ubuntu2.18.04.3) ...\n",
            "(Reading database ... 155049 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-modules_1.1.8-3.6ubuntu2.18.04.3_amd64.deb ...\n",
            "Unpacking libpam-modules:amd64 (1.1.8-3.6ubuntu2.18.04.3) over (1.1.8-3.6ubuntu2.18.04.2) ...\n",
            "Setting up libpam-modules:amd64 (1.1.8-3.6ubuntu2.18.04.3) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../liblz4-1_0.0~r131-2ubuntu3.1_amd64.deb ...\n",
            "Unpacking liblz4-1:amd64 (0.0~r131-2ubuntu3.1) over (0.0~r131-2ubuntu3) ...\n",
            "Setting up liblz4-1:amd64 (0.0~r131-2ubuntu3.1) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libpam-runtime_1.1.8-3.6ubuntu2.18.04.3_all.deb ...\n",
            "Unpacking libpam-runtime (1.1.8-3.6ubuntu2.18.04.3) over (1.1.8-3.6ubuntu2.18.04.2) ...\n",
            "Setting up libpam-runtime (1.1.8-3.6ubuntu2.18.04.3) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libzstd1_1.3.3+dfsg-2ubuntu1.2_amd64.deb ...\n",
            "Unpacking libzstd1:amd64 (1.3.3+dfsg-2ubuntu1.2) over (1.3.3+dfsg-2ubuntu1.1) ...\n",
            "Setting up libzstd1:amd64 (1.3.3+dfsg-2ubuntu1.2) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libnettle6_3.4.1-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libnettle6:amd64 (3.4.1-0ubuntu0.18.04.1) over (3.4-1) ...\n",
            "Setting up libnettle6:amd64 (3.4.1-0ubuntu0.18.04.1) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libhogweed4_3.4.1-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libhogweed4:amd64 (3.4.1-0ubuntu0.18.04.1) over (3.4-1) ...\n",
            "Setting up libhogweed4:amd64 (3.4.1-0ubuntu0.18.04.1) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libp11-kit0_0.23.9-2ubuntu0.1_amd64.deb ...\n",
            "Unpacking libp11-kit0:amd64 (0.23.9-2ubuntu0.1) over (0.23.9-2) ...\n",
            "Setting up libp11-kit0:amd64 (0.23.9-2ubuntu0.1) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libgnutls30_3.5.18-1ubuntu1.5_amd64.deb ...\n",
            "Unpacking libgnutls30:amd64 (3.5.18-1ubuntu1.5) over (3.5.18-1ubuntu1.4) ...\n",
            "Setting up libgnutls30:amd64 (3.5.18-1ubuntu1.5) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../libseccomp2_2.5.1-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libseccomp2:amd64 (2.5.1-1ubuntu1~18.04.1) over (2.4.3-1ubuntu3.18.04.3) ...\n",
            "Setting up libseccomp2:amd64 (2.5.1-1ubuntu1~18.04.1) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../ubuntu-keyring_2018.09.18.1~18.04.2_all.deb ...\n",
            "Unpacking ubuntu-keyring (2018.09.18.1~18.04.2) over (2018.09.18.1~18.04.0) ...\n",
            "Setting up ubuntu-keyring (2018.09.18.1~18.04.2) ...\n",
            "(Reading database ... 155053 files and directories currently installed.)\n",
            "Preparing to unpack .../00-distro-info-data_0.37ubuntu0.13_all.deb ...\n",
            "Unpacking distro-info-data (0.37ubuntu0.13) over (0.37ubuntu0.12) ...\n",
            "Preparing to unpack .../01-openssl_1.1.1-1ubuntu2.1~18.04.13_amd64.deb ...\n",
            "Unpacking openssl (1.1.1-1ubuntu2.1~18.04.13) over (1.1.1-1ubuntu2.1~18.04.7) ...\n",
            "Preparing to unpack .../02-binutils-x86-64-linux-gnu_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../03-binutils-common_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking binutils-common:amd64 (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../04-binutils_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking binutils (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../05-libbinutils_2.30-21ubuntu1~18.04.5_amd64.deb ...\n",
            "Unpacking libbinutils:amd64 (2.30-21ubuntu1~18.04.5) over (2.30-21ubuntu1~18.04.4) ...\n",
            "Preparing to unpack .../06-libcublas10_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas10 (10.2.3.254-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../07-libcublas-dev_10.2.3.254-1_amd64.deb ...\n",
            "Unpacking libcublas-dev (10.2.3.254-1) over (10.2.1.243-1) ...\n",
            "Preparing to unpack .../08-libcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v8.h to provide /usr/include/cudnn.h (libcudnn) in auto mode\n",
            "Unpacking libcudnn7-dev (7.6.5.32-1+cuda10.2) over (7.6.5.32-1+cuda10.1) ...\n",
            "Preparing to unpack .../09-libcudnn7_7.6.5.32-1+cuda10.2_amd64.deb ...\n",
            "Unpacking libcudnn7 (7.6.5.32-1+cuda10.2) over (7.6.5.32-1+cuda10.1) ...\n",
            "Preparing to unpack .../10-libsasl2-modules-db_2.1.27~101-g0780600+dfsg-3ubuntu2.3_amd64.deb ...\n",
            "Unpacking libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) over (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Preparing to unpack .../11-libsasl2-2_2.1.27~101-g0780600+dfsg-3ubuntu2.3_amd64.deb ...\n",
            "Unpacking libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) over (2.1.27~101-g0780600+dfsg-3ubuntu2.1) ...\n",
            "Preparing to unpack .../12-libldap-common_2.4.45+dfsg-1ubuntu1.10_all.deb ...\n",
            "Unpacking libldap-common (2.4.45+dfsg-1ubuntu1.10) over (2.4.45+dfsg-1ubuntu1.8) ...\n",
            "Preparing to unpack .../13-libldap-2.4-2_2.4.45+dfsg-1ubuntu1.10_amd64.deb ...\n",
            "Unpacking libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.10) over (2.4.45+dfsg-1ubuntu1.8) ...\n",
            "Preparing to unpack .../14-linux-libc-dev_4.15.0-161.169_amd64.deb ...\n",
            "Unpacking linux-libc-dev:amd64 (4.15.0-161.169) over (4.15.0-128.131) ...\n",
            "Preparing to unpack .../15-r-cran-cpp11_0.4.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-cpp11 (0.4.0-1cran1.1804.0) over (0.3.1-1cran1.1804.0) ...\n",
            "Preparing to unpack .../16-r-cran-data.table_1.14.2-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-data.table (1.14.2-1cran1.1804.0) over (1.14.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../17-r-cran-desc_1.4.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-desc (1.4.0-1cran1.1804.0) over (1.3.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../18-r-cran-diffobj_0.3.5-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-diffobj (0.3.5-1cran1.1804.0) over (0.3.4-1cran1.1804.0) ...\n",
            "Preparing to unpack .../19-r-cran-digest_0.6.28-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-digest (0.6.28-1cran1.1804.0) over (0.6.27-1cran1.1804.0) ...\n",
            "Preparing to unpack .../20-r-cran-lifecycle_1.0.1-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-lifecycle (1.0.1-1cran1.1804.0) over (1.0.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../21-r-cran-hms_1.1.1-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-hms (1.1.1-1cran1.1804.0) over (1.1.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../22-r-cran-knitr_1.36-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-knitr (1.36-1cran1.1804.0) over (1.34-1cran1.1804.0) ...\n",
            "Preparing to unpack .../23-r-cran-lubridate_1.8.0-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-lubridate (1.8.0-1cran1.1804.0) over (1.7.10-1cran1.1804.0) ...\n",
            "Preparing to unpack .../24-r-cran-mgcv_1.8-38-1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-mgcv (1.8-38-1.1804.0) over (1.8-37-1.1804.0) ...\n",
            "Preparing to unpack .../25-r-cran-mime_0.12-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-mime (0.12-1cran1.1804.0) over (0.11-1cran1.1804.0) ...\n",
            "Preparing to unpack .../26-r-cran-pillar_1.6.3-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-pillar (1.6.3-1cran1.1804.0) over (1.6.2-1cran1.1804.0) ...\n",
            "Preparing to unpack .../27-r-cran-pkgload_1.2.3-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-pkgload (1.2.3-1cran1.1804.0) over (1.2.2-1cran1.1804.0) ...\n",
            "Preparing to unpack .../28-r-cran-rcmdcheck_1.4.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-rcmdcheck (1.4.0-1cran1.1804.0) over (1.3.3-1cran1.1804.0) ...\n",
            "Preparing to unpack .../29-r-cran-tibble_3.1.5-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-tibble (3.1.5-1cran1.1804.0) over (3.1.4-1cran1.1804.0) ...\n",
            "Preparing to unpack .../30-r-cran-readr_2.0.2-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-readr (2.0.2-1cran1.1804.0) over (2.0.1-1cran1.1804.0) ...\n",
            "Preparing to unpack .../31-r-cran-remotes_2.4.1-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-remotes (2.4.1-1cran1.1804.0) over (2.4.0-1cran1.1804.0) ...\n",
            "Preparing to unpack .../32-r-cran-rvest_1.0.2-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-rvest (1.0.2-1cran1.1804.0) over (1.0.1-1cran1.1804.0) ...\n",
            "Preparing to unpack .../33-r-cran-stringi_1.7.5-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-stringi (1.7.5-1cran1.1804.0) over (1.7.4-1cran1.1804.0) ...\n",
            "Preparing to unpack .../34-r-cran-systemfonts_1.0.3-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-systemfonts (1.0.3-1cran1.1804.0) over (1.0.2-1cran1.1804.0) ...\n",
            "Preparing to unpack .../35-r-cran-testthat_3.1.0-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-testthat (3.1.0-1cran1.1804.0) over (3.0.4-1cran1.1804.0) ...\n",
            "Preparing to unpack .../36-r-cran-tidyr_1.1.4-1cran1.1804.0_amd64.deb ...\n",
            "Unpacking r-cran-tidyr (1.1.4-1cran1.1804.0) over (1.1.3-1cran1.1804.0) ...\n",
            "Preparing to unpack .../37-r-cran-tinytex_0.34-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-tinytex (0.34-1cran1.1804.0) over (0.33-1cran1.1804.0) ...\n",
            "Preparing to unpack .../38-r-cran-usethis_2.1.0-1cran1.1804.0_all.deb ...\n",
            "Unpacking r-cran-usethis (2.1.0-1cran1.1804.0) over (2.0.1-1cran1.1804.0) ...\n",
            "Preparing to unpack .../39-gnupg2_2.2.4-1ubuntu1.4_all.deb ...\n",
            "Unpacking gnupg2 (2.2.4-1ubuntu1.4) over (2.2.4-1ubuntu1.3) ...\n",
            "Setting up r-cran-tinytex (0.34-1cran1.1804.0) ...\n",
            "Setting up libcudnn7 (7.6.5.32-1+cuda10.2) ...\n",
            "Setting up r-cran-diffobj (0.3.5-1cran1.1804.0) ...\n",
            "Setting up libldap-common (2.4.45+dfsg-1ubuntu1.10) ...\n",
            "Setting up libsasl2-modules-db:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) ...\n",
            "Setting up r-cran-desc (1.4.0-1cran1.1804.0) ...\n",
            "Setting up r-cran-knitr (1.36-1cran1.1804.0) ...\n",
            "Setting up linux-libc-dev:amd64 (4.15.0-161.169) ...\n",
            "Setting up libsasl2-2:amd64 (2.1.27~101-g0780600+dfsg-3ubuntu2.3) ...\n",
            "Setting up libcudnn7-dev (7.6.5.32-1+cuda10.2) ...\n",
            "update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v7.h to provide /usr/include/cudnn.h (libcudnn) in manual mode\n",
            "Setting up distro-info-data (0.37ubuntu0.13) ...\n",
            "Setting up binutils-common:amd64 (2.30-21ubuntu1~18.04.5) ...\n",
            "Setting up gnupg2 (2.2.4-1ubuntu1.4) ...\n",
            "Setting up r-cran-lifecycle (1.0.1-1cran1.1804.0) ...\n",
            "Setting up libcublas10 (10.2.3.254-1) ...\n",
            "Setting up libcublas-dev (10.2.3.254-1) ...\n",
            "Setting up r-cran-mgcv (1.8-38-1.1804.0) ...\n",
            "Setting up r-cran-hms (1.1.1-1cran1.1804.0) ...\n",
            "Setting up libldap-2.4-2:amd64 (2.4.45+dfsg-1ubuntu1.10) ...\n",
            "Setting up r-cran-stringi (1.7.5-1cran1.1804.0) ...\n",
            "Setting up r-cran-remotes (2.4.1-1cran1.1804.0) ...\n",
            "Setting up r-cran-usethis (2.1.0-1cran1.1804.0) ...\n",
            "Setting up openssl (1.1.1-1ubuntu2.1~18.04.13) ...\n",
            "Setting up r-cran-pillar (1.6.3-1cran1.1804.0) ...\n",
            "Setting up r-cran-cpp11 (0.4.0-1cran1.1804.0) ...\n",
            "Setting up r-cran-mime (0.12-1cran1.1804.0) ...\n",
            "Setting up r-cran-digest (0.6.28-1cran1.1804.0) ...\n",
            "Setting up r-cran-data.table (1.14.2-1cran1.1804.0) ...\n",
            "Setting up r-cran-tibble (3.1.5-1cran1.1804.0) ...\n",
            "Setting up r-cran-pkgload (1.2.3-1cran1.1804.0) ...\n",
            "Setting up libbinutils:amd64 (2.30-21ubuntu1~18.04.5) ...\n",
            "Setting up r-cran-tidyr (1.1.4-1cran1.1804.0) ...\n",
            "Setting up r-cran-rcmdcheck (1.4.0-1cran1.1804.0) ...\n",
            "Setting up r-cran-systemfonts (1.0.3-1cran1.1804.0) ...\n",
            "Setting up r-cran-lubridate (1.8.0-1cran1.1804.0) ...\n",
            "Setting up r-cran-readr (2.0.2-1cran1.1804.0) ...\n",
            "Setting up r-cran-rvest (1.0.2-1cran1.1804.0) ...\n",
            "Setting up r-cran-testthat (3.1.0-1cran1.1804.0) ...\n",
            "Setting up binutils-x86-64-linux-gnu (2.30-21ubuntu1~18.04.5) ...\n",
            "Setting up binutils (2.30-21ubuntu1~18.04.5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkUv0BQlRlNf",
        "outputId": "6fa38eb6-a106-43d9-acd1-375fb26a9411"
      },
      "source": [
        "#CODE\n",
        "\n",
        "#Generate root password\n",
        "import random, string, json, sys\n",
        "# password = ''.join(random.choice(string.ascii_letters + string.digits) for i in range(20))\n",
        "password = '1234'\n",
        "\n",
        "#Download ngrok\n",
        "! wget -q -c -nc https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -qq -n ngrok-stable-linux-amd64.zip\n",
        "#Setup sshd\n",
        "! apt-get install -qq -o=Dpkg::Use-Pty=0 openssh-server pwgen > /dev/null\n",
        "#Set root password\n",
        "! echo root:$password | chpasswd\n",
        "! mkdir -p /var/run/sshd\n",
        "! echo \"PermitRootLogin yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"PasswordAuthentication yes\" >> /etc/ssh/sshd_config\n",
        "! echo \"LD_LIBRARY_PATH=/usr/lib64-nvidia\" >> /root/.bashrc\n",
        "! echo \"export LD_LIBRARY_PATH\" >> /root/.bashrc\n",
        "\n",
        "#Run sshd\n",
        "get_ipython().system_raw('/usr/sbin/sshd -D &')\n",
        "\n",
        "#Ask token\n",
        "print(\"Copy authtoken from https://dashboard.ngrok.com/auth\")\n",
        "import getpass\n",
        "authtoken = getpass.getpass()\n",
        "\n",
        "#Create tunnel\n",
        "get_ipython().system_raw('./ngrok authtoken $authtoken && ./ngrok tcp 22 &')\n",
        "#Print root password\n",
        "print(\"Root password: {}\".format(password))\n",
        "\n",
        "import json, sys\n",
        "# Get public address\n",
        "! curl -s http://localhost:4040/api/tunnels \n",
        "\n",
        "import time\n",
        "time.sleep(5)\n",
        "print(\"\\n\")\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Creating config file /etc/ssh/sshd_config with new version\n",
            "Creating SSH2 RSA key; this may take some time ...\n",
            "2048 SHA256:Fyso2dmEhaJILVDbHxjKgdO9SwK/FKBNIdkqdY+ON/w root@c134e9965f98 (RSA)\n",
            "Creating SSH2 ECDSA key; this may take some time ...\n",
            "256 SHA256:L89acHCAv6G/q2PtpUQk+Y2bPtgtl/CRM+yeioDt63g root@c134e9965f98 (ECDSA)\n",
            "Creating SSH2 ED25519 key; this may take some time ...\n",
            "256 SHA256:BJB7+ZpBsu6BFXE+2JhCwT8dFj2SFSdsCfdxGM0MfC4 root@c134e9965f98 (ED25519)\n",
            "Created symlink /etc/systemd/system/sshd.service → /lib/systemd/system/ssh.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/ssh.service → /lib/systemd/system/ssh.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Copy authtoken from https://dashboard.ngrok.com/auth\n",
            "··········\n",
            "Root password: 1234\n",
            "\n",
            "\n",
            "tcp://4.tcp.ngrok.io:13028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkAi3wMyOLhp",
        "outputId": "5f807bde-f17a-435d-e262-c3320e65c27d"
      },
      "source": [
        "!pip uninstall transformers\n",
        "!pip install huggingface_hub\n",
        "!pip install tensorboardX\n",
        "!pip install tokenizers\n",
        "!pip install sacremoses"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.7.4.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.8.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2021.5.30)\n",
            "Installing collected packages: huggingface-hub\n",
            "Successfully installed huggingface-hub-0.0.19\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 16.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n",
            "Collecting tokenizers\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 12.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.10.3\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 15.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.62.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Installing collected packages: sacremoses\n",
            "Successfully installed sacremoses-0.0.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcb_KnIQgQWn"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/robustqa_change_transformers/robustqa ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqRh4ulazIYZ"
      },
      "source": [
        "!cp -r /content/drive/MyDrive/robustqa_change_transformers/robustqa_albert/ ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2jWbORdO4AR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww63fy_pYiR2"
      },
      "source": [
        "#Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ9t40QT1bVa",
        "outputId": "caa710bc-d4a8-46fe-9e13-ab001b9ea960"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "for i in range(1, 24):\n",
        "    # print('{:02d}'.format(i))\n",
        "    print('/content/robustqa_albert/save/albert_experiment-{:02d}'.format(i))\n",
        "    shutil.rmtree('/content/drive/MyDrive/robustqa_change_transformers/robustqa_albert/save/albert_experiment-{:02d}'.format(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert/save/albert_experiment-01\n",
            "/content/robustqa_albert/save/albert_experiment-02\n",
            "/content/robustqa_albert/save/albert_experiment-03\n",
            "/content/robustqa_albert/save/albert_experiment-04\n",
            "/content/robustqa_albert/save/albert_experiment-05\n",
            "/content/robustqa_albert/save/albert_experiment-06\n",
            "/content/robustqa_albert/save/albert_experiment-07\n",
            "/content/robustqa_albert/save/albert_experiment-08\n",
            "/content/robustqa_albert/save/albert_experiment-09\n",
            "/content/robustqa_albert/save/albert_experiment-10\n",
            "/content/robustqa_albert/save/albert_experiment-11\n",
            "/content/robustqa_albert/save/albert_experiment-12\n",
            "/content/robustqa_albert/save/albert_experiment-13\n",
            "/content/robustqa_albert/save/albert_experiment-14\n",
            "/content/robustqa_albert/save/albert_experiment-15\n",
            "/content/robustqa_albert/save/albert_experiment-16\n",
            "/content/robustqa_albert/save/albert_experiment-17\n",
            "/content/robustqa_albert/save/albert_experiment-18\n",
            "/content/robustqa_albert/save/albert_experiment-19\n",
            "/content/robustqa_albert/save/albert_experiment-20\n",
            "/content/robustqa_albert/save/albert_experiment-21\n",
            "/content/robustqa_albert/save/albert_experiment-22\n",
            "/content/robustqa_albert/save/albert_experiment-23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGOepadnSCSD",
        "outputId": "c7fe9166-9f61-4e0a-e042-07e169e6a9b0"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.2-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.7 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.17 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "882a368f044c4b76a5f01051be6de3f1",
            "b9e3acf5ac884d078ccefba0887752f4",
            "b1e8f1c48c4d46acabbc6125ea8a55f5",
            "134480709fae407c8fb9047fc0f3ac70",
            "3f04e0496cbd4a6eb659b2f8b98080a5",
            "cb3b40323a424016ba3f17439f7fdb5b",
            "be78cf9df2d34a639de5581692043e76",
            "b26ae0761a0c4317b4078e6bd86a70f3",
            "ef8116fc3a944570bfe1efb04530c93a",
            "49b7a4301ecb434fa59ea1fe1b8f2a03",
            "60e962122ecb47bd9029210bd4ba7b8d",
            "17a0e19138194a91af74f5c75b7f4ef6",
            "43cd3768c7f546e8b89d523ced35b0ce",
            "fb6a79f65159421a9672c227332feb9a",
            "f9014986b8ac489fa2d1a814255b8508",
            "b8916d23d35f47a1b87f6d7d3946eba4",
            "83b932f926ee49a6a98e51f132ad31b4",
            "8a50441194b74c468b637c70e2fafe9b",
            "7c0ec22e19eb42469f6c70b18721fccb",
            "25ef59644e21465896ecdb7a81ddff32",
            "7dc3710a3d49469985c2dfd7a111bbd2",
            "6e0ef521bae2492b91a14d8fa813c351"
          ]
        },
        "id": "J7OvA8TUegpP",
        "outputId": "4cdfb7d1-de0b-4122-a213-c100697dfe25"
      },
      "source": [
        "from transformers import RobertaForQuestionAnswering\n",
        "\n",
        "roberta_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
        "print(roberta_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "882a368f044c4b76a5f01051be6de3f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17a0e19138194a91af74f5c75b7f4ef6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaForQuestionAnswering(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bclf28iEet7N",
        "outputId": "12deda54-eec5-43c2-c08b-defd239b95a6"
      },
      "source": [
        "print(roberta_model.config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaConfig {\n",
            "  \"_name_or_path\": \"roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.2\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cBwjnOcjeUs",
        "outputId": "07366511-5ad8-46e5-e6b4-247802909150"
      },
      "source": [
        "%cd /content/robustqa\n",
        "\n",
        "from transformers import RobertaForQuestionAnswering\n",
        "\n",
        "changed_model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
        "print(changed_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.9.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.4.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.0.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.10.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.2.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.6.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.9.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.LayerNorm.bias', 'qa_outputs.bias', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.7.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.4.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.LayerNorm.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.LayerNorm.bias', 'qa_outputs.weight', 'roberta.encoder.layer.10.hyper_columns.LayerNorm.bias', 'roberta.encoder.layer.11.hyper_columns.LayerNorm.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RobertaForQuestionAnswering(\n",
            "  (roberta): RobertaModel(\n",
            "    (embeddings): RobertaEmbeddings(\n",
            "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "      (token_type_embeddings): Embedding(1, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): RobertaEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): RobertaLayer(\n",
            "          (attention): RobertaAttention(\n",
            "            (self): RobertaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): RobertaSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): RobertaIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          )\n",
            "          (output): RobertaOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (hyper_columns): RobertaHyperColumn(\n",
            "            (dense): Linear(in_features=768, out_features=2, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=9984, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGSxBG8aY06u"
      },
      "source": [
        "#Modified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7F1j4j7eRIv",
        "outputId": "1e477bdf-4dea-429d-bc6d-6bd9e6c26cd1"
      },
      "source": [
        "# Modified\n",
        "%cd /content/robustqa\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_complete_13_linear \\\n",
        "    --train-datasets squad01 \\\n",
        "    --eval-datasets race01 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'qa_outputs.weight', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'qa_outputs.bias', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[09.30.21 09:40:17] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_complete_13_linear\",\n",
            "    \"save_dir\": \"save/squad_complete_13_linear-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.30.21 09:40:17] Preparing Training Data...\n",
            "100% 16020/16020 [00:00<00:00, 22901.77it/s]\n",
            "Preprocessing not completely accurate for 150/16020 instances\n",
            "[09.30.21 09:40:21] Preparing Validation Data...\n",
            "100% 4436/4436 [00:00<00:00, 22964.42it/s]\n",
            "[09.30.21 09:40:26] Epoch: 0\n",
            "  0% 0/16020 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[09.30.21 09:40:28] Evaluating at step 0...\n",
            "  0% 16/16020 [00:01<23:28, 11.36it/s, NLL=6.17, epoch=0]\n",
            "  0% 0/4436 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4436 [00:00<00:57, 76.34it/s]\u001b[A\n",
            "  1% 48/4436 [00:00<01:21, 54.05it/s]\u001b[A\n",
            "  1% 64/4436 [00:01<01:32, 47.09it/s]\u001b[A\n",
            "  2% 80/4436 [00:01<01:39, 43.78it/s]\u001b[A\n",
            "  2% 96/4436 [00:02<01:43, 41.99it/s]\u001b[A\n",
            "  3% 112/4436 [00:02<01:45, 41.06it/s]\u001b[A\n",
            "  3% 128/4436 [00:02<01:46, 40.45it/s]\u001b[A\n",
            "  3% 144/4436 [00:03<01:47, 40.08it/s]\u001b[A\n",
            "  4% 160/4436 [00:03<01:48, 39.58it/s]\u001b[A\n",
            "  4% 176/4436 [00:04<01:48, 39.35it/s]\u001b[A\n",
            "  4% 192/4436 [00:04<01:48, 39.23it/s]\u001b[A\n",
            "  5% 208/4436 [00:04<01:48, 38.98it/s]\u001b[A\n",
            "  5% 224/4436 [00:05<01:48, 38.80it/s]\u001b[A\n",
            "  5% 240/4436 [00:05<01:48, 38.57it/s]\u001b[A\n",
            "  6% 256/4436 [00:06<01:49, 38.26it/s]\u001b[A\n",
            "  6% 272/4436 [00:06<01:49, 38.09it/s]\u001b[A\n",
            "  6% 288/4436 [00:07<01:49, 37.92it/s]\u001b[A\n",
            "  7% 304/4436 [00:07<01:49, 37.74it/s]\u001b[A\n",
            "  7% 320/4436 [00:07<01:49, 37.67it/s]\u001b[A\n",
            "  8% 336/4436 [00:08<01:48, 37.80it/s]\u001b[A\n",
            "  8% 352/4436 [00:08<01:47, 38.07it/s]\u001b[A\n",
            "  8% 368/4436 [00:09<01:45, 38.42it/s]\u001b[A\n",
            "  9% 384/4436 [00:09<01:45, 38.45it/s]\u001b[A\n",
            "  9% 400/4436 [00:09<01:44, 38.63it/s]\u001b[A\n",
            "  9% 416/4436 [00:10<01:43, 38.69it/s]\u001b[A\n",
            " 10% 432/4436 [00:10<01:43, 38.61it/s]\u001b[A\n",
            " 10% 448/4436 [00:11<01:43, 38.40it/s]\u001b[A\n",
            " 10% 464/4436 [00:11<01:43, 38.47it/s]\u001b[A\n",
            " 11% 480/4436 [00:12<01:43, 38.32it/s]\u001b[A\n",
            "  0% 16/16020 [00:14<23:28, 11.36it/s, NLL=6.17, epoch=0]\n",
            " 12% 512/4436 [00:12<01:42, 38.26it/s]\u001b[A\n",
            " 12% 528/4436 [00:13<01:41, 38.32it/s]\u001b[A\n",
            " 12% 544/4436 [00:13<01:41, 38.51it/s]\u001b[A\n",
            " 13% 560/4436 [00:14<01:40, 38.47it/s]\u001b[A\n",
            " 13% 576/4436 [00:14<01:40, 38.53it/s]\u001b[A\n",
            " 13% 592/4436 [00:14<01:40, 38.39it/s]\u001b[A\n",
            " 14% 608/4436 [00:15<01:39, 38.39it/s]\u001b[A\n",
            " 14% 624/4436 [00:15<01:39, 38.31it/s]\u001b[A\n",
            " 14% 640/4436 [00:16<01:39, 38.21it/s]\u001b[A\n",
            " 15% 656/4436 [00:16<01:39, 38.14it/s]\u001b[A\n",
            " 15% 672/4436 [00:17<01:38, 38.26it/s]\u001b[A\n",
            " 16% 688/4436 [00:17<01:38, 38.19it/s]\u001b[A\n",
            " 16% 704/4436 [00:17<01:37, 38.24it/s]\u001b[A\n",
            " 16% 720/4436 [00:18<01:37, 38.27it/s]\u001b[A\n",
            " 17% 736/4436 [00:18<01:36, 38.26it/s]\u001b[A\n",
            " 17% 752/4436 [00:19<01:36, 38.13it/s]\u001b[A\n",
            " 17% 768/4436 [00:19<01:36, 38.00it/s]\u001b[A\n",
            " 18% 784/4436 [00:20<01:36, 37.96it/s]\u001b[A\n",
            " 18% 800/4436 [00:20<01:35, 37.89it/s]\u001b[A\n",
            " 18% 816/4436 [00:20<01:35, 37.77it/s]\u001b[A\n",
            " 19% 832/4436 [00:21<01:36, 37.52it/s]\u001b[A\n",
            " 19% 848/4436 [00:21<01:36, 37.30it/s]\u001b[A\n",
            " 19% 864/4436 [00:22<01:36, 37.20it/s]\u001b[A\n",
            " 20% 880/4436 [00:22<01:35, 37.37it/s]\u001b[A\n",
            " 20% 896/4436 [00:23<01:34, 37.53it/s]\u001b[A\n",
            " 21% 912/4436 [00:23<01:33, 37.59it/s]\u001b[A\n",
            " 21% 928/4436 [00:23<01:33, 37.64it/s]\u001b[A\n",
            " 21% 944/4436 [00:24<01:32, 37.70it/s]\u001b[A\n",
            " 22% 960/4436 [00:24<01:32, 37.47it/s]\u001b[A\n",
            " 22% 976/4436 [00:25<01:32, 37.49it/s]\u001b[A\n",
            " 22% 992/4436 [00:25<01:31, 37.53it/s]\u001b[A\n",
            " 23% 1008/4436 [00:26<01:31, 37.45it/s]\u001b[A\n",
            " 23% 1024/4436 [00:26<01:31, 37.44it/s]\u001b[A\n",
            " 23% 1040/4436 [00:26<01:30, 37.45it/s]\u001b[A\n",
            " 24% 1056/4436 [00:27<01:30, 37.35it/s]\u001b[A\n",
            " 24% 1072/4436 [00:27<01:29, 37.38it/s]\u001b[A\n",
            " 25% 1088/4436 [00:28<01:29, 37.31it/s]\u001b[A\n",
            " 25% 1104/4436 [00:28<01:29, 37.38it/s]\u001b[A\n",
            " 25% 1120/4436 [00:29<01:28, 37.27it/s]\u001b[A\n",
            " 26% 1136/4436 [00:29<01:28, 37.26it/s]\u001b[A\n",
            " 26% 1152/4436 [00:29<01:28, 37.31it/s]\u001b[A\n",
            " 26% 1168/4436 [00:30<01:27, 37.36it/s]\u001b[A\n",
            " 27% 1184/4436 [00:30<01:27, 37.32it/s]\u001b[A\n",
            " 27% 1200/4436 [00:31<01:26, 37.48it/s]\u001b[A\n",
            " 27% 1216/4436 [00:31<01:25, 37.60it/s]\u001b[A\n",
            " 28% 1232/4436 [00:31<01:25, 37.49it/s]\u001b[A\n",
            " 28% 1248/4436 [00:32<01:25, 37.47it/s]\u001b[A\n",
            " 28% 1264/4436 [00:32<01:24, 37.49it/s]\u001b[A\n",
            " 29% 1280/4436 [00:33<01:24, 37.49it/s]\u001b[A\n",
            " 29% 1296/4436 [00:33<01:23, 37.38it/s]\u001b[A\n",
            " 30% 1312/4436 [00:34<01:23, 37.28it/s]\u001b[A\n",
            " 30% 1328/4436 [00:34<01:23, 37.42it/s]\u001b[A\n",
            " 30% 1344/4436 [00:34<01:22, 37.45it/s]\u001b[A\n",
            " 31% 1360/4436 [00:35<01:22, 37.47it/s]\u001b[A\n",
            " 31% 1376/4436 [00:35<01:21, 37.42it/s]\u001b[A\n",
            " 31% 1392/4436 [00:36<01:21, 37.34it/s]\u001b[A\n",
            " 32% 1408/4436 [00:36<01:21, 37.27it/s]\u001b[A\n",
            " 32% 1424/4436 [00:37<01:20, 37.26it/s]\u001b[A\n",
            " 32% 1440/4436 [00:37<01:20, 37.08it/s]\u001b[A\n",
            " 33% 1456/4436 [00:38<01:20, 37.07it/s]\u001b[A\n",
            " 33% 1472/4436 [00:38<01:20, 37.03it/s]\u001b[A\n",
            " 34% 1488/4436 [00:38<01:19, 36.97it/s]\u001b[A\n",
            " 34% 1504/4436 [00:39<01:19, 37.09it/s]\u001b[A\n",
            " 34% 1520/4436 [00:39<01:18, 37.07it/s]\u001b[A\n",
            " 35% 1536/4436 [00:40<01:18, 37.10it/s]\u001b[A\n",
            " 35% 1552/4436 [00:40<01:17, 37.08it/s]\u001b[A\n",
            " 35% 1568/4436 [00:41<01:17, 37.09it/s]\u001b[A\n",
            " 36% 1584/4436 [00:41<01:17, 37.03it/s]\u001b[A\n",
            " 36% 1600/4436 [00:41<01:16, 37.05it/s]\u001b[A\n",
            " 36% 1616/4436 [00:42<01:16, 37.05it/s]\u001b[A\n",
            " 37% 1632/4436 [00:42<01:15, 36.97it/s]\u001b[A\n",
            " 37% 1648/4436 [00:43<01:15, 37.07it/s]\u001b[A\n",
            " 38% 1664/4436 [00:43<01:14, 37.10it/s]\u001b[A\n",
            " 38% 1680/4436 [00:44<01:14, 37.03it/s]\u001b[A\n",
            " 38% 1696/4436 [00:44<01:14, 37.02it/s]\u001b[A\n",
            " 39% 1712/4436 [00:44<01:13, 37.05it/s]\u001b[A\n",
            " 39% 1728/4436 [00:45<01:12, 37.26it/s]\u001b[A\n",
            " 39% 1744/4436 [00:45<01:12, 37.15it/s]\u001b[A\n",
            " 40% 1760/4436 [00:46<01:12, 36.98it/s]\u001b[A\n",
            " 40% 1776/4436 [00:46<01:11, 37.04it/s]\u001b[A\n",
            " 40% 1792/4436 [00:47<01:11, 36.84it/s]\u001b[A\n",
            " 41% 1808/4436 [00:47<01:11, 36.90it/s]\u001b[A\n",
            " 41% 1824/4436 [00:47<01:10, 36.86it/s]\u001b[A\n",
            " 41% 1840/4436 [00:48<01:10, 36.91it/s]\u001b[A\n",
            " 42% 1856/4436 [00:48<01:09, 36.93it/s]\u001b[A\n",
            " 42% 1872/4436 [00:49<01:09, 36.91it/s]\u001b[A\n",
            " 43% 1888/4436 [00:49<01:09, 36.71it/s]\u001b[A\n",
            " 43% 1904/4436 [00:50<01:08, 36.82it/s]\u001b[A\n",
            " 43% 1920/4436 [00:50<01:08, 36.83it/s]\u001b[A\n",
            " 44% 1936/4436 [00:50<01:07, 36.83it/s]\u001b[A\n",
            " 44% 1952/4436 [00:51<01:07, 36.89it/s]\u001b[A\n",
            " 44% 1968/4436 [00:51<01:07, 36.82it/s]\u001b[A\n",
            " 45% 1984/4436 [00:52<01:06, 36.77it/s]\u001b[A\n",
            " 45% 2000/4436 [00:52<01:06, 36.80it/s]\u001b[A\n",
            " 45% 2016/4436 [00:53<01:05, 36.80it/s]\u001b[A\n",
            " 46% 2032/4436 [00:53<01:05, 36.87it/s]\u001b[A\n",
            " 46% 2048/4436 [00:54<01:05, 36.68it/s]\u001b[A\n",
            " 47% 2064/4436 [00:54<01:04, 36.64it/s]\u001b[A\n",
            " 47% 2080/4436 [00:54<01:04, 36.67it/s]\u001b[A\n",
            " 47% 2096/4436 [00:55<01:03, 36.68it/s]\u001b[A\n",
            " 48% 2112/4436 [00:55<01:03, 36.71it/s]\u001b[A\n",
            " 48% 2128/4436 [00:56<01:02, 36.75it/s]\u001b[A\n",
            " 48% 2144/4436 [00:56<01:02, 36.69it/s]\u001b[A\n",
            " 49% 2160/4436 [00:57<01:01, 36.72it/s]\u001b[A\n",
            " 49% 2176/4436 [00:57<01:02, 36.04it/s]\u001b[A\n",
            " 49% 2192/4436 [00:57<01:02, 36.11it/s]\u001b[A\n",
            " 50% 2208/4436 [00:58<01:01, 36.12it/s]\u001b[A\n",
            " 50% 2224/4436 [00:58<01:01, 36.17it/s]\u001b[A\n",
            " 50% 2240/4436 [00:59<01:00, 36.01it/s]\u001b[A\n",
            " 51% 2256/4436 [00:59<01:00, 36.17it/s]\u001b[A\n",
            " 51% 2272/4436 [01:00<00:59, 36.23it/s]\u001b[A\n",
            " 52% 2288/4436 [01:00<00:59, 36.33it/s]\u001b[A\n",
            " 52% 2304/4436 [01:01<00:58, 36.25it/s]\u001b[A\n",
            " 52% 2320/4436 [01:01<00:58, 35.96it/s]\u001b[A\n",
            " 53% 2336/4436 [01:01<00:58, 36.09it/s]\u001b[A\n",
            " 53% 2352/4436 [01:02<00:57, 36.16it/s]\u001b[A\n",
            " 53% 2368/4436 [01:02<00:57, 35.94it/s]\u001b[A\n",
            " 54% 2384/4436 [01:03<00:56, 36.13it/s]\u001b[A\n",
            " 54% 2400/4436 [01:03<00:56, 35.91it/s]\u001b[A\n",
            " 54% 2416/4436 [01:04<00:56, 36.05it/s]\u001b[A\n",
            " 55% 2432/4436 [01:04<00:55, 35.82it/s]\u001b[A\n",
            " 55% 2448/4436 [01:05<00:55, 35.72it/s]\u001b[A\n",
            " 56% 2464/4436 [01:05<00:55, 35.71it/s]\u001b[A\n",
            " 56% 2480/4436 [01:05<00:54, 35.78it/s]\u001b[A\n",
            " 56% 2496/4436 [01:06<00:54, 35.72it/s]\u001b[A\n",
            " 57% 2512/4436 [01:06<00:54, 35.41it/s]\u001b[A\n",
            " 57% 2528/4436 [01:07<00:53, 35.49it/s]\u001b[A\n",
            " 57% 2544/4436 [01:07<00:53, 35.53it/s]\u001b[A\n",
            " 58% 2560/4436 [01:08<00:53, 35.35it/s]\u001b[A\n",
            " 58% 2576/4436 [01:08<00:52, 35.40it/s]\u001b[A\n",
            " 58% 2592/4436 [01:09<00:52, 35.38it/s]\u001b[A\n",
            " 59% 2608/4436 [01:09<00:51, 35.25it/s]\u001b[A\n",
            " 59% 2624/4436 [01:10<00:51, 35.19it/s]\u001b[A\n",
            " 60% 2640/4436 [01:10<00:50, 35.33it/s]\u001b[A\n",
            " 60% 2656/4436 [01:10<00:50, 35.47it/s]\u001b[A\n",
            " 60% 2672/4436 [01:11<00:49, 35.60it/s]\u001b[A\n",
            " 61% 2688/4436 [01:11<00:48, 35.70it/s]\u001b[A\n",
            " 61% 2704/4436 [01:12<00:48, 35.67it/s]\u001b[A\n",
            " 61% 2720/4436 [01:12<00:48, 35.66it/s]\u001b[A\n",
            " 62% 2736/4436 [01:13<00:47, 35.68it/s]\u001b[A\n",
            " 62% 2752/4436 [01:13<00:47, 35.68it/s]\u001b[A\n",
            " 62% 2768/4436 [01:14<00:46, 35.72it/s]\u001b[A\n",
            " 63% 2784/4436 [01:14<00:46, 35.75it/s]\u001b[A\n",
            " 63% 2800/4436 [01:14<00:45, 35.80it/s]\u001b[A\n",
            " 63% 2816/4436 [01:15<00:45, 35.88it/s]\u001b[A\n",
            " 64% 2832/4436 [01:15<00:44, 35.84it/s]\u001b[A\n",
            " 64% 2848/4436 [01:16<00:44, 35.85it/s]\u001b[A\n",
            " 65% 2864/4436 [01:16<00:43, 35.76it/s]\u001b[A\n",
            " 65% 2880/4436 [01:17<00:43, 35.79it/s]\u001b[A\n",
            " 65% 2896/4436 [01:17<00:43, 35.58it/s]\u001b[A\n",
            " 66% 2912/4436 [01:18<00:42, 35.56it/s]\u001b[A\n",
            " 66% 2928/4436 [01:18<00:42, 35.50it/s]\u001b[A\n",
            " 66% 2944/4436 [01:19<00:42, 35.39it/s]\u001b[A\n",
            " 67% 2960/4436 [01:19<00:41, 35.35it/s]\u001b[A\n",
            " 67% 2976/4436 [01:19<00:41, 35.28it/s]\u001b[A\n",
            " 67% 2992/4436 [01:20<00:40, 35.35it/s]\u001b[A\n",
            " 68% 3008/4436 [01:20<00:40, 35.36it/s]\u001b[A\n",
            " 68% 3024/4436 [01:21<00:39, 35.33it/s]\u001b[A\n",
            " 69% 3040/4436 [01:21<00:39, 35.33it/s]\u001b[A\n",
            " 69% 3056/4436 [01:22<00:39, 35.34it/s]\u001b[A\n",
            " 69% 3072/4436 [01:22<00:38, 35.23it/s]\u001b[A\n",
            " 70% 3088/4436 [01:23<00:38, 35.41it/s]\u001b[A\n",
            " 70% 3104/4436 [01:23<00:37, 35.19it/s]\u001b[A\n",
            " 70% 3120/4436 [01:24<00:37, 34.78it/s]\u001b[A\n",
            " 71% 3136/4436 [01:24<00:37, 34.95it/s]\u001b[A\n",
            " 71% 3152/4436 [01:24<00:36, 34.90it/s]\u001b[A\n",
            " 71% 3168/4436 [01:25<00:36, 34.69it/s]\u001b[A\n",
            " 72% 3184/4436 [01:25<00:36, 34.73it/s]\u001b[A\n",
            " 72% 3200/4436 [01:26<00:35, 34.51it/s]\u001b[A\n",
            " 72% 3216/4436 [01:26<00:35, 34.53it/s]\u001b[A\n",
            " 73% 3232/4436 [01:27<00:34, 34.47it/s]\u001b[A\n",
            " 73% 3248/4436 [01:27<00:34, 34.50it/s]\u001b[A\n",
            " 74% 3264/4436 [01:28<00:33, 34.52it/s]\u001b[A\n",
            " 74% 3280/4436 [01:28<00:33, 34.17it/s]\u001b[A\n",
            " 74% 3296/4436 [01:29<00:33, 34.33it/s]\u001b[A\n",
            " 75% 3312/4436 [01:29<00:32, 34.20it/s]\u001b[A\n",
            " 75% 3328/4436 [01:30<00:32, 34.46it/s]\u001b[A\n",
            " 75% 3344/4436 [01:30<00:31, 34.34it/s]\u001b[A\n",
            " 76% 3360/4436 [01:31<00:31, 34.43it/s]\u001b[A\n",
            " 76% 3376/4436 [01:31<00:30, 34.44it/s]\u001b[A\n",
            " 76% 3392/4436 [01:31<00:30, 34.50it/s]\u001b[A\n",
            " 77% 3408/4436 [01:32<00:29, 34.45it/s]\u001b[A\n",
            " 77% 3424/4436 [01:32<00:29, 34.37it/s]\u001b[A\n",
            " 78% 3440/4436 [01:33<00:29, 34.30it/s]\u001b[A\n",
            " 78% 3456/4436 [01:33<00:28, 34.17it/s]\u001b[A\n",
            " 78% 3472/4436 [01:34<00:28, 34.05it/s]\u001b[A\n",
            " 79% 3488/4436 [01:34<00:27, 34.09it/s]\u001b[A\n",
            " 79% 3504/4436 [01:35<00:27, 34.23it/s]\u001b[A\n",
            " 79% 3520/4436 [01:35<00:26, 34.17it/s]\u001b[A\n",
            " 80% 3536/4436 [01:36<00:26, 34.03it/s]\u001b[A\n",
            " 80% 3552/4436 [01:36<00:26, 33.94it/s]\u001b[A\n",
            " 80% 3568/4436 [01:37<00:25, 33.91it/s]\u001b[A\n",
            " 81% 3584/4436 [01:37<00:25, 34.06it/s]\u001b[A\n",
            " 81% 3600/4436 [01:38<00:24, 34.06it/s]\u001b[A\n",
            " 82% 3616/4436 [01:38<00:24, 33.73it/s]\u001b[A\n",
            " 82% 3632/4436 [01:39<00:24, 33.48it/s]\u001b[A\n",
            " 82% 3648/4436 [01:39<00:23, 33.54it/s]\u001b[A\n",
            " 83% 3664/4436 [01:39<00:23, 33.49it/s]\u001b[A\n",
            " 83% 3680/4436 [01:40<00:22, 33.61it/s]\u001b[A\n",
            " 83% 3696/4436 [01:40<00:21, 33.70it/s]\u001b[A\n",
            " 84% 3712/4436 [01:41<00:21, 33.73it/s]\u001b[A\n",
            " 84% 3728/4436 [01:41<00:20, 33.79it/s]\u001b[A\n",
            " 84% 3744/4436 [01:42<00:20, 33.77it/s]\u001b[A\n",
            " 85% 3760/4436 [01:42<00:20, 33.56it/s]\u001b[A\n",
            " 85% 3776/4436 [01:43<00:19, 33.59it/s]\u001b[A\n",
            " 85% 3792/4436 [01:43<00:19, 33.46it/s]\u001b[A\n",
            " 86% 3808/4436 [01:44<00:18, 33.56it/s]\u001b[A\n",
            " 86% 3824/4436 [01:44<00:18, 33.76it/s]\u001b[A\n",
            " 87% 3840/4436 [01:45<00:17, 33.71it/s]\u001b[A\n",
            " 87% 3856/4436 [01:45<00:17, 33.58it/s]\u001b[A\n",
            " 87% 3872/4436 [01:46<00:16, 33.34it/s]\u001b[A\n",
            " 88% 3888/4436 [01:46<00:16, 33.52it/s]\u001b[A\n",
            " 88% 3904/4436 [01:47<00:15, 33.46it/s]\u001b[A\n",
            " 88% 3920/4436 [01:47<00:15, 33.61it/s]\u001b[A\n",
            " 89% 3936/4436 [01:48<00:14, 33.58it/s]\u001b[A\n",
            " 89% 3952/4436 [01:48<00:14, 33.53it/s]\u001b[A\n",
            " 89% 3968/4436 [01:49<00:13, 33.48it/s]\u001b[A\n",
            " 90% 3984/4436 [01:49<00:13, 33.63it/s]\u001b[A\n",
            " 90% 4000/4436 [01:49<00:12, 33.59it/s]\u001b[A\n",
            " 91% 4016/4436 [01:50<00:12, 33.68it/s]\u001b[A\n",
            " 91% 4032/4436 [01:50<00:11, 33.69it/s]\u001b[A\n",
            " 91% 4048/4436 [01:51<00:11, 33.67it/s]\u001b[A\n",
            " 92% 4064/4436 [01:51<00:11, 33.70it/s]\u001b[A\n",
            " 92% 4080/4436 [01:52<00:10, 33.76it/s]\u001b[A\n",
            " 92% 4096/4436 [01:52<00:10, 33.88it/s]\u001b[A\n",
            " 93% 4112/4436 [01:53<00:09, 33.96it/s]\u001b[A\n",
            " 93% 4128/4436 [01:53<00:09, 34.10it/s]\u001b[A\n",
            " 93% 4144/4436 [01:54<00:08, 34.10it/s]\u001b[A\n",
            " 94% 4160/4436 [01:54<00:08, 33.47it/s]\u001b[A\n",
            " 94% 4176/4436 [01:55<00:07, 33.57it/s]\u001b[A\n",
            " 94% 4192/4436 [01:55<00:07, 33.18it/s]\u001b[A\n",
            " 95% 4208/4436 [01:56<00:06, 32.98it/s]\u001b[A\n",
            " 95% 4224/4436 [01:56<00:06, 32.51it/s]\u001b[A\n",
            " 96% 4240/4436 [01:57<00:06, 32.53it/s]\u001b[A\n",
            " 96% 4256/4436 [01:57<00:05, 32.28it/s]\u001b[A\n",
            " 96% 4272/4436 [01:58<00:05, 32.49it/s]\u001b[A\n",
            " 97% 4288/4436 [01:58<00:04, 32.51it/s]\u001b[A\n",
            " 97% 4304/4436 [01:59<00:04, 32.74it/s]\u001b[A\n",
            " 97% 4320/4436 [01:59<00:03, 33.06it/s]\u001b[A\n",
            " 98% 4336/4436 [02:00<00:03, 33.25it/s]\u001b[A\n",
            " 98% 4352/4436 [02:00<00:02, 33.15it/s]\u001b[A\n",
            " 98% 4368/4436 [02:01<00:02, 33.02it/s]\u001b[A\n",
            " 99% 4384/4436 [02:01<00:01, 32.88it/s]\u001b[A\n",
            " 99% 4400/4436 [02:02<00:01, 32.88it/s]\u001b[A\n",
            "100% 4416/4436 [02:02<00:00, 32.85it/s]\u001b[A\n",
            "100% 4432/4436 [02:03<00:00, 32.77it/s]\u001b[A\n",
            "100% 4436/4436 [02:03<00:00, 35.92it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 352/4306 [00:00<00:01, 3511.58it/s]\u001b[A\n",
            " 17% 711/4306 [00:00<00:01, 3554.10it/s]\u001b[A\n",
            " 26% 1102/4306 [00:00<00:00, 3712.13it/s]\u001b[A\n",
            " 34% 1474/4306 [00:00<00:00, 3668.78it/s]\u001b[A\n",
            " 43% 1841/4306 [00:00<00:00, 3603.31it/s]\u001b[A\n",
            " 51% 2211/4306 [00:00<00:00, 3633.34it/s]\u001b[A\n",
            " 60% 2602/4306 [00:00<00:00, 3721.94it/s]\u001b[A\n",
            " 69% 2975/4306 [00:00<00:00, 3675.09it/s]\u001b[A\n",
            " 78% 3343/4306 [00:00<00:00, 3644.73it/s]\u001b[A\n",
            " 86% 3708/4306 [00:01<00:00, 3629.35it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 3602.23it/s]\n",
            "validation!!\n",
            "[09.30.21 09:42:33] Visualizing in TensorBoard...\n",
            "[09.30.21 09:42:33] Eval F1: 06.42, EM: 00.14\n",
            "100% 16020/16020 [27:09<00:00,  9.83it/s, NLL=1.13, epoch=0]\n",
            "[09.30.21 10:07:37] Epoch: 1\n",
            "100% 16020/16020 [24:59<00:00, 10.68it/s, NLL=1.36, epoch=1]\n",
            "[09.30.21 10:32:37] Epoch: 2\n",
            "100% 16020/16020 [24:59<00:00, 10.68it/s, NLL=0.358, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fqpELHkmgPi",
        "outputId": "cf1afe3c-63e4-47b1-c18f-6fe1cf4e0ad2"
      },
      "source": [
        "%cd /content/robustqa\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_complete_13_linear \\\n",
        "    --train-datasets squad02 \\\n",
        "    --eval-datasets race02 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "[09.30.21 10:57:46] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race02\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_complete_13_linear\",\n",
            "    \"save_dir\": \"save/squad_complete_13_linear-02\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad02\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.30.21 10:57:46] Preparing Training Data...\n",
            "100% 17496/17496 [00:00<00:00, 21620.24it/s]\n",
            "Preprocessing not completely accurate for 208/17496 instances\n",
            "[09.30.21 10:57:52] Preparing Validation Data...\n",
            "100% 2977/2977 [00:00<00:00, 22602.31it/s]\n",
            "[09.30.21 10:57:56] Epoch: 0\n",
            "  0% 0/17496 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[09.30.21 10:57:58] Evaluating at step 0...\n",
            "  0% 16/17496 [00:01<27:07, 10.74it/s, NLL=0.979, epoch=0]\n",
            "  0% 0/2977 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/2977 [00:00<00:42, 70.09it/s]\u001b[A\n",
            "  2% 48/2977 [00:00<00:59, 49.20it/s]\u001b[A\n",
            "  2% 64/2977 [00:01<01:07, 43.42it/s]\u001b[A\n",
            "  3% 80/2977 [00:01<01:12, 39.96it/s]\u001b[A\n",
            "  3% 96/2977 [00:02<01:15, 38.19it/s]\u001b[A\n",
            "  4% 112/2977 [00:02<01:17, 37.15it/s]\u001b[A\n",
            "  4% 128/2977 [00:03<01:18, 36.44it/s]\u001b[A\n",
            "  5% 144/2977 [00:03<01:18, 36.29it/s]\u001b[A\n",
            "  5% 160/2977 [00:04<01:18, 36.02it/s]\u001b[A\n",
            "  6% 176/2977 [00:04<01:17, 35.99it/s]\u001b[A\n",
            "  6% 192/2977 [00:04<01:17, 35.82it/s]\u001b[A\n",
            "  7% 208/2977 [00:05<01:17, 35.61it/s]\u001b[A\n",
            "  8% 224/2977 [00:05<01:18, 35.12it/s]\u001b[A\n",
            "  8% 240/2977 [00:06<01:18, 35.02it/s]\u001b[A\n",
            "  9% 256/2977 [00:06<01:17, 34.94it/s]\u001b[A\n",
            "  9% 272/2977 [00:07<01:17, 35.07it/s]\u001b[A\n",
            " 10% 288/2977 [00:07<01:16, 35.05it/s]\u001b[A\n",
            " 10% 304/2977 [00:08<01:16, 35.13it/s]\u001b[A\n",
            " 11% 320/2977 [00:08<01:15, 34.96it/s]\u001b[A\n",
            " 11% 336/2977 [00:09<01:15, 34.88it/s]\u001b[A\n",
            " 12% 352/2977 [00:09<01:14, 35.06it/s]\u001b[A\n",
            " 12% 368/2977 [00:10<01:14, 34.87it/s]\u001b[A\n",
            " 13% 384/2977 [00:10<01:14, 34.73it/s]\u001b[A\n",
            " 13% 400/2977 [00:10<01:14, 34.63it/s]\u001b[A\n",
            " 14% 416/2977 [00:11<01:13, 34.86it/s]\u001b[A\n",
            " 15% 432/2977 [00:11<01:12, 34.96it/s]\u001b[A\n",
            " 15% 448/2977 [00:12<01:12, 34.93it/s]\u001b[A\n",
            "  0% 16/17496 [00:14<27:07, 10.74it/s, NLL=0.979, epoch=0]\n",
            " 16% 480/2977 [00:13<01:12, 34.50it/s]\u001b[A\n",
            " 17% 496/2977 [00:13<01:12, 34.19it/s]\u001b[A\n",
            " 17% 512/2977 [00:14<01:12, 34.18it/s]\u001b[A\n",
            " 18% 528/2977 [00:14<01:11, 34.48it/s]\u001b[A\n",
            " 18% 544/2977 [00:15<01:10, 34.62it/s]\u001b[A\n",
            " 19% 560/2977 [00:15<01:09, 34.80it/s]\u001b[A\n",
            " 19% 576/2977 [00:16<01:09, 34.58it/s]\u001b[A\n",
            " 20% 592/2977 [00:16<01:10, 34.07it/s]\u001b[A\n",
            " 20% 608/2977 [00:16<01:09, 34.26it/s]\u001b[A\n",
            " 21% 624/2977 [00:17<01:08, 34.38it/s]\u001b[A\n",
            " 21% 640/2977 [00:17<01:07, 34.47it/s]\u001b[A\n",
            " 22% 656/2977 [00:18<01:07, 34.34it/s]\u001b[A\n",
            " 23% 672/2977 [00:18<01:07, 34.33it/s]\u001b[A\n",
            " 23% 688/2977 [00:19<01:06, 34.21it/s]\u001b[A\n",
            " 24% 704/2977 [00:19<01:06, 34.02it/s]\u001b[A\n",
            " 24% 720/2977 [00:20<01:06, 34.12it/s]\u001b[A\n",
            " 25% 736/2977 [00:20<01:06, 33.69it/s]\u001b[A\n",
            " 25% 752/2977 [00:21<01:06, 33.61it/s]\u001b[A\n",
            " 26% 768/2977 [00:21<01:05, 33.63it/s]\u001b[A\n",
            " 26% 784/2977 [00:22<01:04, 33.79it/s]\u001b[A\n",
            " 27% 800/2977 [00:22<01:04, 33.89it/s]\u001b[A\n",
            " 27% 816/2977 [00:23<01:04, 33.69it/s]\u001b[A\n",
            " 28% 832/2977 [00:23<01:03, 33.61it/s]\u001b[A\n",
            " 28% 848/2977 [00:24<01:02, 33.91it/s]\u001b[A\n",
            " 29% 864/2977 [00:24<01:02, 33.82it/s]\u001b[A\n",
            " 30% 880/2977 [00:25<01:02, 33.79it/s]\u001b[A\n",
            " 30% 896/2977 [00:25<01:01, 33.63it/s]\u001b[A\n",
            " 31% 912/2977 [00:25<01:01, 33.38it/s]\u001b[A\n",
            " 31% 928/2977 [00:26<01:01, 33.27it/s]\u001b[A\n",
            " 32% 944/2977 [00:26<01:00, 33.38it/s]\u001b[A\n",
            " 32% 960/2977 [00:27<01:00, 33.27it/s]\u001b[A\n",
            " 33% 976/2977 [00:27<01:00, 33.07it/s]\u001b[A\n",
            " 33% 992/2977 [00:28<01:00, 32.96it/s]\u001b[A\n",
            " 34% 1008/2977 [00:28<00:59, 33.26it/s]\u001b[A\n",
            " 34% 1024/2977 [00:29<00:58, 33.31it/s]\u001b[A\n",
            " 35% 1040/2977 [00:29<00:57, 33.41it/s]\u001b[A\n",
            " 35% 1056/2977 [00:30<00:57, 33.29it/s]\u001b[A\n",
            " 36% 1072/2977 [00:30<00:57, 33.15it/s]\u001b[A\n",
            " 37% 1088/2977 [00:31<00:57, 32.96it/s]\u001b[A\n",
            " 37% 1104/2977 [00:31<00:56, 32.97it/s]\u001b[A\n",
            " 38% 1120/2977 [00:32<00:56, 33.00it/s]\u001b[A\n",
            " 38% 1136/2977 [00:32<00:55, 32.92it/s]\u001b[A\n",
            " 39% 1152/2977 [00:33<00:55, 32.85it/s]\u001b[A\n",
            " 39% 1168/2977 [00:33<00:55, 32.83it/s]\u001b[A\n",
            " 40% 1184/2977 [00:34<00:54, 32.62it/s]\u001b[A\n",
            " 40% 1200/2977 [00:34<00:54, 32.35it/s]\u001b[A\n",
            " 41% 1216/2977 [00:35<00:54, 32.46it/s]\u001b[A\n",
            " 41% 1232/2977 [00:35<00:54, 32.21it/s]\u001b[A\n",
            " 42% 1248/2977 [00:36<00:53, 32.38it/s]\u001b[A\n",
            " 42% 1264/2977 [00:36<00:52, 32.37it/s]\u001b[A\n",
            " 43% 1280/2977 [00:37<00:52, 32.28it/s]\u001b[A\n",
            " 44% 1296/2977 [00:37<00:51, 32.35it/s]\u001b[A\n",
            " 44% 1312/2977 [00:38<00:51, 32.47it/s]\u001b[A\n",
            " 45% 1328/2977 [00:38<00:50, 32.55it/s]\u001b[A\n",
            " 45% 1344/2977 [00:39<00:49, 32.73it/s]\u001b[A\n",
            " 46% 1360/2977 [00:39<00:49, 32.49it/s]\u001b[A\n",
            " 46% 1376/2977 [00:40<00:49, 32.38it/s]\u001b[A\n",
            " 47% 1392/2977 [00:40<00:48, 32.43it/s]\u001b[A\n",
            " 47% 1408/2977 [00:41<00:48, 32.52it/s]\u001b[A\n",
            " 48% 1424/2977 [00:41<00:47, 32.67it/s]\u001b[A\n",
            " 48% 1440/2977 [00:42<00:46, 32.86it/s]\u001b[A\n",
            " 49% 1456/2977 [00:42<00:46, 32.94it/s]\u001b[A\n",
            " 49% 1472/2977 [00:43<00:45, 33.13it/s]\u001b[A\n",
            " 50% 1488/2977 [00:43<00:45, 32.92it/s]\u001b[A\n",
            " 51% 1504/2977 [00:44<00:44, 33.06it/s]\u001b[A\n",
            " 51% 1520/2977 [00:44<00:44, 32.83it/s]\u001b[A\n",
            " 52% 1536/2977 [00:45<00:43, 32.88it/s]\u001b[A\n",
            " 52% 1552/2977 [00:45<00:43, 32.74it/s]\u001b[A\n",
            " 53% 1568/2977 [00:45<00:42, 33.10it/s]\u001b[A\n",
            " 53% 1584/2977 [00:46<00:42, 33.17it/s]\u001b[A\n",
            " 54% 1600/2977 [00:46<00:41, 33.02it/s]\u001b[A\n",
            " 54% 1616/2977 [00:47<00:41, 32.99it/s]\u001b[A\n",
            " 55% 1632/2977 [00:47<00:40, 33.32it/s]\u001b[A\n",
            " 55% 1648/2977 [00:48<00:39, 33.39it/s]\u001b[A\n",
            " 56% 1664/2977 [00:48<00:39, 33.32it/s]\u001b[A\n",
            " 56% 1680/2977 [00:49<00:39, 33.26it/s]\u001b[A\n",
            " 57% 1696/2977 [00:49<00:38, 33.47it/s]\u001b[A\n",
            " 58% 1712/2977 [00:50<00:37, 33.59it/s]\u001b[A\n",
            " 58% 1728/2977 [00:50<00:36, 33.77it/s]\u001b[A\n",
            " 59% 1744/2977 [00:51<00:36, 33.69it/s]\u001b[A\n",
            " 59% 1760/2977 [00:51<00:36, 33.59it/s]\u001b[A\n",
            " 60% 1776/2977 [00:52<00:35, 33.81it/s]\u001b[A\n",
            " 60% 1792/2977 [00:52<00:35, 33.57it/s]\u001b[A\n",
            " 61% 1808/2977 [00:53<00:35, 33.23it/s]\u001b[A\n",
            " 61% 1824/2977 [00:53<00:34, 33.47it/s]\u001b[A\n",
            " 62% 1840/2977 [00:54<00:33, 33.46it/s]\u001b[A\n",
            " 62% 1856/2977 [00:54<00:33, 33.62it/s]\u001b[A\n",
            " 63% 1872/2977 [00:55<00:32, 33.82it/s]\u001b[A\n",
            " 63% 1888/2977 [00:55<00:32, 33.94it/s]\u001b[A\n",
            " 64% 1904/2977 [00:55<00:31, 33.83it/s]\u001b[A\n",
            " 64% 1920/2977 [00:56<00:31, 34.07it/s]\u001b[A\n",
            " 65% 1936/2977 [00:56<00:30, 33.95it/s]\u001b[A\n",
            " 66% 1952/2977 [00:57<00:30, 33.96it/s]\u001b[A\n",
            " 66% 1968/2977 [00:57<00:29, 34.03it/s]\u001b[A\n",
            " 67% 1984/2977 [00:58<00:29, 33.61it/s]\u001b[A\n",
            " 67% 2000/2977 [00:58<00:28, 33.69it/s]\u001b[A\n",
            " 68% 2016/2977 [00:59<00:28, 33.65it/s]\u001b[A\n",
            " 68% 2032/2977 [00:59<00:27, 33.92it/s]\u001b[A\n",
            " 69% 2048/2977 [01:00<00:27, 34.08it/s]\u001b[A\n",
            " 69% 2064/2977 [01:00<00:26, 34.30it/s]\u001b[A\n",
            " 70% 2080/2977 [01:01<00:26, 34.12it/s]\u001b[A\n",
            " 70% 2096/2977 [01:01<00:25, 34.19it/s]\u001b[A\n",
            " 71% 2112/2977 [01:02<00:25, 34.08it/s]\u001b[A\n",
            " 71% 2128/2977 [01:02<00:24, 34.17it/s]\u001b[A\n",
            " 72% 2144/2977 [01:03<00:24, 34.08it/s]\u001b[A\n",
            " 73% 2160/2977 [01:03<00:23, 34.29it/s]\u001b[A\n",
            " 73% 2176/2977 [01:03<00:23, 34.54it/s]\u001b[A\n",
            " 74% 2192/2977 [01:04<00:22, 34.64it/s]\u001b[A\n",
            " 74% 2208/2977 [01:04<00:22, 34.80it/s]\u001b[A\n",
            " 75% 2224/2977 [01:05<00:21, 34.94it/s]\u001b[A\n",
            " 75% 2240/2977 [01:05<00:21, 34.84it/s]\u001b[A\n",
            " 76% 2256/2977 [01:06<00:20, 34.72it/s]\u001b[A\n",
            " 76% 2272/2977 [01:06<00:20, 34.60it/s]\u001b[A\n",
            " 77% 2288/2977 [01:07<00:19, 34.65it/s]\u001b[A\n",
            " 77% 2304/2977 [01:07<00:19, 34.77it/s]\u001b[A\n",
            " 78% 2320/2977 [01:08<00:18, 34.60it/s]\u001b[A\n",
            " 78% 2336/2977 [01:08<00:18, 34.81it/s]\u001b[A\n",
            " 79% 2352/2977 [01:09<00:18, 34.44it/s]\u001b[A\n",
            " 80% 2368/2977 [01:09<00:17, 34.52it/s]\u001b[A\n",
            " 80% 2384/2977 [01:09<00:17, 34.64it/s]\u001b[A\n",
            " 81% 2400/2977 [01:10<00:16, 34.87it/s]\u001b[A\n",
            " 81% 2416/2977 [01:10<00:16, 34.83it/s]\u001b[A\n",
            " 82% 2432/2977 [01:11<00:15, 34.87it/s]\u001b[A\n",
            " 82% 2448/2977 [01:11<00:15, 34.83it/s]\u001b[A\n",
            " 83% 2464/2977 [01:12<00:14, 34.51it/s]\u001b[A\n",
            " 83% 2480/2977 [01:12<00:14, 34.52it/s]\u001b[A\n",
            " 84% 2496/2977 [01:13<00:13, 34.66it/s]\u001b[A\n",
            " 84% 2512/2977 [01:13<00:13, 34.94it/s]\u001b[A\n",
            " 85% 2528/2977 [01:14<00:12, 34.99it/s]\u001b[A\n",
            " 85% 2544/2977 [01:14<00:12, 34.89it/s]\u001b[A\n",
            " 86% 2560/2977 [01:15<00:11, 34.77it/s]\u001b[A\n",
            " 87% 2576/2977 [01:15<00:11, 34.85it/s]\u001b[A\n",
            " 87% 2592/2977 [01:15<00:11, 34.73it/s]\u001b[A\n",
            " 88% 2608/2977 [01:16<00:10, 34.75it/s]\u001b[A\n",
            " 88% 2624/2977 [01:16<00:10, 34.66it/s]\u001b[A\n",
            " 89% 2640/2977 [01:17<00:09, 34.62it/s]\u001b[A\n",
            " 89% 2656/2977 [01:17<00:09, 34.66it/s]\u001b[A\n",
            " 90% 2672/2977 [01:18<00:08, 34.53it/s]\u001b[A\n",
            " 90% 2688/2977 [01:18<00:08, 34.78it/s]\u001b[A\n",
            " 91% 2704/2977 [01:19<00:07, 35.04it/s]\u001b[A\n",
            " 91% 2720/2977 [01:19<00:07, 35.05it/s]\u001b[A\n",
            " 92% 2736/2977 [01:20<00:06, 35.09it/s]\u001b[A\n",
            " 92% 2752/2977 [01:20<00:06, 35.15it/s]\u001b[A\n",
            " 93% 2768/2977 [01:20<00:05, 35.16it/s]\u001b[A\n",
            " 94% 2784/2977 [01:21<00:05, 35.24it/s]\u001b[A\n",
            " 94% 2800/2977 [01:21<00:05, 35.14it/s]\u001b[A\n",
            " 95% 2816/2977 [01:22<00:04, 35.04it/s]\u001b[A\n",
            " 95% 2832/2977 [01:22<00:04, 35.03it/s]\u001b[A\n",
            " 96% 2848/2977 [01:23<00:03, 35.17it/s]\u001b[A\n",
            " 96% 2864/2977 [01:23<00:03, 35.10it/s]\u001b[A\n",
            " 97% 2880/2977 [01:24<00:02, 35.13it/s]\u001b[A\n",
            " 97% 2896/2977 [01:24<00:02, 35.15it/s]\u001b[A\n",
            " 98% 2912/2977 [01:25<00:01, 35.17it/s]\u001b[A\n",
            " 98% 2928/2977 [01:25<00:01, 35.16it/s]\u001b[A\n",
            " 99% 2944/2977 [01:25<00:00, 35.16it/s]\u001b[A\n",
            " 99% 2960/2977 [01:26<00:00, 35.05it/s]\u001b[A\n",
            "100% 2977/2977 [01:27<00:00, 34.08it/s]\n",
            "\n",
            "  0% 0/2929 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 275/2929 [00:00<00:00, 2745.22it/s]\u001b[A\n",
            " 19% 550/2929 [00:00<00:00, 2714.37it/s]\u001b[A\n",
            " 28% 822/2929 [00:00<00:00, 2662.28it/s]\u001b[A\n",
            " 38% 1113/2929 [00:00<00:00, 2756.40it/s]\u001b[A\n",
            " 47% 1389/2929 [00:00<00:00, 2732.16it/s]\u001b[A\n",
            " 57% 1677/2929 [00:00<00:00, 2778.62it/s]\u001b[A\n",
            " 67% 1962/2929 [00:00<00:00, 2798.79it/s]\u001b[A\n",
            " 77% 2242/2929 [00:00<00:00, 2782.33it/s]\u001b[A\n",
            " 86% 2533/2929 [00:00<00:00, 2821.40it/s]\u001b[A\n",
            "100% 2929/2929 [00:01<00:00, 2768.36it/s]\n",
            "validation!!\n",
            "[09.30.21 10:59:27] Visualizing in TensorBoard...\n",
            "[09.30.21 10:59:27] Eval F1: 83.39, EM: 71.01\n",
            "100% 17496/17496 [28:59<00:00, 10.06it/s, NLL=1.26, epoch=0]\n",
            "[09.30.21 11:26:57] Epoch: 1\n",
            "100% 17496/17496 [27:25<00:00, 10.64it/s, NLL=0.463, epoch=1]\n",
            "[09.30.21 11:54:24] Epoch: 2\n",
            "100% 17496/17496 [27:22<00:00, 10.65it/s, NLL=0.558, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjICFokRSc6M",
        "outputId": "b6009c6b-a783-40c1-c4db-1b0286b13339"
      },
      "source": [
        "%cd /content/robustqa\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_complete_13_linear \\\n",
        "    --train-datasets squad03 \\\n",
        "    --eval-datasets race03 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "[09.30.21 12:21:56] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race03\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_complete_13_linear\",\n",
            "    \"save_dir\": \"save/squad_complete_13_linear-03\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad03\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.30.21 12:21:56] Preparing Training Data...\n",
            "100% 17047/17047 [00:00<00:00, 21995.95it/s]\n",
            "Preprocessing not completely accurate for 198/17047 instances\n",
            "[09.30.21 12:22:01] Preparing Validation Data...\n",
            "100% 3377/3377 [00:00<00:00, 22714.53it/s]\n",
            "[09.30.21 12:22:06] Epoch: 0\n",
            "  0% 0/17047 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[09.30.21 12:22:08] Evaluating at step 0...\n",
            "  0% 16/17047 [00:01<26:32, 10.69it/s, NLL=1.01, epoch=0]\n",
            "  0% 0/3377 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/3377 [00:00<00:46, 71.34it/s]\u001b[A\n",
            "  1% 48/3377 [00:00<01:06, 49.71it/s]\u001b[A\n",
            "  2% 64/3377 [00:01<01:16, 43.27it/s]\u001b[A\n",
            "  2% 80/3377 [00:01<01:21, 40.41it/s]\u001b[A\n",
            "  3% 96/3377 [00:02<01:24, 38.80it/s]\u001b[A\n",
            "  3% 112/3377 [00:02<01:26, 37.66it/s]\u001b[A\n",
            "  4% 128/3377 [00:03<01:29, 36.27it/s]\u001b[A\n",
            "  4% 144/3377 [00:03<01:29, 36.02it/s]\u001b[A\n",
            "  5% 160/3377 [00:04<01:29, 35.97it/s]\u001b[A\n",
            "  5% 176/3377 [00:04<01:29, 35.67it/s]\u001b[A\n",
            "  6% 192/3377 [00:04<01:29, 35.60it/s]\u001b[A\n",
            "  6% 208/3377 [00:05<01:29, 35.53it/s]\u001b[A\n",
            "  7% 224/3377 [00:05<01:28, 35.55it/s]\u001b[A\n",
            "  7% 240/3377 [00:06<01:28, 35.53it/s]\u001b[A\n",
            "  8% 256/3377 [00:06<01:28, 35.35it/s]\u001b[A\n",
            "  8% 272/3377 [00:07<01:27, 35.42it/s]\u001b[A\n",
            "  9% 288/3377 [00:07<01:27, 35.32it/s]\u001b[A\n",
            "  9% 304/3377 [00:08<01:27, 35.31it/s]\u001b[A\n",
            "  9% 320/3377 [00:08<01:26, 35.36it/s]\u001b[A\n",
            " 10% 336/3377 [00:09<01:26, 35.30it/s]\u001b[A\n",
            " 10% 352/3377 [00:09<01:25, 35.32it/s]\u001b[A\n",
            " 11% 368/3377 [00:09<01:25, 35.30it/s]\u001b[A\n",
            " 11% 384/3377 [00:10<01:24, 35.27it/s]\u001b[A\n",
            " 12% 400/3377 [00:10<01:24, 35.25it/s]\u001b[A\n",
            " 12% 416/3377 [00:11<01:23, 35.26it/s]\u001b[A\n",
            " 13% 432/3377 [00:11<01:23, 35.16it/s]\u001b[A\n",
            " 13% 448/3377 [00:12<01:23, 35.10it/s]\u001b[A\n",
            "  0% 16/17047 [00:14<26:32, 10.69it/s, NLL=1.01, epoch=0]\n",
            " 14% 480/3377 [00:13<01:22, 35.00it/s]\u001b[A\n",
            " 15% 496/3377 [00:13<01:22, 34.94it/s]\u001b[A\n",
            " 15% 512/3377 [00:14<01:22, 34.80it/s]\u001b[A\n",
            " 16% 528/3377 [00:14<01:21, 34.92it/s]\u001b[A\n",
            " 16% 544/3377 [00:15<01:21, 34.75it/s]\u001b[A\n",
            " 17% 560/3377 [00:15<01:21, 34.53it/s]\u001b[A\n",
            " 17% 576/3377 [00:15<01:21, 34.29it/s]\u001b[A\n",
            " 18% 592/3377 [00:16<01:21, 34.28it/s]\u001b[A\n",
            " 18% 608/3377 [00:16<01:20, 34.31it/s]\u001b[A\n",
            " 18% 624/3377 [00:17<01:20, 34.06it/s]\u001b[A\n",
            " 19% 640/3377 [00:17<01:20, 34.16it/s]\u001b[A\n",
            " 19% 656/3377 [00:18<01:19, 34.15it/s]\u001b[A\n",
            " 20% 672/3377 [00:18<01:18, 34.36it/s]\u001b[A\n",
            " 20% 688/3377 [00:19<01:18, 34.23it/s]\u001b[A\n",
            " 21% 704/3377 [00:19<01:18, 34.18it/s]\u001b[A\n",
            " 21% 720/3377 [00:20<01:17, 34.37it/s]\u001b[A\n",
            " 22% 736/3377 [00:20<01:17, 34.28it/s]\u001b[A\n",
            " 22% 752/3377 [00:21<01:16, 34.27it/s]\u001b[A\n",
            " 23% 768/3377 [00:21<01:16, 34.02it/s]\u001b[A\n",
            " 23% 784/3377 [00:22<01:16, 33.98it/s]\u001b[A\n",
            " 24% 800/3377 [00:22<01:16, 33.82it/s]\u001b[A\n",
            " 24% 816/3377 [00:22<01:15, 33.87it/s]\u001b[A\n",
            " 25% 832/3377 [00:23<01:15, 33.77it/s]\u001b[A\n",
            " 25% 848/3377 [00:23<01:15, 33.34it/s]\u001b[A\n",
            " 26% 864/3377 [00:24<01:15, 33.36it/s]\u001b[A\n",
            " 26% 880/3377 [00:24<01:14, 33.49it/s]\u001b[A\n",
            " 27% 896/3377 [00:25<01:13, 33.65it/s]\u001b[A\n",
            " 27% 912/3377 [00:25<01:13, 33.75it/s]\u001b[A\n",
            " 27% 928/3377 [00:26<01:12, 33.83it/s]\u001b[A\n",
            " 28% 944/3377 [00:26<01:12, 33.54it/s]\u001b[A\n",
            " 28% 960/3377 [00:27<01:12, 33.24it/s]\u001b[A\n",
            " 29% 976/3377 [00:27<01:12, 33.22it/s]\u001b[A\n",
            " 29% 992/3377 [00:28<01:12, 33.08it/s]\u001b[A\n",
            " 30% 1008/3377 [00:28<01:12, 32.88it/s]\u001b[A\n",
            " 30% 1024/3377 [00:29<01:11, 32.89it/s]\u001b[A\n",
            " 31% 1040/3377 [00:29<01:11, 32.85it/s]\u001b[A\n",
            " 31% 1056/3377 [00:30<01:10, 32.79it/s]\u001b[A\n",
            " 32% 1072/3377 [00:30<01:10, 32.59it/s]\u001b[A\n",
            " 32% 1088/3377 [00:31<01:10, 32.41it/s]\u001b[A\n",
            " 33% 1104/3377 [00:31<01:09, 32.49it/s]\u001b[A\n",
            " 33% 1120/3377 [00:32<01:09, 32.64it/s]\u001b[A\n",
            " 34% 1136/3377 [00:32<01:08, 32.60it/s]\u001b[A\n",
            " 34% 1152/3377 [00:33<01:08, 32.61it/s]\u001b[A\n",
            " 35% 1168/3377 [00:33<01:07, 32.82it/s]\u001b[A\n",
            " 35% 1184/3377 [00:34<01:07, 32.65it/s]\u001b[A\n",
            " 36% 1200/3377 [00:34<01:06, 32.73it/s]\u001b[A\n",
            " 36% 1216/3377 [00:35<01:05, 32.75it/s]\u001b[A\n",
            " 36% 1232/3377 [00:35<01:05, 32.97it/s]\u001b[A\n",
            " 37% 1248/3377 [00:36<01:04, 32.98it/s]\u001b[A\n",
            " 37% 1264/3377 [00:36<01:03, 33.06it/s]\u001b[A\n",
            " 38% 1280/3377 [00:37<01:03, 33.22it/s]\u001b[A\n",
            " 38% 1296/3377 [00:37<01:02, 33.13it/s]\u001b[A\n",
            " 39% 1312/3377 [00:38<01:02, 33.01it/s]\u001b[A\n",
            " 39% 1328/3377 [00:38<01:02, 32.86it/s]\u001b[A\n",
            " 40% 1344/3377 [00:39<01:01, 32.85it/s]\u001b[A\n",
            " 40% 1360/3377 [00:39<01:01, 32.90it/s]\u001b[A\n",
            " 41% 1376/3377 [00:39<01:01, 32.78it/s]\u001b[A\n",
            " 41% 1392/3377 [00:40<01:00, 32.66it/s]\u001b[A\n",
            " 42% 1408/3377 [00:40<01:00, 32.80it/s]\u001b[A\n",
            " 42% 1424/3377 [00:41<00:58, 33.10it/s]\u001b[A\n",
            " 43% 1440/3377 [00:41<00:58, 33.05it/s]\u001b[A\n",
            " 43% 1456/3377 [00:42<00:57, 33.40it/s]\u001b[A\n",
            " 44% 1472/3377 [00:42<00:57, 33.11it/s]\u001b[A\n",
            " 44% 1488/3377 [00:43<00:56, 33.30it/s]\u001b[A\n",
            " 45% 1504/3377 [00:43<00:56, 33.33it/s]\u001b[A\n",
            " 45% 1520/3377 [00:44<00:55, 33.28it/s]\u001b[A\n",
            " 45% 1536/3377 [00:44<00:55, 33.26it/s]\u001b[A\n",
            " 46% 1552/3377 [00:45<00:54, 33.18it/s]\u001b[A\n",
            " 46% 1568/3377 [00:45<00:54, 33.27it/s]\u001b[A\n",
            " 47% 1584/3377 [00:46<00:54, 32.92it/s]\u001b[A\n",
            " 47% 1600/3377 [00:46<00:54, 32.83it/s]\u001b[A\n",
            " 48% 1616/3377 [00:47<00:53, 33.11it/s]\u001b[A\n",
            " 48% 1632/3377 [00:47<00:52, 33.16it/s]\u001b[A\n",
            " 49% 1648/3377 [00:48<00:51, 33.32it/s]\u001b[A\n",
            " 49% 1664/3377 [00:48<00:51, 33.40it/s]\u001b[A\n",
            " 50% 1680/3377 [00:49<00:50, 33.45it/s]\u001b[A\n",
            " 50% 1696/3377 [00:49<00:49, 33.67it/s]\u001b[A\n",
            " 51% 1712/3377 [00:50<00:49, 33.56it/s]\u001b[A\n",
            " 51% 1728/3377 [00:50<00:48, 33.68it/s]\u001b[A\n",
            " 52% 1744/3377 [00:51<00:48, 33.72it/s]\u001b[A\n",
            " 52% 1760/3377 [00:51<00:47, 33.88it/s]\u001b[A\n",
            " 53% 1776/3377 [00:51<00:47, 33.77it/s]\u001b[A\n",
            " 53% 1792/3377 [00:52<00:46, 33.92it/s]\u001b[A\n",
            " 54% 1808/3377 [00:52<00:46, 33.89it/s]\u001b[A\n",
            " 54% 1824/3377 [00:53<00:45, 34.05it/s]\u001b[A\n",
            " 54% 1840/3377 [00:53<00:45, 33.81it/s]\u001b[A\n",
            " 55% 1856/3377 [00:54<00:45, 33.63it/s]\u001b[A\n",
            " 55% 1872/3377 [00:54<00:44, 33.84it/s]\u001b[A\n",
            " 56% 1888/3377 [00:55<00:44, 33.75it/s]\u001b[A\n",
            " 56% 1904/3377 [00:55<00:43, 33.73it/s]\u001b[A\n",
            " 57% 1920/3377 [00:56<00:43, 33.83it/s]\u001b[A\n",
            " 57% 1936/3377 [00:56<00:42, 33.96it/s]\u001b[A\n",
            " 58% 1952/3377 [00:57<00:41, 34.07it/s]\u001b[A\n",
            " 58% 1968/3377 [00:57<00:41, 33.82it/s]\u001b[A\n",
            " 59% 1984/3377 [00:58<00:41, 33.93it/s]\u001b[A\n",
            " 59% 2000/3377 [00:58<00:40, 33.98it/s]\u001b[A\n",
            " 60% 2016/3377 [00:59<00:40, 33.99it/s]\u001b[A\n",
            " 60% 2032/3377 [00:59<00:39, 33.95it/s]\u001b[A\n",
            " 61% 2048/3377 [00:59<00:39, 34.03it/s]\u001b[A\n",
            " 61% 2064/3377 [01:00<00:38, 34.19it/s]\u001b[A\n",
            " 62% 2080/3377 [01:00<00:37, 34.28it/s]\u001b[A\n",
            " 62% 2096/3377 [01:01<00:37, 34.32it/s]\u001b[A\n",
            " 63% 2112/3377 [01:01<00:37, 34.13it/s]\u001b[A\n",
            " 63% 2128/3377 [01:02<00:36, 34.20it/s]\u001b[A\n",
            " 63% 2144/3377 [01:02<00:36, 34.02it/s]\u001b[A\n",
            " 64% 2160/3377 [01:03<00:35, 34.18it/s]\u001b[A\n",
            " 64% 2176/3377 [01:03<00:35, 34.29it/s]\u001b[A\n",
            " 65% 2192/3377 [01:04<00:34, 34.38it/s]\u001b[A\n",
            " 65% 2208/3377 [01:04<00:34, 34.35it/s]\u001b[A\n",
            " 66% 2224/3377 [01:05<00:33, 34.40it/s]\u001b[A\n",
            " 66% 2240/3377 [01:05<00:32, 34.52it/s]\u001b[A\n",
            " 67% 2256/3377 [01:06<00:32, 34.49it/s]\u001b[A\n",
            " 67% 2272/3377 [01:06<00:31, 34.71it/s]\u001b[A\n",
            " 68% 2288/3377 [01:06<00:31, 34.68it/s]\u001b[A\n",
            " 68% 2304/3377 [01:07<00:31, 34.52it/s]\u001b[A\n",
            " 69% 2320/3377 [01:07<00:30, 34.50it/s]\u001b[A\n",
            " 69% 2336/3377 [01:08<00:30, 34.63it/s]\u001b[A\n",
            " 70% 2352/3377 [01:08<00:29, 34.77it/s]\u001b[A\n",
            " 70% 2368/3377 [01:09<00:28, 34.85it/s]\u001b[A\n",
            " 71% 2384/3377 [01:09<00:28, 34.83it/s]\u001b[A\n",
            " 71% 2400/3377 [01:10<00:28, 34.63it/s]\u001b[A\n",
            " 72% 2416/3377 [01:10<00:27, 34.58it/s]\u001b[A\n",
            " 72% 2432/3377 [01:11<00:27, 34.77it/s]\u001b[A\n",
            " 72% 2448/3377 [01:11<00:26, 34.69it/s]\u001b[A\n",
            " 73% 2464/3377 [01:12<00:26, 34.78it/s]\u001b[A\n",
            " 73% 2480/3377 [01:12<00:25, 34.84it/s]\u001b[A\n",
            " 74% 2496/3377 [01:12<00:25, 34.76it/s]\u001b[A\n",
            " 74% 2512/3377 [01:13<00:24, 34.98it/s]\u001b[A\n",
            " 75% 2528/3377 [01:13<00:24, 35.10it/s]\u001b[A\n",
            " 75% 2544/3377 [01:14<00:23, 35.10it/s]\u001b[A\n",
            " 76% 2560/3377 [01:14<00:23, 35.10it/s]\u001b[A\n",
            " 76% 2576/3377 [01:15<00:22, 35.07it/s]\u001b[A\n",
            " 77% 2592/3377 [01:15<00:22, 35.22it/s]\u001b[A\n",
            " 77% 2608/3377 [01:16<00:21, 34.99it/s]\u001b[A\n",
            " 78% 2624/3377 [01:16<00:21, 35.04it/s]\u001b[A\n",
            " 78% 2640/3377 [01:17<00:21, 34.93it/s]\u001b[A\n",
            " 79% 2656/3377 [01:17<00:20, 34.51it/s]\u001b[A\n",
            " 79% 2672/3377 [01:17<00:20, 34.55it/s]\u001b[A\n",
            " 80% 2688/3377 [01:18<00:19, 34.73it/s]\u001b[A\n",
            " 80% 2704/3377 [01:18<00:19, 34.84it/s]\u001b[A\n",
            " 81% 2720/3377 [01:19<00:18, 34.86it/s]\u001b[A\n",
            " 81% 2736/3377 [01:19<00:18, 34.84it/s]\u001b[A\n",
            " 81% 2752/3377 [01:20<00:17, 35.01it/s]\u001b[A\n",
            " 82% 2768/3377 [01:20<00:17, 35.02it/s]\u001b[A\n",
            " 82% 2784/3377 [01:21<00:16, 34.93it/s]\u001b[A\n",
            " 83% 2800/3377 [01:21<00:16, 34.67it/s]\u001b[A\n",
            " 83% 2816/3377 [01:22<00:16, 34.67it/s]\u001b[A\n",
            " 84% 2832/3377 [01:22<00:15, 34.90it/s]\u001b[A\n",
            " 84% 2848/3377 [01:23<00:15, 34.96it/s]\u001b[A\n",
            " 85% 2864/3377 [01:23<00:14, 34.93it/s]\u001b[A\n",
            " 85% 2880/3377 [01:23<00:14, 35.04it/s]\u001b[A\n",
            " 86% 2896/3377 [01:24<00:13, 34.91it/s]\u001b[A\n",
            " 86% 2912/3377 [01:24<00:13, 34.77it/s]\u001b[A\n",
            " 87% 2928/3377 [01:25<00:12, 34.87it/s]\u001b[A\n",
            " 87% 2944/3377 [01:25<00:12, 35.07it/s]\u001b[A\n",
            " 88% 2960/3377 [01:26<00:11, 35.01it/s]\u001b[A\n",
            " 88% 2976/3377 [01:26<00:11, 35.06it/s]\u001b[A\n",
            " 89% 2992/3377 [01:27<00:11, 34.98it/s]\u001b[A\n",
            " 89% 3008/3377 [01:27<00:10, 35.04it/s]\u001b[A\n",
            " 90% 3024/3377 [01:28<00:10, 35.09it/s]\u001b[A\n",
            " 90% 3040/3377 [01:28<00:09, 35.12it/s]\u001b[A\n",
            " 90% 3056/3377 [01:28<00:09, 35.05it/s]\u001b[A\n",
            " 91% 3072/3377 [01:29<00:08, 35.09it/s]\u001b[A\n",
            " 91% 3088/3377 [01:29<00:08, 34.88it/s]\u001b[A\n",
            " 92% 3104/3377 [01:30<00:07, 34.88it/s]\u001b[A\n",
            " 92% 3120/3377 [01:30<00:07, 34.99it/s]\u001b[A\n",
            " 93% 3136/3377 [01:31<00:06, 34.88it/s]\u001b[A\n",
            " 93% 3152/3377 [01:31<00:06, 34.96it/s]\u001b[A\n",
            " 94% 3168/3377 [01:32<00:05, 35.03it/s]\u001b[A\n",
            " 94% 3184/3377 [01:32<00:05, 34.97it/s]\u001b[A\n",
            " 95% 3200/3377 [01:33<00:05, 34.99it/s]\u001b[A\n",
            " 95% 3216/3377 [01:33<00:04, 34.86it/s]\u001b[A\n",
            " 96% 3232/3377 [01:34<00:04, 34.75it/s]\u001b[A\n",
            " 96% 3248/3377 [01:34<00:03, 34.73it/s]\u001b[A\n",
            " 97% 3264/3377 [01:34<00:03, 34.81it/s]\u001b[A\n",
            " 97% 3280/3377 [01:35<00:02, 34.74it/s]\u001b[A\n",
            " 98% 3296/3377 [01:35<00:02, 34.44it/s]\u001b[A\n",
            " 98% 3312/3377 [01:36<00:01, 34.71it/s]\u001b[A\n",
            " 99% 3328/3377 [01:36<00:01, 34.85it/s]\u001b[A\n",
            " 99% 3344/3377 [01:37<00:00, 34.96it/s]\u001b[A\n",
            " 99% 3360/3377 [01:37<00:00, 35.01it/s]\u001b[A\n",
            "100% 3377/3377 [01:38<00:00, 34.25it/s]\n",
            "\n",
            "  0% 0/3335 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 247/3335 [00:00<00:01, 2462.50it/s]\u001b[A\n",
            " 15% 502/3335 [00:00<00:01, 2508.72it/s]\u001b[A\n",
            " 23% 765/3335 [00:00<00:01, 2562.71it/s]\u001b[A\n",
            " 31% 1022/3335 [00:00<00:00, 2551.50it/s]\u001b[A\n",
            " 38% 1278/3335 [00:00<00:00, 2508.48it/s]\u001b[A\n",
            " 46% 1536/3335 [00:00<00:00, 2530.38it/s]\u001b[A\n",
            " 54% 1790/3335 [00:00<00:00, 2528.65it/s]\u001b[A\n",
            " 61% 2043/3335 [00:00<00:00, 2524.00it/s]\u001b[A\n",
            " 69% 2307/3335 [00:00<00:00, 2558.30it/s]\u001b[A\n",
            " 77% 2563/3335 [00:01<00:00, 2556.02it/s]\u001b[A\n",
            " 85% 2820/3335 [00:01<00:00, 2557.90it/s]\u001b[A\n",
            "100% 3335/3335 [00:01<00:00, 2555.89it/s]\n",
            "validation!!\n",
            "[09.30.21 12:23:48] Visualizing in TensorBoard...\n",
            "[09.30.21 12:23:48] Eval F1: 82.60, EM: 68.91\n",
            "100% 17047/17047 [28:25<00:00,  9.99it/s, NLL=0.481, epoch=0]\n",
            "[09.30.21 12:50:33] Epoch: 1\n",
            "100% 17047/17047 [26:41<00:00, 10.64it/s, NLL=0.193, epoch=1]\n",
            "[09.30.21 13:17:16] Epoch: 2\n",
            "100% 17047/17047 [26:42<00:00, 10.64it/s, NLL=0.115, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kVcH52MAfmP"
      },
      "source": [
        "!cp -r /content/robustqa /content/drive/MyDrive/robustqa_change_transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxlkdDrDNSEV",
        "outputId": "536bc551-85aa-4382-bd7a-b2fafe13fd24"
      },
      "source": [
        "%%writefile /content/robustqa/robustqa_original/robustqa/args.py\n",
        "import argparse\n",
        "\n",
        "def get_train_test_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--saved', type=str, default='NO')\n",
        "    parser.add_argument('--batch-size', type=int, default=16)\n",
        "    parser.add_argument('--num-epochs', type=int, default=3)\n",
        "    parser.add_argument('--lr', type=float, default=3e-5)\n",
        "    parser.add_argument('--num-visuals', type=int, default=10)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--save-dir', type=str, default='save/')\n",
        "    parser.add_argument('--train', action='store_true')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--train-datasets', type=str, default='squad01')#,nat_questions,newsqa')\n",
        "    parser.add_argument('--run-name', type=str, default='multitask_distilbert')\n",
        "    parser.add_argument('--recompute-features', action='store_true')\n",
        "    parser.add_argument('--train-dir', type=str, default='./datasets/indomain_train')\n",
        "    parser.add_argument('--val-dir', type=str, default='./datasets/indomain_val')\n",
        "    parser.add_argument('--eval-dir', type=str, default='./datasets/oodomain_test')\n",
        "    parser.add_argument('--eval-datasets', type=str, default='race01')#,relation_extraction,duorc')\n",
        "    parser.add_argument('--do-train', action='store_true')\n",
        "    parser.add_argument('--do-eval', action='store_true')\n",
        "    parser.add_argument('--sub-file', type=str, default='')\n",
        "    parser.add_argument('--visualize-predictions', action='store_true')\n",
        "    parser.add_argument('--eval-every', type=int, default=1000)\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/robustqa/robustqa_original/robustqa/args.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrW3AHFCuSv0"
      },
      "source": [
        "# rm -r /content/robustqa/robustqa_original/robustqa/save/squad_complete_original2-01/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-D3GccwMeTN",
        "outputId": "63f90409-2187-4141-e542-4b0fa4a533fa"
      },
      "source": [
        "# Original\n",
        "%cd /content/robustqa/robustqa_original/robustqa\n",
        "!python3 train.py \\\n",
        "     --do-train \\\n",
        "     --recompute-features \\\n",
        "     --run-name squad_complete_original2 \\\n",
        "     --train-datasets squad01 \\\n",
        "     --eval-datasets race01 \\\n",
        "     --eval-every 500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa/robustqa_original/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[09.25.21 02:13:58] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"./datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_complete_original2\",\n",
            "    \"save_dir\": \"save/squad_complete_original2-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"./datasets/indomain_train\",\n",
            "    \"val_dir\": \"./datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.25.21 02:13:58] Preparing Training Data...\n",
            "100% 16020/16020 [00:00<00:00, 16932.72it/s]\n",
            "Preprocessing not completely accurate for 150/16020 instances\n",
            "[09.25.21 02:14:04] Preparing Validation Data...\n",
            "100% 4436/4436 [00:00<00:00, 16967.10it/s]\n",
            "[09.25.21 02:14:13] Epoch: 0\n",
            "[09.25.21 02:14:14] Evaluating at step 0...\n",
            "  0% 16/16020 [00:00<12:29, 21.35it/s, NLL=5.97, epoch=0]\n",
            "  0% 0/4436 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4436 [00:00<00:30, 142.75it/s]\u001b[A\n",
            "  1% 48/4436 [00:00<00:41, 104.99it/s]\u001b[A\n",
            "  1% 64/4436 [00:00<00:47, 92.35it/s] \u001b[A\n",
            "  2% 80/4436 [00:00<00:50, 86.08it/s]\u001b[A\n",
            "  2% 96/4436 [00:01<00:52, 82.68it/s]\u001b[A\n",
            "  3% 112/4436 [00:01<00:53, 80.55it/s]\u001b[A\n",
            "  3% 128/4436 [00:01<00:54, 79.29it/s]\u001b[A\n",
            "  3% 144/4436 [00:01<00:54, 78.42it/s]\u001b[A\n",
            "  4% 160/4436 [00:01<00:54, 77.84it/s]\u001b[A\n",
            "  4% 176/4436 [00:02<00:55, 77.31it/s]\u001b[A\n",
            "  4% 192/4436 [00:02<00:55, 77.12it/s]\u001b[A\n",
            "  5% 208/4436 [00:02<00:54, 76.93it/s]\u001b[A\n",
            "  5% 224/4436 [00:02<00:54, 76.78it/s]\u001b[A\n",
            "  5% 240/4436 [00:02<00:54, 76.89it/s]\u001b[A\n",
            "  6% 256/4436 [00:03<00:54, 76.89it/s]\u001b[A\n",
            "  6% 272/4436 [00:03<00:54, 76.94it/s]\u001b[A\n",
            "  6% 288/4436 [00:03<00:53, 76.87it/s]\u001b[A\n",
            "  7% 304/4436 [00:03<00:53, 76.84it/s]\u001b[A\n",
            "  7% 320/4436 [00:03<00:53, 76.90it/s]\u001b[A\n",
            "  8% 336/4436 [00:04<00:53, 76.89it/s]\u001b[A\n",
            "  8% 352/4436 [00:04<00:53, 76.65it/s]\u001b[A\n",
            "  8% 368/4436 [00:04<00:53, 76.54it/s]\u001b[A\n",
            "  9% 384/4436 [00:04<00:52, 76.72it/s]\u001b[A\n",
            "  9% 400/4436 [00:05<00:52, 76.61it/s]\u001b[A\n",
            "  9% 416/4436 [00:05<00:52, 76.61it/s]\u001b[A\n",
            " 10% 432/4436 [00:05<00:52, 76.75it/s]\u001b[A\n",
            " 10% 448/4436 [00:05<00:52, 76.64it/s]\u001b[A\n",
            " 10% 464/4436 [00:05<00:51, 76.57it/s]\u001b[A\n",
            " 11% 480/4436 [00:06<00:51, 76.46it/s]\u001b[A\n",
            " 11% 496/4436 [00:06<00:51, 76.56it/s]\u001b[A\n",
            " 12% 512/4436 [00:06<00:51, 76.60it/s]\u001b[A\n",
            " 12% 528/4436 [00:06<00:51, 76.46it/s]\u001b[A\n",
            " 12% 544/4436 [00:06<00:50, 76.54it/s]\u001b[A\n",
            " 13% 560/4436 [00:07<00:50, 76.43it/s]\u001b[A\n",
            " 13% 576/4436 [00:07<00:50, 76.61it/s]\u001b[A\n",
            " 13% 592/4436 [00:07<00:50, 76.51it/s]\u001b[A\n",
            " 14% 608/4436 [00:07<00:50, 76.42it/s]\u001b[A\n",
            " 14% 624/4436 [00:07<00:49, 76.53it/s]\u001b[A\n",
            " 14% 640/4436 [00:08<00:49, 76.72it/s]\u001b[A\n",
            " 15% 656/4436 [00:08<00:49, 76.74it/s]\u001b[A\n",
            " 15% 672/4436 [00:08<00:49, 76.67it/s]\u001b[A\n",
            " 16% 688/4436 [00:08<00:48, 76.66it/s]\u001b[A\n",
            " 16% 704/4436 [00:08<00:48, 76.64it/s]\u001b[A\n",
            " 16% 720/4436 [00:09<00:48, 76.59it/s]\u001b[A\n",
            " 17% 736/4436 [00:09<00:48, 76.47it/s]\u001b[A\n",
            " 17% 752/4436 [00:09<00:48, 76.57it/s]\u001b[A\n",
            " 17% 768/4436 [00:09<00:47, 76.66it/s]\u001b[A\n",
            " 18% 784/4436 [00:10<00:47, 76.60it/s]\u001b[A\n",
            " 18% 800/4436 [00:10<00:47, 76.64it/s]\u001b[A\n",
            " 18% 816/4436 [00:10<00:47, 76.78it/s]\u001b[A\n",
            " 19% 832/4436 [00:10<00:46, 76.79it/s]\u001b[A\n",
            " 19% 848/4436 [00:10<00:46, 76.72it/s]\u001b[A\n",
            " 19% 864/4436 [00:11<00:46, 76.89it/s]\u001b[A\n",
            " 20% 880/4436 [00:11<00:46, 76.80it/s]\u001b[A\n",
            " 20% 896/4436 [00:11<00:46, 76.87it/s]\u001b[A\n",
            " 21% 912/4436 [00:11<00:45, 76.78it/s]\u001b[A\n",
            " 21% 928/4436 [00:11<00:45, 76.38it/s]\u001b[A\n",
            " 21% 944/4436 [00:12<00:45, 76.72it/s]\u001b[A\n",
            " 22% 960/4436 [00:12<00:45, 76.75it/s]\u001b[A\n",
            " 22% 976/4436 [00:12<00:45, 76.71it/s]\u001b[A\n",
            " 22% 992/4436 [00:12<00:44, 76.69it/s]\u001b[A\n",
            " 23% 1008/4436 [00:12<00:44, 76.47it/s]\u001b[A\n",
            " 23% 1024/4436 [00:13<00:44, 76.70it/s]\u001b[A\n",
            " 23% 1040/4436 [00:13<00:44, 76.76it/s]\u001b[A\n",
            " 24% 1056/4436 [00:13<00:44, 76.69it/s]\u001b[A\n",
            " 24% 1072/4436 [00:13<00:43, 76.66it/s]\u001b[A\n",
            " 25% 1088/4436 [00:14<00:43, 76.75it/s]\u001b[A\n",
            " 25% 1104/4436 [00:14<00:43, 76.76it/s]\u001b[A\n",
            " 25% 1120/4436 [00:14<00:43, 76.85it/s]\u001b[A\n",
            " 26% 1136/4436 [00:14<00:42, 76.90it/s]\u001b[A\n",
            " 26% 1152/4436 [00:14<00:42, 76.81it/s]\u001b[A\n",
            " 26% 1168/4436 [00:15<00:42, 76.64it/s]\u001b[A\n",
            " 27% 1184/4436 [00:15<00:42, 76.64it/s]\u001b[A\n",
            " 27% 1200/4436 [00:15<00:42, 76.54it/s]\u001b[A\n",
            " 27% 1216/4436 [00:15<00:42, 76.59it/s]\u001b[A\n",
            " 28% 1232/4436 [00:15<00:41, 76.35it/s]\u001b[A\n",
            " 28% 1248/4436 [00:16<00:41, 76.65it/s]\u001b[A\n",
            " 28% 1264/4436 [00:16<00:41, 76.57it/s]\u001b[A\n",
            " 29% 1280/4436 [00:16<00:41, 76.66it/s]\u001b[A\n",
            " 29% 1296/4436 [00:16<00:40, 76.61it/s]\u001b[A\n",
            " 30% 1312/4436 [00:16<00:40, 76.32it/s]\u001b[A\n",
            " 30% 1328/4436 [00:17<00:40, 76.67it/s]\u001b[A\n",
            " 30% 1344/4436 [00:17<00:40, 76.63it/s]\u001b[A\n",
            " 31% 1360/4436 [00:17<00:40, 76.65it/s]\u001b[A\n",
            " 31% 1376/4436 [00:17<00:39, 76.74it/s]\u001b[A\n",
            " 31% 1392/4436 [00:17<00:39, 76.77it/s]\u001b[A\n",
            " 32% 1408/4436 [00:18<00:39, 76.62it/s]\u001b[A\n",
            " 32% 1424/4436 [00:18<00:39, 76.43it/s]\u001b[A\n",
            " 32% 1440/4436 [00:18<00:39, 76.69it/s]\u001b[A\n",
            " 33% 1456/4436 [00:18<00:38, 76.61it/s]\u001b[A\n",
            " 33% 1472/4436 [00:19<00:38, 76.64it/s]\u001b[A\n",
            "  0% 16/16020 [00:20<12:29, 21.35it/s, NLL=5.97, epoch=0]\n",
            " 34% 1504/4436 [00:19<00:38, 76.47it/s]\u001b[A\n",
            " 34% 1520/4436 [00:19<00:38, 76.60it/s]\u001b[A\n",
            " 35% 1536/4436 [00:19<00:37, 76.63it/s]\u001b[A\n",
            " 35% 1552/4436 [00:20<00:37, 76.55it/s]\u001b[A\n",
            " 35% 1568/4436 [00:20<00:37, 76.63it/s]\u001b[A\n",
            " 36% 1584/4436 [00:20<00:37, 76.58it/s]\u001b[A\n",
            " 36% 1600/4436 [00:20<00:37, 76.61it/s]\u001b[A\n",
            " 36% 1616/4436 [00:20<00:36, 76.71it/s]\u001b[A\n",
            " 37% 1632/4436 [00:21<00:36, 76.60it/s]\u001b[A\n",
            " 37% 1648/4436 [00:21<00:36, 76.62it/s]\u001b[A\n",
            " 38% 1664/4436 [00:21<00:36, 76.58it/s]\u001b[A\n",
            " 38% 1680/4436 [00:21<00:36, 76.25it/s]\u001b[A\n",
            " 38% 1696/4436 [00:21<00:35, 76.58it/s]\u001b[A\n",
            " 39% 1712/4436 [00:22<00:35, 76.56it/s]\u001b[A\n",
            " 39% 1728/4436 [00:22<00:35, 76.50it/s]\u001b[A\n",
            " 39% 1744/4436 [00:22<00:35, 76.55it/s]\u001b[A\n",
            " 40% 1760/4436 [00:22<00:34, 76.63it/s]\u001b[A\n",
            " 40% 1776/4436 [00:22<00:34, 76.31it/s]\u001b[A\n",
            " 40% 1792/4436 [00:23<00:34, 76.73it/s]\u001b[A\n",
            " 41% 1808/4436 [00:23<00:34, 76.83it/s]\u001b[A\n",
            " 41% 1824/4436 [00:23<00:34, 76.72it/s]\u001b[A\n",
            " 41% 1840/4436 [00:23<00:33, 76.82it/s]\u001b[A\n",
            " 42% 1856/4436 [00:24<00:33, 76.53it/s]\u001b[A\n",
            " 42% 1872/4436 [00:24<00:33, 76.48it/s]\u001b[A\n",
            " 43% 1888/4436 [00:24<00:33, 76.71it/s]\u001b[A\n",
            " 43% 1904/4436 [00:24<00:33, 76.64it/s]\u001b[A\n",
            " 43% 1920/4436 [00:24<00:32, 76.68it/s]\u001b[A\n",
            " 44% 1936/4436 [00:25<00:32, 76.52it/s]\u001b[A\n",
            " 44% 1952/4436 [00:25<00:32, 76.50it/s]\u001b[A\n",
            " 44% 1968/4436 [00:25<00:32, 76.52it/s]\u001b[A\n",
            " 45% 1984/4436 [00:25<00:32, 76.58it/s]\u001b[A\n",
            " 45% 2000/4436 [00:25<00:31, 76.65it/s]\u001b[A\n",
            " 45% 2016/4436 [00:26<00:31, 76.59it/s]\u001b[A\n",
            " 46% 2032/4436 [00:26<00:31, 76.68it/s]\u001b[A\n",
            " 46% 2048/4436 [00:26<00:31, 76.65it/s]\u001b[A\n",
            " 47% 2064/4436 [00:26<00:30, 76.53it/s]\u001b[A\n",
            " 47% 2080/4436 [00:26<00:30, 76.48it/s]\u001b[A\n",
            " 47% 2096/4436 [00:27<00:30, 76.66it/s]\u001b[A\n",
            " 48% 2112/4436 [00:27<00:30, 76.71it/s]\u001b[A\n",
            " 48% 2128/4436 [00:27<00:30, 76.62it/s]\u001b[A\n",
            " 48% 2144/4436 [00:27<00:29, 76.52it/s]\u001b[A\n",
            " 49% 2160/4436 [00:27<00:29, 76.66it/s]\u001b[A\n",
            " 49% 2176/4436 [00:28<00:29, 76.72it/s]\u001b[A\n",
            " 49% 2192/4436 [00:28<00:29, 76.73it/s]\u001b[A\n",
            " 50% 2208/4436 [00:28<00:29, 76.68it/s]\u001b[A\n",
            " 50% 2224/4436 [00:28<00:28, 76.55it/s]\u001b[A\n",
            " 50% 2240/4436 [00:29<00:28, 76.38it/s]\u001b[A\n",
            " 51% 2256/4436 [00:29<00:28, 76.60it/s]\u001b[A\n",
            " 51% 2272/4436 [00:29<00:28, 76.50it/s]\u001b[A\n",
            " 52% 2288/4436 [00:29<00:28, 76.58it/s]\u001b[A\n",
            " 52% 2304/4436 [00:29<00:27, 76.57it/s]\u001b[A\n",
            " 52% 2320/4436 [00:30<00:27, 76.52it/s]\u001b[A\n",
            " 53% 2336/4436 [00:30<00:27, 76.62it/s]\u001b[A\n",
            " 53% 2352/4436 [00:30<00:27, 76.47it/s]\u001b[A\n",
            " 53% 2368/4436 [00:30<00:26, 76.64it/s]\u001b[A\n",
            " 54% 2384/4436 [00:30<00:26, 76.63it/s]\u001b[A\n",
            " 54% 2400/4436 [00:31<00:26, 76.75it/s]\u001b[A\n",
            " 54% 2416/4436 [00:31<00:26, 76.58it/s]\u001b[A\n",
            " 55% 2432/4436 [00:31<00:26, 76.71it/s]\u001b[A\n",
            " 55% 2448/4436 [00:31<00:25, 76.70it/s]\u001b[A\n",
            " 56% 2464/4436 [00:31<00:25, 76.65it/s]\u001b[A\n",
            " 56% 2480/4436 [00:32<00:25, 76.60it/s]\u001b[A\n",
            " 56% 2496/4436 [00:32<00:25, 76.63it/s]\u001b[A\n",
            " 57% 2512/4436 [00:32<00:25, 76.71it/s]\u001b[A\n",
            " 57% 2528/4436 [00:32<00:24, 76.63it/s]\u001b[A\n",
            " 57% 2544/4436 [00:33<00:24, 76.48it/s]\u001b[A\n",
            " 58% 2560/4436 [00:33<00:24, 76.84it/s]\u001b[A\n",
            " 58% 2576/4436 [00:33<00:24, 76.77it/s]\u001b[A\n",
            " 58% 2592/4436 [00:33<00:24, 76.82it/s]\u001b[A\n",
            " 59% 2608/4436 [00:33<00:23, 76.81it/s]\u001b[A\n",
            " 59% 2624/4436 [00:34<00:23, 76.92it/s]\u001b[A\n",
            " 60% 2640/4436 [00:34<00:23, 76.54it/s]\u001b[A\n",
            " 60% 2656/4436 [00:34<00:23, 76.86it/s]\u001b[A\n",
            " 60% 2672/4436 [00:34<00:23, 76.63it/s]\u001b[A\n",
            " 61% 2688/4436 [00:34<00:22, 76.75it/s]\u001b[A\n",
            " 61% 2704/4436 [00:35<00:22, 76.49it/s]\u001b[A\n",
            " 61% 2720/4436 [00:35<00:22, 76.63it/s]\u001b[A\n",
            " 62% 2736/4436 [00:35<00:22, 76.52it/s]\u001b[A\n",
            " 62% 2752/4436 [00:35<00:22, 76.39it/s]\u001b[A\n",
            " 62% 2768/4436 [00:35<00:21, 76.70it/s]\u001b[A\n",
            " 63% 2784/4436 [00:36<00:21, 76.56it/s]\u001b[A\n",
            " 63% 2800/4436 [00:36<00:21, 76.77it/s]\u001b[A\n",
            " 63% 2816/4436 [00:36<00:21, 76.60it/s]\u001b[A\n",
            " 64% 2832/4436 [00:36<00:20, 76.63it/s]\u001b[A\n",
            " 64% 2848/4436 [00:36<00:20, 76.53it/s]\u001b[A\n",
            " 65% 2864/4436 [00:37<00:20, 76.40it/s]\u001b[A\n",
            " 65% 2880/4436 [00:37<00:20, 76.71it/s]\u001b[A\n",
            " 65% 2896/4436 [00:37<00:20, 76.76it/s]\u001b[A\n",
            " 66% 2912/4436 [00:37<00:19, 76.72it/s]\u001b[A\n",
            " 66% 2928/4436 [00:38<00:19, 76.43it/s]\u001b[A\n",
            " 66% 2944/4436 [00:38<00:19, 76.52it/s]\u001b[A\n",
            " 67% 2960/4436 [00:38<00:19, 76.76it/s]\u001b[A\n",
            " 67% 2976/4436 [00:38<00:19, 76.67it/s]\u001b[A\n",
            " 67% 2992/4436 [00:38<00:18, 76.49it/s]\u001b[A\n",
            " 68% 3008/4436 [00:39<00:18, 76.33it/s]\u001b[A\n",
            " 68% 3024/4436 [00:39<00:18, 76.78it/s]\u001b[A\n",
            " 69% 3040/4436 [00:39<00:18, 76.79it/s]\u001b[A\n",
            " 69% 3056/4436 [00:39<00:17, 76.70it/s]\u001b[A\n",
            " 69% 3072/4436 [00:39<00:17, 76.69it/s]\u001b[A\n",
            " 70% 3088/4436 [00:40<00:17, 76.64it/s]\u001b[A\n",
            " 70% 3104/4436 [00:40<00:17, 76.73it/s]\u001b[A\n",
            " 70% 3120/4436 [00:40<00:17, 76.87it/s]\u001b[A\n",
            " 71% 3136/4436 [00:40<00:16, 76.82it/s]\u001b[A\n",
            " 71% 3152/4436 [00:40<00:16, 76.73it/s]\u001b[A\n",
            " 71% 3168/4436 [00:41<00:16, 76.48it/s]\u001b[A\n",
            " 72% 3184/4436 [00:41<00:16, 76.73it/s]\u001b[A\n",
            " 72% 3200/4436 [00:41<00:16, 76.63it/s]\u001b[A\n",
            " 72% 3216/4436 [00:41<00:15, 76.67it/s]\u001b[A\n",
            " 73% 3232/4436 [00:41<00:15, 76.73it/s]\u001b[A\n",
            " 73% 3248/4436 [00:42<00:15, 76.62it/s]\u001b[A\n",
            " 74% 3264/4436 [00:42<00:15, 76.82it/s]\u001b[A\n",
            " 74% 3280/4436 [00:42<00:15, 76.71it/s]\u001b[A\n",
            " 74% 3296/4436 [00:42<00:14, 76.53it/s]\u001b[A\n",
            " 75% 3312/4436 [00:43<00:14, 76.66it/s]\u001b[A\n",
            " 75% 3328/4436 [00:43<00:14, 76.73it/s]\u001b[A\n",
            " 75% 3344/4436 [00:43<00:14, 76.80it/s]\u001b[A\n",
            " 76% 3360/4436 [00:43<00:14, 76.81it/s]\u001b[A\n",
            " 76% 3376/4436 [00:43<00:13, 76.73it/s]\u001b[A\n",
            " 76% 3392/4436 [00:44<00:13, 76.51it/s]\u001b[A\n",
            " 77% 3408/4436 [00:44<00:13, 76.39it/s]\u001b[A\n",
            " 77% 3424/4436 [00:44<00:13, 76.44it/s]\u001b[A\n",
            " 78% 3440/4436 [00:44<00:13, 76.59it/s]\u001b[A\n",
            " 78% 3456/4436 [00:44<00:12, 76.64it/s]\u001b[A\n",
            " 78% 3472/4436 [00:45<00:12, 76.70it/s]\u001b[A\n",
            " 79% 3488/4436 [00:45<00:12, 76.72it/s]\u001b[A\n",
            " 79% 3504/4436 [00:45<00:12, 76.65it/s]\u001b[A\n",
            " 79% 3520/4436 [00:45<00:11, 76.68it/s]\u001b[A\n",
            " 80% 3536/4436 [00:45<00:11, 76.69it/s]\u001b[A\n",
            " 80% 3552/4436 [00:46<00:11, 76.67it/s]\u001b[A\n",
            " 80% 3568/4436 [00:46<00:11, 76.68it/s]\u001b[A\n",
            " 81% 3584/4436 [00:46<00:11, 76.87it/s]\u001b[A\n",
            " 81% 3600/4436 [00:46<00:10, 76.85it/s]\u001b[A\n",
            " 82% 3616/4436 [00:46<00:10, 76.71it/s]\u001b[A\n",
            " 82% 3632/4436 [00:47<00:10, 76.70it/s]\u001b[A\n",
            " 82% 3648/4436 [00:47<00:10, 76.73it/s]\u001b[A\n",
            " 83% 3664/4436 [00:47<00:10, 76.55it/s]\u001b[A\n",
            " 83% 3680/4436 [00:47<00:09, 76.58it/s]\u001b[A\n",
            " 83% 3696/4436 [00:48<00:09, 76.69it/s]\u001b[A\n",
            " 84% 3712/4436 [00:48<00:09, 76.76it/s]\u001b[A\n",
            " 84% 3728/4436 [00:48<00:09, 76.78it/s]\u001b[A\n",
            " 84% 3744/4436 [00:48<00:09, 76.71it/s]\u001b[A\n",
            " 85% 3760/4436 [00:48<00:08, 76.79it/s]\u001b[A\n",
            " 85% 3776/4436 [00:49<00:08, 76.63it/s]\u001b[A\n",
            " 85% 3792/4436 [00:49<00:08, 76.69it/s]\u001b[A\n",
            " 86% 3808/4436 [00:49<00:08, 76.68it/s]\u001b[A\n",
            " 86% 3824/4436 [00:49<00:07, 76.69it/s]\u001b[A\n",
            " 87% 3840/4436 [00:49<00:07, 76.73it/s]\u001b[A\n",
            " 87% 3856/4436 [00:50<00:07, 76.67it/s]\u001b[A\n",
            " 87% 3872/4436 [00:50<00:07, 76.78it/s]\u001b[A\n",
            " 88% 3888/4436 [00:50<00:07, 76.76it/s]\u001b[A\n",
            " 88% 3904/4436 [00:50<00:06, 76.65it/s]\u001b[A\n",
            " 88% 3920/4436 [00:50<00:06, 76.43it/s]\u001b[A\n",
            " 89% 3936/4436 [00:51<00:06, 76.47it/s]\u001b[A\n",
            " 89% 3952/4436 [00:51<00:06, 76.66it/s]\u001b[A\n",
            " 89% 3968/4436 [00:51<00:06, 76.63it/s]\u001b[A\n",
            " 90% 3984/4436 [00:51<00:05, 76.68it/s]\u001b[A\n",
            " 90% 4000/4436 [00:52<00:05, 76.60it/s]\u001b[A\n",
            " 91% 4016/4436 [00:52<00:05, 76.33it/s]\u001b[A\n",
            " 91% 4032/4436 [00:52<00:05, 76.65it/s]\u001b[A\n",
            " 91% 4048/4436 [00:52<00:05, 76.83it/s]\u001b[A\n",
            " 92% 4064/4436 [00:52<00:04, 76.73it/s]\u001b[A\n",
            " 92% 4080/4436 [00:53<00:04, 76.73it/s]\u001b[A\n",
            " 92% 4096/4436 [00:53<00:04, 76.61it/s]\u001b[A\n",
            " 93% 4112/4436 [00:53<00:04, 76.56it/s]\u001b[A\n",
            " 93% 4128/4436 [00:53<00:04, 76.44it/s]\u001b[A\n",
            " 93% 4144/4436 [00:53<00:03, 76.73it/s]\u001b[A\n",
            " 94% 4160/4436 [00:54<00:03, 76.81it/s]\u001b[A\n",
            " 94% 4176/4436 [00:54<00:03, 76.71it/s]\u001b[A\n",
            " 94% 4192/4436 [00:54<00:03, 76.73it/s]\u001b[A\n",
            " 95% 4208/4436 [00:54<00:02, 76.79it/s]\u001b[A\n",
            " 95% 4224/4436 [00:54<00:02, 76.87it/s]\u001b[A\n",
            " 96% 4240/4436 [00:55<00:02, 76.98it/s]\u001b[A\n",
            " 96% 4256/4436 [00:55<00:02, 76.83it/s]\u001b[A\n",
            " 96% 4272/4436 [00:55<00:02, 76.90it/s]\u001b[A\n",
            " 97% 4288/4436 [00:55<00:01, 76.88it/s]\u001b[A\n",
            " 97% 4304/4436 [00:55<00:01, 76.82it/s]\u001b[A\n",
            " 97% 4320/4436 [00:56<00:01, 76.94it/s]\u001b[A\n",
            " 98% 4336/4436 [00:56<00:01, 76.97it/s]\u001b[A\n",
            " 98% 4352/4436 [00:56<00:01, 76.93it/s]\u001b[A\n",
            " 98% 4368/4436 [00:56<00:00, 76.74it/s]\u001b[A\n",
            " 99% 4384/4436 [00:56<00:00, 77.02it/s]\u001b[A\n",
            " 99% 4400/4436 [00:57<00:00, 76.88it/s]\u001b[A\n",
            "100% 4416/4436 [00:57<00:00, 76.83it/s]\u001b[A\n",
            "100% 4436/4436 [00:57<00:00, 76.70it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 227/4306 [00:00<00:01, 2265.21it/s]\u001b[A\n",
            " 11% 459/4306 [00:00<00:01, 2294.40it/s]\u001b[A\n",
            " 16% 708/4306 [00:00<00:01, 2379.80it/s]\u001b[A\n",
            " 22% 964/4306 [00:00<00:01, 2450.35it/s]\u001b[A\n",
            " 28% 1214/4306 [00:00<00:01, 2464.87it/s]\u001b[A\n",
            " 34% 1461/4306 [00:00<00:01, 2332.42it/s]\u001b[A\n",
            " 39% 1696/4306 [00:00<00:01, 2310.13it/s]\u001b[A\n",
            " 45% 1948/4306 [00:00<00:00, 2372.26it/s]\u001b[A\n",
            " 51% 2186/4306 [00:00<00:00, 2327.89it/s]\u001b[A\n",
            " 56% 2420/4306 [00:01<00:00, 2306.41it/s]\u001b[A\n",
            " 62% 2661/4306 [00:01<00:00, 2336.76it/s]\u001b[A\n",
            " 68% 2907/4306 [00:01<00:00, 2372.23it/s]\u001b[A\n",
            " 73% 3154/4306 [00:01<00:00, 2400.93it/s]\u001b[A\n",
            " 79% 3417/4306 [00:01<00:00, 2467.12it/s]\u001b[A\n",
            " 86% 3684/4306 [00:01<00:00, 2526.02it/s]\u001b[A\n",
            " 91% 3937/4306 [00:01<00:00, 2445.52it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 2370.29it/s]\n",
            "[09.25.21 02:15:14] Visualizing in TensorBoard...\n",
            "[09.25.21 02:15:14] Eval F1: 07.81, EM: 00.07\n",
            "100% 16020/16020 [11:30<00:00, 23.20it/s, NLL=0.207, epoch=0]\n",
            "[09.25.21 02:25:45] Epoch: 1\n",
            "100% 16020/16020 [10:25<00:00, 25.60it/s, NLL=1.09, epoch=1]\n",
            "[09.25.21 02:36:11] Epoch: 2\n",
            "100% 16020/16020 [10:26<00:00, 25.58it/s, NLL=0.821, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BroFepOliLFR",
        "outputId": "6e0f810d-75f5-4b34-ac64-0802730ab2c1"
      },
      "source": [
        "%cd /content/robustqa/robustqa_original/robustqa\n",
        "!python3 train.py \\\n",
        "     --do-train \\\n",
        "     --recompute-features \\\n",
        "     --run-name squad_complete_original2 \\\n",
        "     --train-datasets squad02 \\\n",
        "     --eval-datasets race02 \\\n",
        "     --eval-every 500 \\\n",
        "     --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa/robustqa_original/robustqa\n",
            "[09.25.21 02:47:23] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race02\",\n",
            "    \"eval_dir\": \"./datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_complete_original2\",\n",
            "    \"save_dir\": \"save/squad_complete_original2-02\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad02\",\n",
            "    \"train_dir\": \"./datasets/indomain_train\",\n",
            "    \"val_dir\": \"./datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.25.21 02:47:23] Preparing Training Data...\n",
            "100% 17496/17496 [00:01<00:00, 15189.37it/s]\n",
            "Preprocessing not completely accurate for 208/17496 instances\n",
            "[09.25.21 02:47:31] Preparing Validation Data...\n",
            "100% 2977/2977 [00:00<00:00, 15577.33it/s]\n",
            "[09.25.21 02:47:36] Epoch: 0\n",
            "[09.25.21 02:47:37] Evaluating at step 0...\n",
            "  0% 16/17496 [00:00<12:30, 23.28it/s, NLL=1.62, epoch=0]\n",
            "  0% 0/2977 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/2977 [00:00<00:20, 143.90it/s]\u001b[A\n",
            "  2% 48/2977 [00:00<00:27, 105.42it/s]\u001b[A\n",
            "  2% 64/2977 [00:00<00:31, 92.05it/s] \u001b[A\n",
            "  3% 80/2977 [00:00<00:33, 86.59it/s]\u001b[A\n",
            "  3% 96/2977 [00:01<00:34, 83.18it/s]\u001b[A\n",
            "  4% 112/2977 [00:01<00:35, 80.93it/s]\u001b[A\n",
            "  4% 128/2977 [00:01<00:35, 79.48it/s]\u001b[A\n",
            "  5% 144/2977 [00:01<00:35, 78.87it/s]\u001b[A\n",
            "  5% 160/2977 [00:01<00:35, 78.26it/s]\u001b[A\n",
            "  6% 176/2977 [00:02<00:36, 77.76it/s]\u001b[A\n",
            "  6% 192/2977 [00:02<00:35, 77.65it/s]\u001b[A\n",
            "  7% 208/2977 [00:02<00:35, 77.20it/s]\u001b[A\n",
            "  8% 224/2977 [00:02<00:35, 77.04it/s]\u001b[A\n",
            "  8% 240/2977 [00:02<00:35, 77.22it/s]\u001b[A\n",
            "  9% 256/2977 [00:03<00:35, 77.09it/s]\u001b[A\n",
            "  9% 272/2977 [00:03<00:35, 76.93it/s]\u001b[A\n",
            " 10% 288/2977 [00:03<00:34, 76.89it/s]\u001b[A\n",
            " 10% 304/2977 [00:03<00:34, 77.01it/s]\u001b[A\n",
            " 11% 320/2977 [00:03<00:34, 76.84it/s]\u001b[A\n",
            " 11% 336/2977 [00:04<00:34, 76.92it/s]\u001b[A\n",
            " 12% 352/2977 [00:04<00:34, 76.96it/s]\u001b[A\n",
            " 12% 368/2977 [00:04<00:33, 77.16it/s]\u001b[A\n",
            " 13% 384/2977 [00:04<00:33, 77.08it/s]\u001b[A\n",
            " 13% 400/2977 [00:05<00:33, 77.15it/s]\u001b[A\n",
            " 14% 416/2977 [00:05<00:33, 76.79it/s]\u001b[A\n",
            " 15% 432/2977 [00:05<00:33, 76.81it/s]\u001b[A\n",
            " 15% 448/2977 [00:05<00:32, 76.98it/s]\u001b[A\n",
            " 16% 464/2977 [00:05<00:32, 76.78it/s]\u001b[A\n",
            " 16% 480/2977 [00:06<00:32, 76.87it/s]\u001b[A\n",
            " 17% 496/2977 [00:06<00:32, 76.73it/s]\u001b[A\n",
            " 17% 512/2977 [00:06<00:32, 76.88it/s]\u001b[A\n",
            " 18% 528/2977 [00:06<00:31, 76.81it/s]\u001b[A\n",
            " 18% 544/2977 [00:06<00:31, 76.91it/s]\u001b[A\n",
            " 19% 560/2977 [00:07<00:31, 76.77it/s]\u001b[A\n",
            " 19% 576/2977 [00:07<00:31, 76.78it/s]\u001b[A\n",
            " 20% 592/2977 [00:07<00:31, 76.87it/s]\u001b[A\n",
            " 20% 608/2977 [00:07<00:30, 76.77it/s]\u001b[A\n",
            " 21% 624/2977 [00:07<00:30, 76.82it/s]\u001b[A\n",
            " 21% 640/2977 [00:08<00:30, 76.89it/s]\u001b[A\n",
            " 22% 656/2977 [00:08<00:30, 76.85it/s]\u001b[A\n",
            " 23% 672/2977 [00:08<00:30, 76.57it/s]\u001b[A\n",
            " 23% 688/2977 [00:08<00:29, 76.86it/s]\u001b[A\n",
            " 24% 704/2977 [00:08<00:29, 76.88it/s]\u001b[A\n",
            " 24% 720/2977 [00:09<00:29, 77.01it/s]\u001b[A\n",
            " 25% 736/2977 [00:09<00:29, 76.90it/s]\u001b[A\n",
            " 25% 752/2977 [00:09<00:28, 76.92it/s]\u001b[A\n",
            " 26% 768/2977 [00:09<00:28, 76.84it/s]\u001b[A\n",
            " 26% 784/2977 [00:10<00:28, 76.94it/s]\u001b[A\n",
            " 27% 800/2977 [00:10<00:28, 76.92it/s]\u001b[A\n",
            " 27% 816/2977 [00:10<00:28, 76.73it/s]\u001b[A\n",
            " 28% 832/2977 [00:10<00:27, 76.87it/s]\u001b[A\n",
            " 28% 848/2977 [00:10<00:27, 76.84it/s]\u001b[A\n",
            " 29% 864/2977 [00:11<00:27, 76.93it/s]\u001b[A\n",
            " 30% 880/2977 [00:11<00:27, 76.84it/s]\u001b[A\n",
            " 30% 896/2977 [00:11<00:27, 77.06it/s]\u001b[A\n",
            " 31% 912/2977 [00:11<00:26, 76.86it/s]\u001b[A\n",
            " 31% 928/2977 [00:11<00:26, 76.80it/s]\u001b[A\n",
            " 32% 944/2977 [00:12<00:26, 76.79it/s]\u001b[A\n",
            " 32% 960/2977 [00:12<00:26, 76.73it/s]\u001b[A\n",
            " 33% 976/2977 [00:12<00:26, 76.78it/s]\u001b[A\n",
            " 33% 992/2977 [00:12<00:25, 76.80it/s]\u001b[A\n",
            " 34% 1008/2977 [00:12<00:25, 76.70it/s]\u001b[A\n",
            "  0% 16/17496 [00:13<12:30, 23.28it/s, NLL=1.62, epoch=0]\n",
            " 35% 1040/2977 [00:13<00:25, 76.94it/s]\u001b[A\n",
            " 35% 1056/2977 [00:13<00:25, 76.76it/s]\u001b[A\n",
            " 36% 1072/2977 [00:13<00:24, 76.88it/s]\u001b[A\n",
            " 37% 1088/2977 [00:13<00:24, 76.96it/s]\u001b[A\n",
            " 37% 1104/2977 [00:14<00:24, 76.81it/s]\u001b[A\n",
            " 38% 1120/2977 [00:14<00:24, 76.96it/s]\u001b[A\n",
            " 38% 1136/2977 [00:14<00:23, 77.07it/s]\u001b[A\n",
            " 39% 1152/2977 [00:14<00:23, 76.84it/s]\u001b[A\n",
            " 39% 1168/2977 [00:15<00:23, 76.87it/s]\u001b[A\n",
            " 40% 1184/2977 [00:15<00:23, 76.73it/s]\u001b[A\n",
            " 40% 1200/2977 [00:15<00:23, 76.91it/s]\u001b[A\n",
            " 41% 1216/2977 [00:15<00:22, 77.05it/s]\u001b[A\n",
            " 41% 1232/2977 [00:15<00:22, 76.84it/s]\u001b[A\n",
            " 42% 1248/2977 [00:16<00:22, 76.68it/s]\u001b[A\n",
            " 42% 1264/2977 [00:16<00:22, 76.91it/s]\u001b[A\n",
            " 43% 1280/2977 [00:16<00:22, 76.99it/s]\u001b[A\n",
            " 44% 1296/2977 [00:16<00:21, 76.94it/s]\u001b[A\n",
            " 44% 1312/2977 [00:16<00:21, 76.88it/s]\u001b[A\n",
            " 45% 1328/2977 [00:17<00:21, 77.06it/s]\u001b[A\n",
            " 45% 1344/2977 [00:17<00:21, 77.00it/s]\u001b[A\n",
            " 46% 1360/2977 [00:17<00:20, 77.05it/s]\u001b[A\n",
            " 46% 1376/2977 [00:17<00:20, 77.18it/s]\u001b[A\n",
            " 47% 1392/2977 [00:17<00:20, 76.94it/s]\u001b[A\n",
            " 47% 1408/2977 [00:18<00:20, 76.92it/s]\u001b[A\n",
            " 48% 1424/2977 [00:18<00:20, 76.48it/s]\u001b[A\n",
            " 48% 1440/2977 [00:18<00:19, 76.87it/s]\u001b[A\n",
            " 49% 1456/2977 [00:18<00:19, 76.81it/s]\u001b[A\n",
            " 49% 1472/2977 [00:18<00:19, 76.87it/s]\u001b[A\n",
            " 50% 1488/2977 [00:19<00:19, 76.82it/s]\u001b[A\n",
            " 51% 1504/2977 [00:19<00:19, 76.79it/s]\u001b[A\n",
            " 51% 1520/2977 [00:19<00:18, 76.99it/s]\u001b[A\n",
            " 52% 1536/2977 [00:19<00:18, 76.95it/s]\u001b[A\n",
            " 52% 1552/2977 [00:19<00:18, 76.77it/s]\u001b[A\n",
            " 53% 1568/2977 [00:20<00:18, 77.00it/s]\u001b[A\n",
            " 53% 1584/2977 [00:20<00:18, 76.96it/s]\u001b[A\n",
            " 54% 1600/2977 [00:20<00:17, 77.01it/s]\u001b[A\n",
            " 54% 1616/2977 [00:20<00:17, 77.07it/s]\u001b[A\n",
            " 55% 1632/2977 [00:21<00:17, 77.02it/s]\u001b[A\n",
            " 55% 1648/2977 [00:21<00:17, 76.85it/s]\u001b[A\n",
            " 56% 1664/2977 [00:21<00:17, 76.96it/s]\u001b[A\n",
            " 56% 1680/2977 [00:21<00:16, 76.77it/s]\u001b[A\n",
            " 57% 1696/2977 [00:21<00:16, 76.74it/s]\u001b[A\n",
            " 58% 1712/2977 [00:22<00:16, 76.74it/s]\u001b[A\n",
            " 58% 1728/2977 [00:22<00:16, 76.92it/s]\u001b[A\n",
            " 59% 1744/2977 [00:22<00:16, 76.91it/s]\u001b[A\n",
            " 59% 1760/2977 [00:22<00:15, 76.92it/s]\u001b[A\n",
            " 60% 1776/2977 [00:22<00:15, 76.90it/s]\u001b[A\n",
            " 60% 1792/2977 [00:23<00:15, 76.83it/s]\u001b[A\n",
            " 61% 1808/2977 [00:23<00:15, 77.08it/s]\u001b[A\n",
            " 61% 1824/2977 [00:23<00:14, 77.01it/s]\u001b[A\n",
            " 62% 1840/2977 [00:23<00:14, 76.98it/s]\u001b[A\n",
            " 62% 1856/2977 [00:23<00:14, 77.07it/s]\u001b[A\n",
            " 63% 1872/2977 [00:24<00:14, 76.89it/s]\u001b[A\n",
            " 63% 1888/2977 [00:24<00:14, 76.86it/s]\u001b[A\n",
            " 64% 1904/2977 [00:24<00:13, 76.90it/s]\u001b[A\n",
            " 64% 1920/2977 [00:24<00:13, 76.77it/s]\u001b[A\n",
            " 65% 1936/2977 [00:24<00:13, 76.87it/s]\u001b[A\n",
            " 66% 1952/2977 [00:25<00:13, 76.75it/s]\u001b[A\n",
            " 66% 1968/2977 [00:25<00:13, 76.57it/s]\u001b[A\n",
            " 67% 1984/2977 [00:25<00:12, 76.95it/s]\u001b[A\n",
            " 67% 2000/2977 [00:25<00:12, 76.95it/s]\u001b[A\n",
            " 68% 2016/2977 [00:26<00:12, 76.70it/s]\u001b[A\n",
            " 68% 2032/2977 [00:26<00:12, 76.99it/s]\u001b[A\n",
            " 69% 2048/2977 [00:26<00:12, 76.80it/s]\u001b[A\n",
            " 69% 2064/2977 [00:26<00:11, 77.00it/s]\u001b[A\n",
            " 70% 2080/2977 [00:26<00:11, 76.94it/s]\u001b[A\n",
            " 70% 2096/2977 [00:27<00:11, 76.91it/s]\u001b[A\n",
            " 71% 2112/2977 [00:27<00:11, 77.08it/s]\u001b[A\n",
            " 71% 2128/2977 [00:27<00:11, 76.91it/s]\u001b[A\n",
            " 72% 2144/2977 [00:27<00:10, 76.99it/s]\u001b[A\n",
            " 73% 2160/2977 [00:27<00:10, 76.47it/s]\u001b[A\n",
            " 73% 2176/2977 [00:28<00:10, 76.84it/s]\u001b[A\n",
            " 74% 2192/2977 [00:28<00:10, 76.88it/s]\u001b[A\n",
            " 74% 2208/2977 [00:28<00:10, 76.73it/s]\u001b[A\n",
            " 75% 2224/2977 [00:28<00:09, 76.78it/s]\u001b[A\n",
            " 75% 2240/2977 [00:28<00:09, 76.92it/s]\u001b[A\n",
            " 76% 2256/2977 [00:29<00:09, 76.66it/s]\u001b[A\n",
            " 76% 2272/2977 [00:29<00:09, 76.90it/s]\u001b[A\n",
            " 77% 2288/2977 [00:29<00:08, 76.94it/s]\u001b[A\n",
            " 77% 2304/2977 [00:29<00:08, 76.95it/s]\u001b[A\n",
            " 78% 2320/2977 [00:29<00:08, 76.93it/s]\u001b[A\n",
            " 78% 2336/2977 [00:30<00:08, 76.94it/s]\u001b[A\n",
            " 79% 2352/2977 [00:30<00:08, 76.96it/s]\u001b[A\n",
            " 80% 2368/2977 [00:30<00:07, 76.90it/s]\u001b[A\n",
            " 80% 2384/2977 [00:30<00:07, 76.74it/s]\u001b[A\n",
            " 81% 2400/2977 [00:31<00:07, 76.66it/s]\u001b[A\n",
            " 81% 2416/2977 [00:31<00:07, 76.87it/s]\u001b[A\n",
            " 82% 2432/2977 [00:31<00:07, 76.74it/s]\u001b[A\n",
            " 82% 2448/2977 [00:31<00:06, 76.89it/s]\u001b[A\n",
            " 83% 2464/2977 [00:31<00:06, 76.87it/s]\u001b[A\n",
            " 83% 2480/2977 [00:32<00:06, 76.87it/s]\u001b[A\n",
            " 84% 2496/2977 [00:32<00:06, 77.11it/s]\u001b[A\n",
            " 84% 2512/2977 [00:32<00:06, 77.02it/s]\u001b[A\n",
            " 85% 2528/2977 [00:32<00:05, 76.83it/s]\u001b[A\n",
            " 85% 2544/2977 [00:32<00:05, 76.93it/s]\u001b[A\n",
            " 86% 2560/2977 [00:33<00:05, 76.74it/s]\u001b[A\n",
            " 87% 2576/2977 [00:33<00:05, 76.92it/s]\u001b[A\n",
            " 87% 2592/2977 [00:33<00:05, 76.78it/s]\u001b[A\n",
            " 88% 2608/2977 [00:33<00:04, 76.53it/s]\u001b[A\n",
            " 88% 2624/2977 [00:33<00:04, 77.01it/s]\u001b[A\n",
            " 89% 2640/2977 [00:34<00:04, 76.87it/s]\u001b[A\n",
            " 89% 2656/2977 [00:34<00:04, 76.97it/s]\u001b[A\n",
            " 90% 2672/2977 [00:34<00:03, 76.88it/s]\u001b[A\n",
            " 90% 2688/2977 [00:34<00:03, 76.79it/s]\u001b[A\n",
            " 91% 2704/2977 [00:34<00:03, 76.77it/s]\u001b[A\n",
            " 91% 2720/2977 [00:35<00:03, 76.85it/s]\u001b[A\n",
            " 92% 2736/2977 [00:35<00:03, 76.91it/s]\u001b[A\n",
            " 92% 2752/2977 [00:35<00:02, 76.75it/s]\u001b[A\n",
            " 93% 2768/2977 [00:35<00:02, 76.79it/s]\u001b[A\n",
            " 94% 2784/2977 [00:36<00:02, 76.71it/s]\u001b[A\n",
            " 94% 2800/2977 [00:36<00:02, 76.74it/s]\u001b[A\n",
            " 95% 2816/2977 [00:36<00:02, 76.86it/s]\u001b[A\n",
            " 95% 2832/2977 [00:36<00:01, 76.78it/s]\u001b[A\n",
            " 96% 2848/2977 [00:36<00:01, 76.79it/s]\u001b[A\n",
            " 96% 2864/2977 [00:37<00:01, 76.95it/s]\u001b[A\n",
            " 97% 2880/2977 [00:37<00:01, 76.79it/s]\u001b[A\n",
            " 97% 2896/2977 [00:37<00:01, 76.86it/s]\u001b[A\n",
            " 98% 2912/2977 [00:37<00:00, 76.88it/s]\u001b[A\n",
            " 98% 2928/2977 [00:37<00:00, 76.77it/s]\u001b[A\n",
            " 99% 2944/2977 [00:38<00:00, 76.80it/s]\u001b[A\n",
            " 99% 2960/2977 [00:38<00:00, 76.89it/s]\u001b[A\n",
            "100% 2977/2977 [00:38<00:00, 76.86it/s]\n",
            "\n",
            "  0% 0/2929 [00:00<?, ?it/s]\u001b[A\n",
            "  6% 186/2929 [00:00<00:01, 1852.37it/s]\u001b[A\n",
            " 13% 383/2929 [00:00<00:01, 1918.13it/s]\u001b[A\n",
            " 20% 577/2929 [00:00<00:01, 1925.34it/s]\u001b[A\n",
            " 27% 778/2929 [00:00<00:01, 1957.38it/s]\u001b[A\n",
            " 34% 991/2929 [00:00<00:00, 2019.36it/s]\u001b[A\n",
            " 41% 1196/2929 [00:00<00:00, 2028.41it/s]\u001b[A\n",
            " 48% 1403/2929 [00:00<00:00, 2040.85it/s]\u001b[A\n",
            " 55% 1608/2929 [00:00<00:00, 2040.26it/s]\u001b[A\n",
            " 62% 1821/2929 [00:00<00:00, 2066.15it/s]\u001b[A\n",
            " 70% 2040/2929 [00:01<00:00, 2103.86it/s]\u001b[A\n",
            " 77% 2251/2929 [00:01<00:00, 2091.26it/s]\u001b[A\n",
            " 84% 2464/2929 [00:01<00:00, 2099.47it/s]\u001b[A\n",
            " 92% 2685/2929 [00:01<00:00, 2132.13it/s]\u001b[A\n",
            "100% 2929/2929 [00:01<00:00, 2064.29it/s]\n",
            "[09.25.21 02:48:17] Visualizing in TensorBoard...\n",
            "[09.25.21 02:48:17] Eval F1: 83.41, EM: 71.18\n",
            "100% 17496/17496 [12:07<00:00, 24.04it/s, NLL=0.905, epoch=0]\n",
            "[09.25.21 02:59:45] Epoch: 1\n",
            "100% 17496/17496 [11:21<00:00, 25.67it/s, NLL=1.14, epoch=1]\n",
            "[09.25.21 03:11:08] Epoch: 2\n",
            "100% 17496/17496 [11:21<00:00, 25.66it/s, NLL=0.265, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEDckiMmiKcM",
        "outputId": "11018116-ef39-43d4-d81b-9f27c652a801"
      },
      "source": [
        "%cd /content/robustqa/robustqa_original/robustqa\n",
        "!python3 train.py \\\n",
        "     --do-train \\\n",
        "     --recompute-features \\\n",
        "     --run-name squad_complete_original2 \\\n",
        "     --train-datasets squad03 \\\n",
        "     --eval-datasets race03 \\\n",
        "     --eval-every 500 \\\n",
        "     --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa/robustqa_original/robustqa\n",
            "[09.25.21 03:22:39] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race03\",\n",
            "    \"eval_dir\": \"./datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_complete_original2\",\n",
            "    \"save_dir\": \"save/squad_complete_original2-03\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad03\",\n",
            "    \"train_dir\": \"./datasets/indomain_train\",\n",
            "    \"val_dir\": \"./datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.25.21 03:22:39] Preparing Training Data...\n",
            "100% 17047/17047 [00:01<00:00, 16244.26it/s]\n",
            "Preprocessing not completely accurate for 198/17047 instances\n",
            "[09.25.21 03:22:46] Preparing Validation Data...\n",
            "100% 3377/3377 [00:00<00:00, 15976.54it/s]\n",
            "[09.25.21 03:22:52] Epoch: 0\n",
            "[09.25.21 03:22:52] Evaluating at step 0...\n",
            "  0% 16/17047 [00:00<11:48, 24.02it/s, NLL=1.3, epoch=0]\n",
            "  0% 0/3377 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/3377 [00:00<00:23, 143.55it/s]\u001b[A\n",
            "  1% 48/3377 [00:00<00:31, 105.57it/s]\u001b[A\n",
            "  2% 64/3377 [00:00<00:35, 92.12it/s] \u001b[A\n",
            "  2% 80/3377 [00:00<00:38, 86.62it/s]\u001b[A\n",
            "  3% 96/3377 [00:01<00:39, 82.90it/s]\u001b[A\n",
            "  3% 112/3377 [00:01<00:40, 80.88it/s]\u001b[A\n",
            "  4% 128/3377 [00:01<00:40, 79.51it/s]\u001b[A\n",
            "  4% 144/3377 [00:01<00:41, 78.12it/s]\u001b[A\n",
            "  5% 160/3377 [00:01<00:41, 77.96it/s]\u001b[A\n",
            "  5% 176/3377 [00:02<00:41, 77.75it/s]\u001b[A\n",
            "  6% 192/3377 [00:02<00:41, 77.40it/s]\u001b[A\n",
            "  6% 208/3377 [00:02<00:41, 77.08it/s]\u001b[A\n",
            "  7% 224/3377 [00:02<00:40, 77.09it/s]\u001b[A\n",
            "  7% 240/3377 [00:02<00:40, 76.96it/s]\u001b[A\n",
            "  8% 256/3377 [00:03<00:40, 76.96it/s]\u001b[A\n",
            "  8% 272/3377 [00:03<00:40, 76.93it/s]\u001b[A\n",
            "  9% 288/3377 [00:03<00:40, 76.67it/s]\u001b[A\n",
            "  9% 304/3377 [00:03<00:39, 76.89it/s]\u001b[A\n",
            "  9% 320/3377 [00:03<00:39, 76.93it/s]\u001b[A\n",
            " 10% 336/3377 [00:04<00:39, 76.88it/s]\u001b[A\n",
            " 10% 352/3377 [00:04<00:39, 76.83it/s]\u001b[A\n",
            " 11% 368/3377 [00:04<00:39, 76.62it/s]\u001b[A\n",
            " 11% 384/3377 [00:04<00:39, 76.68it/s]\u001b[A\n",
            " 12% 400/3377 [00:05<00:38, 76.58it/s]\u001b[A\n",
            " 12% 416/3377 [00:05<00:38, 76.71it/s]\u001b[A\n",
            " 13% 432/3377 [00:05<00:38, 76.63it/s]\u001b[A\n",
            " 13% 448/3377 [00:05<00:38, 76.80it/s]\u001b[A\n",
            " 14% 464/3377 [00:05<00:37, 76.78it/s]\u001b[A\n",
            " 14% 480/3377 [00:06<00:37, 76.62it/s]\u001b[A\n",
            " 15% 496/3377 [00:06<00:37, 76.79it/s]\u001b[A\n",
            " 15% 512/3377 [00:06<00:37, 76.81it/s]\u001b[A\n",
            " 16% 528/3377 [00:06<00:37, 76.68it/s]\u001b[A\n",
            " 16% 544/3377 [00:06<00:36, 76.80it/s]\u001b[A\n",
            " 17% 560/3377 [00:07<00:36, 76.89it/s]\u001b[A\n",
            " 17% 576/3377 [00:07<00:36, 76.76it/s]\u001b[A\n",
            " 18% 592/3377 [00:07<00:36, 76.66it/s]\u001b[A\n",
            " 18% 608/3377 [00:07<00:36, 76.58it/s]\u001b[A\n",
            " 18% 624/3377 [00:07<00:35, 76.61it/s]\u001b[A\n",
            " 19% 640/3377 [00:08<00:35, 76.74it/s]\u001b[A\n",
            " 19% 656/3377 [00:08<00:35, 76.70it/s]\u001b[A\n",
            " 20% 672/3377 [00:08<00:35, 76.76it/s]\u001b[A\n",
            " 20% 688/3377 [00:08<00:34, 76.87it/s]\u001b[A\n",
            " 21% 704/3377 [00:08<00:34, 76.80it/s]\u001b[A\n",
            " 21% 720/3377 [00:09<00:34, 76.74it/s]\u001b[A\n",
            " 22% 736/3377 [00:09<00:34, 76.80it/s]\u001b[A\n",
            " 22% 752/3377 [00:09<00:34, 76.77it/s]\u001b[A\n",
            " 23% 768/3377 [00:09<00:34, 76.64it/s]\u001b[A\n",
            " 23% 784/3377 [00:10<00:33, 76.86it/s]\u001b[A\n",
            " 24% 800/3377 [00:10<00:33, 76.74it/s]\u001b[A\n",
            " 24% 816/3377 [00:10<00:33, 76.62it/s]\u001b[A\n",
            " 25% 832/3377 [00:10<00:33, 76.82it/s]\u001b[A\n",
            " 25% 848/3377 [00:10<00:32, 76.83it/s]\u001b[A\n",
            " 26% 864/3377 [00:11<00:32, 76.76it/s]\u001b[A\n",
            " 26% 880/3377 [00:11<00:32, 76.72it/s]\u001b[A\n",
            " 27% 896/3377 [00:11<00:32, 76.43it/s]\u001b[A\n",
            " 27% 912/3377 [00:11<00:32, 76.72it/s]\u001b[A\n",
            " 27% 928/3377 [00:11<00:31, 76.81it/s]\u001b[A\n",
            " 28% 944/3377 [00:12<00:31, 76.71it/s]\u001b[A\n",
            " 28% 960/3377 [00:12<00:31, 76.91it/s]\u001b[A\n",
            " 29% 976/3377 [00:12<00:31, 76.92it/s]\u001b[A\n",
            " 29% 992/3377 [00:12<00:31, 76.92it/s]\u001b[A\n",
            "  0% 16/17047 [00:13<11:48, 24.02it/s, NLL=1.3, epoch=0]\n",
            " 30% 1024/3377 [00:13<00:30, 76.65it/s]\u001b[A\n",
            " 31% 1040/3377 [00:13<00:30, 76.64it/s]\u001b[A\n",
            " 31% 1056/3377 [00:13<00:30, 76.75it/s]\u001b[A\n",
            " 32% 1072/3377 [00:13<00:29, 76.93it/s]\u001b[A\n",
            " 32% 1088/3377 [00:13<00:29, 76.79it/s]\u001b[A\n",
            " 33% 1104/3377 [00:14<00:29, 76.82it/s]\u001b[A\n",
            " 33% 1120/3377 [00:14<00:29, 76.77it/s]\u001b[A\n",
            " 34% 1136/3377 [00:14<00:29, 76.46it/s]\u001b[A\n",
            " 34% 1152/3377 [00:14<00:28, 76.92it/s]\u001b[A\n",
            " 35% 1168/3377 [00:15<00:28, 76.41it/s]\u001b[A\n",
            " 35% 1184/3377 [00:15<00:28, 76.86it/s]\u001b[A\n",
            " 36% 1200/3377 [00:15<00:28, 76.79it/s]\u001b[A\n",
            " 36% 1216/3377 [00:15<00:28, 76.86it/s]\u001b[A\n",
            " 36% 1232/3377 [00:15<00:27, 76.87it/s]\u001b[A\n",
            " 37% 1248/3377 [00:16<00:27, 76.85it/s]\u001b[A\n",
            " 37% 1264/3377 [00:16<00:27, 76.66it/s]\u001b[A\n",
            " 38% 1280/3377 [00:16<00:27, 76.70it/s]\u001b[A\n",
            " 38% 1296/3377 [00:16<00:27, 76.71it/s]\u001b[A\n",
            " 39% 1312/3377 [00:16<00:26, 76.66it/s]\u001b[A\n",
            " 39% 1328/3377 [00:17<00:26, 76.76it/s]\u001b[A\n",
            " 40% 1344/3377 [00:17<00:26, 76.58it/s]\u001b[A\n",
            " 40% 1360/3377 [00:17<00:26, 76.58it/s]\u001b[A\n",
            " 41% 1376/3377 [00:17<00:26, 76.74it/s]\u001b[A\n",
            " 41% 1392/3377 [00:17<00:25, 76.77it/s]\u001b[A\n",
            " 42% 1408/3377 [00:18<00:25, 76.68it/s]\u001b[A\n",
            " 42% 1424/3377 [00:18<00:25, 76.74it/s]\u001b[A\n",
            " 43% 1440/3377 [00:18<00:25, 76.78it/s]\u001b[A\n",
            " 43% 1456/3377 [00:18<00:25, 76.74it/s]\u001b[A\n",
            " 44% 1472/3377 [00:18<00:24, 76.88it/s]\u001b[A\n",
            " 44% 1488/3377 [00:19<00:24, 76.84it/s]\u001b[A\n",
            " 45% 1504/3377 [00:19<00:24, 76.69it/s]\u001b[A\n",
            " 45% 1520/3377 [00:19<00:24, 76.75it/s]\u001b[A\n",
            " 45% 1536/3377 [00:19<00:23, 76.71it/s]\u001b[A\n",
            " 46% 1552/3377 [00:20<00:23, 76.63it/s]\u001b[A\n",
            " 46% 1568/3377 [00:20<00:23, 76.70it/s]\u001b[A\n",
            " 47% 1584/3377 [00:20<00:23, 76.69it/s]\u001b[A\n",
            " 47% 1600/3377 [00:20<00:23, 76.47it/s]\u001b[A\n",
            " 48% 1616/3377 [00:20<00:22, 76.75it/s]\u001b[A\n",
            " 48% 1632/3377 [00:21<00:22, 76.81it/s]\u001b[A\n",
            " 49% 1648/3377 [00:21<00:22, 76.84it/s]\u001b[A\n",
            " 49% 1664/3377 [00:21<00:22, 76.70it/s]\u001b[A\n",
            " 50% 1680/3377 [00:21<00:22, 76.79it/s]\u001b[A\n",
            " 50% 1696/3377 [00:21<00:21, 76.81it/s]\u001b[A\n",
            " 51% 1712/3377 [00:22<00:21, 76.87it/s]\u001b[A\n",
            " 51% 1728/3377 [00:22<00:21, 76.67it/s]\u001b[A\n",
            " 52% 1744/3377 [00:22<00:21, 76.83it/s]\u001b[A\n",
            " 52% 1760/3377 [00:22<00:21, 76.80it/s]\u001b[A\n",
            " 53% 1776/3377 [00:22<00:20, 76.67it/s]\u001b[A\n",
            " 53% 1792/3377 [00:23<00:20, 76.63it/s]\u001b[A\n",
            " 54% 1808/3377 [00:23<00:20, 76.59it/s]\u001b[A\n",
            " 54% 1824/3377 [00:23<00:20, 76.66it/s]\u001b[A\n",
            " 54% 1840/3377 [00:23<00:20, 76.71it/s]\u001b[A\n",
            " 55% 1856/3377 [00:23<00:19, 76.74it/s]\u001b[A\n",
            " 55% 1872/3377 [00:24<00:19, 76.73it/s]\u001b[A\n",
            " 56% 1888/3377 [00:24<00:19, 76.74it/s]\u001b[A\n",
            " 56% 1904/3377 [00:24<00:19, 76.73it/s]\u001b[A\n",
            " 57% 1920/3377 [00:24<00:18, 76.82it/s]\u001b[A\n",
            " 57% 1936/3377 [00:25<00:18, 76.89it/s]\u001b[A\n",
            " 58% 1952/3377 [00:25<00:18, 76.39it/s]\u001b[A\n",
            " 58% 1968/3377 [00:25<00:18, 76.84it/s]\u001b[A\n",
            " 59% 1984/3377 [00:25<00:18, 76.62it/s]\u001b[A\n",
            " 59% 2000/3377 [00:25<00:17, 76.69it/s]\u001b[A\n",
            " 60% 2016/3377 [00:26<00:17, 76.51it/s]\u001b[A\n",
            " 60% 2032/3377 [00:26<00:17, 76.78it/s]\u001b[A\n",
            " 61% 2048/3377 [00:26<00:17, 76.77it/s]\u001b[A\n",
            " 61% 2064/3377 [00:26<00:17, 76.67it/s]\u001b[A\n",
            " 62% 2080/3377 [00:26<00:16, 76.63it/s]\u001b[A\n",
            " 62% 2096/3377 [00:27<00:16, 76.67it/s]\u001b[A\n",
            " 63% 2112/3377 [00:27<00:16, 76.73it/s]\u001b[A\n",
            " 63% 2128/3377 [00:27<00:16, 76.58it/s]\u001b[A\n",
            " 63% 2144/3377 [00:27<00:16, 76.85it/s]\u001b[A\n",
            " 64% 2160/3377 [00:27<00:15, 76.84it/s]\u001b[A\n",
            " 64% 2176/3377 [00:28<00:15, 76.81it/s]\u001b[A\n",
            " 65% 2192/3377 [00:28<00:15, 76.86it/s]\u001b[A\n",
            " 65% 2208/3377 [00:28<00:15, 76.79it/s]\u001b[A\n",
            " 66% 2224/3377 [00:28<00:15, 76.68it/s]\u001b[A\n",
            " 66% 2240/3377 [00:28<00:14, 76.86it/s]\u001b[A\n",
            " 67% 2256/3377 [00:29<00:14, 76.67it/s]\u001b[A\n",
            " 67% 2272/3377 [00:29<00:14, 76.70it/s]\u001b[A\n",
            " 68% 2288/3377 [00:29<00:14, 76.54it/s]\u001b[A\n",
            " 68% 2304/3377 [00:29<00:13, 76.69it/s]\u001b[A\n",
            " 69% 2320/3377 [00:30<00:13, 76.63it/s]\u001b[A\n",
            " 69% 2336/3377 [00:30<00:13, 76.65it/s]\u001b[A\n",
            " 70% 2352/3377 [00:30<00:13, 76.59it/s]\u001b[A\n",
            " 70% 2368/3377 [00:30<00:13, 76.77it/s]\u001b[A\n",
            " 71% 2384/3377 [00:30<00:12, 76.71it/s]\u001b[A\n",
            " 71% 2400/3377 [00:31<00:12, 76.73it/s]\u001b[A\n",
            " 72% 2416/3377 [00:31<00:12, 76.73it/s]\u001b[A\n",
            " 72% 2432/3377 [00:31<00:12, 76.74it/s]\u001b[A\n",
            " 72% 2448/3377 [00:31<00:12, 76.71it/s]\u001b[A\n",
            " 73% 2464/3377 [00:31<00:11, 76.67it/s]\u001b[A\n",
            " 73% 2480/3377 [00:32<00:11, 76.62it/s]\u001b[A\n",
            " 74% 2496/3377 [00:32<00:11, 76.60it/s]\u001b[A\n",
            " 74% 2512/3377 [00:32<00:11, 76.48it/s]\u001b[A\n",
            " 75% 2528/3377 [00:32<00:11, 76.76it/s]\u001b[A\n",
            " 75% 2544/3377 [00:32<00:10, 76.68it/s]\u001b[A\n",
            " 76% 2560/3377 [00:33<00:10, 76.47it/s]\u001b[A\n",
            " 76% 2576/3377 [00:33<00:10, 76.70it/s]\u001b[A\n",
            " 77% 2592/3377 [00:33<00:10, 76.37it/s]\u001b[A\n",
            " 77% 2608/3377 [00:33<00:09, 76.93it/s]\u001b[A\n",
            " 78% 2624/3377 [00:34<00:09, 76.49it/s]\u001b[A\n",
            " 78% 2640/3377 [00:34<00:09, 76.92it/s]\u001b[A\n",
            " 79% 2656/3377 [00:34<00:09, 76.98it/s]\u001b[A\n",
            " 79% 2672/3377 [00:34<00:09, 76.86it/s]\u001b[A\n",
            " 80% 2688/3377 [00:34<00:08, 76.78it/s]\u001b[A\n",
            " 80% 2704/3377 [00:35<00:08, 76.71it/s]\u001b[A\n",
            " 81% 2720/3377 [00:35<00:08, 76.72it/s]\u001b[A\n",
            " 81% 2736/3377 [00:35<00:08, 76.71it/s]\u001b[A\n",
            " 81% 2752/3377 [00:35<00:08, 76.63it/s]\u001b[A\n",
            " 82% 2768/3377 [00:35<00:07, 76.67it/s]\u001b[A\n",
            " 82% 2784/3377 [00:36<00:07, 76.70it/s]\u001b[A\n",
            " 83% 2800/3377 [00:36<00:07, 76.80it/s]\u001b[A\n",
            " 83% 2816/3377 [00:36<00:07, 76.81it/s]\u001b[A\n",
            " 84% 2832/3377 [00:36<00:07, 76.57it/s]\u001b[A\n",
            " 84% 2848/3377 [00:36<00:06, 76.77it/s]\u001b[A\n",
            " 85% 2864/3377 [00:37<00:06, 76.72it/s]\u001b[A\n",
            " 85% 2880/3377 [00:37<00:06, 76.22it/s]\u001b[A\n",
            " 86% 2896/3377 [00:37<00:06, 77.10it/s]\u001b[A\n",
            " 86% 2912/3377 [00:37<00:06, 76.88it/s]\u001b[A\n",
            " 87% 2928/3377 [00:37<00:05, 76.86it/s]\u001b[A\n",
            " 87% 2944/3377 [00:38<00:05, 76.69it/s]\u001b[A\n",
            " 88% 2960/3377 [00:38<00:05, 76.46it/s]\u001b[A\n",
            " 88% 2976/3377 [00:38<00:05, 76.76it/s]\u001b[A\n",
            " 89% 2992/3377 [00:38<00:05, 76.74it/s]\u001b[A\n",
            " 89% 3008/3377 [00:39<00:04, 76.71it/s]\u001b[A\n",
            " 90% 3024/3377 [00:39<00:04, 76.75it/s]\u001b[A\n",
            " 90% 3040/3377 [00:39<00:04, 76.60it/s]\u001b[A\n",
            " 90% 3056/3377 [00:39<00:04, 76.64it/s]\u001b[A\n",
            " 91% 3072/3377 [00:39<00:03, 76.75it/s]\u001b[A\n",
            " 91% 3088/3377 [00:40<00:03, 76.82it/s]\u001b[A\n",
            " 92% 3104/3377 [00:40<00:03, 76.74it/s]\u001b[A\n",
            " 92% 3120/3377 [00:40<00:03, 76.80it/s]\u001b[A\n",
            " 93% 3136/3377 [00:40<00:03, 76.86it/s]\u001b[A\n",
            " 93% 3152/3377 [00:40<00:02, 76.82it/s]\u001b[A\n",
            " 94% 3168/3377 [00:41<00:02, 76.60it/s]\u001b[A\n",
            " 94% 3184/3377 [00:41<00:02, 76.80it/s]\u001b[A\n",
            " 95% 3200/3377 [00:41<00:02, 76.83it/s]\u001b[A\n",
            " 95% 3216/3377 [00:41<00:02, 76.35it/s]\u001b[A\n",
            " 96% 3232/3377 [00:41<00:01, 76.81it/s]\u001b[A\n",
            " 96% 3248/3377 [00:42<00:01, 76.78it/s]\u001b[A\n",
            " 97% 3264/3377 [00:42<00:01, 76.80it/s]\u001b[A\n",
            " 97% 3280/3377 [00:42<00:01, 76.81it/s]\u001b[A\n",
            " 98% 3296/3377 [00:42<00:01, 76.76it/s]\u001b[A\n",
            " 98% 3312/3377 [00:42<00:00, 76.81it/s]\u001b[A\n",
            " 99% 3328/3377 [00:43<00:00, 76.83it/s]\u001b[A\n",
            " 99% 3344/3377 [00:43<00:00, 76.85it/s]\u001b[A\n",
            " 99% 3360/3377 [00:43<00:00, 76.75it/s]\u001b[A\n",
            "100% 3377/3377 [00:44<00:00, 76.72it/s]\n",
            "\n",
            "  0% 0/3335 [00:00<?, ?it/s]\u001b[A\n",
            "  5% 170/3335 [00:00<00:01, 1691.06it/s]\u001b[A\n",
            " 11% 355/3335 [00:00<00:01, 1781.68it/s]\u001b[A\n",
            " 16% 534/3335 [00:00<00:01, 1756.23it/s]\u001b[A\n",
            " 21% 710/3335 [00:00<00:01, 1753.76it/s]\u001b[A\n",
            " 27% 886/3335 [00:00<00:01, 1709.27it/s]\u001b[A\n",
            " 32% 1063/3335 [00:00<00:01, 1727.76it/s]\u001b[A\n",
            " 37% 1249/3335 [00:00<00:01, 1769.28it/s]\u001b[A\n",
            " 43% 1432/3335 [00:00<00:01, 1788.15it/s]\u001b[A\n",
            " 49% 1622/3335 [00:00<00:00, 1820.44it/s]\u001b[A\n",
            " 54% 1805/3335 [00:01<00:00, 1809.67it/s]\u001b[A\n",
            " 60% 1987/3335 [00:01<00:00, 1807.27it/s]\u001b[A\n",
            " 65% 2178/3335 [00:01<00:00, 1836.66it/s]\u001b[A\n",
            " 71% 2362/3335 [00:01<00:00, 1820.06it/s]\u001b[A\n",
            " 76% 2545/3335 [00:01<00:00, 1756.49it/s]\u001b[A\n",
            " 82% 2727/3335 [00:01<00:00, 1773.77it/s]\u001b[A\n",
            " 87% 2909/3335 [00:01<00:00, 1787.12it/s]\u001b[A\n",
            " 93% 3089/3335 [00:01<00:00, 1761.43it/s]\u001b[A\n",
            "100% 3335/3335 [00:01<00:00, 1769.12it/s]\n",
            "[09.25.21 03:23:39] Visualizing in TensorBoard...\n",
            "[09.25.21 03:23:39] Eval F1: 82.69, EM: 69.48\n",
            "100% 17047/17047 [11:56<00:00, 23.81it/s, NLL=1.73, epoch=0]\n",
            "[09.25.21 03:34:49] Epoch: 1\n",
            "100% 17047/17047 [11:05<00:00, 25.63it/s, NLL=0.839, epoch=1]\n",
            "[09.25.21 03:45:55] Epoch: 2\n",
            "100% 17047/17047 [11:04<00:00, 25.64it/s, NLL=0.179, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz3jIiSPirpR"
      },
      "source": [
        "cp -R /content/robustqa /content/drive/MyDrive/robustqa_change_transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4MxE2hoixii",
        "outputId": "31e49f54-dfb6-4bd4-d788-c0f8ee805df7"
      },
      "source": [
        "# Evaluate out domain\n",
        "%cd /content/robustqa\n",
        "!python3 train.py \\\n",
        "    --run-name squad_complete_bicubic \\\n",
        "    --do-eval \\\n",
        "    --eval-dir ./datasets/oodomain_val/ \\\n",
        "    --eval-datasets race"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'qa_outputs.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'qa_outputs.weight', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 181/181 [00:00<00:00, 24431.79it/s]\n",
            "  0% 0/181 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "100% 181/181 [00:04<00:00, 37.41it/s]\n",
            "100% 128/128 [00:00<00:00, 2379.54it/s]\n",
            "validation!!\n",
            "[09.29.21 20:40:40] Eval F1: 40.95, EM: 25.00\n",
            "[09.29.21 20:40:40] Writing submission file to save/validation_...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7gUZqlyHZQH",
        "outputId": "063c639c-a5c9-422a-805f-440b8c83d903"
      },
      "source": [
        "# Evaluate in domain\n",
        "%cd /content/robustqa\n",
        "!python3 train.py \\\n",
        "    --do-eval \\\n",
        "    --eval-dir ./datasets/indomain_val/ \\\n",
        "    --eval-datasets squad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'qa_outputs.bias', 'qa_outputs.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 10790/10790 [00:00<00:00, 24019.12it/s]\n",
            "  0% 0/10790 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "100% 10790/10790 [05:02<00:00, 35.62it/s]\n",
            "100% 10570/10570 [00:04<00:00, 2519.19it/s]\n",
            "[10.01.21 04:50:19] Eval F1: 77.74, EM: 63.36\n",
            "[10.01.21 04:50:19] Writing submission file to save/validation_...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3CUUT5JHZHO",
        "outputId": "a2cec5cc-74b9-4239-ac19-75abd87c5047"
      },
      "source": [
        "# Evaluate original\n",
        "%cd /content/robustqa/robustqa_original/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-eval \\\n",
        "    --eval-dir ./datasets/indomain_val/ \\\n",
        "    --eval-datasets squad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa/robustqa_original/robustqa\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 2.86MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 1.42MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 4.06MB/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 612kB/s]\n",
            "Downloading: 100% 478M/478M [00:07<00:00, 63.2MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 10790/10790 [00:00<00:00, 24434.90it/s]\n",
            "100% 10790/10790 [04:24<00:00, 40.83it/s]\n",
            "100% 10570/10570 [00:04<00:00, 2615.97it/s]\n",
            "[09.30.21 08:42:54] Eval F1: 83.53, EM: 70.45\n",
            "[09.30.21 08:42:54] Writing submission file to save/validation_...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NrII4prHY2K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROlV2Ma3MlYK",
        "outputId": "c41525c1-1595-4046-abd8-7a30f55556eb"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1891lzg8M5Ag",
        "outputId": "6e09225f-8619-49fc-8abb-f7f771b0e0fd"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 84890, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 84890 (delta 17), reused 21 (delta 8), pack-reused 84831\u001b[K\n",
            "Receiving objects: 100% (84890/84890), 67.86 MiB | 31.98 MiB/s, done.\n",
            "Resolving deltas: 100% (61000/61000), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yntn8boRNZF1"
      },
      "source": [
        "!cp -R transformers/src/transformers /content/robustqa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anm2z7sxNvIU"
      },
      "source": [
        "!rm -rf /content/robustqa/src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPaRdgNKZHNH"
      },
      "source": [
        "#Albert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grebPsxb2CaA"
      },
      "source": [
        "# Albert modification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878,
          "referenced_widgets": [
            "b660c0f2f141430bb9f11c7d33f3c7c1",
            "14b805938d0c45c1a3bdd0cd7ef04319",
            "6122db1a902c4015be491c3205631cc4",
            "22277833e0db4e67b143d25833aff8c3",
            "fdf9173b370546468dd71bb78dbaca2c",
            "8f362c331fcb4a8a9928c6a8022bf6ad",
            "85bd5999b9594f3cbca1f35f243b8c3f",
            "ed69199f4b7d403ca87d97c012714e82",
            "e55012c4b86f458eb7befff8d8ad60d0",
            "f5daee2157134dd682422777b5de8af3",
            "ba3404899150404fa10d65997e3be72d",
            "27766d1aa5194364bba282c2457fc122",
            "bd65aeea4552409381c294cb0944e801",
            "09bc2185ea2d46bf86251b2b0da0fe6a",
            "e436d277fb6645baae2c250e53797e61",
            "db9a306341664f7c9b235c57869bc525",
            "208276e991b049b887786420e85c354f",
            "fb8b416ccb2a4548ae40545d796870db",
            "574d370e2c564b1b93456d411d52c11d",
            "2225b66a5b6349b6b4eee4fc4a6ef41c",
            "62386326714a4c14a1bb0e477f28f8df",
            "ba3e96b2e598429f955983fd95502df1",
            "8b2dcd8b4c144da99ed5c086e1c9bd0b",
            "df822d7f7d0041e78ef01da89f9c93fb",
            "b52457ffda764e31be5249350180dd38",
            "6023e710c87a4f3ba8b03113db24f39a",
            "006b937d51754c6eabf95d723ba7da52",
            "e357ebdc16b24f92906ff412c9d6d36d",
            "1b7d1ac0feb747cdad0b759e69accd3a",
            "a6957b0c8ece478baab6f6b90467d7e6",
            "192aee0856c94ca8913bd011a560a6fb",
            "5039c028b177416a99642fbe3981d752",
            "e61dc9ddb43c4e2ca9a650078492ee9a",
            "d16bee6af022404d8c02e4f0987ccb11",
            "ead7f91407ef422abdf89cfee9640890",
            "84e73d14c0b44fb1a2193407a99245e8",
            "005a6a768671465490bd50f019b2d94b",
            "61ffc44c76e547ddadf6a8f8b2741464",
            "4349c04e1dce41058daf326e4e18d6d9",
            "b27e79f0f1034190a890e8ee9651bf4a",
            "0d4f989223674af59048e3d7ab6a8757",
            "192e5466651d4c0f92a307c2aa6dd456",
            "23ef7feef4f84aabac5efaa1fa998dc1",
            "44dbbba842df4e07ae1b125a0d3a2e8a"
          ]
        },
        "id": "LMooDt-S1Gsk",
        "outputId": "c4c6c38f-8b86-4fb9-c721-3e033a6a5519"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "from transformers import AlbertForQuestionAnswering, AlbertTokenizerFast\n",
        "\n",
        "tokenizer = AlbertTokenizerFast.from_pretrained('albert-xxlarge-v2')\n",
        "albert = AlbertForQuestionAnswering.from_pretrained('albert-xxlarge-v2')\n",
        "albert_config = albert.config"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.10.3-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 47.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Installing collected packages: pyyaml, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed pyyaml-5.4.1 transformers-4.10.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b660c0f2f141430bb9f11c7d33f3c7c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/742k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27766d1aa5194364bba282c2457fc122",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b2dcd8b4c144da99ed5c086e1c9bd0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/710 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d16bee6af022404d8c02e4f0987ccb11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/851M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.LayerNorm.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.dense.bias', 'predictions.dense.weight', 'predictions.decoder.weight']\n",
            "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-xxlarge-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glBvJxLh2Hxn",
        "outputId": "59da3cf3-ac95-48f4-a8c8-df9df40c4daf"
      },
      "source": [
        "print(albert_config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AlbertConfig {\n",
            "  \"_name_or_path\": \"albert-xxlarge-v2\",\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 16384,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"layers_to_keep\": [],\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 64,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIvDuyzY2Iq2",
        "outputId": "fbe05634-497e-469a-d661-758df5d54be8"
      },
      "source": [
        "print(albert)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AlbertForQuestionAnswering(\n",
            "  (albert): AlbertModel(\n",
            "    (embeddings): AlbertEmbeddings(\n",
            "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 128)\n",
            "      (token_type_embeddings): Embedding(2, 128)\n",
            "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "    (encoder): AlbertTransformer(\n",
            "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=4096, bias=True)\n",
            "      (albert_layer_groups): ModuleList(\n",
            "        (0): AlbertLayerGroup(\n",
            "          (albert_layers): ModuleList(\n",
            "            (0): AlbertLayer(\n",
            "              (full_layer_layer_norm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
            "              (attention): AlbertAttention(\n",
            "                (query): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "                (key): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "                (value): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "                (attention_dropout): Dropout(p=0, inplace=False)\n",
            "                (output_dropout): Dropout(p=0, inplace=False)\n",
            "                (dense): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "                (LayerNorm): LayerNorm((4096,), eps=1e-12, elementwise_affine=True)\n",
            "              )\n",
            "              (ffn): Linear(in_features=4096, out_features=16384, bias=True)\n",
            "              (ffn_output): Linear(in_features=16384, out_features=4096, bias=True)\n",
            "              (dropout): Dropout(p=0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (qa_outputs): Linear(in_features=4096, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKTi9RQ22TXO",
        "outputId": "2b1dcade-dcc7-411e-8975-22ee6cf915ee"
      },
      "source": [
        "!pip uninstall transformers -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: transformers 4.10.3\n",
            "Uninstalling transformers-4.10.3:\n",
            "  Successfully uninstalled transformers-4.10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLs1j8aQ2vPN",
        "outputId": "2590f833-0841-4c9a-821a-5c558039c001"
      },
      "source": [
        "%cd /content/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name albert_squad_hyper_column_skip_attention \\\n",
        "    --train-datasets squad01 \\\n",
        "    --eval-datasets race01 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert\n",
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.decoder.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.dense.weight', 'predictions.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.bias']\n",
            "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'albert.encoder.albert_layer_groups.0.albert_layers.0.hyper_column_layer.dense.bias', 'albert.encoder.albert_layer_groups.0.albert_layers.0.hyper_column_layer.dense.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[09.28.21 10:44:42] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"albert_squad_hyper_column_skip_attention\",\n",
            "    \"save_dir\": \"save/albert_squad_hyper_column_skip_attention-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.28.21 10:44:42] Preparing Training Data...\n",
            "100% 16037/16037 [00:00<00:00, 22030.80it/s]\n",
            "Preprocessing not completely accurate for 25/16037 instances\n",
            "[09.28.21 10:44:48] Preparing Validation Data...\n",
            "100% 4453/4453 [00:00<00:00, 21715.24it/s]\n",
            "[09.28.21 10:44:53] Epoch: 0\n",
            "  0% 0/16037 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[09.28.21 10:44:54] Evaluating at step 0...\n",
            "  0% 16/16037 [00:01<22:35, 11.82it/s, NLL=5.99, epoch=0]\n",
            "  0% 0/4453 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4453 [00:00<01:00, 73.21it/s]\u001b[A\n",
            "  1% 48/4453 [00:00<01:25, 51.50it/s]\u001b[A\n",
            "  1% 64/4453 [00:01<01:38, 44.56it/s]\u001b[A\n",
            "  2% 80/4453 [00:01<01:45, 41.46it/s]\u001b[A\n",
            "  2% 96/4453 [00:02<01:50, 39.49it/s]\u001b[A\n",
            "  3% 112/4453 [00:02<01:52, 38.66it/s]\u001b[A\n",
            "  3% 128/4453 [00:03<01:54, 37.74it/s]\u001b[A\n",
            "  3% 144/4453 [00:03<01:54, 37.48it/s]\u001b[A\n",
            "  4% 160/4453 [00:03<01:55, 37.07it/s]\u001b[A\n",
            "  4% 176/4453 [00:04<01:56, 36.75it/s]\u001b[A\n",
            "  4% 192/4453 [00:04<01:55, 36.75it/s]\u001b[A\n",
            "  5% 208/4453 [00:05<01:56, 36.43it/s]\u001b[A\n",
            "  5% 224/4453 [00:05<01:56, 36.29it/s]\u001b[A\n",
            "  5% 240/4453 [00:06<01:56, 36.10it/s]\u001b[A\n",
            "  6% 256/4453 [00:06<01:56, 35.95it/s]\u001b[A\n",
            "  6% 272/4453 [00:07<01:56, 35.88it/s]\u001b[A\n",
            "  6% 288/4453 [00:07<01:56, 35.72it/s]\u001b[A\n",
            "  7% 304/4453 [00:07<01:55, 35.78it/s]\u001b[A\n",
            "  7% 320/4453 [00:08<01:55, 35.77it/s]\u001b[A\n",
            "  8% 336/4453 [00:08<01:55, 35.76it/s]\u001b[A\n",
            "  8% 352/4453 [00:09<01:54, 35.82it/s]\u001b[A\n",
            "  8% 368/4453 [00:09<01:54, 35.70it/s]\u001b[A\n",
            "  9% 384/4453 [00:10<01:53, 35.86it/s]\u001b[A\n",
            "  9% 400/4453 [00:10<01:53, 35.78it/s]\u001b[A\n",
            "  9% 416/4453 [00:11<01:53, 35.72it/s]\u001b[A\n",
            " 10% 432/4453 [00:11<01:52, 35.62it/s]\u001b[A\n",
            " 10% 448/4453 [00:12<01:52, 35.57it/s]\u001b[A\n",
            " 10% 464/4453 [00:12<01:51, 35.63it/s]\u001b[A\n",
            "  0% 16/16037 [00:14<22:35, 11.82it/s, NLL=5.99, epoch=0]\n",
            " 11% 496/4453 [00:13<01:51, 35.50it/s]\u001b[A\n",
            " 11% 512/4453 [00:13<01:51, 35.48it/s]\u001b[A\n",
            " 12% 528/4453 [00:14<01:50, 35.41it/s]\u001b[A\n",
            " 12% 544/4453 [00:14<01:50, 35.42it/s]\u001b[A\n",
            " 13% 560/4453 [00:15<01:49, 35.43it/s]\u001b[A\n",
            " 13% 576/4453 [00:15<01:49, 35.37it/s]\u001b[A\n",
            " 13% 592/4453 [00:16<01:49, 35.36it/s]\u001b[A\n",
            " 14% 608/4453 [00:16<01:49, 35.22it/s]\u001b[A\n",
            " 14% 624/4453 [00:16<01:48, 35.22it/s]\u001b[A\n",
            " 14% 640/4453 [00:17<01:48, 35.08it/s]\u001b[A\n",
            " 15% 656/4453 [00:17<01:48, 35.16it/s]\u001b[A\n",
            " 15% 672/4453 [00:18<01:47, 35.17it/s]\u001b[A\n",
            " 15% 688/4453 [00:18<01:47, 35.12it/s]\u001b[A\n",
            " 16% 704/4453 [00:19<01:46, 35.04it/s]\u001b[A\n",
            " 16% 720/4453 [00:19<01:46, 35.01it/s]\u001b[A\n",
            " 17% 736/4453 [00:20<01:46, 34.99it/s]\u001b[A\n",
            " 17% 752/4453 [00:20<01:45, 35.03it/s]\u001b[A\n",
            " 17% 768/4453 [00:21<01:44, 35.10it/s]\u001b[A\n",
            " 18% 784/4453 [00:21<01:44, 35.13it/s]\u001b[A\n",
            " 18% 800/4453 [00:22<01:44, 35.05it/s]\u001b[A\n",
            " 18% 816/4453 [00:22<01:44, 34.96it/s]\u001b[A\n",
            " 19% 832/4453 [00:22<01:43, 34.98it/s]\u001b[A\n",
            " 19% 848/4453 [00:23<01:43, 34.85it/s]\u001b[A\n",
            " 19% 864/4453 [00:23<01:43, 34.81it/s]\u001b[A\n",
            " 20% 880/4453 [00:24<01:42, 34.91it/s]\u001b[A\n",
            " 20% 896/4453 [00:24<01:42, 34.83it/s]\u001b[A\n",
            " 20% 912/4453 [00:25<01:41, 34.87it/s]\u001b[A\n",
            " 21% 928/4453 [00:25<01:41, 34.83it/s]\u001b[A\n",
            " 21% 944/4453 [00:26<01:41, 34.72it/s]\u001b[A\n",
            " 22% 960/4453 [00:26<01:40, 34.59it/s]\u001b[A\n",
            " 22% 976/4453 [00:27<01:40, 34.63it/s]\u001b[A\n",
            " 22% 992/4453 [00:27<01:39, 34.61it/s]\u001b[A\n",
            " 23% 1008/4453 [00:27<01:39, 34.57it/s]\u001b[A\n",
            " 23% 1024/4453 [00:28<01:39, 34.54it/s]\u001b[A\n",
            " 23% 1040/4453 [00:28<01:39, 34.46it/s]\u001b[A\n",
            " 24% 1056/4453 [00:29<01:38, 34.38it/s]\u001b[A\n",
            " 24% 1072/4453 [00:29<01:38, 34.42it/s]\u001b[A\n",
            " 24% 1088/4453 [00:30<01:37, 34.38it/s]\u001b[A\n",
            " 25% 1104/4453 [00:30<01:37, 34.31it/s]\u001b[A\n",
            " 25% 1120/4453 [00:31<01:37, 34.25it/s]\u001b[A\n",
            " 26% 1136/4453 [00:31<01:37, 34.17it/s]\u001b[A\n",
            " 26% 1152/4453 [00:32<01:36, 34.28it/s]\u001b[A\n",
            " 26% 1168/4453 [00:32<01:35, 34.26it/s]\u001b[A\n",
            " 27% 1184/4453 [00:33<01:35, 34.20it/s]\u001b[A\n",
            " 27% 1200/4453 [00:33<01:35, 34.23it/s]\u001b[A\n",
            " 27% 1216/4453 [00:34<01:34, 34.21it/s]\u001b[A\n",
            " 28% 1232/4453 [00:34<01:33, 34.45it/s]\u001b[A\n",
            " 28% 1248/4453 [00:34<01:32, 34.50it/s]\u001b[A\n",
            " 28% 1264/4453 [00:35<01:32, 34.53it/s]\u001b[A\n",
            " 29% 1280/4453 [00:35<01:31, 34.54it/s]\u001b[A\n",
            " 29% 1296/4453 [00:36<01:31, 34.58it/s]\u001b[A\n",
            " 29% 1312/4453 [00:36<01:31, 34.47it/s]\u001b[A\n",
            " 30% 1328/4453 [00:37<01:30, 34.54it/s]\u001b[A\n",
            " 30% 1344/4453 [00:37<01:30, 34.49it/s]\u001b[A\n",
            " 31% 1360/4453 [00:38<01:29, 34.40it/s]\u001b[A\n",
            " 31% 1376/4453 [00:38<01:29, 34.32it/s]\u001b[A\n",
            " 31% 1392/4453 [00:39<01:29, 34.27it/s]\u001b[A\n",
            " 32% 1408/4453 [00:39<01:28, 34.28it/s]\u001b[A\n",
            " 32% 1424/4453 [00:40<01:28, 34.22it/s]\u001b[A\n",
            " 32% 1440/4453 [00:40<01:28, 34.22it/s]\u001b[A\n",
            " 33% 1456/4453 [00:41<01:27, 34.16it/s]\u001b[A\n",
            " 33% 1472/4453 [00:41<01:27, 34.02it/s]\u001b[A\n",
            " 33% 1488/4453 [00:41<01:27, 33.98it/s]\u001b[A\n",
            " 34% 1504/4453 [00:42<01:26, 34.04it/s]\u001b[A\n",
            " 34% 1520/4453 [00:42<01:26, 33.93it/s]\u001b[A\n",
            " 34% 1536/4453 [00:43<01:25, 33.93it/s]\u001b[A\n",
            " 35% 1552/4453 [00:43<01:25, 33.99it/s]\u001b[A\n",
            " 35% 1568/4453 [00:44<01:25, 33.94it/s]\u001b[A\n",
            " 36% 1584/4453 [00:44<01:24, 33.89it/s]\u001b[A\n",
            " 36% 1600/4453 [00:45<01:24, 33.91it/s]\u001b[A\n",
            " 36% 1616/4453 [00:45<01:23, 33.81it/s]\u001b[A\n",
            " 37% 1632/4453 [00:46<01:23, 33.65it/s]\u001b[A\n",
            " 37% 1648/4453 [00:46<01:23, 33.79it/s]\u001b[A\n",
            " 37% 1664/4453 [00:47<01:22, 33.61it/s]\u001b[A\n",
            " 38% 1680/4453 [00:47<01:22, 33.53it/s]\u001b[A\n",
            " 38% 1696/4453 [00:48<01:22, 33.45it/s]\u001b[A\n",
            " 38% 1712/4453 [00:48<01:22, 33.38it/s]\u001b[A\n",
            " 39% 1728/4453 [00:49<01:21, 33.29it/s]\u001b[A\n",
            " 39% 1744/4453 [00:49<01:21, 33.34it/s]\u001b[A\n",
            " 40% 1760/4453 [00:50<01:20, 33.33it/s]\u001b[A\n",
            " 40% 1776/4453 [00:50<01:20, 33.13it/s]\u001b[A\n",
            " 40% 1792/4453 [00:51<01:20, 33.10it/s]\u001b[A\n",
            " 41% 1808/4453 [00:51<01:20, 32.98it/s]\u001b[A\n",
            " 41% 1824/4453 [00:52<01:19, 33.17it/s]\u001b[A\n",
            " 41% 1840/4453 [00:52<01:19, 33.05it/s]\u001b[A\n",
            " 42% 1856/4453 [00:53<01:18, 33.03it/s]\u001b[A\n",
            " 42% 1872/4453 [00:53<01:18, 32.91it/s]\u001b[A\n",
            " 42% 1888/4453 [00:53<01:17, 32.94it/s]\u001b[A\n",
            " 43% 1904/4453 [00:54<01:17, 32.92it/s]\u001b[A\n",
            " 43% 1920/4453 [00:54<01:17, 32.84it/s]\u001b[A\n",
            " 43% 1936/4453 [00:55<01:16, 32.83it/s]\u001b[A\n",
            " 44% 1952/4453 [00:55<01:16, 32.75it/s]\u001b[A\n",
            " 44% 1968/4453 [00:56<01:15, 32.79it/s]\u001b[A\n",
            " 45% 1984/4453 [00:56<01:15, 32.65it/s]\u001b[A\n",
            " 45% 2000/4453 [00:57<01:15, 32.60it/s]\u001b[A\n",
            " 45% 2016/4453 [00:57<01:14, 32.64it/s]\u001b[A\n",
            " 46% 2032/4453 [00:58<01:14, 32.53it/s]\u001b[A\n",
            " 46% 2048/4453 [00:58<01:13, 32.50it/s]\u001b[A\n",
            " 46% 2064/4453 [00:59<01:13, 32.49it/s]\u001b[A\n",
            " 47% 2080/4453 [00:59<01:13, 32.27it/s]\u001b[A\n",
            " 47% 2096/4453 [01:00<01:12, 32.31it/s]\u001b[A\n",
            " 47% 2112/4453 [01:00<01:12, 32.25it/s]\u001b[A\n",
            " 48% 2128/4453 [01:01<01:12, 32.28it/s]\u001b[A\n",
            " 48% 2144/4453 [01:01<01:11, 32.18it/s]\u001b[A\n",
            " 49% 2160/4453 [01:02<01:11, 32.23it/s]\u001b[A\n",
            " 49% 2176/4453 [01:02<01:10, 32.14it/s]\u001b[A\n",
            " 49% 2192/4453 [01:03<01:10, 32.20it/s]\u001b[A\n",
            " 50% 2208/4453 [01:03<01:10, 31.94it/s]\u001b[A\n",
            " 50% 2224/4453 [01:04<01:09, 32.03it/s]\u001b[A\n",
            " 50% 2240/4453 [01:04<01:08, 32.13it/s]\u001b[A\n",
            " 51% 2256/4453 [01:05<01:08, 32.06it/s]\u001b[A\n",
            " 51% 2272/4453 [01:05<01:07, 32.18it/s]\u001b[A\n",
            " 51% 2288/4453 [01:06<01:07, 32.08it/s]\u001b[A\n",
            " 52% 2304/4453 [01:06<01:06, 32.17it/s]\u001b[A\n",
            " 52% 2320/4453 [01:07<01:06, 32.29it/s]\u001b[A\n",
            " 52% 2336/4453 [01:07<01:05, 32.32it/s]\u001b[A\n",
            " 53% 2352/4453 [01:08<01:05, 32.24it/s]\u001b[A\n",
            " 53% 2368/4453 [01:08<01:04, 32.18it/s]\u001b[A\n",
            " 54% 2384/4453 [01:09<01:04, 32.13it/s]\u001b[A\n",
            " 54% 2400/4453 [01:09<01:03, 32.11it/s]\u001b[A\n",
            " 54% 2416/4453 [01:10<01:03, 32.14it/s]\u001b[A\n",
            " 55% 2432/4453 [01:10<01:03, 31.96it/s]\u001b[A\n",
            " 55% 2448/4453 [01:11<01:02, 31.94it/s]\u001b[A\n",
            " 55% 2464/4453 [01:11<01:02, 32.04it/s]\u001b[A\n",
            " 56% 2480/4453 [01:12<01:01, 32.01it/s]\u001b[A\n",
            " 56% 2496/4453 [01:12<01:01, 32.07it/s]\u001b[A\n",
            " 56% 2512/4453 [01:13<01:00, 32.04it/s]\u001b[A\n",
            " 57% 2528/4453 [01:13<00:59, 32.10it/s]\u001b[A\n",
            " 57% 2544/4453 [01:14<00:59, 31.93it/s]\u001b[A\n",
            " 57% 2560/4453 [01:14<00:59, 31.89it/s]\u001b[A\n",
            " 58% 2576/4453 [01:15<00:58, 32.03it/s]\u001b[A\n",
            " 58% 2592/4453 [01:15<00:58, 31.85it/s]\u001b[A\n",
            " 59% 2608/4453 [01:16<00:58, 31.74it/s]\u001b[A\n",
            " 59% 2624/4453 [01:16<00:57, 31.65it/s]\u001b[A\n",
            " 59% 2640/4453 [01:17<00:57, 31.63it/s]\u001b[A\n",
            " 60% 2656/4453 [01:17<00:57, 31.51it/s]\u001b[A\n",
            " 60% 2672/4453 [01:18<00:56, 31.56it/s]\u001b[A\n",
            " 60% 2688/4453 [01:18<00:55, 31.54it/s]\u001b[A\n",
            " 61% 2704/4453 [01:19<00:55, 31.54it/s]\u001b[A\n",
            " 61% 2720/4453 [01:19<00:54, 31.53it/s]\u001b[A\n",
            " 61% 2736/4453 [01:20<00:54, 31.56it/s]\u001b[A\n",
            " 62% 2752/4453 [01:20<00:53, 31.55it/s]\u001b[A\n",
            " 62% 2768/4453 [01:21<00:53, 31.53it/s]\u001b[A\n",
            " 63% 2784/4453 [01:21<00:52, 31.50it/s]\u001b[A\n",
            " 63% 2800/4453 [01:22<00:52, 31.49it/s]\u001b[A\n",
            " 63% 2816/4453 [01:22<00:51, 31.54it/s]\u001b[A\n",
            " 64% 2832/4453 [01:23<00:51, 31.50it/s]\u001b[A\n",
            " 64% 2848/4453 [01:23<00:50, 31.57it/s]\u001b[A\n",
            " 64% 2864/4453 [01:24<00:50, 31.58it/s]\u001b[A\n",
            " 65% 2880/4453 [01:24<00:50, 31.34it/s]\u001b[A\n",
            " 65% 2896/4453 [01:25<00:49, 31.22it/s]\u001b[A\n",
            " 65% 2912/4453 [01:26<00:49, 31.02it/s]\u001b[A\n",
            " 66% 2928/4453 [01:26<00:49, 31.03it/s]\u001b[A\n",
            " 66% 2944/4453 [01:27<00:48, 30.83it/s]\u001b[A\n",
            " 66% 2960/4453 [01:27<00:48, 30.87it/s]\u001b[A\n",
            " 67% 2976/4453 [01:28<00:47, 30.82it/s]\u001b[A\n",
            " 67% 2992/4453 [01:28<00:47, 30.91it/s]\u001b[A\n",
            " 68% 3008/4453 [01:29<00:46, 30.79it/s]\u001b[A\n",
            " 68% 3024/4453 [01:29<00:46, 30.89it/s]\u001b[A\n",
            " 68% 3040/4453 [01:30<00:46, 30.57it/s]\u001b[A\n",
            " 69% 3056/4453 [01:30<00:45, 30.63it/s]\u001b[A\n",
            " 69% 3072/4453 [01:31<00:45, 30.60it/s]\u001b[A\n",
            " 69% 3088/4453 [01:31<00:44, 30.66it/s]\u001b[A\n",
            " 70% 3104/4453 [01:32<00:44, 30.48it/s]\u001b[A\n",
            " 70% 3120/4453 [01:32<00:43, 30.43it/s]\u001b[A\n",
            " 70% 3136/4453 [01:33<00:43, 30.40it/s]\u001b[A\n",
            " 71% 3152/4453 [01:33<00:42, 30.30it/s]\u001b[A\n",
            " 71% 3168/4453 [01:34<00:42, 30.16it/s]\u001b[A\n",
            " 72% 3184/4453 [01:34<00:42, 30.12it/s]\u001b[A\n",
            " 72% 3200/4453 [01:35<00:41, 30.02it/s]\u001b[A\n",
            " 72% 3216/4453 [01:36<00:41, 29.97it/s]\u001b[A\n",
            " 73% 3232/4453 [01:36<00:40, 29.91it/s]\u001b[A\n",
            " 73% 3248/4453 [01:37<00:40, 29.81it/s]\u001b[A\n",
            " 73% 3264/4453 [01:37<00:39, 29.90it/s]\u001b[A\n",
            " 74% 3280/4453 [01:38<00:39, 29.72it/s]\u001b[A\n",
            " 74% 3296/4453 [01:38<00:38, 29.72it/s]\u001b[A\n",
            " 74% 3312/4453 [01:39<00:38, 29.60it/s]\u001b[A\n",
            " 75% 3328/4453 [01:39<00:37, 29.61it/s]\u001b[A\n",
            " 75% 3344/4453 [01:40<00:37, 29.50it/s]\u001b[A\n",
            " 75% 3360/4453 [01:40<00:37, 29.46it/s]\u001b[A\n",
            " 76% 3376/4453 [01:41<00:36, 29.28it/s]\u001b[A\n",
            " 76% 3392/4453 [01:41<00:36, 29.31it/s]\u001b[A\n",
            " 77% 3408/4453 [01:42<00:35, 29.28it/s]\u001b[A\n",
            " 77% 3424/4453 [01:43<00:35, 29.16it/s]\u001b[A\n",
            " 77% 3440/4453 [01:43<00:34, 29.13it/s]\u001b[A\n",
            " 78% 3456/4453 [01:44<00:34, 29.11it/s]\u001b[A\n",
            " 78% 3472/4453 [01:44<00:33, 29.08it/s]\u001b[A\n",
            " 78% 3488/4453 [01:45<00:33, 29.06it/s]\u001b[A\n",
            " 79% 3504/4453 [01:45<00:32, 29.08it/s]\u001b[A\n",
            " 79% 3520/4453 [01:46<00:32, 28.99it/s]\u001b[A\n",
            " 79% 3536/4453 [01:46<00:31, 28.90it/s]\u001b[A\n",
            " 80% 3552/4453 [01:47<00:31, 28.81it/s]\u001b[A\n",
            " 80% 3568/4453 [01:48<00:30, 28.78it/s]\u001b[A\n",
            " 80% 3584/4453 [01:48<00:30, 28.78it/s]\u001b[A\n",
            " 81% 3600/4453 [01:49<00:29, 28.63it/s]\u001b[A\n",
            " 81% 3616/4453 [01:49<00:29, 28.65it/s]\u001b[A\n",
            " 82% 3632/4453 [01:50<00:28, 28.57it/s]\u001b[A\n",
            " 82% 3648/4453 [01:50<00:28, 28.57it/s]\u001b[A\n",
            " 82% 3664/4453 [01:51<00:27, 28.49it/s]\u001b[A\n",
            " 83% 3680/4453 [01:52<00:27, 28.50it/s]\u001b[A\n",
            " 83% 3696/4453 [01:52<00:26, 28.48it/s]\u001b[A\n",
            " 83% 3712/4453 [01:53<00:26, 28.38it/s]\u001b[A\n",
            " 84% 3728/4453 [01:53<00:25, 28.37it/s]\u001b[A\n",
            " 84% 3744/4453 [01:54<00:25, 28.33it/s]\u001b[A\n",
            " 84% 3760/4453 [01:54<00:24, 28.27it/s]\u001b[A\n",
            " 85% 3776/4453 [01:55<00:23, 28.33it/s]\u001b[A\n",
            " 85% 3792/4453 [01:55<00:23, 28.29it/s]\u001b[A\n",
            " 86% 3808/4453 [01:56<00:22, 28.20it/s]\u001b[A\n",
            " 86% 3824/4453 [01:57<00:22, 28.24it/s]\u001b[A\n",
            " 86% 3840/4453 [01:57<00:21, 28.24it/s]\u001b[A\n",
            " 87% 3856/4453 [01:58<00:21, 28.17it/s]\u001b[A\n",
            " 87% 3872/4453 [01:58<00:20, 28.23it/s]\u001b[A\n",
            " 87% 3888/4453 [01:59<00:20, 28.25it/s]\u001b[A\n",
            " 88% 3904/4453 [01:59<00:19, 28.17it/s]\u001b[A\n",
            " 88% 3920/4453 [02:00<00:18, 28.17it/s]\u001b[A\n",
            " 88% 3936/4453 [02:01<00:18, 28.23it/s]\u001b[A\n",
            " 89% 3952/4453 [02:01<00:17, 28.15it/s]\u001b[A\n",
            " 89% 3968/4453 [02:02<00:17, 28.24it/s]\u001b[A\n",
            " 89% 3984/4453 [02:02<00:16, 28.34it/s]\u001b[A\n",
            " 90% 4000/4453 [02:03<00:15, 28.44it/s]\u001b[A\n",
            " 90% 4016/4453 [02:03<00:15, 28.42it/s]\u001b[A\n",
            " 91% 4032/4453 [02:04<00:14, 28.56it/s]\u001b[A\n",
            " 91% 4048/4453 [02:05<00:14, 28.52it/s]\u001b[A\n",
            " 91% 4064/4453 [02:05<00:13, 28.57it/s]\u001b[A\n",
            " 92% 4080/4453 [02:06<00:13, 28.63it/s]\u001b[A\n",
            " 92% 4096/4453 [02:06<00:12, 28.69it/s]\u001b[A\n",
            " 92% 4112/4453 [02:07<00:11, 28.70it/s]\u001b[A\n",
            " 93% 4128/4453 [02:07<00:11, 28.80it/s]\u001b[A\n",
            " 93% 4144/4453 [02:08<00:10, 28.88it/s]\u001b[A\n",
            " 93% 4160/4453 [02:08<00:10, 28.88it/s]\u001b[A\n",
            " 94% 4176/4453 [02:09<00:09, 28.89it/s]\u001b[A\n",
            " 94% 4192/4453 [02:10<00:09, 28.87it/s]\u001b[A\n",
            " 94% 4208/4453 [02:10<00:08, 28.83it/s]\u001b[A\n",
            " 95% 4224/4453 [02:11<00:07, 28.94it/s]\u001b[A\n",
            " 95% 4240/4453 [02:11<00:07, 28.82it/s]\u001b[A\n",
            " 96% 4256/4453 [02:12<00:06, 28.85it/s]\u001b[A\n",
            " 96% 4272/4453 [02:12<00:06, 28.89it/s]\u001b[A\n",
            " 96% 4288/4453 [02:13<00:05, 28.92it/s]\u001b[A\n",
            " 97% 4304/4453 [02:13<00:05, 28.93it/s]\u001b[A\n",
            " 97% 4320/4453 [02:14<00:04, 28.97it/s]\u001b[A\n",
            " 97% 4336/4453 [02:14<00:04, 29.02it/s]\u001b[A\n",
            " 98% 4352/4453 [02:15<00:03, 29.17it/s]\u001b[A\n",
            " 98% 4368/4453 [02:16<00:02, 29.09it/s]\u001b[A\n",
            " 98% 4384/4453 [02:16<00:02, 29.23it/s]\u001b[A\n",
            " 99% 4400/4453 [02:17<00:01, 29.20it/s]\u001b[A\n",
            " 99% 4416/4453 [02:17<00:01, 29.23it/s]\u001b[A\n",
            "100% 4432/4453 [02:18<00:00, 29.25it/s]\u001b[A\n",
            "100% 4448/4453 [02:18<00:00, 29.28it/s]\u001b[A\n",
            "100% 4453/4453 [02:19<00:00, 31.95it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 287/4306 [00:00<00:01, 2864.88it/s]\u001b[A\n",
            " 14% 588/4306 [00:00<00:01, 2949.24it/s]\u001b[A\n",
            " 21% 910/4306 [00:00<00:01, 3072.41it/s]\u001b[A\n",
            " 29% 1229/4306 [00:00<00:00, 3115.16it/s]\u001b[A\n",
            " 36% 1548/4306 [00:00<00:00, 3140.33it/s]\u001b[A\n",
            " 43% 1863/4306 [00:00<00:00, 3124.31it/s]\u001b[A\n",
            " 51% 2182/4306 [00:00<00:00, 3143.57it/s]\u001b[A\n",
            " 59% 2525/4306 [00:00<00:00, 3233.70it/s]\u001b[A\n",
            " 67% 2869/4306 [00:00<00:00, 3297.53it/s]\u001b[A\n",
            " 75% 3221/4306 [00:01<00:00, 3365.62it/s]\u001b[A\n",
            " 83% 3558/4306 [00:01<00:00, 3190.42it/s]\u001b[A\n",
            " 90% 3892/4306 [00:01<00:00, 3233.85it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 3167.85it/s]\n",
            "[09.28.21 10:47:16] Visualizing in TensorBoard...\n",
            "[09.28.21 10:47:16] Eval F1: 08.06, EM: 00.37\n",
            "100% 16037/16037 [27:45<00:00,  9.63it/s, NLL=0.313, epoch=0]\n",
            "[09.28.21 11:12:38] Epoch: 1\n",
            "100% 16037/16037 [25:29<00:00, 10.48it/s, NLL=0.213, epoch=1]\n",
            "[09.28.21 11:38:08] Epoch: 2\n",
            "100% 16037/16037 [25:28<00:00, 10.49it/s, NLL=0.598, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhsuPhRV6WXD",
        "outputId": "52f33a91-a178-4779-b37d-303d6a7791c1"
      },
      "source": [
        "%cd /content/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name albert_squad_hyper_column_skip_attention \\\n",
        "    --train-datasets squad02 \\\n",
        "    --eval-datasets race02 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert\n",
            "[09.28.21 12:03:42] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race02\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"albert_squad_hyper_column_skip_attention\",\n",
            "    \"save_dir\": \"save/albert_squad_hyper_column_skip_attention-02\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad02\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.28.21 12:03:42] Preparing Training Data...\n",
            "100% 17524/17524 [00:00<00:00, 21865.91it/s]\n",
            "Preprocessing not completely accurate for 16/17524 instances\n",
            "[09.28.21 12:03:50] Preparing Validation Data...\n",
            "100% 2982/2982 [00:00<00:00, 22380.07it/s]\n",
            "[09.28.21 12:03:54] Epoch: 0\n",
            "  0% 0/17524 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[09.28.21 12:03:55] Evaluating at step 0...\n",
            "  0% 16/17524 [00:01<27:48, 10.49it/s, NLL=1.51, epoch=0]\n",
            "  0% 0/2982 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/2982 [00:00<00:46, 63.88it/s]\u001b[A\n",
            "  2% 48/2982 [00:01<01:06, 44.06it/s]\u001b[A\n",
            "  2% 64/2982 [00:01<01:15, 38.79it/s]\u001b[A\n",
            "  3% 80/2982 [00:02<01:21, 35.52it/s]\u001b[A\n",
            "  3% 96/2982 [00:02<01:23, 34.42it/s]\u001b[A\n",
            "  4% 112/2982 [00:03<01:26, 33.14it/s]\u001b[A\n",
            "  4% 128/2982 [00:03<01:26, 32.89it/s]\u001b[A\n",
            "  5% 144/2982 [00:04<01:27, 32.31it/s]\u001b[A\n",
            "  5% 160/2982 [00:04<01:27, 32.13it/s]\u001b[A\n",
            "  6% 176/2982 [00:05<01:27, 31.93it/s]\u001b[A\n",
            "  6% 192/2982 [00:05<01:27, 31.98it/s]\u001b[A\n",
            "  7% 208/2982 [00:06<01:27, 31.62it/s]\u001b[A\n",
            "  8% 224/2982 [00:06<01:27, 31.62it/s]\u001b[A\n",
            "  8% 240/2982 [00:07<01:26, 31.58it/s]\u001b[A\n",
            "  9% 256/2982 [00:07<01:26, 31.57it/s]\u001b[A\n",
            "  9% 272/2982 [00:08<01:25, 31.51it/s]\u001b[A\n",
            " 10% 288/2982 [00:08<01:25, 31.56it/s]\u001b[A\n",
            " 10% 304/2982 [00:09<01:25, 31.26it/s]\u001b[A\n",
            " 11% 320/2982 [00:09<01:25, 31.25it/s]\u001b[A\n",
            " 11% 336/2982 [00:10<01:25, 31.06it/s]\u001b[A\n",
            " 12% 352/2982 [00:10<01:24, 31.07it/s]\u001b[A\n",
            " 12% 368/2982 [00:11<01:24, 30.85it/s]\u001b[A\n",
            " 13% 384/2982 [00:11<01:23, 31.05it/s]\u001b[A\n",
            " 13% 400/2982 [00:12<01:23, 30.88it/s]\u001b[A\n",
            " 14% 416/2982 [00:12<01:22, 31.03it/s]\u001b[A\n",
            "  0% 16/17524 [00:14<27:48, 10.49it/s, NLL=1.51, epoch=0]\n",
            " 15% 448/2982 [00:13<01:21, 31.02it/s]\u001b[A\n",
            " 16% 464/2982 [00:14<01:21, 30.80it/s]\u001b[A\n",
            " 16% 480/2982 [00:14<01:20, 31.03it/s]\u001b[A\n",
            " 17% 496/2982 [00:15<01:20, 30.81it/s]\u001b[A\n",
            " 17% 512/2982 [00:15<01:19, 31.02it/s]\u001b[A\n",
            " 18% 528/2982 [00:16<01:19, 30.86it/s]\u001b[A\n",
            " 18% 544/2982 [00:16<01:18, 31.03it/s]\u001b[A\n",
            " 19% 560/2982 [00:17<01:18, 30.88it/s]\u001b[A\n",
            " 19% 576/2982 [00:17<01:17, 31.05it/s]\u001b[A\n",
            " 20% 592/2982 [00:18<01:17, 30.78it/s]\u001b[A\n",
            " 20% 608/2982 [00:18<01:16, 31.02it/s]\u001b[A\n",
            " 21% 624/2982 [00:19<01:16, 30.83it/s]\u001b[A\n",
            " 21% 640/2982 [00:20<01:15, 31.01it/s]\u001b[A\n",
            " 22% 656/2982 [00:20<01:15, 30.82it/s]\u001b[A\n",
            " 23% 672/2982 [00:21<01:15, 30.70it/s]\u001b[A\n",
            " 23% 688/2982 [00:21<01:15, 30.55it/s]\u001b[A\n",
            " 24% 704/2982 [00:22<01:14, 30.48it/s]\u001b[A\n",
            " 24% 720/2982 [00:22<01:14, 30.44it/s]\u001b[A\n",
            " 25% 736/2982 [00:23<01:13, 30.37it/s]\u001b[A\n",
            " 25% 752/2982 [00:23<01:13, 30.38it/s]\u001b[A\n",
            " 26% 768/2982 [00:24<01:13, 30.27it/s]\u001b[A\n",
            " 26% 784/2982 [00:24<01:12, 30.37it/s]\u001b[A\n",
            " 27% 800/2982 [00:25<01:11, 30.35it/s]\u001b[A\n",
            " 27% 816/2982 [00:25<01:11, 30.35it/s]\u001b[A\n",
            " 28% 832/2982 [00:26<01:11, 30.24it/s]\u001b[A\n",
            " 28% 848/2982 [00:26<01:10, 30.35it/s]\u001b[A\n",
            " 29% 864/2982 [00:27<01:09, 30.32it/s]\u001b[A\n",
            " 30% 880/2982 [00:27<01:09, 30.29it/s]\u001b[A\n",
            " 30% 896/2982 [00:28<01:08, 30.34it/s]\u001b[A\n",
            " 31% 912/2982 [00:28<01:08, 30.32it/s]\u001b[A\n",
            " 31% 928/2982 [00:29<01:07, 30.26it/s]\u001b[A\n",
            " 32% 944/2982 [00:30<01:07, 30.34it/s]\u001b[A\n",
            " 32% 960/2982 [00:30<01:06, 30.34it/s]\u001b[A\n",
            " 33% 976/2982 [00:31<01:06, 30.26it/s]\u001b[A\n",
            " 33% 992/2982 [00:31<01:05, 30.25it/s]\u001b[A\n",
            " 34% 1008/2982 [00:32<01:05, 30.32it/s]\u001b[A\n",
            " 34% 1024/2982 [00:32<01:04, 30.24it/s]\u001b[A\n",
            " 35% 1040/2982 [00:33<01:03, 30.42it/s]\u001b[A\n",
            " 35% 1056/2982 [00:33<01:03, 30.35it/s]\u001b[A\n",
            " 36% 1072/2982 [00:34<01:03, 30.28it/s]\u001b[A\n",
            " 36% 1088/2982 [00:34<01:02, 30.25it/s]\u001b[A\n",
            " 37% 1104/2982 [00:35<01:01, 30.34it/s]\u001b[A\n",
            " 38% 1120/2982 [00:35<01:01, 30.28it/s]\u001b[A\n",
            " 38% 1136/2982 [00:36<01:00, 30.32it/s]\u001b[A\n",
            " 39% 1152/2982 [00:36<01:00, 30.19it/s]\u001b[A\n",
            " 39% 1168/2982 [00:37<00:59, 30.29it/s]\u001b[A\n",
            " 40% 1184/2982 [00:37<00:59, 30.19it/s]\u001b[A\n",
            " 40% 1200/2982 [00:38<00:59, 30.20it/s]\u001b[A\n",
            " 41% 1216/2982 [00:39<00:58, 30.10it/s]\u001b[A\n",
            " 41% 1232/2982 [00:39<00:58, 29.93it/s]\u001b[A\n",
            " 42% 1248/2982 [00:40<00:57, 30.05it/s]\u001b[A\n",
            " 42% 1264/2982 [00:40<00:57, 29.96it/s]\u001b[A\n",
            " 43% 1280/2982 [00:41<00:56, 29.99it/s]\u001b[A\n",
            " 43% 1296/2982 [00:41<00:56, 29.99it/s]\u001b[A\n",
            " 44% 1312/2982 [00:42<00:55, 30.00it/s]\u001b[A\n",
            " 45% 1328/2982 [00:42<00:55, 29.93it/s]\u001b[A\n",
            " 45% 1344/2982 [00:43<00:54, 30.06it/s]\u001b[A\n",
            " 46% 1360/2982 [00:43<00:54, 29.96it/s]\u001b[A\n",
            " 46% 1376/2982 [00:44<00:53, 29.97it/s]\u001b[A\n",
            " 47% 1392/2982 [00:44<00:53, 29.97it/s]\u001b[A\n",
            " 47% 1408/2982 [00:45<00:52, 30.01it/s]\u001b[A\n",
            " 48% 1424/2982 [00:45<00:51, 30.00it/s]\u001b[A\n",
            " 48% 1440/2982 [00:46<00:51, 29.96it/s]\u001b[A\n",
            " 49% 1456/2982 [00:47<00:50, 30.06it/s]\u001b[A\n",
            " 49% 1472/2982 [00:47<00:50, 30.09it/s]\u001b[A\n",
            " 50% 1488/2982 [00:48<00:49, 30.03it/s]\u001b[A\n",
            " 50% 1504/2982 [00:48<00:49, 30.05it/s]\u001b[A\n",
            " 51% 1520/2982 [00:49<00:48, 29.96it/s]\u001b[A\n",
            " 52% 1536/2982 [00:49<00:48, 30.07it/s]\u001b[A\n",
            " 52% 1552/2982 [00:50<00:47, 29.98it/s]\u001b[A\n",
            " 53% 1568/2982 [00:50<00:47, 30.00it/s]\u001b[A\n",
            " 53% 1584/2982 [00:51<00:46, 30.04it/s]\u001b[A\n",
            " 54% 1600/2982 [00:51<00:46, 30.02it/s]\u001b[A\n",
            " 54% 1616/2982 [00:52<00:45, 29.95it/s]\u001b[A\n",
            " 55% 1632/2982 [00:52<00:44, 30.06it/s]\u001b[A\n",
            " 55% 1648/2982 [00:53<00:44, 30.09it/s]\u001b[A\n",
            " 56% 1664/2982 [00:53<00:43, 29.99it/s]\u001b[A\n",
            " 56% 1680/2982 [00:54<00:43, 29.99it/s]\u001b[A\n",
            " 57% 1696/2982 [00:55<00:42, 30.01it/s]\u001b[A\n",
            " 57% 1712/2982 [00:55<00:42, 30.05it/s]\u001b[A\n",
            " 58% 1728/2982 [00:56<00:41, 29.93it/s]\u001b[A\n",
            " 58% 1744/2982 [00:56<00:41, 29.98it/s]\u001b[A\n",
            " 59% 1760/2982 [00:57<00:40, 29.96it/s]\u001b[A\n",
            " 60% 1776/2982 [00:57<00:40, 30.01it/s]\u001b[A\n",
            " 60% 1792/2982 [00:58<00:39, 29.97it/s]\u001b[A\n",
            " 61% 1808/2982 [00:58<00:39, 29.92it/s]\u001b[A\n",
            " 61% 1824/2982 [00:59<00:38, 29.88it/s]\u001b[A\n",
            " 62% 1840/2982 [00:59<00:38, 29.95it/s]\u001b[A\n",
            " 62% 1856/2982 [01:00<00:37, 29.87it/s]\u001b[A\n",
            " 63% 1872/2982 [01:00<00:37, 29.91it/s]\u001b[A\n",
            " 63% 1888/2982 [01:01<00:36, 29.98it/s]\u001b[A\n",
            " 64% 1904/2982 [01:01<00:35, 30.05it/s]\u001b[A\n",
            " 64% 1920/2982 [01:02<00:35, 30.05it/s]\u001b[A\n",
            " 65% 1936/2982 [01:03<00:34, 29.91it/s]\u001b[A\n",
            " 65% 1952/2982 [01:03<00:34, 29.98it/s]\u001b[A\n",
            " 66% 1968/2982 [01:04<00:33, 29.97it/s]\u001b[A\n",
            " 67% 1984/2982 [01:04<00:33, 29.89it/s]\u001b[A\n",
            " 67% 2000/2982 [01:05<00:32, 29.87it/s]\u001b[A\n",
            " 68% 2016/2982 [01:05<00:32, 29.89it/s]\u001b[A\n",
            " 68% 2032/2982 [01:06<00:31, 29.89it/s]\u001b[A\n",
            " 69% 2048/2982 [01:06<00:31, 29.89it/s]\u001b[A\n",
            " 69% 2064/2982 [01:07<00:30, 29.88it/s]\u001b[A\n",
            " 70% 2080/2982 [01:07<00:30, 29.83it/s]\u001b[A\n",
            " 70% 2096/2982 [01:08<00:29, 29.90it/s]\u001b[A\n",
            " 71% 2112/2982 [01:08<00:29, 29.95it/s]\u001b[A\n",
            " 71% 2128/2982 [01:09<00:28, 29.88it/s]\u001b[A\n",
            " 72% 2144/2982 [01:10<00:28, 29.91it/s]\u001b[A\n",
            " 72% 2160/2982 [01:10<00:27, 29.98it/s]\u001b[A\n",
            " 73% 2176/2982 [01:11<00:26, 30.00it/s]\u001b[A\n",
            " 74% 2192/2982 [01:11<00:26, 29.89it/s]\u001b[A\n",
            " 74% 2208/2982 [01:12<00:25, 29.99it/s]\u001b[A\n",
            " 75% 2224/2982 [01:12<00:25, 29.97it/s]\u001b[A\n",
            " 75% 2240/2982 [01:13<00:24, 29.90it/s]\u001b[A\n",
            " 76% 2256/2982 [01:13<00:24, 29.89it/s]\u001b[A\n",
            " 76% 2272/2982 [01:14<00:23, 29.94it/s]\u001b[A\n",
            " 77% 2288/2982 [01:14<00:23, 29.87it/s]\u001b[A\n",
            " 77% 2304/2982 [01:15<00:22, 29.90it/s]\u001b[A\n",
            " 78% 2320/2982 [01:15<00:22, 29.84it/s]\u001b[A\n",
            " 78% 2336/2982 [01:16<00:21, 29.86it/s]\u001b[A\n",
            " 79% 2352/2982 [01:16<00:21, 29.84it/s]\u001b[A\n",
            " 79% 2368/2982 [01:17<00:20, 29.90it/s]\u001b[A\n",
            " 80% 2384/2982 [01:18<00:20, 29.83it/s]\u001b[A\n",
            " 80% 2400/2982 [01:18<00:19, 29.91it/s]\u001b[A\n",
            " 81% 2416/2982 [01:19<00:18, 29.83it/s]\u001b[A\n",
            " 82% 2432/2982 [01:19<00:18, 29.85it/s]\u001b[A\n",
            " 82% 2448/2982 [01:20<00:17, 29.82it/s]\u001b[A\n",
            " 83% 2464/2982 [01:20<00:17, 29.87it/s]\u001b[A\n",
            " 83% 2480/2982 [01:21<00:16, 29.84it/s]\u001b[A\n",
            " 84% 2496/2982 [01:21<00:16, 29.86it/s]\u001b[A\n",
            " 84% 2512/2982 [01:22<00:15, 29.89it/s]\u001b[A\n",
            " 85% 2528/2982 [01:22<00:15, 29.93it/s]\u001b[A\n",
            " 85% 2544/2982 [01:23<00:14, 29.85it/s]\u001b[A\n",
            " 86% 2560/2982 [01:23<00:14, 29.79it/s]\u001b[A\n",
            " 86% 2576/2982 [01:24<00:13, 29.73it/s]\u001b[A\n",
            " 87% 2592/2982 [01:25<00:13, 29.74it/s]\u001b[A\n",
            " 87% 2608/2982 [01:25<00:12, 29.79it/s]\u001b[A\n",
            " 88% 2624/2982 [01:26<00:12, 29.79it/s]\u001b[A\n",
            " 89% 2640/2982 [01:26<00:11, 29.76it/s]\u001b[A\n",
            " 89% 2656/2982 [01:27<00:10, 29.70it/s]\u001b[A\n",
            " 90% 2672/2982 [01:27<00:10, 29.74it/s]\u001b[A\n",
            " 90% 2688/2982 [01:28<00:09, 29.79it/s]\u001b[A\n",
            " 91% 2704/2982 [01:28<00:09, 29.80it/s]\u001b[A\n",
            " 91% 2720/2982 [01:29<00:08, 29.79it/s]\u001b[A\n",
            " 92% 2736/2982 [01:29<00:08, 29.80it/s]\u001b[A\n",
            " 92% 2752/2982 [01:30<00:07, 29.77it/s]\u001b[A\n",
            " 93% 2768/2982 [01:30<00:07, 29.75it/s]\u001b[A\n",
            " 93% 2784/2982 [01:31<00:06, 29.80it/s]\u001b[A\n",
            " 94% 2800/2982 [01:32<00:06, 29.71it/s]\u001b[A\n",
            " 94% 2816/2982 [01:32<00:05, 29.71it/s]\u001b[A\n",
            " 95% 2832/2982 [01:33<00:05, 29.70it/s]\u001b[A\n",
            " 96% 2848/2982 [01:33<00:04, 29.69it/s]\u001b[A\n",
            " 96% 2864/2982 [01:34<00:03, 29.74it/s]\u001b[A\n",
            " 97% 2880/2982 [01:34<00:03, 29.71it/s]\u001b[A\n",
            " 97% 2896/2982 [01:35<00:02, 29.76it/s]\u001b[A\n",
            " 98% 2912/2982 [01:35<00:02, 29.75it/s]\u001b[A\n",
            " 98% 2928/2982 [01:36<00:01, 29.72it/s]\u001b[A\n",
            " 99% 2944/2982 [01:36<00:01, 29.71it/s]\u001b[A\n",
            " 99% 2960/2982 [01:37<00:00, 29.82it/s]\u001b[A\n",
            "100% 2976/2982 [01:37<00:00, 29.69it/s]\u001b[A\n",
            "100% 2982/2982 [01:38<00:00, 30.29it/s]\n",
            "\n",
            "  0% 0/2929 [00:00<?, ?it/s]\u001b[A\n",
            " 10% 292/2929 [00:00<00:00, 2908.02it/s]\u001b[A\n",
            " 21% 607/2929 [00:00<00:00, 3049.46it/s]\u001b[A\n",
            " 32% 938/2929 [00:00<00:00, 3167.87it/s]\u001b[A\n",
            " 43% 1258/2929 [00:00<00:00, 3178.53it/s]\u001b[A\n",
            " 54% 1588/2929 [00:00<00:00, 3219.36it/s]\u001b[A\n",
            " 66% 1921/2929 [00:00<00:00, 3253.17it/s]\u001b[A\n",
            " 77% 2247/2929 [00:00<00:00, 3208.17it/s]\u001b[A\n",
            " 88% 2582/2929 [00:00<00:00, 3250.26it/s]\u001b[A\n",
            "100% 2929/2929 [00:00<00:00, 3214.85it/s]\n",
            "[09.28.21 12:05:35] Visualizing in TensorBoard...\n",
            "[09.28.21 12:05:35] Eval F1: 81.12, EM: 67.02\n",
            "100% 17524/17524 [29:33<00:00,  9.88it/s, NLL=0.422, epoch=0]\n",
            "[09.28.21 12:33:27] Epoch: 1\n",
            "100% 17524/17524 [27:53<00:00, 10.47it/s, NLL=0.32, epoch=1]\n",
            "[09.28.21 13:01:21] Epoch: 2\n",
            "100% 17524/17524 [27:53<00:00, 10.47it/s, NLL=0.538, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo42Zq0-6Zvf",
        "outputId": "dd3c6453-3404-4807-e38d-6906302b48b2"
      },
      "source": [
        "%cd /content/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name albert_squad_hyper_column_skip_attention \\\n",
        "    --train-datasets squad03 \\\n",
        "    --eval-datasets race03 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert\n",
            "[09.28.21 13:29:20] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race03\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"albert_squad_hyper_column_skip_attention\",\n",
            "    \"save_dir\": \"save/albert_squad_hyper_column_skip_attention-03\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad03\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.28.21 13:29:20] Preparing Training Data...\n",
            "100% 17044/17044 [00:00<00:00, 21537.45it/s]\n",
            "Preprocessing not completely accurate for 15/17044 instances\n",
            "[09.28.21 13:29:27] Preparing Validation Data...\n",
            "100% 3373/3373 [00:00<00:00, 22487.08it/s]\n",
            "[09.28.21 13:29:32] Epoch: 0\n",
            "  0% 0/17044 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[09.28.21 13:29:33] Evaluating at step 0...\n",
            "  0% 16/17044 [00:01<26:25, 10.74it/s, NLL=2.12, epoch=0]\n",
            "  0% 0/3373 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/3373 [00:00<00:52, 63.79it/s]\u001b[A\n",
            "  1% 48/3373 [00:01<01:15, 44.19it/s]\u001b[A\n",
            "  2% 64/3373 [00:01<01:25, 38.81it/s]\u001b[A\n",
            "  2% 80/3373 [00:02<01:31, 35.89it/s]\u001b[A\n",
            "  3% 96/3373 [00:02<01:34, 34.55it/s]\u001b[A\n",
            "  3% 112/3373 [00:03<01:37, 33.59it/s]\u001b[A\n",
            "  4% 128/3373 [00:03<01:38, 33.07it/s]\u001b[A\n",
            "  4% 144/3373 [00:04<01:39, 32.58it/s]\u001b[A\n",
            "  5% 160/3373 [00:04<01:39, 32.28it/s]\u001b[A\n",
            "  5% 176/3373 [00:05<01:39, 32.00it/s]\u001b[A\n",
            "  6% 192/3373 [00:05<01:39, 31.87it/s]\u001b[A\n",
            "  6% 208/3373 [00:06<01:39, 31.78it/s]\u001b[A\n",
            "  7% 224/3373 [00:06<01:39, 31.66it/s]\u001b[A\n",
            "  7% 240/3373 [00:07<01:38, 31.66it/s]\u001b[A\n",
            "  8% 256/3373 [00:07<01:38, 31.64it/s]\u001b[A\n",
            "  8% 272/3373 [00:08<01:38, 31.61it/s]\u001b[A\n",
            "  9% 288/3373 [00:08<01:37, 31.58it/s]\u001b[A\n",
            "  9% 304/3373 [00:09<01:37, 31.60it/s]\u001b[A\n",
            "  9% 320/3373 [00:09<01:36, 31.56it/s]\u001b[A\n",
            " 10% 336/3373 [00:10<01:36, 31.54it/s]\u001b[A\n",
            " 10% 352/3373 [00:10<01:36, 31.46it/s]\u001b[A\n",
            " 11% 368/3373 [00:11<01:35, 31.55it/s]\u001b[A\n",
            " 11% 384/3373 [00:11<01:34, 31.53it/s]\u001b[A\n",
            " 12% 400/3373 [00:12<01:34, 31.54it/s]\u001b[A\n",
            " 12% 416/3373 [00:12<01:33, 31.50it/s]\u001b[A\n",
            "  0% 16/17044 [00:14<26:25, 10.74it/s, NLL=2.12, epoch=0]\n",
            " 13% 448/3373 [00:13<01:32, 31.45it/s]\u001b[A\n",
            " 14% 464/3373 [00:14<01:33, 31.22it/s]\u001b[A\n",
            " 14% 480/3373 [00:14<01:32, 31.22it/s]\u001b[A\n",
            " 15% 496/3373 [00:15<01:32, 30.98it/s]\u001b[A\n",
            " 15% 512/3373 [00:15<01:32, 31.04it/s]\u001b[A\n",
            " 16% 528/3373 [00:16<01:31, 31.02it/s]\u001b[A\n",
            " 16% 544/3373 [00:16<01:31, 31.00it/s]\u001b[A\n",
            " 17% 560/3373 [00:17<01:31, 30.80it/s]\u001b[A\n",
            " 17% 576/3373 [00:17<01:30, 30.79it/s]\u001b[A\n",
            " 18% 592/3373 [00:18<01:30, 30.64it/s]\u001b[A\n",
            " 18% 608/3373 [00:18<01:30, 30.72it/s]\u001b[A\n",
            " 18% 624/3373 [00:19<01:29, 30.65it/s]\u001b[A\n",
            " 19% 640/3373 [00:19<01:29, 30.66it/s]\u001b[A\n",
            " 19% 656/3373 [00:20<01:28, 30.67it/s]\u001b[A\n",
            " 20% 672/3373 [00:20<01:28, 30.63it/s]\u001b[A\n",
            " 20% 688/3373 [00:21<01:28, 30.51it/s]\u001b[A\n",
            " 21% 704/3373 [00:22<01:27, 30.50it/s]\u001b[A\n",
            " 21% 720/3373 [00:22<01:27, 30.49it/s]\u001b[A\n",
            " 22% 736/3373 [00:23<01:26, 30.43it/s]\u001b[A\n",
            " 22% 752/3373 [00:23<01:26, 30.46it/s]\u001b[A\n",
            " 23% 768/3373 [00:24<01:25, 30.53it/s]\u001b[A\n",
            " 23% 784/3373 [00:24<01:24, 30.58it/s]\u001b[A\n",
            " 24% 800/3373 [00:25<01:24, 30.48it/s]\u001b[A\n",
            " 24% 816/3373 [00:25<01:24, 30.40it/s]\u001b[A\n",
            " 25% 832/3373 [00:26<01:23, 30.38it/s]\u001b[A\n",
            " 25% 848/3373 [00:26<01:23, 30.39it/s]\u001b[A\n",
            " 26% 864/3373 [00:27<01:22, 30.31it/s]\u001b[A\n",
            " 26% 880/3373 [00:27<01:22, 30.32it/s]\u001b[A\n",
            " 27% 896/3373 [00:28<01:21, 30.38it/s]\u001b[A\n",
            " 27% 912/3373 [00:28<01:21, 30.36it/s]\u001b[A\n",
            " 28% 928/3373 [00:29<01:20, 30.36it/s]\u001b[A\n",
            " 28% 944/3373 [00:29<01:20, 30.20it/s]\u001b[A\n",
            " 28% 960/3373 [00:30<01:19, 30.37it/s]\u001b[A\n",
            " 29% 976/3373 [00:30<01:19, 30.33it/s]\u001b[A\n",
            " 29% 992/3373 [00:31<01:18, 30.34it/s]\u001b[A\n",
            " 30% 1008/3373 [00:32<01:18, 30.31it/s]\u001b[A\n",
            " 30% 1024/3373 [00:32<01:17, 30.33it/s]\u001b[A\n",
            " 31% 1040/3373 [00:33<01:16, 30.30it/s]\u001b[A\n",
            " 31% 1056/3373 [00:33<01:16, 30.30it/s]\u001b[A\n",
            " 32% 1072/3373 [00:34<01:15, 30.34it/s]\u001b[A\n",
            " 32% 1088/3373 [00:34<01:15, 30.28it/s]\u001b[A\n",
            " 33% 1104/3373 [00:35<01:14, 30.36it/s]\u001b[A\n",
            " 33% 1120/3373 [00:35<01:14, 30.32it/s]\u001b[A\n",
            " 34% 1136/3373 [00:36<01:13, 30.34it/s]\u001b[A\n",
            " 34% 1152/3373 [00:36<01:13, 30.32it/s]\u001b[A\n",
            " 35% 1168/3373 [00:37<01:12, 30.30it/s]\u001b[A\n",
            " 35% 1184/3373 [00:37<01:12, 30.32it/s]\u001b[A\n",
            " 36% 1200/3373 [00:38<01:11, 30.28it/s]\u001b[A\n",
            " 36% 1216/3373 [00:38<01:11, 30.31it/s]\u001b[A\n",
            " 37% 1232/3373 [00:39<01:10, 30.28it/s]\u001b[A\n",
            " 37% 1248/3373 [00:39<01:10, 30.33it/s]\u001b[A\n",
            " 37% 1264/3373 [00:40<01:09, 30.28it/s]\u001b[A\n",
            " 38% 1280/3373 [00:41<01:08, 30.38it/s]\u001b[A\n",
            " 38% 1296/3373 [00:41<01:08, 30.27it/s]\u001b[A\n",
            " 39% 1312/3373 [00:42<01:07, 30.34it/s]\u001b[A\n",
            " 39% 1328/3373 [00:42<01:07, 30.29it/s]\u001b[A\n",
            " 40% 1344/3373 [00:43<01:06, 30.36it/s]\u001b[A\n",
            " 40% 1360/3373 [00:43<01:06, 30.32it/s]\u001b[A\n",
            " 41% 1376/3373 [00:44<01:05, 30.39it/s]\u001b[A\n",
            " 41% 1392/3373 [00:44<01:05, 30.25it/s]\u001b[A\n",
            " 42% 1408/3373 [00:45<01:04, 30.39it/s]\u001b[A\n",
            " 42% 1424/3373 [00:45<01:04, 30.35it/s]\u001b[A\n",
            " 43% 1440/3373 [00:46<01:03, 30.33it/s]\u001b[A\n",
            " 43% 1456/3373 [00:46<01:03, 30.22it/s]\u001b[A\n",
            " 44% 1472/3373 [00:47<01:02, 30.30it/s]\u001b[A\n",
            " 44% 1488/3373 [00:47<01:02, 30.32it/s]\u001b[A\n",
            " 45% 1504/3373 [00:48<01:01, 30.40it/s]\u001b[A\n",
            " 45% 1520/3373 [00:48<01:01, 30.35it/s]\u001b[A\n",
            " 46% 1536/3373 [00:49<01:00, 30.22it/s]\u001b[A\n",
            " 46% 1552/3373 [00:49<00:59, 30.37it/s]\u001b[A\n",
            " 46% 1568/3373 [00:50<00:59, 30.24it/s]\u001b[A\n",
            " 47% 1584/3373 [00:51<00:58, 30.33it/s]\u001b[A\n",
            " 47% 1600/3373 [00:51<00:58, 30.35it/s]\u001b[A\n",
            " 48% 1616/3373 [00:52<00:57, 30.36it/s]\u001b[A\n",
            " 48% 1632/3373 [00:52<00:57, 30.20it/s]\u001b[A\n",
            " 49% 1648/3373 [00:53<00:56, 30.32it/s]\u001b[A\n",
            " 49% 1664/3373 [00:53<00:56, 30.31it/s]\u001b[A\n",
            " 50% 1680/3373 [00:54<00:55, 30.34it/s]\u001b[A\n",
            " 50% 1696/3373 [00:54<00:55, 30.31it/s]\u001b[A\n",
            " 51% 1712/3373 [00:55<00:54, 30.30it/s]\u001b[A\n",
            " 51% 1728/3373 [00:55<00:54, 30.33it/s]\u001b[A\n",
            " 52% 1744/3373 [00:56<00:53, 30.29it/s]\u001b[A\n",
            " 52% 1760/3373 [00:56<00:53, 30.27it/s]\u001b[A\n",
            " 53% 1776/3373 [00:57<00:52, 30.24it/s]\u001b[A\n",
            " 53% 1792/3373 [00:57<00:52, 30.25it/s]\u001b[A\n",
            " 54% 1808/3373 [00:58<00:51, 30.18it/s]\u001b[A\n",
            " 54% 1824/3373 [00:58<00:51, 30.12it/s]\u001b[A\n",
            " 55% 1840/3373 [00:59<00:50, 30.09it/s]\u001b[A\n",
            " 55% 1856/3373 [01:00<00:50, 30.09it/s]\u001b[A\n",
            " 55% 1872/3373 [01:00<00:50, 30.00it/s]\u001b[A\n",
            " 56% 1888/3373 [01:01<00:49, 30.01it/s]\u001b[A\n",
            " 56% 1904/3373 [01:01<00:48, 29.99it/s]\u001b[A\n",
            " 57% 1920/3373 [01:02<00:48, 30.04it/s]\u001b[A\n",
            " 57% 1936/3373 [01:02<00:47, 30.08it/s]\u001b[A\n",
            " 58% 1952/3373 [01:03<00:47, 30.07it/s]\u001b[A\n",
            " 58% 1968/3373 [01:03<00:46, 30.03it/s]\u001b[A\n",
            " 59% 1984/3373 [01:04<00:46, 30.07it/s]\u001b[A\n",
            " 59% 2000/3373 [01:04<00:45, 30.07it/s]\u001b[A\n",
            " 60% 2016/3373 [01:05<00:45, 30.01it/s]\u001b[A\n",
            " 60% 2032/3373 [01:05<00:44, 29.96it/s]\u001b[A\n",
            " 61% 2048/3373 [01:06<00:44, 29.97it/s]\u001b[A\n",
            " 61% 2064/3373 [01:06<00:43, 30.01it/s]\u001b[A\n",
            " 62% 2080/3373 [01:07<00:43, 30.01it/s]\u001b[A\n",
            " 62% 2096/3373 [01:08<00:42, 30.19it/s]\u001b[A\n",
            " 63% 2112/3373 [01:08<00:41, 30.02it/s]\u001b[A\n",
            " 63% 2128/3373 [01:09<00:41, 30.17it/s]\u001b[A\n",
            " 64% 2144/3373 [01:09<00:40, 30.10it/s]\u001b[A\n",
            " 64% 2160/3373 [01:10<00:40, 30.09it/s]\u001b[A\n",
            " 65% 2176/3373 [01:10<00:39, 30.08it/s]\u001b[A\n",
            " 65% 2192/3373 [01:11<00:39, 30.08it/s]\u001b[A\n",
            " 65% 2208/3373 [01:11<00:38, 30.09it/s]\u001b[A\n",
            " 66% 2224/3373 [01:12<00:38, 30.12it/s]\u001b[A\n",
            " 66% 2240/3373 [01:12<00:37, 30.07it/s]\u001b[A\n",
            " 67% 2256/3373 [01:13<00:37, 29.93it/s]\u001b[A\n",
            " 67% 2272/3373 [01:13<00:36, 30.03it/s]\u001b[A\n",
            " 68% 2288/3373 [01:14<00:36, 30.03it/s]\u001b[A\n",
            " 68% 2304/3373 [01:14<00:35, 29.99it/s]\u001b[A\n",
            " 69% 2320/3373 [01:15<00:35, 29.89it/s]\u001b[A\n",
            " 69% 2336/3373 [01:16<00:34, 29.95it/s]\u001b[A\n",
            " 70% 2352/3373 [01:16<00:34, 29.98it/s]\u001b[A\n",
            " 70% 2368/3373 [01:17<00:33, 29.98it/s]\u001b[A\n",
            " 71% 2384/3373 [01:17<00:33, 29.87it/s]\u001b[A\n",
            " 71% 2400/3373 [01:18<00:32, 29.92it/s]\u001b[A\n",
            " 72% 2416/3373 [01:18<00:31, 29.92it/s]\u001b[A\n",
            " 72% 2432/3373 [01:19<00:31, 29.91it/s]\u001b[A\n",
            " 73% 2448/3373 [01:19<00:30, 30.02it/s]\u001b[A\n",
            " 73% 2464/3373 [01:20<00:30, 29.95it/s]\u001b[A\n",
            " 74% 2480/3373 [01:20<00:29, 29.89it/s]\u001b[A\n",
            " 74% 2496/3373 [01:21<00:29, 29.92it/s]\u001b[A\n",
            " 74% 2512/3373 [01:21<00:28, 30.11it/s]\u001b[A\n",
            " 75% 2528/3373 [01:22<00:28, 30.02it/s]\u001b[A\n",
            " 75% 2544/3373 [01:22<00:27, 30.02it/s]\u001b[A\n",
            " 76% 2560/3373 [01:23<00:27, 29.98it/s]\u001b[A\n",
            " 76% 2576/3373 [01:24<00:26, 29.98it/s]\u001b[A\n",
            " 77% 2592/3373 [01:24<00:26, 29.91it/s]\u001b[A\n",
            " 77% 2608/3373 [01:25<00:25, 29.94it/s]\u001b[A\n",
            " 78% 2624/3373 [01:25<00:25, 29.95it/s]\u001b[A\n",
            " 78% 2640/3373 [01:26<00:24, 29.89it/s]\u001b[A\n",
            " 79% 2656/3373 [01:26<00:23, 29.89it/s]\u001b[A\n",
            " 79% 2672/3373 [01:27<00:23, 29.86it/s]\u001b[A\n",
            " 80% 2688/3373 [01:27<00:22, 29.81it/s]\u001b[A\n",
            " 80% 2704/3373 [01:28<00:22, 29.93it/s]\u001b[A\n",
            " 81% 2720/3373 [01:28<00:21, 29.88it/s]\u001b[A\n",
            " 81% 2736/3373 [01:29<00:21, 29.87it/s]\u001b[A\n",
            " 82% 2752/3373 [01:29<00:20, 29.89it/s]\u001b[A\n",
            " 82% 2768/3373 [01:30<00:20, 29.85it/s]\u001b[A\n",
            " 83% 2784/3373 [01:30<00:19, 29.79it/s]\u001b[A\n",
            " 83% 2800/3373 [01:31<00:19, 29.79it/s]\u001b[A\n",
            " 83% 2816/3373 [01:32<00:18, 29.78it/s]\u001b[A\n",
            " 84% 2832/3373 [01:32<00:18, 29.75it/s]\u001b[A\n",
            " 84% 2848/3373 [01:33<00:17, 29.83it/s]\u001b[A\n",
            " 85% 2864/3373 [01:33<00:17, 29.85it/s]\u001b[A\n",
            " 85% 2880/3373 [01:34<00:16, 29.80it/s]\u001b[A\n",
            " 86% 2896/3373 [01:34<00:15, 29.97it/s]\u001b[A\n",
            " 86% 2912/3373 [01:35<00:15, 29.87it/s]\u001b[A\n",
            " 87% 2928/3373 [01:35<00:14, 29.95it/s]\u001b[A\n",
            " 87% 2944/3373 [01:36<00:14, 29.84it/s]\u001b[A\n",
            " 88% 2960/3373 [01:36<00:13, 29.87it/s]\u001b[A\n",
            " 88% 2976/3373 [01:37<00:13, 29.82it/s]\u001b[A\n",
            " 89% 2992/3373 [01:37<00:12, 29.81it/s]\u001b[A\n",
            " 89% 3008/3373 [01:38<00:12, 29.83it/s]\u001b[A\n",
            " 90% 3024/3373 [01:39<00:11, 29.96it/s]\u001b[A\n",
            " 90% 3040/3373 [01:39<00:11, 29.84it/s]\u001b[A\n",
            " 91% 3056/3373 [01:40<00:10, 29.80it/s]\u001b[A\n",
            " 91% 3072/3373 [01:40<00:10, 29.84it/s]\u001b[A\n",
            " 92% 3088/3373 [01:41<00:09, 29.80it/s]\u001b[A\n",
            " 92% 3104/3373 [01:41<00:09, 29.77it/s]\u001b[A\n",
            " 92% 3120/3373 [01:42<00:08, 29.71it/s]\u001b[A\n",
            " 93% 3136/3373 [01:42<00:07, 29.84it/s]\u001b[A\n",
            " 93% 3152/3373 [01:43<00:07, 29.77it/s]\u001b[A\n",
            " 94% 3168/3373 [01:43<00:06, 29.74it/s]\u001b[A\n",
            " 94% 3184/3373 [01:44<00:06, 29.63it/s]\u001b[A\n",
            " 95% 3200/3373 [01:44<00:05, 29.66it/s]\u001b[A\n",
            " 95% 3216/3373 [01:45<00:05, 29.66it/s]\u001b[A\n",
            " 96% 3232/3373 [01:46<00:04, 29.79it/s]\u001b[A\n",
            " 96% 3248/3373 [01:46<00:04, 29.70it/s]\u001b[A\n",
            " 97% 3264/3373 [01:47<00:03, 29.68it/s]\u001b[A\n",
            " 97% 3280/3373 [01:47<00:03, 29.71it/s]\u001b[A\n",
            " 98% 3296/3373 [01:48<00:02, 29.62it/s]\u001b[A\n",
            " 98% 3312/3373 [01:48<00:02, 29.70it/s]\u001b[A\n",
            " 99% 3328/3373 [01:49<00:01, 29.65it/s]\u001b[A\n",
            " 99% 3344/3373 [01:49<00:00, 29.77it/s]\u001b[A\n",
            "100% 3360/3373 [01:50<00:00, 29.64it/s]\u001b[A\n",
            "100% 3373/3373 [01:50<00:00, 30.42it/s]\n",
            "\n",
            "  0% 0/3335 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 250/3335 [00:00<00:01, 2499.53it/s]\u001b[A\n",
            " 16% 520/3335 [00:00<00:01, 2615.36it/s]\u001b[A\n",
            " 24% 807/3335 [00:00<00:00, 2729.83it/s]\u001b[A\n",
            " 33% 1096/3335 [00:00<00:00, 2790.60it/s]\u001b[A\n",
            " 41% 1379/3335 [00:00<00:00, 2803.24it/s]\u001b[A\n",
            " 50% 1660/3335 [00:00<00:00, 2773.75it/s]\u001b[A\n",
            " 58% 1938/3335 [00:00<00:00, 2759.27it/s]\u001b[A\n",
            " 66% 2216/3335 [00:00<00:00, 2765.30it/s]\u001b[A\n",
            " 75% 2493/3335 [00:00<00:00, 2745.40it/s]\u001b[A\n",
            " 83% 2774/3335 [00:01<00:00, 2763.83it/s]\u001b[A\n",
            " 91% 3051/3335 [00:01<00:00, 2752.22it/s]\u001b[A\n",
            "100% 3335/3335 [00:01<00:00, 2745.58it/s]\n",
            "[09.28.21 13:31:26] Visualizing in TensorBoard...\n",
            "[09.28.21 13:31:26] Eval F1: 77.64, EM: 62.19\n",
            "100% 17044/17044 [28:59<00:00,  9.80it/s, NLL=0.166, epoch=0]\n",
            "[09.28.21 13:58:31] Epoch: 1\n",
            "100% 17044/17044 [27:07<00:00, 10.47it/s, NLL=0.958, epoch=1]\n",
            "[09.28.21 14:25:39] Epoch: 2\n",
            "100% 17044/17044 [27:05<00:00, 10.48it/s, NLL=0.172, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsdtpXFTrfMI"
      },
      "source": [
        "cp -R /content/robustqa_albert /content/drive/MyDrive/robustqa_change_transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh8Xgij7uwap"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdghsiuJqMdj",
        "outputId": "89885a6d-9433-492a-946c-fb27ed74a51d"
      },
      "source": [
        "!pip uninstall transformers -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixy_2Ta1C1H3",
        "outputId": "57727a3c-4de9-4812-fea3-214a614b6cda"
      },
      "source": [
        "%cd /content/robustqa_albert/robustqa_original/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name albert_squad_complete \\\n",
        "    --train-datasets squad01 \\\n",
        "    --eval-datasets race01 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert/robustqa_original/robustqa_albert\n",
            "Downloading: 100% 742k/742k [00:00<00:00, 2.90MB/s]\n",
            "Downloading: 100% 1.25M/1.25M [00:00<00:00, 3.92MB/s]\n",
            "Downloading: 100% 684/684 [00:00<00:00, 810kB/s]\n",
            "Downloading: 100% 45.2M/45.2M [00:00<00:00, 62.1MB/s]\n",
            "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForQuestionAnswering: ['predictions.dense.bias', 'predictions.LayerNorm.weight', 'predictions.bias', 'predictions.dense.weight', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.decoder.weight']\n",
            "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of AlbertForQuestionAnswering were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[09.26.21 03:23:39] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"./datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"albert_squad_complete\",\n",
            "    \"save_dir\": \"save/albert_squad_complete-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"./datasets/indomain_train\",\n",
            "    \"val_dir\": \"./datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.26.21 03:23:39] Preparing Training Data...\n",
            "100% 16037/16037 [00:00<00:00, 22837.36it/s]\n",
            "Preprocessing not completely accurate for 25/16037 instances\n",
            "[09.26.21 03:23:45] Preparing Validation Data...\n",
            "100% 4453/4453 [00:00<00:00, 23162.89it/s]\n",
            "[09.26.21 03:23:52] Epoch: 0\n",
            "[09.26.21 03:23:53] Evaluating at step 0...\n",
            "  0% 16/16037 [00:00<12:39, 21.08it/s, NLL=5.96, epoch=0]\n",
            "  0% 0/4453 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4453 [00:00<00:33, 131.07it/s]\u001b[A\n",
            "  1% 48/4453 [00:00<00:46, 94.95it/s] \u001b[A\n",
            "  1% 64/4453 [00:00<00:52, 82.92it/s]\u001b[A\n",
            "  2% 80/4453 [00:00<00:56, 77.24it/s]\u001b[A\n",
            "  2% 96/4453 [00:01<00:58, 74.01it/s]\u001b[A\n",
            "  3% 112/4453 [00:01<01:00, 72.12it/s]\u001b[A\n",
            "  3% 128/4453 [00:01<01:00, 70.90it/s]\u001b[A\n",
            "  3% 144/4453 [00:01<01:01, 70.06it/s]\u001b[A\n",
            "  4% 160/4453 [00:02<01:01, 69.50it/s]\u001b[A\n",
            "  4% 176/4453 [00:02<01:01, 69.21it/s]\u001b[A\n",
            "  4% 192/4453 [00:02<01:01, 68.94it/s]\u001b[A\n",
            "  5% 208/4453 [00:02<01:01, 68.74it/s]\u001b[A\n",
            "  5% 224/4453 [00:03<01:01, 68.73it/s]\u001b[A\n",
            "  5% 240/4453 [00:03<01:01, 68.68it/s]\u001b[A\n",
            "  6% 256/4453 [00:03<01:01, 68.67it/s]\u001b[A\n",
            "  6% 272/4453 [00:03<01:00, 68.70it/s]\u001b[A\n",
            "  6% 288/4453 [00:03<01:00, 68.62it/s]\u001b[A\n",
            "  7% 304/4453 [00:04<01:00, 68.65it/s]\u001b[A\n",
            "  7% 320/4453 [00:04<01:00, 68.65it/s]\u001b[A\n",
            "  8% 336/4453 [00:04<00:59, 68.62it/s]\u001b[A\n",
            "  8% 352/4453 [00:04<00:59, 68.43it/s]\u001b[A\n",
            "  8% 368/4453 [00:05<00:59, 68.54it/s]\u001b[A\n",
            "  9% 384/4453 [00:05<00:59, 68.48it/s]\u001b[A\n",
            "  9% 400/4453 [00:05<00:59, 68.43it/s]\u001b[A\n",
            "  9% 416/4453 [00:05<00:58, 68.42it/s]\u001b[A\n",
            " 10% 432/4453 [00:06<00:58, 68.27it/s]\u001b[A\n",
            " 10% 448/4453 [00:06<00:58, 68.47it/s]\u001b[A\n",
            " 10% 464/4453 [00:06<00:58, 68.34it/s]\u001b[A\n",
            " 11% 480/4453 [00:06<00:58, 68.43it/s]\u001b[A\n",
            " 11% 496/4453 [00:07<00:57, 68.43it/s]\u001b[A\n",
            " 11% 512/4453 [00:07<00:57, 68.45it/s]\u001b[A\n",
            " 12% 528/4453 [00:07<00:57, 68.40it/s]\u001b[A\n",
            " 12% 544/4453 [00:07<00:57, 68.40it/s]\u001b[A\n",
            " 13% 560/4453 [00:07<00:56, 68.35it/s]\u001b[A\n",
            " 13% 576/4453 [00:08<00:56, 68.38it/s]\u001b[A\n",
            " 13% 592/4453 [00:08<00:56, 68.41it/s]\u001b[A\n",
            " 14% 608/4453 [00:08<00:56, 68.45it/s]\u001b[A\n",
            " 14% 624/4453 [00:08<00:55, 68.43it/s]\u001b[A\n",
            " 14% 640/4453 [00:09<00:55, 68.35it/s]\u001b[A\n",
            " 15% 656/4453 [00:09<00:55, 68.42it/s]\u001b[A\n",
            " 15% 672/4453 [00:09<00:55, 68.43it/s]\u001b[A\n",
            " 15% 688/4453 [00:09<00:55, 68.39it/s]\u001b[A\n",
            " 16% 704/4453 [00:10<00:54, 68.43it/s]\u001b[A\n",
            " 16% 720/4453 [00:10<00:54, 68.41it/s]\u001b[A\n",
            " 17% 736/4453 [00:10<00:54, 68.38it/s]\u001b[A\n",
            " 17% 752/4453 [00:10<00:54, 68.43it/s]\u001b[A\n",
            " 17% 768/4453 [00:10<00:53, 68.48it/s]\u001b[A\n",
            "  0% 16/16037 [00:12<12:39, 21.08it/s, NLL=5.96, epoch=0]\n",
            " 18% 800/4453 [00:11<00:53, 68.50it/s]\u001b[A\n",
            " 18% 816/4453 [00:11<00:53, 68.53it/s]\u001b[A\n",
            " 19% 832/4453 [00:11<00:52, 68.57it/s]\u001b[A\n",
            " 19% 848/4453 [00:12<00:52, 68.58it/s]\u001b[A\n",
            " 19% 864/4453 [00:12<00:52, 68.57it/s]\u001b[A\n",
            " 20% 880/4453 [00:12<00:52, 68.32it/s]\u001b[A\n",
            " 20% 896/4453 [00:12<00:51, 68.55it/s]\u001b[A\n",
            " 20% 912/4453 [00:13<00:51, 68.53it/s]\u001b[A\n",
            " 21% 928/4453 [00:13<00:51, 68.43it/s]\u001b[A\n",
            " 21% 944/4453 [00:13<00:51, 68.49it/s]\u001b[A\n",
            " 22% 960/4453 [00:13<00:51, 68.43it/s]\u001b[A\n",
            " 22% 976/4453 [00:14<00:50, 68.22it/s]\u001b[A\n",
            " 22% 992/4453 [00:14<00:50, 68.57it/s]\u001b[A\n",
            " 23% 1008/4453 [00:14<00:50, 68.52it/s]\u001b[A\n",
            " 23% 1024/4453 [00:14<00:50, 68.55it/s]\u001b[A\n",
            " 23% 1040/4453 [00:14<00:49, 68.54it/s]\u001b[A\n",
            " 24% 1056/4453 [00:15<00:49, 68.52it/s]\u001b[A\n",
            " 24% 1072/4453 [00:15<00:49, 68.54it/s]\u001b[A\n",
            " 24% 1088/4453 [00:15<00:49, 68.54it/s]\u001b[A\n",
            " 25% 1104/4453 [00:15<00:48, 68.53it/s]\u001b[A\n",
            " 25% 1120/4453 [00:16<00:48, 68.48it/s]\u001b[A\n",
            " 26% 1136/4453 [00:16<00:48, 68.52it/s]\u001b[A\n",
            " 26% 1152/4453 [00:16<00:48, 68.50it/s]\u001b[A\n",
            " 26% 1168/4453 [00:16<00:47, 68.48it/s]\u001b[A\n",
            " 27% 1184/4453 [00:17<00:47, 68.47it/s]\u001b[A\n",
            " 27% 1200/4453 [00:17<00:47, 68.44it/s]\u001b[A\n",
            " 27% 1216/4453 [00:17<00:47, 68.41it/s]\u001b[A\n",
            " 28% 1232/4453 [00:17<00:47, 68.36it/s]\u001b[A\n",
            " 28% 1248/4453 [00:18<00:46, 68.34it/s]\u001b[A\n",
            " 28% 1264/4453 [00:18<00:46, 68.37it/s]\u001b[A\n",
            " 29% 1280/4453 [00:18<00:46, 68.36it/s]\u001b[A\n",
            " 29% 1296/4453 [00:18<00:46, 68.40it/s]\u001b[A\n",
            " 29% 1312/4453 [00:18<00:45, 68.41it/s]\u001b[A\n",
            " 30% 1328/4453 [00:19<00:45, 68.46it/s]\u001b[A\n",
            " 30% 1344/4453 [00:19<00:45, 68.41it/s]\u001b[A\n",
            " 31% 1360/4453 [00:19<00:45, 68.44it/s]\u001b[A\n",
            " 31% 1376/4453 [00:19<00:44, 68.41it/s]\u001b[A\n",
            " 31% 1392/4453 [00:20<00:44, 68.30it/s]\u001b[A\n",
            " 32% 1408/4453 [00:20<00:44, 68.41it/s]\u001b[A\n",
            " 32% 1424/4453 [00:20<00:44, 68.43it/s]\u001b[A\n",
            " 32% 1440/4453 [00:20<00:44, 68.44it/s]\u001b[A\n",
            " 33% 1456/4453 [00:21<00:43, 68.42it/s]\u001b[A\n",
            " 33% 1472/4453 [00:21<00:43, 68.44it/s]\u001b[A\n",
            " 33% 1488/4453 [00:21<00:43, 68.46it/s]\u001b[A\n",
            " 34% 1504/4453 [00:21<00:43, 68.43it/s]\u001b[A\n",
            " 34% 1520/4453 [00:21<00:42, 68.23it/s]\u001b[A\n",
            " 34% 1536/4453 [00:22<00:42, 68.51it/s]\u001b[A\n",
            " 35% 1552/4453 [00:22<00:42, 68.46it/s]\u001b[A\n",
            " 35% 1568/4453 [00:22<00:42, 68.45it/s]\u001b[A\n",
            " 36% 1584/4453 [00:22<00:41, 68.36it/s]\u001b[A\n",
            " 36% 1600/4453 [00:23<00:41, 68.44it/s]\u001b[A\n",
            " 36% 1616/4453 [00:23<00:41, 68.40it/s]\u001b[A\n",
            " 37% 1632/4453 [00:23<00:41, 68.43it/s]\u001b[A\n",
            " 37% 1648/4453 [00:23<00:41, 68.38it/s]\u001b[A\n",
            " 37% 1664/4453 [00:24<00:40, 68.40it/s]\u001b[A\n",
            " 38% 1680/4453 [00:24<00:40, 68.43it/s]\u001b[A\n",
            " 38% 1696/4453 [00:24<00:40, 68.40it/s]\u001b[A\n",
            " 38% 1712/4453 [00:24<00:40, 68.39it/s]\u001b[A\n",
            " 39% 1728/4453 [00:25<00:39, 68.16it/s]\u001b[A\n",
            " 39% 1744/4453 [00:25<00:39, 68.47it/s]\u001b[A\n",
            " 40% 1760/4453 [00:25<00:39, 68.45it/s]\u001b[A\n",
            " 40% 1776/4453 [00:25<00:39, 68.43it/s]\u001b[A\n",
            " 40% 1792/4453 [00:25<00:38, 68.45it/s]\u001b[A\n",
            " 41% 1808/4453 [00:26<00:38, 68.41it/s]\u001b[A\n",
            " 41% 1824/4453 [00:26<00:38, 68.45it/s]\u001b[A\n",
            " 41% 1840/4453 [00:26<00:38, 68.41it/s]\u001b[A\n",
            " 42% 1856/4453 [00:26<00:37, 68.38it/s]\u001b[A\n",
            " 42% 1872/4453 [00:27<00:37, 68.46it/s]\u001b[A\n",
            " 42% 1888/4453 [00:27<00:37, 68.47it/s]\u001b[A\n",
            " 43% 1904/4453 [00:27<00:37, 68.46it/s]\u001b[A\n",
            " 43% 1920/4453 [00:27<00:37, 68.45it/s]\u001b[A\n",
            " 43% 1936/4453 [00:28<00:36, 68.37it/s]\u001b[A\n",
            " 44% 1952/4453 [00:28<00:36, 68.33it/s]\u001b[A\n",
            " 44% 1968/4453 [00:28<00:36, 68.34it/s]\u001b[A\n",
            " 45% 1984/4453 [00:28<00:36, 68.41it/s]\u001b[A\n",
            " 45% 2000/4453 [00:29<00:35, 68.40it/s]\u001b[A\n",
            " 45% 2016/4453 [00:29<00:35, 68.15it/s]\u001b[A\n",
            " 46% 2032/4453 [00:29<00:35, 68.46it/s]\u001b[A\n",
            " 46% 2048/4453 [00:29<00:35, 68.43it/s]\u001b[A\n",
            " 46% 2064/4453 [00:29<00:34, 68.50it/s]\u001b[A\n",
            " 47% 2080/4453 [00:30<00:34, 68.47it/s]\u001b[A\n",
            " 47% 2096/4453 [00:30<00:34, 68.41it/s]\u001b[A\n",
            " 47% 2112/4453 [00:30<00:34, 68.40it/s]\u001b[A\n",
            " 48% 2128/4453 [00:30<00:34, 68.35it/s]\u001b[A\n",
            " 48% 2144/4453 [00:31<00:33, 68.29it/s]\u001b[A\n",
            " 49% 2160/4453 [00:31<00:33, 68.42it/s]\u001b[A\n",
            " 49% 2176/4453 [00:31<00:33, 68.42it/s]\u001b[A\n",
            " 49% 2192/4453 [00:31<00:33, 68.28it/s]\u001b[A\n",
            " 50% 2208/4453 [00:32<00:32, 68.48it/s]\u001b[A\n",
            " 50% 2224/4453 [00:32<00:32, 68.48it/s]\u001b[A\n",
            " 50% 2240/4453 [00:32<00:32, 68.37it/s]\u001b[A\n",
            " 51% 2256/4453 [00:32<00:32, 68.42it/s]\u001b[A\n",
            " 51% 2272/4453 [00:32<00:31, 68.45it/s]\u001b[A\n",
            " 51% 2288/4453 [00:33<00:31, 68.30it/s]\u001b[A\n",
            " 52% 2304/4453 [00:33<00:31, 68.43it/s]\u001b[A\n",
            " 52% 2320/4453 [00:33<00:31, 68.47it/s]\u001b[A\n",
            " 52% 2336/4453 [00:33<00:30, 68.31it/s]\u001b[A\n",
            " 53% 2352/4453 [00:34<00:30, 68.35it/s]\u001b[A\n",
            " 53% 2368/4453 [00:34<00:30, 68.49it/s]\u001b[A\n",
            " 54% 2384/4453 [00:34<00:30, 68.31it/s]\u001b[A\n",
            " 54% 2400/4453 [00:34<00:29, 68.50it/s]\u001b[A\n",
            " 54% 2416/4453 [00:35<00:29, 68.48it/s]\u001b[A\n",
            " 55% 2432/4453 [00:35<00:29, 68.42it/s]\u001b[A\n",
            " 55% 2448/4453 [00:35<00:29, 68.53it/s]\u001b[A\n",
            " 55% 2464/4453 [00:35<00:29, 68.54it/s]\u001b[A\n",
            " 56% 2480/4453 [00:36<00:28, 68.37it/s]\u001b[A\n",
            " 56% 2496/4453 [00:36<00:28, 68.52it/s]\u001b[A\n",
            " 56% 2512/4453 [00:36<00:28, 68.55it/s]\u001b[A\n",
            " 57% 2528/4453 [00:36<00:28, 68.39it/s]\u001b[A\n",
            " 57% 2544/4453 [00:36<00:27, 68.53it/s]\u001b[A\n",
            " 57% 2560/4453 [00:37<00:27, 68.54it/s]\u001b[A\n",
            " 58% 2576/4453 [00:37<00:27, 68.57it/s]\u001b[A\n",
            " 58% 2592/4453 [00:37<00:27, 68.55it/s]\u001b[A\n",
            " 59% 2608/4453 [00:37<00:26, 68.56it/s]\u001b[A\n",
            " 59% 2624/4453 [00:38<00:26, 68.58it/s]\u001b[A\n",
            " 59% 2640/4453 [00:38<00:26, 68.54it/s]\u001b[A\n",
            " 60% 2656/4453 [00:38<00:26, 68.52it/s]\u001b[A\n",
            " 60% 2672/4453 [00:38<00:25, 68.50it/s]\u001b[A\n",
            " 60% 2688/4453 [00:39<00:25, 68.47it/s]\u001b[A\n",
            " 61% 2704/4453 [00:39<00:25, 68.45it/s]\u001b[A\n",
            " 61% 2720/4453 [00:39<00:25, 68.48it/s]\u001b[A\n",
            " 61% 2736/4453 [00:39<00:25, 68.38it/s]\u001b[A\n",
            " 62% 2752/4453 [00:39<00:24, 68.41it/s]\u001b[A\n",
            " 62% 2768/4453 [00:40<00:24, 68.47it/s]\u001b[A\n",
            " 63% 2784/4453 [00:40<00:24, 68.44it/s]\u001b[A\n",
            " 63% 2800/4453 [00:40<00:24, 68.41it/s]\u001b[A\n",
            " 63% 2816/4453 [00:40<00:23, 68.39it/s]\u001b[A\n",
            " 64% 2832/4453 [00:41<00:23, 68.40it/s]\u001b[A\n",
            " 64% 2848/4453 [00:41<00:23, 68.40it/s]\u001b[A\n",
            " 64% 2864/4453 [00:41<00:23, 68.31it/s]\u001b[A\n",
            " 65% 2880/4453 [00:41<00:23, 68.37it/s]\u001b[A\n",
            " 65% 2896/4453 [00:42<00:22, 68.44it/s]\u001b[A\n",
            " 65% 2912/4453 [00:42<00:22, 68.45it/s]\u001b[A\n",
            " 66% 2928/4453 [00:42<00:22, 68.46it/s]\u001b[A\n",
            " 66% 2944/4453 [00:42<00:22, 68.48it/s]\u001b[A\n",
            " 66% 2960/4453 [00:43<00:21, 68.47it/s]\u001b[A\n",
            " 67% 2976/4453 [00:43<00:21, 68.45it/s]\u001b[A\n",
            " 67% 2992/4453 [00:43<00:21, 68.46it/s]\u001b[A\n",
            " 68% 3008/4453 [00:43<00:21, 68.45it/s]\u001b[A\n",
            " 68% 3024/4453 [00:43<00:20, 68.39it/s]\u001b[A\n",
            " 68% 3040/4453 [00:44<00:20, 68.49it/s]\u001b[A\n",
            " 69% 3056/4453 [00:44<00:20, 68.46it/s]\u001b[A\n",
            " 69% 3072/4453 [00:44<00:20, 68.43it/s]\u001b[A\n",
            " 69% 3088/4453 [00:44<00:19, 68.43it/s]\u001b[A\n",
            " 70% 3104/4453 [00:45<00:19, 68.44it/s]\u001b[A\n",
            " 70% 3120/4453 [00:45<00:19, 68.48it/s]\u001b[A\n",
            " 70% 3136/4453 [00:45<00:19, 68.50it/s]\u001b[A\n",
            " 71% 3152/4453 [00:45<00:19, 68.43it/s]\u001b[A\n",
            " 71% 3168/4453 [00:46<00:18, 68.43it/s]\u001b[A\n",
            " 72% 3184/4453 [00:46<00:18, 68.47it/s]\u001b[A\n",
            " 72% 3200/4453 [00:46<00:18, 68.42it/s]\u001b[A\n",
            " 72% 3216/4453 [00:46<00:18, 68.46it/s]\u001b[A\n",
            " 73% 3232/4453 [00:47<00:17, 68.42it/s]\u001b[A\n",
            " 73% 3248/4453 [00:47<00:17, 68.40it/s]\u001b[A\n",
            " 73% 3264/4453 [00:47<00:17, 68.50it/s]\u001b[A\n",
            " 74% 3280/4453 [00:47<00:17, 68.51it/s]\u001b[A\n",
            " 74% 3296/4453 [00:47<00:16, 68.37it/s]\u001b[A\n",
            " 74% 3312/4453 [00:48<00:16, 68.43it/s]\u001b[A\n",
            " 75% 3328/4453 [00:48<00:16, 68.44it/s]\u001b[A\n",
            " 75% 3344/4453 [00:48<00:16, 68.48it/s]\u001b[A\n",
            " 75% 3360/4453 [00:48<00:15, 68.46it/s]\u001b[A\n",
            " 76% 3376/4453 [00:49<00:15, 68.47it/s]\u001b[A\n",
            " 76% 3392/4453 [00:49<00:15, 68.40it/s]\u001b[A\n",
            " 77% 3408/4453 [00:49<00:15, 68.48it/s]\u001b[A\n",
            " 77% 3424/4453 [00:49<00:15, 68.45it/s]\u001b[A\n",
            " 77% 3440/4453 [00:50<00:14, 68.49it/s]\u001b[A\n",
            " 78% 3456/4453 [00:50<00:14, 68.38it/s]\u001b[A\n",
            " 78% 3472/4453 [00:50<00:14, 68.51it/s]\u001b[A\n",
            " 78% 3488/4453 [00:50<00:14, 68.52it/s]\u001b[A\n",
            " 79% 3504/4453 [00:50<00:13, 68.32it/s]\u001b[A\n",
            " 79% 3520/4453 [00:51<00:13, 68.53it/s]\u001b[A\n",
            " 79% 3536/4453 [00:51<00:13, 68.55it/s]\u001b[A\n",
            " 80% 3552/4453 [00:51<00:13, 68.39it/s]\u001b[A\n",
            " 80% 3568/4453 [00:51<00:12, 68.57it/s]\u001b[A\n",
            " 80% 3584/4453 [00:52<00:12, 68.54it/s]\u001b[A\n",
            " 81% 3600/4453 [00:52<00:12, 68.41it/s]\u001b[A\n",
            " 81% 3616/4453 [00:52<00:12, 68.56it/s]\u001b[A\n",
            " 82% 3632/4453 [00:52<00:11, 68.53it/s]\u001b[A\n",
            " 82% 3648/4453 [00:53<00:11, 68.46it/s]\u001b[A\n",
            " 82% 3664/4453 [00:53<00:11, 68.53it/s]\u001b[A\n",
            " 83% 3680/4453 [00:53<00:11, 68.52it/s]\u001b[A\n",
            " 83% 3696/4453 [00:53<00:11, 68.38it/s]\u001b[A\n",
            " 83% 3712/4453 [00:54<00:10, 68.48it/s]\u001b[A\n",
            " 84% 3728/4453 [00:54<00:10, 68.46it/s]\u001b[A\n",
            " 84% 3744/4453 [00:54<00:10, 68.40it/s]\u001b[A\n",
            " 84% 3760/4453 [00:54<00:10, 68.46it/s]\u001b[A\n",
            " 85% 3776/4453 [00:54<00:09, 68.45it/s]\u001b[A\n",
            " 85% 3792/4453 [00:55<00:09, 68.49it/s]\u001b[A\n",
            " 86% 3808/4453 [00:55<00:09, 68.51it/s]\u001b[A\n",
            " 86% 3824/4453 [00:55<00:09, 68.46it/s]\u001b[A\n",
            " 86% 3840/4453 [00:55<00:08, 68.45it/s]\u001b[A\n",
            " 87% 3856/4453 [00:56<00:08, 68.48it/s]\u001b[A\n",
            " 87% 3872/4453 [00:56<00:08, 68.48it/s]\u001b[A\n",
            " 87% 3888/4453 [00:56<00:08, 68.48it/s]\u001b[A\n",
            " 88% 3904/4453 [00:56<00:08, 68.45it/s]\u001b[A\n",
            " 88% 3920/4453 [00:57<00:07, 68.45it/s]\u001b[A\n",
            " 88% 3936/4453 [00:57<00:07, 68.52it/s]\u001b[A\n",
            " 89% 3952/4453 [00:57<00:07, 68.53it/s]\u001b[A\n",
            " 89% 3968/4453 [00:57<00:07, 68.50it/s]\u001b[A\n",
            " 89% 3984/4453 [00:57<00:06, 68.50it/s]\u001b[A\n",
            " 90% 4000/4453 [00:58<00:06, 68.48it/s]\u001b[A\n",
            " 90% 4016/4453 [00:58<00:06, 68.49it/s]\u001b[A\n",
            " 91% 4032/4453 [00:58<00:06, 68.50it/s]\u001b[A\n",
            " 91% 4048/4453 [00:58<00:05, 68.48it/s]\u001b[A\n",
            " 91% 4064/4453 [00:59<00:05, 68.52it/s]\u001b[A\n",
            " 92% 4080/4453 [00:59<00:05, 68.46it/s]\u001b[A\n",
            " 92% 4096/4453 [00:59<00:05, 68.51it/s]\u001b[A\n",
            " 92% 4112/4453 [00:59<00:04, 68.51it/s]\u001b[A\n",
            " 93% 4128/4453 [01:00<00:04, 68.44it/s]\u001b[A\n",
            " 93% 4144/4453 [01:00<00:04, 68.48it/s]\u001b[A\n",
            " 93% 4160/4453 [01:00<00:04, 68.50it/s]\u001b[A\n",
            " 94% 4176/4453 [01:00<00:04, 68.52it/s]\u001b[A\n",
            " 94% 4192/4453 [01:01<00:03, 68.56it/s]\u001b[A\n",
            " 94% 4208/4453 [01:01<00:03, 68.63it/s]\u001b[A\n",
            " 95% 4224/4453 [01:01<00:03, 68.64it/s]\u001b[A\n",
            " 95% 4240/4453 [01:01<00:03, 68.65it/s]\u001b[A\n",
            " 96% 4256/4453 [01:01<00:02, 68.68it/s]\u001b[A\n",
            " 96% 4272/4453 [01:02<00:02, 68.71it/s]\u001b[A\n",
            " 96% 4288/4453 [01:02<00:02, 68.69it/s]\u001b[A\n",
            " 97% 4304/4453 [01:02<00:02, 68.69it/s]\u001b[A\n",
            " 97% 4320/4453 [01:02<00:01, 68.73it/s]\u001b[A\n",
            " 97% 4336/4453 [01:03<00:01, 68.52it/s]\u001b[A\n",
            " 98% 4352/4453 [01:03<00:01, 68.66it/s]\u001b[A\n",
            " 98% 4368/4453 [01:03<00:01, 68.66it/s]\u001b[A\n",
            " 98% 4384/4453 [01:03<00:01, 68.51it/s]\u001b[A\n",
            " 99% 4400/4453 [01:04<00:00, 68.72it/s]\u001b[A\n",
            " 99% 4416/4453 [01:04<00:00, 68.74it/s]\u001b[A\n",
            "100% 4432/4453 [01:04<00:00, 68.55it/s]\u001b[A\n",
            "100% 4453/4453 [01:04<00:00, 68.53it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 344/4306 [00:00<00:01, 3432.16it/s]\u001b[A\n",
            " 16% 699/4306 [00:00<00:01, 3497.59it/s]\u001b[A\n",
            " 25% 1072/4306 [00:00<00:00, 3599.59it/s]\u001b[A\n",
            " 33% 1432/4306 [00:00<00:00, 3519.37it/s]\u001b[A\n",
            " 41% 1785/4306 [00:00<00:00, 3503.47it/s]\u001b[A\n",
            " 50% 2136/4306 [00:00<00:00, 3501.99it/s]\u001b[A\n",
            " 58% 2498/4306 [00:00<00:00, 3535.26it/s]\u001b[A\n",
            " 66% 2852/4306 [00:00<00:00, 3525.83it/s]\u001b[A\n",
            " 75% 3223/4306 [00:00<00:00, 3582.72it/s]\u001b[A\n",
            " 83% 3595/4306 [00:01<00:00, 3624.25it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 3524.40it/s]\n",
            "[09.26.21 03:25:00] Visualizing in TensorBoard...\n",
            "[09.26.21 03:25:00] Eval F1: 07.19, EM: 00.16\n",
            "100% 16037/16037 [12:01<00:00, 22.21it/s, NLL=1.44, epoch=0]\n",
            "[09.26.21 03:35:54] Epoch: 1\n",
            "100% 16037/16037 [10:54<00:00, 24.50it/s, NLL=0.229, epoch=1]\n",
            "[09.26.21 03:46:49] Epoch: 2\n",
            "100% 16037/16037 [10:54<00:00, 24.50it/s, NLL=1.17, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaidySem6nHq",
        "outputId": "3cd04c62-1e57-4534-a085-de7d79c9cd7e"
      },
      "source": [
        "%cd /content/robustqa_albert/robustqa_original/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name albert_squad_complete \\\n",
        "    --train-datasets squad02 \\\n",
        "    --eval-datasets race02 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert/robustqa_original/robustqa_albert\n",
            "[09.26.21 03:57:49] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race02\",\n",
            "    \"eval_dir\": \"./datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"albert_squad_complete\",\n",
            "    \"save_dir\": \"save/albert_squad_complete-02\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad02\",\n",
            "    \"train_dir\": \"./datasets/indomain_train\",\n",
            "    \"val_dir\": \"./datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.26.21 03:57:49] Preparing Training Data...\n",
            "100% 17524/17524 [00:00<00:00, 21660.60it/s]\n",
            "Preprocessing not completely accurate for 16/17524 instances\n",
            "[09.26.21 03:57:56] Preparing Validation Data...\n",
            "100% 2982/2982 [00:00<00:00, 22379.83it/s]\n",
            "[09.26.21 03:58:00] Epoch: 0\n",
            "[09.26.21 03:58:01] Evaluating at step 0...\n",
            "  0% 16/17524 [00:00<12:27, 23.41it/s, NLL=0.93, epoch=0]\n",
            "  0% 0/2982 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/2982 [00:00<00:24, 121.91it/s]\u001b[A\n",
            "  2% 48/2982 [00:00<00:31, 91.99it/s] \u001b[A\n",
            "  2% 64/2982 [00:00<00:35, 81.63it/s]\u001b[A\n",
            "  3% 80/2982 [00:00<00:37, 76.55it/s]\u001b[A\n",
            "  3% 96/2982 [00:01<00:39, 73.66it/s]\u001b[A\n",
            "  4% 112/2982 [00:01<00:39, 71.83it/s]\u001b[A\n",
            "  4% 128/2982 [00:01<00:40, 70.85it/s]\u001b[A\n",
            "  5% 144/2982 [00:01<00:40, 70.06it/s]\u001b[A\n",
            "  5% 160/2982 [00:02<00:40, 69.50it/s]\u001b[A\n",
            "  6% 176/2982 [00:02<00:40, 69.27it/s]\u001b[A\n",
            "  6% 192/2982 [00:02<00:40, 68.92it/s]\u001b[A\n",
            "  7% 208/2982 [00:02<00:40, 68.69it/s]\u001b[A\n",
            "  8% 224/2982 [00:03<00:40, 68.88it/s]\u001b[A\n",
            "  8% 240/2982 [00:03<00:39, 68.81it/s]\u001b[A\n",
            "  9% 256/2982 [00:03<00:39, 68.68it/s]\u001b[A\n",
            "  9% 272/2982 [00:03<00:39, 68.57it/s]\u001b[A\n",
            " 10% 288/2982 [00:03<00:39, 68.56it/s]\u001b[A\n",
            " 10% 304/2982 [00:04<00:39, 68.54it/s]\u001b[A\n",
            " 11% 320/2982 [00:04<00:38, 68.51it/s]\u001b[A\n",
            " 11% 336/2982 [00:04<00:38, 68.55it/s]\u001b[A\n",
            " 12% 352/2982 [00:04<00:38, 68.50it/s]\u001b[A\n",
            " 12% 368/2982 [00:05<00:38, 68.49it/s]\u001b[A\n",
            " 13% 384/2982 [00:05<00:37, 68.52it/s]\u001b[A\n",
            " 13% 400/2982 [00:05<00:37, 68.51it/s]\u001b[A\n",
            " 14% 416/2982 [00:05<00:37, 68.50it/s]\u001b[A\n",
            " 14% 432/2982 [00:06<00:37, 68.53it/s]\u001b[A\n",
            " 15% 448/2982 [00:06<00:37, 68.46it/s]\u001b[A\n",
            " 16% 464/2982 [00:06<00:36, 68.50it/s]\u001b[A\n",
            " 16% 480/2982 [00:06<00:36, 68.53it/s]\u001b[A\n",
            " 17% 496/2982 [00:07<00:36, 68.44it/s]\u001b[A\n",
            " 17% 512/2982 [00:07<00:36, 68.51it/s]\u001b[A\n",
            " 18% 528/2982 [00:07<00:35, 68.47it/s]\u001b[A\n",
            " 18% 544/2982 [00:07<00:35, 68.56it/s]\u001b[A\n",
            " 19% 560/2982 [00:07<00:35, 68.54it/s]\u001b[A\n",
            " 19% 576/2982 [00:08<00:35, 68.53it/s]\u001b[A\n",
            " 20% 592/2982 [00:08<00:34, 68.57it/s]\u001b[A\n",
            " 20% 608/2982 [00:08<00:34, 68.51it/s]\u001b[A\n",
            " 21% 624/2982 [00:08<00:34, 68.54it/s]\u001b[A\n",
            " 21% 640/2982 [00:09<00:34, 68.55it/s]\u001b[A\n",
            " 22% 656/2982 [00:09<00:33, 68.53it/s]\u001b[A\n",
            " 23% 672/2982 [00:09<00:33, 68.51it/s]\u001b[A\n",
            " 23% 688/2982 [00:09<00:33, 68.56it/s]\u001b[A\n",
            " 24% 704/2982 [00:10<00:33, 68.51it/s]\u001b[A\n",
            " 24% 720/2982 [00:10<00:33, 68.46it/s]\u001b[A\n",
            " 25% 736/2982 [00:10<00:32, 68.54it/s]\u001b[A\n",
            " 25% 752/2982 [00:10<00:32, 68.56it/s]\u001b[A\n",
            " 26% 768/2982 [00:11<00:32, 68.53it/s]\u001b[A\n",
            " 26% 784/2982 [00:11<00:32, 68.56it/s]\u001b[A\n",
            " 27% 800/2982 [00:11<00:31, 68.48it/s]\u001b[A\n",
            " 27% 816/2982 [00:11<00:31, 68.55it/s]\u001b[A\n",
            " 28% 832/2982 [00:11<00:31, 68.57it/s]\u001b[A\n",
            " 28% 848/2982 [00:12<00:31, 68.49it/s]\u001b[A\n",
            " 29% 864/2982 [00:12<00:30, 68.50it/s]\u001b[A\n",
            " 30% 880/2982 [00:12<00:30, 68.51it/s]\u001b[A\n",
            " 30% 896/2982 [00:12<00:30, 68.36it/s]\u001b[A\n",
            " 31% 912/2982 [00:13<00:30, 68.55it/s]\u001b[A\n",
            " 31% 928/2982 [00:13<00:29, 68.53it/s]\u001b[A\n",
            " 32% 944/2982 [00:13<00:29, 68.45it/s]\u001b[A\n",
            " 32% 960/2982 [00:13<00:29, 68.51it/s]\u001b[A\n",
            " 33% 976/2982 [00:14<00:29, 68.41it/s]\u001b[A\n",
            " 33% 992/2982 [00:14<00:29, 68.48it/s]\u001b[A\n",
            " 34% 1008/2982 [00:14<00:28, 68.51it/s]\u001b[A\n",
            "  0% 16/17524 [00:15<12:27, 23.41it/s, NLL=0.93, epoch=0]\n",
            " 35% 1040/2982 [00:14<00:28, 68.29it/s]\u001b[A\n",
            " 35% 1056/2982 [00:15<00:28, 68.42it/s]\u001b[A\n",
            " 36% 1072/2982 [00:15<00:27, 68.46it/s]\u001b[A\n",
            " 36% 1088/2982 [00:15<00:27, 68.46it/s]\u001b[A\n",
            " 37% 1104/2982 [00:15<00:27, 68.38it/s]\u001b[A\n",
            " 38% 1120/2982 [00:16<00:27, 68.54it/s]\u001b[A\n",
            " 38% 1136/2982 [00:16<00:26, 68.51it/s]\u001b[A\n",
            " 39% 1152/2982 [00:16<00:26, 68.50it/s]\u001b[A\n",
            " 39% 1168/2982 [00:16<00:26, 68.52it/s]\u001b[A\n",
            " 40% 1184/2982 [00:17<00:26, 68.54it/s]\u001b[A\n",
            " 40% 1200/2982 [00:17<00:26, 68.53it/s]\u001b[A\n",
            " 41% 1216/2982 [00:17<00:25, 68.61it/s]\u001b[A\n",
            " 41% 1232/2982 [00:17<00:25, 68.62it/s]\u001b[A\n",
            " 42% 1248/2982 [00:18<00:25, 68.55it/s]\u001b[A\n",
            " 42% 1264/2982 [00:18<00:25, 68.62it/s]\u001b[A\n",
            " 43% 1280/2982 [00:18<00:24, 68.57it/s]\u001b[A\n",
            " 43% 1296/2982 [00:18<00:24, 68.52it/s]\u001b[A\n",
            " 44% 1312/2982 [00:18<00:24, 68.57it/s]\u001b[A\n",
            " 45% 1328/2982 [00:19<00:24, 68.56it/s]\u001b[A\n",
            " 45% 1344/2982 [00:19<00:23, 68.53it/s]\u001b[A\n",
            " 46% 1360/2982 [00:19<00:23, 68.56it/s]\u001b[A\n",
            " 46% 1376/2982 [00:19<00:23, 68.59it/s]\u001b[A\n",
            " 47% 1392/2982 [00:20<00:23, 68.54it/s]\u001b[A\n",
            " 47% 1408/2982 [00:20<00:22, 68.57it/s]\u001b[A\n",
            " 48% 1424/2982 [00:20<00:22, 68.48it/s]\u001b[A\n",
            " 48% 1440/2982 [00:20<00:22, 68.54it/s]\u001b[A\n",
            " 49% 1456/2982 [00:21<00:22, 68.49it/s]\u001b[A\n",
            " 49% 1472/2982 [00:21<00:22, 68.47it/s]\u001b[A\n",
            " 50% 1488/2982 [00:21<00:21, 68.53it/s]\u001b[A\n",
            " 50% 1504/2982 [00:21<00:21, 68.53it/s]\u001b[A\n",
            " 51% 1520/2982 [00:21<00:21, 68.49it/s]\u001b[A\n",
            " 52% 1536/2982 [00:22<00:21, 68.50it/s]\u001b[A\n",
            " 52% 1552/2982 [00:22<00:20, 68.51it/s]\u001b[A\n",
            " 53% 1568/2982 [00:22<00:20, 68.53it/s]\u001b[A\n",
            " 53% 1584/2982 [00:22<00:20, 68.44it/s]\u001b[A\n",
            " 54% 1600/2982 [00:23<00:20, 68.55it/s]\u001b[A\n",
            " 54% 1616/2982 [00:23<00:19, 68.55it/s]\u001b[A\n",
            " 55% 1632/2982 [00:23<00:19, 68.54it/s]\u001b[A\n",
            " 55% 1648/2982 [00:23<00:19, 68.44it/s]\u001b[A\n",
            " 56% 1664/2982 [00:24<00:19, 68.53it/s]\u001b[A\n",
            " 56% 1680/2982 [00:24<00:18, 68.53it/s]\u001b[A\n",
            " 57% 1696/2982 [00:24<00:18, 68.54it/s]\u001b[A\n",
            " 57% 1712/2982 [00:24<00:18, 68.51it/s]\u001b[A\n",
            " 58% 1728/2982 [00:25<00:18, 68.43it/s]\u001b[A\n",
            " 58% 1744/2982 [00:25<00:18, 68.53it/s]\u001b[A\n",
            " 59% 1760/2982 [00:25<00:17, 68.54it/s]\u001b[A\n",
            " 60% 1776/2982 [00:25<00:17, 68.42it/s]\u001b[A\n",
            " 60% 1792/2982 [00:25<00:17, 68.61it/s]\u001b[A\n",
            " 61% 1808/2982 [00:26<00:17, 68.48it/s]\u001b[A\n",
            " 61% 1824/2982 [00:26<00:16, 68.57it/s]\u001b[A\n",
            " 62% 1840/2982 [00:26<00:16, 68.61it/s]\u001b[A\n",
            " 62% 1856/2982 [00:26<00:16, 68.65it/s]\u001b[A\n",
            " 63% 1872/2982 [00:27<00:16, 68.60it/s]\u001b[A\n",
            " 63% 1888/2982 [00:27<00:15, 68.45it/s]\u001b[A\n",
            " 64% 1904/2982 [00:27<00:15, 68.55it/s]\u001b[A\n",
            " 64% 1920/2982 [00:27<00:15, 68.57it/s]\u001b[A\n",
            " 65% 1936/2982 [00:28<00:15, 68.57it/s]\u001b[A\n",
            " 65% 1952/2982 [00:28<00:15, 68.56it/s]\u001b[A\n",
            " 66% 1968/2982 [00:28<00:14, 68.41it/s]\u001b[A\n",
            " 67% 1984/2982 [00:28<00:14, 68.54it/s]\u001b[A\n",
            " 67% 2000/2982 [00:28<00:14, 68.58it/s]\u001b[A\n",
            " 68% 2016/2982 [00:29<00:14, 68.63it/s]\u001b[A\n",
            " 68% 2032/2982 [00:29<00:13, 68.58it/s]\u001b[A\n",
            " 69% 2048/2982 [00:29<00:13, 68.40it/s]\u001b[A\n",
            " 69% 2064/2982 [00:29<00:13, 68.56it/s]\u001b[A\n",
            " 70% 2080/2982 [00:30<00:13, 68.59it/s]\u001b[A\n",
            " 70% 2096/2982 [00:30<00:12, 68.51it/s]\u001b[A\n",
            " 71% 2112/2982 [00:30<00:12, 68.58it/s]\u001b[A\n",
            " 71% 2128/2982 [00:30<00:12, 68.55it/s]\u001b[A\n",
            " 72% 2144/2982 [00:31<00:12, 68.59it/s]\u001b[A\n",
            " 72% 2160/2982 [00:31<00:11, 68.56it/s]\u001b[A\n",
            " 73% 2176/2982 [00:31<00:11, 68.52it/s]\u001b[A\n",
            " 74% 2192/2982 [00:31<00:11, 68.51it/s]\u001b[A\n",
            " 74% 2208/2982 [00:32<00:11, 68.50it/s]\u001b[A\n",
            " 75% 2224/2982 [00:32<00:11, 68.45it/s]\u001b[A\n",
            " 75% 2240/2982 [00:32<00:10, 68.48it/s]\u001b[A\n",
            " 76% 2256/2982 [00:32<00:10, 68.53it/s]\u001b[A\n",
            " 76% 2272/2982 [00:32<00:10, 68.49it/s]\u001b[A\n",
            " 77% 2288/2982 [00:33<00:10, 68.45it/s]\u001b[A\n",
            " 77% 2304/2982 [00:33<00:09, 68.53it/s]\u001b[A\n",
            " 78% 2320/2982 [00:33<00:09, 68.55it/s]\u001b[A\n",
            " 78% 2336/2982 [00:33<00:09, 68.52it/s]\u001b[A\n",
            " 79% 2352/2982 [00:34<00:09, 68.58it/s]\u001b[A\n",
            " 79% 2368/2982 [00:34<00:08, 68.51it/s]\u001b[A\n",
            " 80% 2384/2982 [00:34<00:08, 68.55it/s]\u001b[A\n",
            " 80% 2400/2982 [00:34<00:08, 68.54it/s]\u001b[A\n",
            " 81% 2416/2982 [00:35<00:08, 68.43it/s]\u001b[A\n",
            " 82% 2432/2982 [00:35<00:08, 68.55it/s]\u001b[A\n",
            " 82% 2448/2982 [00:35<00:07, 68.50it/s]\u001b[A\n",
            " 83% 2464/2982 [00:35<00:07, 68.57it/s]\u001b[A\n",
            " 83% 2480/2982 [00:35<00:07, 68.61it/s]\u001b[A\n",
            " 84% 2496/2982 [00:36<00:07, 68.57it/s]\u001b[A\n",
            " 84% 2512/2982 [00:36<00:06, 68.43it/s]\u001b[A\n",
            " 85% 2528/2982 [00:36<00:06, 68.55it/s]\u001b[A\n",
            " 85% 2544/2982 [00:36<00:06, 68.51it/s]\u001b[A\n",
            " 86% 2560/2982 [00:37<00:06, 68.43it/s]\u001b[A\n",
            " 86% 2576/2982 [00:37<00:05, 68.57it/s]\u001b[A\n",
            " 87% 2592/2982 [00:37<00:05, 68.63it/s]\u001b[A\n",
            " 87% 2608/2982 [00:37<00:05, 68.58it/s]\u001b[A\n",
            " 88% 2624/2982 [00:38<00:05, 68.59it/s]\u001b[A\n",
            " 89% 2640/2982 [00:38<00:04, 68.58it/s]\u001b[A\n",
            " 89% 2656/2982 [00:38<00:04, 68.59it/s]\u001b[A\n",
            " 90% 2672/2982 [00:38<00:04, 68.60it/s]\u001b[A\n",
            " 90% 2688/2982 [00:39<00:04, 68.51it/s]\u001b[A\n",
            " 91% 2704/2982 [00:39<00:04, 68.52it/s]\u001b[A\n",
            " 91% 2720/2982 [00:39<00:03, 68.51it/s]\u001b[A\n",
            " 92% 2736/2982 [00:39<00:03, 68.52it/s]\u001b[A\n",
            " 92% 2752/2982 [00:39<00:03, 68.53it/s]\u001b[A\n",
            " 93% 2768/2982 [00:40<00:03, 68.48it/s]\u001b[A\n",
            " 93% 2784/2982 [00:40<00:02, 68.46it/s]\u001b[A\n",
            " 94% 2800/2982 [00:40<00:02, 68.45it/s]\u001b[A\n",
            " 94% 2816/2982 [00:40<00:02, 68.51it/s]\u001b[A\n",
            " 95% 2832/2982 [00:41<00:02, 68.49it/s]\u001b[A\n",
            " 96% 2848/2982 [00:41<00:01, 68.52it/s]\u001b[A\n",
            " 96% 2864/2982 [00:41<00:01, 68.49it/s]\u001b[A\n",
            " 97% 2880/2982 [00:41<00:01, 68.52it/s]\u001b[A\n",
            " 97% 2896/2982 [00:42<00:01, 68.50it/s]\u001b[A\n",
            " 98% 2912/2982 [00:42<00:01, 68.47it/s]\u001b[A\n",
            " 98% 2928/2982 [00:42<00:00, 68.51it/s]\u001b[A\n",
            " 99% 2944/2982 [00:42<00:00, 68.50it/s]\u001b[A\n",
            " 99% 2960/2982 [00:42<00:00, 68.50it/s]\u001b[A\n",
            "100% 2982/2982 [00:43<00:00, 68.61it/s]\n",
            "\n",
            "  0% 0/2929 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 274/2929 [00:00<00:00, 2731.42it/s]\u001b[A\n",
            " 19% 553/2929 [00:00<00:00, 2763.66it/s]\u001b[A\n",
            " 29% 861/2929 [00:00<00:00, 2907.66it/s]\u001b[A\n",
            " 40% 1160/2929 [00:00<00:00, 2938.20it/s]\u001b[A\n",
            " 50% 1465/2929 [00:00<00:00, 2977.40it/s]\u001b[A\n",
            " 61% 1786/2929 [00:00<00:00, 3055.60it/s]\u001b[A\n",
            " 71% 2094/2929 [00:00<00:00, 3060.55it/s]\u001b[A\n",
            " 82% 2401/2929 [00:00<00:00, 2999.06it/s]\u001b[A\n",
            "100% 2929/2929 [00:00<00:00, 2991.19it/s]\n",
            "[09.26.21 03:58:45] Visualizing in TensorBoard...\n",
            "[09.26.21 03:58:45] Eval F1: 80.29, EM: 67.26\n",
            "100% 17524/17524 [12:38<00:00, 23.11it/s, NLL=0.489, epoch=0]\n",
            "[09.26.21 04:10:38] Epoch: 1\n",
            "100% 17524/17524 [11:53<00:00, 24.57it/s, NLL=0.579, epoch=1]\n",
            "[09.26.21 04:22:32] Epoch: 2\n",
            "100% 17524/17524 [11:53<00:00, 24.56it/s, NLL=0.333, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJoD26HW6rRB",
        "outputId": "7348b674-6617-460e-cf97-b7391a0f4466"
      },
      "source": [
        "%cd /content/robustqa_albert/robustqa_original/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name albert_squad_complete \\\n",
        "    --train-datasets squad03 \\\n",
        "    --eval-datasets race03 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa_albert/robustqa_original/robustqa_albert\n",
            "[09.26.21 04:34:31] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race03\",\n",
            "    \"eval_dir\": \"./datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"albert_squad_complete\",\n",
            "    \"save_dir\": \"save/albert_squad_complete-03\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad03\",\n",
            "    \"train_dir\": \"./datasets/indomain_train\",\n",
            "    \"val_dir\": \"./datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.26.21 04:34:31] Preparing Training Data...\n",
            "100% 17044/17044 [00:00<00:00, 21672.15it/s]\n",
            "Preprocessing not completely accurate for 15/17044 instances\n",
            "[09.26.21 04:34:38] Preparing Validation Data...\n",
            "100% 3373/3373 [00:00<00:00, 22695.15it/s]\n",
            "[09.26.21 04:34:42] Epoch: 0\n",
            "[09.26.21 04:34:42] Evaluating at step 0...\n",
            "  0% 16/17044 [00:00<12:06, 23.42it/s, NLL=1.62, epoch=0]\n",
            "  0% 0/3373 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/3373 [00:00<00:27, 120.66it/s]\u001b[A\n",
            "  1% 48/3373 [00:00<00:36, 91.46it/s] \u001b[A\n",
            "  2% 64/3373 [00:00<00:40, 81.16it/s]\u001b[A\n",
            "  2% 80/3373 [00:00<00:43, 76.21it/s]\u001b[A\n",
            "  3% 96/3373 [00:01<00:44, 73.33it/s]\u001b[A\n",
            "  3% 112/3373 [00:01<00:45, 71.72it/s]\u001b[A\n",
            "  4% 128/3373 [00:01<00:45, 70.73it/s]\u001b[A\n",
            "  4% 144/3373 [00:01<00:46, 69.95it/s]\u001b[A\n",
            "  5% 160/3373 [00:02<00:46, 69.38it/s]\u001b[A\n",
            "  5% 176/3373 [00:02<00:46, 69.11it/s]\u001b[A\n",
            "  6% 192/3373 [00:02<00:46, 68.81it/s]\u001b[A\n",
            "  6% 208/3373 [00:02<00:46, 68.66it/s]\u001b[A\n",
            "  7% 224/3373 [00:03<00:45, 68.60it/s]\u001b[A\n",
            "  7% 240/3373 [00:03<00:45, 68.55it/s]\u001b[A\n",
            "  8% 256/3373 [00:03<00:45, 68.50it/s]\u001b[A\n",
            "  8% 272/3373 [00:03<00:45, 68.39it/s]\u001b[A\n",
            "  9% 288/3373 [00:04<00:45, 68.37it/s]\u001b[A\n",
            "  9% 304/3373 [00:04<00:44, 68.35it/s]\u001b[A\n",
            "  9% 320/3373 [00:04<00:44, 68.37it/s]\u001b[A\n",
            " 10% 336/3373 [00:04<00:44, 68.41it/s]\u001b[A\n",
            " 10% 352/3373 [00:04<00:44, 68.42it/s]\u001b[A\n",
            " 11% 368/3373 [00:05<00:43, 68.39it/s]\u001b[A\n",
            " 11% 384/3373 [00:05<00:43, 68.37it/s]\u001b[A\n",
            " 12% 400/3373 [00:05<00:43, 68.40it/s]\u001b[A\n",
            " 12% 416/3373 [00:05<00:43, 68.35it/s]\u001b[A\n",
            " 13% 432/3373 [00:06<00:43, 68.38it/s]\u001b[A\n",
            " 13% 448/3373 [00:06<00:42, 68.41it/s]\u001b[A\n",
            " 14% 464/3373 [00:06<00:42, 68.37it/s]\u001b[A\n",
            " 14% 480/3373 [00:06<00:42, 68.32it/s]\u001b[A\n",
            " 15% 496/3373 [00:07<00:42, 68.38it/s]\u001b[A\n",
            " 15% 512/3373 [00:07<00:41, 68.41it/s]\u001b[A\n",
            " 16% 528/3373 [00:07<00:41, 68.31it/s]\u001b[A\n",
            " 16% 544/3373 [00:07<00:41, 68.40it/s]\u001b[A\n",
            " 17% 560/3373 [00:07<00:41, 68.39it/s]\u001b[A\n",
            " 17% 576/3373 [00:08<00:40, 68.41it/s]\u001b[A\n",
            " 18% 592/3373 [00:08<00:40, 68.43it/s]\u001b[A\n",
            " 18% 608/3373 [00:08<00:40, 68.41it/s]\u001b[A\n",
            " 18% 624/3373 [00:08<00:40, 68.38it/s]\u001b[A\n",
            " 19% 640/3373 [00:09<00:39, 68.43it/s]\u001b[A\n",
            " 19% 656/3373 [00:09<00:39, 68.43it/s]\u001b[A\n",
            " 20% 672/3373 [00:09<00:39, 68.36it/s]\u001b[A\n",
            " 20% 688/3373 [00:09<00:39, 68.41it/s]\u001b[A\n",
            " 21% 704/3373 [00:10<00:39, 68.40it/s]\u001b[A\n",
            " 21% 720/3373 [00:10<00:38, 68.36it/s]\u001b[A\n",
            " 22% 736/3373 [00:10<00:38, 68.33it/s]\u001b[A\n",
            " 22% 752/3373 [00:10<00:38, 68.38it/s]\u001b[A\n",
            " 23% 768/3373 [00:11<00:38, 68.36it/s]\u001b[A\n",
            " 23% 784/3373 [00:11<00:37, 68.41it/s]\u001b[A\n",
            " 24% 800/3373 [00:11<00:37, 68.40it/s]\u001b[A\n",
            " 24% 816/3373 [00:11<00:37, 68.39it/s]\u001b[A\n",
            " 25% 832/3373 [00:11<00:37, 68.40it/s]\u001b[A\n",
            " 25% 848/3373 [00:12<00:36, 68.43it/s]\u001b[A\n",
            " 26% 864/3373 [00:12<00:36, 68.39it/s]\u001b[A\n",
            " 26% 880/3373 [00:12<00:36, 68.38it/s]\u001b[A\n",
            " 27% 896/3373 [00:12<00:36, 68.33it/s]\u001b[A\n",
            " 27% 912/3373 [00:13<00:35, 68.38it/s]\u001b[A\n",
            " 28% 928/3373 [00:13<00:35, 68.30it/s]\u001b[A\n",
            " 28% 944/3373 [00:13<00:35, 68.42it/s]\u001b[A\n",
            " 28% 960/3373 [00:13<00:35, 68.39it/s]\u001b[A\n",
            " 29% 976/3373 [00:14<00:35, 68.37it/s]\u001b[A\n",
            " 29% 992/3373 [00:14<00:34, 68.39it/s]\u001b[A\n",
            " 30% 1008/3373 [00:14<00:34, 68.38it/s]\u001b[A\n",
            "  0% 16/17044 [00:15<12:06, 23.42it/s, NLL=1.62, epoch=0]\n",
            " 31% 1040/3373 [00:15<00:34, 68.45it/s]\u001b[A\n",
            " 31% 1056/3373 [00:15<00:33, 68.42it/s]\u001b[A\n",
            " 32% 1072/3373 [00:15<00:33, 68.48it/s]\u001b[A\n",
            " 32% 1088/3373 [00:15<00:33, 68.46it/s]\u001b[A\n",
            " 33% 1104/3373 [00:15<00:33, 68.35it/s]\u001b[A\n",
            " 33% 1120/3373 [00:16<00:32, 68.40it/s]\u001b[A\n",
            " 34% 1136/3373 [00:16<00:32, 68.48it/s]\u001b[A\n",
            " 34% 1152/3373 [00:16<00:32, 68.33it/s]\u001b[A\n",
            " 35% 1168/3373 [00:16<00:32, 68.45it/s]\u001b[A\n",
            " 35% 1184/3373 [00:17<00:31, 68.43it/s]\u001b[A\n",
            " 36% 1200/3373 [00:17<00:31, 68.34it/s]\u001b[A\n",
            " 36% 1216/3373 [00:17<00:31, 68.41it/s]\u001b[A\n",
            " 37% 1232/3373 [00:17<00:31, 68.38it/s]\u001b[A\n",
            " 37% 1248/3373 [00:18<00:31, 68.23it/s]\u001b[A\n",
            " 37% 1264/3373 [00:18<00:30, 68.35it/s]\u001b[A\n",
            " 38% 1280/3373 [00:18<00:30, 68.33it/s]\u001b[A\n",
            " 38% 1296/3373 [00:18<00:30, 68.22it/s]\u001b[A\n",
            " 39% 1312/3373 [00:18<00:30, 68.38it/s]\u001b[A\n",
            " 39% 1328/3373 [00:19<00:29, 68.38it/s]\u001b[A\n",
            " 40% 1344/3373 [00:19<00:29, 68.27it/s]\u001b[A\n",
            " 40% 1360/3373 [00:19<00:29, 68.41it/s]\u001b[A\n",
            " 41% 1376/3373 [00:19<00:29, 68.36it/s]\u001b[A\n",
            " 41% 1392/3373 [00:20<00:28, 68.44it/s]\u001b[A\n",
            " 42% 1408/3373 [00:20<00:28, 68.39it/s]\u001b[A\n",
            " 42% 1424/3373 [00:20<00:28, 68.40it/s]\u001b[A\n",
            " 43% 1440/3373 [00:20<00:28, 68.20it/s]\u001b[A\n",
            " 43% 1456/3373 [00:21<00:28, 68.43it/s]\u001b[A\n",
            " 44% 1472/3373 [00:21<00:27, 68.45it/s]\u001b[A\n",
            " 44% 1488/3373 [00:21<00:27, 68.36it/s]\u001b[A\n",
            " 45% 1504/3373 [00:21<00:27, 68.41it/s]\u001b[A\n",
            " 45% 1520/3373 [00:22<00:27, 68.39it/s]\u001b[A\n",
            " 46% 1536/3373 [00:22<00:26, 68.24it/s]\u001b[A\n",
            " 46% 1552/3373 [00:22<00:26, 68.46it/s]\u001b[A\n",
            " 46% 1568/3373 [00:22<00:26, 68.38it/s]\u001b[A\n",
            " 47% 1584/3373 [00:22<00:26, 68.43it/s]\u001b[A\n",
            " 47% 1600/3373 [00:23<00:25, 68.44it/s]\u001b[A\n",
            " 48% 1616/3373 [00:23<00:25, 68.37it/s]\u001b[A\n",
            " 48% 1632/3373 [00:23<00:25, 68.25it/s]\u001b[A\n",
            " 49% 1648/3373 [00:23<00:25, 68.38it/s]\u001b[A\n",
            " 49% 1664/3373 [00:24<00:25, 68.36it/s]\u001b[A\n",
            " 50% 1680/3373 [00:24<00:24, 68.26it/s]\u001b[A\n",
            " 50% 1696/3373 [00:24<00:24, 68.41it/s]\u001b[A\n",
            " 51% 1712/3373 [00:24<00:24, 68.42it/s]\u001b[A\n",
            " 51% 1728/3373 [00:25<00:24, 68.26it/s]\u001b[A\n",
            " 52% 1744/3373 [00:25<00:23, 68.44it/s]\u001b[A\n",
            " 52% 1760/3373 [00:25<00:23, 68.34it/s]\u001b[A\n",
            " 53% 1776/3373 [00:25<00:23, 68.36it/s]\u001b[A\n",
            " 53% 1792/3373 [00:26<00:23, 68.38it/s]\u001b[A\n",
            " 54% 1808/3373 [00:26<00:22, 68.36it/s]\u001b[A\n",
            " 54% 1824/3373 [00:26<00:22, 68.38it/s]\u001b[A\n",
            " 55% 1840/3373 [00:26<00:22, 68.37it/s]\u001b[A\n",
            " 55% 1856/3373 [00:26<00:22, 68.35it/s]\u001b[A\n",
            " 55% 1872/3373 [00:27<00:21, 68.31it/s]\u001b[A\n",
            " 56% 1888/3373 [00:27<00:21, 68.28it/s]\u001b[A\n",
            " 56% 1904/3373 [00:27<00:21, 68.41it/s]\u001b[A\n",
            " 57% 1920/3373 [00:27<00:21, 68.40it/s]\u001b[A\n",
            " 57% 1936/3373 [00:28<00:21, 68.39it/s]\u001b[A\n",
            " 58% 1952/3373 [00:28<00:20, 68.34it/s]\u001b[A\n",
            " 58% 1968/3373 [00:28<00:20, 68.27it/s]\u001b[A\n",
            " 59% 1984/3373 [00:28<00:20, 68.41it/s]\u001b[A\n",
            " 59% 2000/3373 [00:29<00:20, 68.40it/s]\u001b[A\n",
            " 60% 2016/3373 [00:29<00:19, 68.39it/s]\u001b[A\n",
            " 60% 2032/3373 [00:29<00:19, 68.42it/s]\u001b[A\n",
            " 61% 2048/3373 [00:29<00:19, 68.38it/s]\u001b[A\n",
            " 61% 2064/3373 [00:29<00:19, 68.37it/s]\u001b[A\n",
            " 62% 2080/3373 [00:30<00:18, 68.37it/s]\u001b[A\n",
            " 62% 2096/3373 [00:30<00:18, 68.36it/s]\u001b[A\n",
            " 63% 2112/3373 [00:30<00:18, 68.37it/s]\u001b[A\n",
            " 63% 2128/3373 [00:30<00:18, 68.39it/s]\u001b[A\n",
            " 64% 2144/3373 [00:31<00:17, 68.41it/s]\u001b[A\n",
            " 64% 2160/3373 [00:31<00:17, 68.39it/s]\u001b[A\n",
            " 65% 2176/3373 [00:31<00:17, 68.37it/s]\u001b[A\n",
            " 65% 2192/3373 [00:31<00:17, 68.34it/s]\u001b[A\n",
            " 65% 2208/3373 [00:32<00:17, 68.39it/s]\u001b[A\n",
            " 66% 2224/3373 [00:32<00:16, 68.39it/s]\u001b[A\n",
            " 66% 2240/3373 [00:32<00:16, 68.38it/s]\u001b[A\n",
            " 67% 2256/3373 [00:32<00:16, 68.36it/s]\u001b[A\n",
            " 67% 2272/3373 [00:33<00:16, 68.38it/s]\u001b[A\n",
            " 68% 2288/3373 [00:33<00:15, 68.24it/s]\u001b[A\n",
            " 68% 2304/3373 [00:33<00:15, 68.38it/s]\u001b[A\n",
            " 69% 2320/3373 [00:33<00:15, 68.35it/s]\u001b[A\n",
            " 69% 2336/3373 [00:33<00:15, 68.26it/s]\u001b[A\n",
            " 70% 2352/3373 [00:34<00:14, 68.30it/s]\u001b[A\n",
            " 70% 2368/3373 [00:34<00:14, 68.27it/s]\u001b[A\n",
            " 71% 2384/3373 [00:34<00:14, 68.36it/s]\u001b[A\n",
            " 71% 2400/3373 [00:34<00:14, 68.37it/s]\u001b[A\n",
            " 72% 2416/3373 [00:35<00:13, 68.36it/s]\u001b[A\n",
            " 72% 2432/3373 [00:35<00:13, 68.33it/s]\u001b[A\n",
            " 73% 2448/3373 [00:35<00:13, 68.24it/s]\u001b[A\n",
            " 73% 2464/3373 [00:35<00:13, 68.43it/s]\u001b[A\n",
            " 74% 2480/3373 [00:36<00:13, 68.41it/s]\u001b[A\n",
            " 74% 2496/3373 [00:36<00:12, 68.36it/s]\u001b[A\n",
            " 74% 2512/3373 [00:36<00:12, 68.35it/s]\u001b[A\n",
            " 75% 2528/3373 [00:36<00:12, 68.31it/s]\u001b[A\n",
            " 75% 2544/3373 [00:37<00:12, 68.33it/s]\u001b[A\n",
            " 76% 2560/3373 [00:37<00:11, 68.36it/s]\u001b[A\n",
            " 76% 2576/3373 [00:37<00:11, 68.32it/s]\u001b[A\n",
            " 77% 2592/3373 [00:37<00:11, 68.37it/s]\u001b[A\n",
            " 77% 2608/3373 [00:37<00:11, 68.38it/s]\u001b[A\n",
            " 78% 2624/3373 [00:38<00:10, 68.31it/s]\u001b[A\n",
            " 78% 2640/3373 [00:38<00:10, 68.40it/s]\u001b[A\n",
            " 79% 2656/3373 [00:38<00:10, 68.46it/s]\u001b[A\n",
            " 79% 2672/3373 [00:38<00:10, 68.36it/s]\u001b[A\n",
            " 80% 2688/3373 [00:39<00:10, 68.28it/s]\u001b[A\n",
            " 80% 2704/3373 [00:39<00:09, 68.36it/s]\u001b[A\n",
            " 81% 2720/3373 [00:39<00:09, 68.40it/s]\u001b[A\n",
            " 81% 2736/3373 [00:39<00:09, 68.41it/s]\u001b[A\n",
            " 82% 2752/3373 [00:40<00:09, 68.37it/s]\u001b[A\n",
            " 82% 2768/3373 [00:40<00:08, 68.37it/s]\u001b[A\n",
            " 83% 2784/3373 [00:40<00:08, 68.42it/s]\u001b[A\n",
            " 83% 2800/3373 [00:40<00:08, 68.42it/s]\u001b[A\n",
            " 83% 2816/3373 [00:40<00:08, 68.39it/s]\u001b[A\n",
            " 84% 2832/3373 [00:41<00:07, 68.39it/s]\u001b[A\n",
            " 84% 2848/3373 [00:41<00:07, 68.36it/s]\u001b[A\n",
            " 85% 2864/3373 [00:41<00:07, 68.37it/s]\u001b[A\n",
            " 85% 2880/3373 [00:41<00:07, 68.36it/s]\u001b[A\n",
            " 86% 2896/3373 [00:42<00:06, 68.31it/s]\u001b[A\n",
            " 86% 2912/3373 [00:42<00:06, 68.37it/s]\u001b[A\n",
            " 87% 2928/3373 [00:42<00:06, 68.38it/s]\u001b[A\n",
            " 87% 2944/3373 [00:42<00:06, 68.36it/s]\u001b[A\n",
            " 88% 2960/3373 [00:43<00:06, 68.39it/s]\u001b[A\n",
            " 88% 2976/3373 [00:43<00:05, 68.33it/s]\u001b[A\n",
            " 89% 2992/3373 [00:43<00:05, 68.39it/s]\u001b[A\n",
            " 89% 3008/3373 [00:43<00:05, 68.31it/s]\u001b[A\n",
            " 90% 3024/3373 [00:44<00:05, 68.31it/s]\u001b[A\n",
            " 90% 3040/3373 [00:44<00:04, 68.33it/s]\u001b[A\n",
            " 91% 3056/3373 [00:44<00:04, 68.32it/s]\u001b[A\n",
            " 91% 3072/3373 [00:44<00:04, 68.39it/s]\u001b[A\n",
            " 92% 3088/3373 [00:44<00:04, 68.38it/s]\u001b[A\n",
            " 92% 3104/3373 [00:45<00:03, 68.33it/s]\u001b[A\n",
            " 92% 3120/3373 [00:45<00:03, 68.37it/s]\u001b[A\n",
            " 93% 3136/3373 [00:45<00:03, 68.41it/s]\u001b[A\n",
            " 93% 3152/3373 [00:45<00:03, 68.42it/s]\u001b[A\n",
            " 94% 3168/3373 [00:46<00:02, 68.39it/s]\u001b[A\n",
            " 94% 3184/3373 [00:46<00:02, 68.39it/s]\u001b[A\n",
            " 95% 3200/3373 [00:46<00:02, 68.39it/s]\u001b[A\n",
            " 95% 3216/3373 [00:46<00:02, 68.43it/s]\u001b[A\n",
            " 96% 3232/3373 [00:47<00:02, 68.40it/s]\u001b[A\n",
            " 96% 3248/3373 [00:47<00:01, 68.41it/s]\u001b[A\n",
            " 97% 3264/3373 [00:47<00:01, 68.43it/s]\u001b[A\n",
            " 97% 3280/3373 [00:47<00:01, 68.45it/s]\u001b[A\n",
            " 98% 3296/3373 [00:48<00:01, 68.45it/s]\u001b[A\n",
            " 98% 3312/3373 [00:48<00:00, 68.40it/s]\u001b[A\n",
            " 99% 3328/3373 [00:48<00:00, 68.33it/s]\u001b[A\n",
            " 99% 3344/3373 [00:48<00:00, 68.39it/s]\u001b[A\n",
            "100% 3360/3373 [00:48<00:00, 68.39it/s]\u001b[A\n",
            "100% 3373/3373 [00:49<00:00, 68.59it/s]\n",
            "\n",
            "  0% 0/3335 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 271/3335 [00:00<00:01, 2705.48it/s]\u001b[A\n",
            " 16% 550/3335 [00:00<00:01, 2754.82it/s]\u001b[A\n",
            " 25% 834/3335 [00:00<00:00, 2792.37it/s]\u001b[A\n",
            " 33% 1117/3335 [00:00<00:00, 2804.50it/s]\u001b[A\n",
            " 42% 1400/3335 [00:00<00:00, 2809.88it/s]\u001b[A\n",
            " 50% 1682/3335 [00:00<00:00, 2810.66it/s]\u001b[A\n",
            " 59% 1965/3335 [00:00<00:00, 2815.16it/s]\u001b[A\n",
            " 67% 2247/3335 [00:00<00:00, 2809.35it/s]\u001b[A\n",
            " 76% 2528/3335 [00:00<00:00, 2787.43it/s]\u001b[A\n",
            " 84% 2807/3335 [00:01<00:00, 2780.77it/s]\u001b[A\n",
            "100% 3335/3335 [00:01<00:00, 2788.25it/s]\n",
            "[09.26.21 04:35:33] Visualizing in TensorBoard...\n",
            "[09.26.21 04:35:33] Eval F1: 78.20, EM: 62.94\n",
            "100% 17044/17044 [12:25<00:00, 22.86it/s, NLL=0.495, epoch=0]\n",
            "[09.26.21 04:47:07] Epoch: 1\n",
            "100% 17044/17044 [11:34<00:00, 24.55it/s, NLL=0.24, epoch=1]\n",
            "[09.26.21 04:58:41] Epoch: 2\n",
            "100% 17044/17044 [11:34<00:00, 24.55it/s, NLL=0.0776, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKbi6QuL707l"
      },
      "source": [
        "cp -R /content/robustqa_albert/ /content/drive/MyDrive/robustqa_change_transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0729dDvXZXPK"
      },
      "source": [
        "#Official Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXzuEKyx-JjF",
        "outputId": "ae5ca936-1b21-437c-833b-e81c23188153"
      },
      "source": [
        "%%writefile /content/robustqa/eval/official_eval.py\n",
        "\"\"\"Official evaluation script for SQuAD version 2.0.\n",
        "\n",
        "In addition to basic functionality, we also compute additional statistics and\n",
        "plot precision-recall curves if an additional na_prob.json file is provided.\n",
        "This file is expected to map question ID's to the model's predicted probability\n",
        "that a question is unanswerable.\n",
        "\"\"\"\n",
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "\n",
        "OPTS = None\n",
        "\n",
        "def parse_args():\n",
        "  parser = argparse.ArgumentParser('Official evaluation script for SQuAD version 2.0.')\n",
        "  parser.add_argument('data_file', metavar='data.json', help='Input data JSON file.')\n",
        "  parser.add_argument('pred_file', metavar='pred.json', help='Model predictions.')\n",
        "  parser.add_argument('--out-file', '-o', metavar='eval.json',\n",
        "                      help='Write accuracy metrics to file (default is stdout).')\n",
        "  parser.add_argument('--na-prob-file', '-n', metavar='na_prob.json',\n",
        "                      help='Model estimates of probability of no answer.')\n",
        "  parser.add_argument('--na-prob-thresh', '-t', type=float, default=1.0,\n",
        "                      help='Predict \"\" if no-answer probability exceeds this (default = 1.0).')\n",
        "  parser.add_argument('--out-image-dir', '-p', metavar='out_images', default=None,\n",
        "                      help='Save precision-recall curves to directory.')\n",
        "  parser.add_argument('--verbose', '-v', action='store_true')\n",
        "  if len(sys.argv) == 1:\n",
        "    parser.print_help()\n",
        "    sys.exit(1)\n",
        "  return parser.parse_args()\n",
        "\n",
        "def make_qid_to_has_ans(dataset):\n",
        "  qid_to_has_ans = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid_to_has_ans[qa['id']] = bool(qa['answers'])\n",
        "  return qid_to_has_ans\n",
        "\n",
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()\n",
        "\n",
        "def compute_exact(a_gold, a_pred):\n",
        "  return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  num_same = sum(common.values())\n",
        "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "def get_raw_scores(dataset, preds):\n",
        "  exact_scores = {}\n",
        "  f1_scores = {}\n",
        "  for article in dataset:\n",
        "    for p in article['paragraphs']:\n",
        "      for qa in p['qas']:\n",
        "        qid = qa['id']\n",
        "        gold_answers = [a['text'] for a in qa['answers']\n",
        "                        if normalize_answer(a['text'])]\n",
        "        if not gold_answers:\n",
        "          # For unanswerable questions, only correct answer is empty string\n",
        "          gold_answers = ['']\n",
        "        if qid not in preds:\n",
        "          print('Missing prediction for %s' % qid)\n",
        "          continue\n",
        "        a_pred = preds[qid]\n",
        "        # Take max over all gold answers\n",
        "        exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n",
        "        f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n",
        "  return exact_scores, f1_scores\n",
        "\n",
        "def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n",
        "  new_scores = {}\n",
        "  for qid, s in scores.items():\n",
        "    pred_na = na_probs[qid] > na_prob_thresh\n",
        "    if pred_na:\n",
        "      new_scores[qid] = float(not qid_to_has_ans[qid])\n",
        "    else:\n",
        "      new_scores[qid] = s\n",
        "  return new_scores\n",
        "\n",
        "def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n",
        "  if not qid_list:\n",
        "    total = len(exact_scores)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "  else:\n",
        "    total = len(qid_list)\n",
        "    return collections.OrderedDict([\n",
        "        ('exact', 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n",
        "        ('f1', 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n",
        "        ('total', total),\n",
        "    ])\n",
        "\n",
        "def merge_eval(main_eval, new_eval, prefix):\n",
        "  for k in new_eval:\n",
        "    main_eval['%s_%s' % (prefix, k)] = new_eval[k]\n",
        "\n",
        "def plot_pr_curve(precisions, recalls, out_image, title):\n",
        "  plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n",
        "  plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='b')\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.xlim([0.0, 1.05])\n",
        "  plt.ylim([0.0, 1.05])\n",
        "  plt.title(title)\n",
        "  plt.savefig(out_image)\n",
        "  plt.clf()\n",
        "\n",
        "def make_precision_recall_eval(scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "                               out_image=None, title=None):\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  true_pos = 0.0\n",
        "  cur_p = 1.0\n",
        "  cur_r = 0.0\n",
        "  precisions = [1.0]\n",
        "  recalls = [0.0]\n",
        "  avg_prec = 0.0\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid_to_has_ans[qid]:\n",
        "      true_pos += scores[qid]\n",
        "    cur_p = true_pos / float(i+1)\n",
        "    cur_r = true_pos / float(num_true_pos)\n",
        "    if i == len(qid_list) - 1 or na_probs[qid] != na_probs[qid_list[i+1]]:\n",
        "      # i.e., if we can put a threshold after this point\n",
        "      avg_prec += cur_p * (cur_r - recalls[-1])\n",
        "      precisions.append(cur_p)\n",
        "      recalls.append(cur_r)\n",
        "  if out_image:\n",
        "    plot_pr_curve(precisions, recalls, out_image, title)\n",
        "  return {'ap': 100.0 * avg_prec}\n",
        "\n",
        "def run_precision_recall_analysis(main_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, out_image_dir):\n",
        "  if out_image_dir and not os.path.exists(out_image_dir):\n",
        "    os.makedirs(out_image_dir)\n",
        "  num_true_pos = sum(1 for v in qid_to_has_ans.values() if v)\n",
        "  if num_true_pos == 0:\n",
        "    return\n",
        "  pr_exact = make_precision_recall_eval(\n",
        "      exact_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_exact.png'),\n",
        "      title='Precision-Recall curve for Exact Match score')\n",
        "  pr_f1 = make_precision_recall_eval(\n",
        "      f1_raw, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_f1.png'),\n",
        "      title='Precision-Recall curve for F1 score')\n",
        "  oracle_scores = {k: float(v) for k, v in qid_to_has_ans.items()}\n",
        "  pr_oracle = make_precision_recall_eval(\n",
        "      oracle_scores, na_probs, num_true_pos, qid_to_has_ans,\n",
        "      out_image=os.path.join(out_image_dir, 'pr_oracle.png'),\n",
        "      title='Oracle Precision-Recall curve (binary task of HasAns vs. NoAns)')\n",
        "  merge_eval(main_eval, pr_exact, 'pr_exact')\n",
        "  merge_eval(main_eval, pr_f1, 'pr_f1')\n",
        "  merge_eval(main_eval, pr_oracle, 'pr_oracle')\n",
        "\n",
        "def histogram_na_prob(na_probs, qid_list, image_dir, name):\n",
        "  if not qid_list:\n",
        "    return\n",
        "  x = [na_probs[k] for k in qid_list]\n",
        "  weights = np.ones_like(x) / float(len(x))\n",
        "  plt.hist(x, weights=weights, bins=20, range=(0.0, 1.0))\n",
        "  plt.xlabel('Model probability of no-answer')\n",
        "  plt.ylabel('Proportion of dataset')\n",
        "  plt.title('Histogram of no-answer probability: %s' % name)\n",
        "  plt.savefig(os.path.join(image_dir, 'na_prob_hist_%s.png' % name))\n",
        "  plt.clf()\n",
        "\n",
        "def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n",
        "  num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n",
        "  cur_score = num_no_ans\n",
        "  best_score = cur_score\n",
        "  best_thresh = 0.0\n",
        "  qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n",
        "  for i, qid in enumerate(qid_list):\n",
        "    if qid not in scores: continue\n",
        "    if qid_to_has_ans[qid]:\n",
        "      diff = scores[qid]\n",
        "    else:\n",
        "      if preds[qid]:\n",
        "        diff = -1\n",
        "      else:\n",
        "        diff = 0\n",
        "    cur_score += diff\n",
        "    if cur_score > best_score:\n",
        "      best_score = cur_score\n",
        "      best_thresh = na_probs[qid]\n",
        "  return 100.0 * best_score / len(scores), best_thresh\n",
        "\n",
        "def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n",
        "  best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n",
        "  best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n",
        "  main_eval['best_exact'] = best_exact\n",
        "  main_eval['best_exact_thresh'] = exact_thresh\n",
        "  main_eval['best_f1'] = best_f1\n",
        "  main_eval['best_f1_thresh'] = f1_thresh\n",
        "\n",
        "def main():\n",
        "  with open(OPTS.data_file) as f:\n",
        "    dataset_json = json.load(f)\n",
        "    dataset = dataset_json['data']\n",
        "  with open(OPTS.pred_file) as f:\n",
        "    preds = json.load(f)\n",
        "  if OPTS.na_prob_file:\n",
        "    with open(OPTS.na_prob_file) as f:\n",
        "      na_probs = json.load(f)\n",
        "  else:\n",
        "    na_probs = {k: 0.0 for k in preds}\n",
        "  qid_to_has_ans = make_qid_to_has_ans(dataset)  # maps qid to True/False\n",
        "  has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "  no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "  exact_raw, f1_raw = get_raw_scores(dataset, preds)\n",
        "  exact_thresh = apply_no_ans_threshold(exact_raw, na_probs, qid_to_has_ans,\n",
        "                                        OPTS.na_prob_thresh)\n",
        "  f1_thresh = apply_no_ans_threshold(f1_raw, na_probs, qid_to_has_ans,\n",
        "                                     OPTS.na_prob_thresh)\n",
        "  out_eval = make_eval_dict(exact_thresh, f1_thresh)\n",
        "  if has_ans_qids:\n",
        "    has_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=has_ans_qids)\n",
        "    merge_eval(out_eval, has_ans_eval, 'HasAns')\n",
        "  if no_ans_qids:\n",
        "    no_ans_eval = make_eval_dict(exact_thresh, f1_thresh, qid_list=no_ans_qids)\n",
        "    merge_eval(out_eval, no_ans_eval, 'NoAns')\n",
        "  if OPTS.na_prob_file:\n",
        "    find_all_best_thresh(out_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans)\n",
        "  if OPTS.na_prob_file and OPTS.out_image_dir:\n",
        "    run_precision_recall_analysis(out_eval, exact_raw, f1_raw, na_probs, \n",
        "                                  qid_to_has_ans, OPTS.out_image_dir)\n",
        "    histogram_na_prob(na_probs, has_ans_qids, OPTS.out_image_dir, 'hasAns')\n",
        "    histogram_na_prob(na_probs, no_ans_qids, OPTS.out_image_dir, 'noAns')\n",
        "  if OPTS.out_file:\n",
        "    with open(OPTS.out_file, 'w') as f:\n",
        "      json.dump(out_eval, f)\n",
        "  else:\n",
        "    print(json.dumps(out_eval, indent=2))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  OPTS = parse_args()\n",
        "  if OPTS.out_image_dir:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')\n",
        "    import matplotlib.pyplot as plt \n",
        "  main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing /content/robustqa/eval/official_eval.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAiPF33lZcKr"
      },
      "source": [
        "#Roberta HyperColumn Resnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2ayD5obpNj2"
      },
      "source": [
        "%pycat /content/robustqa/transformers/models/roberta/modeling_roberta.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAuXweEcI9Bv"
      },
      "source": [
        "# Hypercolumn Resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfmyHmGKphRR",
        "outputId": "0b0f4693-92e0-4a5a-9781-c0fbe8b0fe98"
      },
      "source": [
        "%%writefile /content/robustqa/transformers/models/roberta/modeling_roberta.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model. \"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from ...activations import ACT2FN, gelu\n",
        "from ...file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from ...utils import logging\n",
        "from .configuration_roberta import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        # End copy\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
        "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # if self.config.hyper_column:\n",
        "        # print(\"embedding = \" + str(embeddings.shape))\n",
        "        # hyper_columns = torch.zeros(embeddings.shape)\n",
        "        hyper_columns = torch.clone(embeddings) # [batch, 384, 768]\n",
        "        hyper_columns.fill_(0.0)\n",
        "        embeddings = torch.cat((embeddings, hyper_columns), dim=1) # [batch, 768, 768]\n",
        "        # print(\"embedding after = \" + str(embeddings.shape))\n",
        "        return embeddings\n",
        "\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n",
        "        \"\"\"\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n",
        "\n",
        "        Args:\n",
        "            inputs_embeds: torch.Tensor\n",
        "\n",
        "        Returns: torch.Tensor\n",
        "        \"\"\"\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        sequence_length = input_shape[1]\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n",
        "        )\n",
        "        return position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        # mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        # hidden_states = mixed_states[0]\n",
        "        # hyper_columns = mixed_states[1]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        # print(\"self attention = \" + str(len(outputs))) # len = 1\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print(\"self ouput = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"attention hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"attention hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        # print(\"attention outputs = \" + str(len(outputs)))\n",
        "        # print(outputs[0].shape)\n",
        "        outputs = (torch.cat((outputs[0], hyper_columns), dim=1),)\n",
        "        # print(\"ret outputs = \" + str(len(outputs)))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(hidden_states.shape)\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"intermediate hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"intermediate hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        # hidden_states.shape = [batch, 384, 3072]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=-1)\n",
        "        # print(\"intermediate after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print(\"input_tensor = \" + str(input_tensor.shape))\n",
        "        # print(\"hidden_states = \" + str(hidden_states.shape))\n",
        "        mixed_states = torch.split(hidden_states, 3072, dim=-1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"output hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"output hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print(hidden_states.shape)\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        # input_tensor.shape = [batch, 384, 768]\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor.split(384, dim=1)[0]) #problem!!\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=1)\n",
        "        # print(\"roberta output after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaHyperColumn(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 2)\n",
        "        # self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(len(hidden_states))\n",
        "        # print(hidden_states[0].shape)\n",
        "        mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"hyper columns hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"hyper column hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hyper_columns = self.dense(hidden_states)\n",
        "        # print(hyper_columns.shape)\n",
        "        # hyper_columns = nn.functional.interpolate(hyper_columns, 768)\n",
        "        \n",
        "        # hyper_columns = hyper_columns.unsqueeze(1)\n",
        "        # hyper_columns = self.interp(hyper_columns, size=(384, 768), mode='bicubic')\n",
        "        # hyper_columns = hyper_columns.squeeze(1)\n",
        "        \n",
        "        hyper_columns = self.interp(hyper_columns, size=768, mode='linear')\n",
        "        \n",
        "        # print(hyper_columns.shape)\n",
        "        hyper_columns = self.dropout(hyper_columns)\n",
        "        # hyper_columns = self.LayerNorm(hyper_columns)\n",
        "\n",
        "        hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "        self.hyper_columns = RobertaHyperColumn(config)\n",
        "        # self.linear = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # if isinstance(config.hidden_act, str):\n",
        "        #     self.resnet_act_fn = ACT2FN[config.hidden_act]\n",
        "        # else:\n",
        "        #     self.resnet_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # print(\"roberta layer hidden states = \" + str(hidden_states.shape))\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            assert hasattr(\n",
        "                self, \"crossattention\"\n",
        "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        \n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        outputs = self.hyper_columns(outputs)\n",
        "        # Implement Residual Net\n",
        "        mixed_tensor = torch.split(outputs[0], 384, dim=1)\n",
        "        outputs_tensor = mixed_tensor[0]\n",
        "        hyper_columns_tensor = mixed_tensor[1]\n",
        "        # mixed_attention = torch.split(attention_output, 384, dim=1)\n",
        "        # identity = mixed_attention[0]\n",
        "        identity = torch.split(attention_output, 384, dim=1)[0]\n",
        "        outputs_tensor = torch.add(outputs_tensor, identity)\n",
        "        # outputs_tensor = torch.cat((outputs_tensor, outputs_tensor_add), dim=-1)\n",
        "        # outputs_tensor = self.linear(outputs_tensor)\n",
        "        # outputs_tensor = self.resnet_act_fn(outputs_tensor)\n",
        "        outputs_tensor = torch.cat((outputs_tensor, hyper_columns_tensor), dim=1)\n",
        "        outputs = (outputs_tensor,)\n",
        "\n",
        "        # print(\"roberta layer outputs = \" + str(len(outputs)) + \" \" + str(outputs[0].shape))\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        # print(\"roberta encoder hidden states = \" + str(hidden_states.shape))\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        all_hyper_columns = ()\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            # print(\"roberta encoder after hidden states = \" + str(hidden_states.shape))\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            # if (i + 1) % 4 == 0:\n",
        "            hyper_column = torch.split(hidden_states, 384, dim=1)[1]\n",
        "            # print(\"encoder hyper column shape = \" + str(hyper_column.shape))\n",
        "            all_hyper_columns = all_hyper_columns + (hyper_column,)\n",
        "\n",
        "        # hidden_states = torch.split(hidden_states, 384, dim=1)[0]\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        hidden_states = torch.split(hidden_states, 384, dim=1)[0] # [batch, 384, 768]\n",
        "        # print(\"before = \" + str(hidden_states.shape))\n",
        "        to_add = (hidden_states,) + all_hyper_columns # len = 13\n",
        "        # print(\"len to add = \" + str(len(to_add)))\n",
        "        hidden_states = torch.cat(to_add, dim=1)\n",
        "        # print(\"after = \" + str(hidden_states.shape))\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(RobertaPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "\n",
        "    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", ROBERTA_START_DOCSTRING\n",
        ")\n",
        "class RobertaForCausalLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "            >>> import torch\n",
        "\n",
        "            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "            >>> config.is_decoder = True\n",
        "            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n",
        "\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dense(features)\n",
        "        x = gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # project back to size of vocabulary with bias\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _tie_weights(self):\n",
        "        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n",
        "        self.bias = self.decoder.bias\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
        "    pooled output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            position_ids=flat_position_ids,\n",
        "            token_type_ids=flat_token_type_ids,\n",
        "            attention_mask=flat_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=flat_inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size * config.num_hidden_layers, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # print(len(outputs))\n",
        "        # print(\"outputs shape = \" + str(outputs[0].shape))\n",
        "        sequence_output = torch.split(outputs[0], 384, dim=1)[0]\n",
        "        hyper_columns = torch.split(outputs[0], 384, dim=1)[1:]\n",
        "        # print(\"qa sequence = \" + str(sequence_output.shape))\n",
        "        # print(\"qa hyper columns len = \" + str(len(hyper_columns)))\n",
        "        hyper_columns_tensor = torch.cat(hyper_columns, dim=-1)\n",
        "\n",
        "        sequence_output = hyper_columns_tensor\n",
        "        \n",
        "        # sequence_output = torch.cat((sequence_output, hyper_columns_tensor), dim=-1)\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
        "    \"\"\"\n",
        "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
        "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
        "\n",
        "    Args:\n",
        "        x: torch.Tensor x:\n",
        "\n",
        "    Returns: torch.Tensor\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
        "    return incremental_indices.long() + padding_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/robustqa/transformers/models/roberta/modeling_roberta.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQAw4LImgnu_",
        "outputId": "7c6df5a1-4f3d-47fc-fc95-84c4406aa1d0"
      },
      "source": [
        "# Hypercolumn \n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name experiment \\\n",
        "    --train-datasets squad_experiment \\\n",
        "    --eval-datasets race_experiment \\\n",
        "    --eval-every 50 \\\n",
        "    --batch-size 4 \\\n",
        "    --num-epochs 1 \\\n",
        "    --save-dir delete/ \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/robustqa\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 11, in <module>\n",
            "    from transformers import RobertaForQuestionAnswering\n",
            "  File \"<frozen importlib._bootstrap>\", line 1032, in _handle_fromlist\n",
            "  File \"/content/robustqa/transformers/file_utils.py\", line 2141, in __getattr__\n",
            "    value = getattr(module, name)\n",
            "  File \"/content/robustqa/transformers/file_utils.py\", line 2140, in __getattr__\n",
            "    module = self._get_module(self._class_to_module[name])\n",
            "  File \"/content/robustqa/transformers/file_utils.py\", line 2149, in _get_module\n",
            "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
            "  File \"/usr/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"/content/robustqa/transformers/models/roberta/modeling_roberta.py\", line 813, in <module>\n",
            "    class RobertaModel(RobertaPreTrainedModel):\n",
            "  File \"/content/robustqa/transformers/models/roberta/modeling_roberta.py\", line 863, in RobertaModel\n",
            "    config_class=_CONFIG_FOR_DOC,\n",
            "TypeError: add_code_sample_docstrings() got an unexpected keyword argument 'tokenizer_class'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZNAxgRVZpCx"
      },
      "source": [
        "#Albert Hypercolumn CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0aM_vOgJLoV"
      },
      "source": [
        "# Hypercolumn CNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ7jGFytJH8K",
        "outputId": "833c4655-10b9-49d2-c3e1-f0c9fbf61b9a"
      },
      "source": [
        "%%writefile /content/robustqa/transformers/models/roberta/modeling_roberta.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model. \"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from ...activations import ACT2FN, gelu\n",
        "from ...file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from ...utils import logging\n",
        "from .configuration_roberta import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        # End copy\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
        "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # if self.config.hyper_column:\n",
        "        # print(\"embedding = \" + str(embeddings.shape))\n",
        "        # hyper_columns = torch.zeros(embeddings.shape)\n",
        "        hyper_columns = torch.clone(embeddings) # [batch, 384, 768]\n",
        "        hyper_columns.fill_(0.0)\n",
        "        embeddings = torch.cat((embeddings, hyper_columns), dim=1) # [batch, 768, 768]\n",
        "        # print(\"embedding after = \" + str(embeddings.shape))\n",
        "        return embeddings\n",
        "\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n",
        "        \"\"\"\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n",
        "\n",
        "        Args:\n",
        "            inputs_embeds: torch.Tensor\n",
        "\n",
        "        Returns: torch.Tensor\n",
        "        \"\"\"\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        sequence_length = input_shape[1]\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n",
        "        )\n",
        "        return position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        # mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        # hidden_states = mixed_states[0]\n",
        "        # hyper_columns = mixed_states[1]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        # print(\"self attention = \" + str(len(outputs))) # len = 1\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print(\"self ouput = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"attention hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"attention hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        # print(\"attention outputs = \" + str(len(outputs)))\n",
        "        # print(outputs[0].shape)\n",
        "        outputs = (torch.cat((outputs[0], hyper_columns), dim=1),)\n",
        "        # print(\"ret outputs = \" + str(len(outputs)))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(hidden_states.shape)\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"intermediate hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"intermediate hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        # hidden_states.shape = [batch, 384, 3072]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=-1)\n",
        "        # print(\"intermediate after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print(\"input_tensor = \" + str(input_tensor.shape))\n",
        "        # print(\"hidden_states = \" + str(hidden_states.shape))\n",
        "        mixed_states = torch.split(hidden_states, 3072, dim=-1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"output hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"output hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print(hidden_states.shape)\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        # input_tensor.shape = [batch, 384, 768]\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor.split(384, dim=1)[0]) #problem!!\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=1)\n",
        "        # print(\"roberta output after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaHyperColumn(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 2)\n",
        "        # self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(len(hidden_states))\n",
        "        # print(hidden_states[0].shape)\n",
        "        mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"hyper columns hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"hyper column hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hyper_columns = self.dense(hidden_states)\n",
        "        # print(hyper_columns.shape)\n",
        "        # hyper_columns = nn.functional.interpolate(hyper_columns, 768)\n",
        "        \n",
        "        # hyper_columns = hyper_columns.unsqueeze(1)\n",
        "        # hyper_columns = self.interp(hyper_columns, size=(384, 768), mode='bicubic')\n",
        "        # hyper_columns = hyper_columns.squeeze(1)\n",
        "        \n",
        "        hyper_columns = self.interp(hyper_columns, size=768, mode='linear')\n",
        "        \n",
        "        # print(hyper_columns.shape)\n",
        "        hyper_columns = self.dropout(hyper_columns)\n",
        "        # hyper_columns = self.LayerNorm(hyper_columns)\n",
        "\n",
        "        hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "        self.hyper_columns = RobertaHyperColumn(config)\n",
        "        # self.linear = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # if isinstance(config.hidden_act, str):\n",
        "        #     self.resnet_act_fn = ACT2FN[config.hidden_act]\n",
        "        # else:\n",
        "        #     self.resnet_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # print(\"roberta layer hidden states = \" + str(hidden_states.shape))\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            assert hasattr(\n",
        "                self, \"crossattention\"\n",
        "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        \n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        outputs = self.hyper_columns(outputs)\n",
        "        # Implement Residual Net\n",
        "        # mixed_tensor = torch.split(outputs[0], 384, dim=1)\n",
        "        # outputs_tensor = mixed_tensor[0]\n",
        "        # hyper_columns_tensor = mixed_tensor[1]\n",
        "        # # mixed_attention = torch.split(attention_output, 384, dim=1)\n",
        "        # # identity = mixed_attention[0]\n",
        "        # identity = torch.split(attention_output, 384, dim=1)[0]\n",
        "        # outputs_tensor = torch.add(outputs_tensor, identity)\n",
        "        # # outputs_tensor = torch.cat((outputs_tensor, outputs_tensor_add), dim=-1)\n",
        "        # # outputs_tensor = self.linear(outputs_tensor)\n",
        "        # # outputs_tensor = self.resnet_act_fn(outputs_tensor)\n",
        "        # outputs_tensor = torch.cat((outputs_tensor, hyper_columns_tensor), dim=1)\n",
        "        # outputs = (outputs_tensor,)\n",
        "\n",
        "        # print(\"roberta layer outputs = \" + str(len(outputs)) + \" \" + str(outputs[0].shape))\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        # print(\"roberta encoder hidden states = \" + str(hidden_states.shape))\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        all_hyper_columns = ()\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            # print(\"roberta encoder after hidden states = \" + str(hidden_states.shape))\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            # if (i + 1) % 4 == 0:\n",
        "            hyper_column = torch.split(hidden_states, 384, dim=1)[1]\n",
        "            # print(\"encoder hyper column shape = \" + str(hyper_column.shape))\n",
        "            all_hyper_columns = all_hyper_columns + (hyper_column,)\n",
        "\n",
        "        # hidden_states = torch.split(hidden_states, 384, dim=1)[0]\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        hidden_states = torch.split(hidden_states, 384, dim=1)[0] # [batch, 384, 768]\n",
        "        # print(\"before = \" + str(hidden_states.shape))\n",
        "        to_add = (hidden_states,) + all_hyper_columns # len = 13\n",
        "        # print(\"len to add = \" + str(len(to_add)))\n",
        "        hidden_states = torch.cat(to_add, dim=1)\n",
        "        # print(\"after = \" + str(hidden_states.shape))\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(RobertaPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "\n",
        "    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", ROBERTA_START_DOCSTRING\n",
        ")\n",
        "class RobertaForCausalLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "            >>> import torch\n",
        "\n",
        "            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "            >>> config.is_decoder = True\n",
        "            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n",
        "\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dense(features)\n",
        "        x = gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # project back to size of vocabulary with bias\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _tie_weights(self):\n",
        "        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n",
        "        self.bias = self.decoder.bias\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
        "    pooled output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            position_ids=flat_position_ids,\n",
        "            token_type_ids=flat_token_type_ids,\n",
        "            attention_mask=flat_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=flat_inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.qa_outputs = nn.Conv1d(in_channels=config.hidden_size * config.num_hidden_layers, out_channels=2,\n",
        "                                    kernel_size=5, padding='same')\n",
        "        # self.qa_outputs = nn.Linear(config.hidden_size * config.num_hidden_layers, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # print(len(outputs))\n",
        "        # print(\"outputs shape = \" + str(outputs[0].shape))\n",
        "        sequence_output = torch.split(outputs[0], 384, dim=1)[0]\n",
        "        hyper_columns = torch.split(outputs[0], 384, dim=1)[1:]\n",
        "        # print(\"qa sequence = \" + str(sequence_output.shape))\n",
        "        # print(\"qa hyper columns len = \" + str(len(hyper_columns)))\n",
        "        hyper_columns_tensor = torch.cat(hyper_columns, dim=-1)\n",
        "\n",
        "        sequence_output = hyper_columns_tensor\n",
        "        \n",
        "        # sequence_output = torch.cat((sequence_output, hyper_columns_tensor), dim=-1)\n",
        "\n",
        "        # CNN\n",
        "        sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
        "    \"\"\"\n",
        "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
        "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
        "\n",
        "    Args:\n",
        "        x: torch.Tensor x:\n",
        "\n",
        "    Returns: torch.Tensor\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
        "    return incremental_indices.long() + padding_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/robustqa/transformers/models/roberta/modeling_roberta.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOX3YhDsXLPv",
        "outputId": "355ec1ae-3aa8-4852-9b45-71b6c380058d"
      },
      "source": [
        "%cd /content/robustqa_albert/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name experiment \\\n",
        "    --train-datasets squad_experiment \\\n",
        "    --eval-datasets race_experiment \\\n",
        "    --eval-every 50 \\\n",
        "    --batch-size 4 \\\n",
        "    --num-epochs 1 \\\n",
        "    --save-dir delete/ \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/robustqa_albert/'\n",
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'qa_outputs.bias', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'qa_outputs.weight', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[09.30.21 18:24:36] Args: {\n",
            "    \"batch_size\": 4,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race_experiment\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 50,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 1,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"experiment\",\n",
            "    \"save_dir\": \"delete/experiment-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad_experiment\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[09.30.21 18:24:36] Preparing Training Data...\n",
            "100% 40/40 [00:00<00:00, 21925.27it/s]\n",
            "Preprocessing not completely accurate for 1/40 instances\n",
            "[09.30.21 18:24:36] Preparing Validation Data...\n",
            "100% 18/18 [00:00<00:00, 24456.58it/s]\n",
            "[09.30.21 18:24:39] Epoch: 0\n",
            "  0% 0/40 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 360, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 330, in main\n",
            "    best_scores = trainer.train(model, train_loader, val_loader, val_dict)\n",
            "  File \"train.py\", line 237, in train\n",
            "    end_positions=end_positions)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/robustqa/transformers/models/roberta/modeling_roberta.py\", line 1631, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/robustqa/transformers/models/roberta/modeling_roberta.py\", line 972, in forward\n",
            "    return_dict=return_dict,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/robustqa/transformers/models/roberta/modeling_roberta.py\", line 635, in forward\n",
            "    output_attentions,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/robustqa/transformers/models/roberta/modeling_roberta.py\", line 550, in forward\n",
            "    printf(\"output before hyper columns = \" + str(outputs[0].shape))\n",
            "NameError: name 'printf' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGeKVoVVXLE-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZMzrAiC8-ba",
        "outputId": "b034e796-cb9e-429a-8223-bd16d104ac7e"
      },
      "source": [
        "# Resnet + Hypercolumn\n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name experiment \\\n",
        "    --train-datasets squad_experiment \\\n",
        "    --eval-datasets race_experiment \\\n",
        "    --eval-every 50 \\\n",
        "    --batch-size 4 \\\n",
        "    --num-epochs 1 \\\n",
        "    --save-dir delete/ \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 957kB/s] \n",
            "Downloading: 100% 446k/446k [00:00<00:00, 607kB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 1.44MB/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 426kB/s]\n",
            "Downloading: 100% 478M/478M [00:07<00:00, 67.8MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'qa_outputs.bias', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'qa_outputs.weight', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[10.02.21 01:43:01] Args: {\n",
            "    \"batch_size\": 4,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race_experiment\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 50,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 1,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"experiment\",\n",
            "    \"save_dir\": \"delete/experiment-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad_experiment\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.02.21 01:43:01] Preparing Training Data...\n",
            "100% 40/40 [00:00<00:00, 19373.23it/s]\n",
            "Preprocessing not completely accurate for 1/40 instances\n",
            "[10.02.21 01:43:02] Preparing Validation Data...\n",
            "100% 18/18 [00:00<00:00, 21870.65it/s]\n",
            "[10.02.21 01:43:11] Epoch: 0\n",
            "  0% 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.02.21 01:43:12] Evaluating at step 0...\n",
            " 10% 4/40 [00:01<00:09,  3.95it/s, NLL=5.84, epoch=0]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            " 44% 8/18 [00:00<00:00, 28.08it/s]\u001b[A\n",
            " 67% 12/18 [00:00<00:00, 19.50it/s]\u001b[A\n",
            " 89% 16/18 [00:00<00:00, 15.79it/s]\u001b[A\n",
            "100% 18/18 [00:01<00:00, 15.11it/s]\n",
            "\n",
            "100% 18/18 [00:00<00:00, 2583.41it/s]\n",
            "[10.02.21 01:43:13] Visualizing in TensorBoard...\n",
            "[10.02.21 01:43:13] Eval F1: 03.96, EM: 00.00\n",
            "100% 40/40 [00:15<00:00,  2.53it/s, NLL=5.7, epoch=0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoXoSsh8sWDt"
      },
      "source": [
        "!rm -r /content/robustqa/delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WRFcl0-XK3y",
        "outputId": "163cb533-efd8-428a-c767-21e696bad9e2"
      },
      "source": [
        "# Resnet + Hypercolumn\n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_resnet_hypercolumn_before_intermediate \\\n",
        "    --train-datasets squad01 \\\n",
        "    --eval-datasets race01 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 6.18MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 3.89MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 7.49MB/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 717kB/s]\n",
            "Downloading: 100% 478M/478M [00:16<00:00, 30.2MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.10.hyper_columns.dense.bias', 'qa_outputs.bias', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'qa_outputs.weight', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[10.05.21 01:54:28] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_resnet_hypercolumn_before_intermediate\",\n",
            "    \"save_dir\": \"save/squad_resnet_hypercolumn_before_intermediate-02\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.05.21 01:54:28] Preparing Training Data...\n",
            "100% 16020/16020 [00:00<00:00, 21977.12it/s]\n",
            "Preprocessing not completely accurate for 150/16020 instances\n",
            "[10.05.21 01:54:35] Preparing Validation Data...\n",
            "100% 4436/4436 [00:00<00:00, 22236.03it/s]\n",
            "[10.05.21 01:54:44] Epoch: 0\n",
            "  0% 0/16020 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.05.21 01:54:45] Evaluating at step 0...\n",
            "  0% 16/16020 [00:01<18:55, 14.09it/s, NLL=6, epoch=0]\n",
            "  0% 0/4436 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4436 [00:00<00:40, 109.49it/s]\u001b[A\n",
            "  1% 48/4436 [00:00<00:55, 79.35it/s] \u001b[A\n",
            "  1% 64/4436 [00:00<01:03, 69.27it/s]\u001b[A\n",
            "  2% 80/4436 [00:01<01:08, 64.03it/s]\u001b[A\n",
            "  2% 96/4436 [00:01<01:10, 61.87it/s]\u001b[A\n",
            "  3% 112/4436 [00:01<01:11, 60.31it/s]\u001b[A\n",
            "  3% 128/4436 [00:01<01:12, 59.33it/s]\u001b[A\n",
            "  3% 144/4436 [00:02<01:13, 58.74it/s]\u001b[A\n",
            "  4% 160/4436 [00:02<01:13, 58.35it/s]\u001b[A\n",
            "  4% 176/4436 [00:02<01:13, 58.01it/s]\u001b[A\n",
            "  4% 192/4436 [00:03<01:13, 57.74it/s]\u001b[A\n",
            "  5% 208/4436 [00:03<01:13, 57.64it/s]\u001b[A\n",
            "  5% 224/4436 [00:03<01:13, 57.32it/s]\u001b[A\n",
            "  5% 240/4436 [00:03<01:13, 57.38it/s]\u001b[A\n",
            "  6% 256/4436 [00:04<01:12, 57.51it/s]\u001b[A\n",
            "  6% 272/4436 [00:04<01:12, 57.46it/s]\u001b[A\n",
            "  6% 288/4436 [00:04<01:12, 57.48it/s]\u001b[A\n",
            "  7% 304/4436 [00:05<01:11, 57.49it/s]\u001b[A\n",
            "  7% 320/4436 [00:05<01:11, 57.47it/s]\u001b[A\n",
            "  8% 336/4436 [00:05<01:11, 57.43it/s]\u001b[A\n",
            "  8% 352/4436 [00:05<01:11, 57.36it/s]\u001b[A\n",
            "  8% 368/4436 [00:06<01:11, 57.12it/s]\u001b[A\n",
            "  9% 384/4436 [00:06<01:10, 57.37it/s]\u001b[A\n",
            "  9% 400/4436 [00:06<01:10, 57.32it/s]\u001b[A\n",
            "  9% 416/4436 [00:06<01:10, 57.13it/s]\u001b[A\n",
            " 10% 432/4436 [00:07<01:09, 57.34it/s]\u001b[A\n",
            " 10% 448/4436 [00:07<01:09, 57.18it/s]\u001b[A\n",
            " 10% 464/4436 [00:07<01:09, 57.32it/s]\u001b[A\n",
            " 11% 480/4436 [00:08<01:08, 57.36it/s]\u001b[A\n",
            " 11% 496/4436 [00:08<01:08, 57.31it/s]\u001b[A\n",
            " 12% 512/4436 [00:08<01:08, 57.33it/s]\u001b[A\n",
            " 12% 528/4436 [00:08<01:08, 57.29it/s]\u001b[A\n",
            " 12% 544/4436 [00:09<01:07, 57.27it/s]\u001b[A\n",
            " 13% 560/4436 [00:09<01:07, 57.27it/s]\u001b[A\n",
            " 13% 576/4436 [00:09<01:07, 57.27it/s]\u001b[A\n",
            " 13% 592/4436 [00:10<01:07, 57.10it/s]\u001b[A\n",
            " 14% 608/4436 [00:10<01:06, 57.31it/s]\u001b[A\n",
            " 14% 624/4436 [00:10<01:06, 57.30it/s]\u001b[A\n",
            " 14% 640/4436 [00:10<01:06, 57.27it/s]\u001b[A\n",
            " 15% 656/4436 [00:11<01:06, 57.22it/s]\u001b[A\n",
            " 15% 672/4436 [00:11<01:05, 57.33it/s]\u001b[A\n",
            " 16% 688/4436 [00:11<01:05, 57.32it/s]\u001b[A\n",
            " 16% 704/4436 [00:12<01:05, 57.26it/s]\u001b[A\n",
            " 16% 720/4436 [00:12<01:04, 57.26it/s]\u001b[A\n",
            " 17% 736/4436 [00:12<01:04, 57.15it/s]\u001b[A\n",
            " 17% 752/4436 [00:12<01:04, 57.14it/s]\u001b[A\n",
            "  0% 16/16020 [00:14<18:55, 14.09it/s, NLL=6, epoch=0]\n",
            " 18% 784/4436 [00:13<01:03, 57.38it/s]\u001b[A\n",
            " 18% 800/4436 [00:13<01:03, 57.30it/s]\u001b[A\n",
            " 18% 816/4436 [00:13<01:03, 57.27it/s]\u001b[A\n",
            " 19% 832/4436 [00:14<01:02, 57.45it/s]\u001b[A\n",
            " 19% 848/4436 [00:14<01:02, 57.45it/s]\u001b[A\n",
            " 19% 864/4436 [00:14<01:02, 57.43it/s]\u001b[A\n",
            " 20% 880/4436 [00:15<01:02, 57.02it/s]\u001b[A\n",
            " 20% 896/4436 [00:15<01:01, 57.34it/s]\u001b[A\n",
            " 21% 912/4436 [00:15<01:01, 57.48it/s]\u001b[A\n",
            " 21% 928/4436 [00:15<01:01, 57.33it/s]\u001b[A\n",
            " 21% 944/4436 [00:16<01:01, 57.15it/s]\u001b[A\n",
            " 22% 960/4436 [00:16<01:00, 57.31it/s]\u001b[A\n",
            " 22% 976/4436 [00:16<01:00, 57.43it/s]\u001b[A\n",
            " 22% 992/4436 [00:17<01:00, 57.40it/s]\u001b[A\n",
            " 23% 1008/4436 [00:17<00:59, 57.35it/s]\u001b[A\n",
            " 23% 1024/4436 [00:17<00:59, 57.23it/s]\u001b[A\n",
            " 23% 1040/4436 [00:17<00:59, 57.40it/s]\u001b[A\n",
            " 24% 1056/4436 [00:18<00:58, 57.41it/s]\u001b[A\n",
            " 24% 1072/4436 [00:18<00:58, 57.38it/s]\u001b[A\n",
            " 25% 1088/4436 [00:18<00:58, 57.38it/s]\u001b[A\n",
            " 25% 1104/4436 [00:19<00:58, 57.21it/s]\u001b[A\n",
            " 25% 1120/4436 [00:19<00:57, 57.40it/s]\u001b[A\n",
            " 26% 1136/4436 [00:19<00:57, 57.41it/s]\u001b[A\n",
            " 26% 1152/4436 [00:19<00:57, 57.22it/s]\u001b[A\n",
            " 26% 1168/4436 [00:20<00:56, 57.38it/s]\u001b[A\n",
            " 27% 1184/4436 [00:20<00:56, 57.36it/s]\u001b[A\n",
            " 27% 1200/4436 [00:20<00:56, 57.34it/s]\u001b[A\n",
            " 27% 1216/4436 [00:20<00:56, 57.31it/s]\u001b[A\n",
            " 28% 1232/4436 [00:21<00:55, 57.32it/s]\u001b[A\n",
            " 28% 1248/4436 [00:21<00:55, 57.09it/s]\u001b[A\n",
            " 28% 1264/4436 [00:21<00:55, 57.38it/s]\u001b[A\n",
            " 29% 1280/4436 [00:22<00:55, 57.33it/s]\u001b[A\n",
            " 29% 1296/4436 [00:22<00:54, 57.25it/s]\u001b[A\n",
            " 30% 1312/4436 [00:22<00:54, 57.37it/s]\u001b[A\n",
            " 30% 1328/4436 [00:22<00:54, 57.23it/s]\u001b[A\n",
            " 30% 1344/4436 [00:23<00:54, 57.06it/s]\u001b[A\n",
            " 31% 1360/4436 [00:23<00:53, 57.42it/s]\u001b[A\n",
            " 31% 1376/4436 [00:23<00:53, 57.30it/s]\u001b[A\n",
            " 31% 1392/4436 [00:24<00:53, 57.21it/s]\u001b[A\n",
            " 32% 1408/4436 [00:24<00:52, 57.32it/s]\u001b[A\n",
            " 32% 1424/4436 [00:24<00:52, 57.32it/s]\u001b[A\n",
            " 32% 1440/4436 [00:24<00:52, 57.35it/s]\u001b[A\n",
            " 33% 1456/4436 [00:25<00:51, 57.32it/s]\u001b[A\n",
            " 33% 1472/4436 [00:25<00:51, 57.13it/s]\u001b[A\n",
            " 34% 1488/4436 [00:25<00:51, 57.31it/s]\u001b[A\n",
            " 34% 1504/4436 [00:25<00:51, 57.36it/s]\u001b[A\n",
            " 34% 1520/4436 [00:26<00:50, 57.34it/s]\u001b[A\n",
            " 35% 1536/4436 [00:26<00:50, 57.29it/s]\u001b[A\n",
            " 35% 1552/4436 [00:26<00:50, 57.03it/s]\u001b[A\n",
            " 35% 1568/4436 [00:27<00:50, 57.24it/s]\u001b[A\n",
            " 36% 1584/4436 [00:27<00:49, 57.40it/s]\u001b[A\n",
            " 36% 1600/4436 [00:27<00:49, 57.34it/s]\u001b[A\n",
            " 36% 1616/4436 [00:27<00:49, 57.18it/s]\u001b[A\n",
            " 37% 1632/4436 [00:28<00:48, 57.38it/s]\u001b[A\n",
            " 37% 1648/4436 [00:28<00:48, 57.31it/s]\u001b[A\n",
            " 38% 1664/4436 [00:28<00:48, 57.35it/s]\u001b[A\n",
            " 38% 1680/4436 [00:29<00:48, 57.31it/s]\u001b[A\n",
            " 38% 1696/4436 [00:29<00:48, 57.06it/s]\u001b[A\n",
            " 39% 1712/4436 [00:29<00:47, 57.34it/s]\u001b[A\n",
            " 39% 1728/4436 [00:29<00:47, 57.29it/s]\u001b[A\n",
            " 39% 1744/4436 [00:30<00:47, 57.25it/s]\u001b[A\n",
            " 40% 1760/4436 [00:30<00:46, 57.15it/s]\u001b[A\n",
            " 40% 1776/4436 [00:30<00:46, 57.34it/s]\u001b[A\n",
            " 40% 1792/4436 [00:31<00:46, 57.18it/s]\u001b[A\n",
            " 41% 1808/4436 [00:31<00:46, 57.10it/s]\u001b[A\n",
            " 41% 1824/4436 [00:31<00:45, 57.43it/s]\u001b[A\n",
            " 41% 1840/4436 [00:31<00:45, 57.21it/s]\u001b[A\n",
            " 42% 1856/4436 [00:32<00:45, 57.33it/s]\u001b[A\n",
            " 42% 1872/4436 [00:32<00:44, 57.35it/s]\u001b[A\n",
            " 43% 1888/4436 [00:32<00:44, 57.31it/s]\u001b[A\n",
            " 43% 1904/4436 [00:32<00:44, 57.28it/s]\u001b[A\n",
            " 43% 1920/4436 [00:33<00:44, 57.16it/s]\u001b[A\n",
            " 44% 1936/4436 [00:33<00:43, 57.35it/s]\u001b[A\n",
            " 44% 1952/4436 [00:33<00:43, 57.14it/s]\u001b[A\n",
            " 44% 1968/4436 [00:34<00:43, 57.35it/s]\u001b[A\n",
            " 45% 1984/4436 [00:34<00:42, 57.33it/s]\u001b[A\n",
            " 45% 2000/4436 [00:34<00:42, 57.14it/s]\u001b[A\n",
            " 45% 2016/4436 [00:34<00:42, 57.16it/s]\u001b[A\n",
            " 46% 2032/4436 [00:35<00:41, 57.38it/s]\u001b[A\n",
            " 46% 2048/4436 [00:35<00:41, 57.18it/s]\u001b[A\n",
            " 47% 2064/4436 [00:35<00:41, 57.11it/s]\u001b[A\n",
            " 47% 2080/4436 [00:36<00:41, 57.21it/s]\u001b[A\n",
            " 47% 2096/4436 [00:36<00:40, 57.36it/s]\u001b[A\n",
            " 48% 2112/4436 [00:36<00:40, 57.35it/s]\u001b[A\n",
            " 48% 2128/4436 [00:36<00:40, 57.27it/s]\u001b[A\n",
            " 48% 2144/4436 [00:37<00:40, 57.11it/s]\u001b[A\n",
            " 49% 2160/4436 [00:37<00:39, 57.36it/s]\u001b[A\n",
            " 49% 2176/4436 [00:37<00:39, 57.36it/s]\u001b[A\n",
            " 49% 2192/4436 [00:37<00:39, 57.40it/s]\u001b[A\n",
            " 50% 2208/4436 [00:38<00:39, 57.08it/s]\u001b[A\n",
            " 50% 2224/4436 [00:38<00:38, 57.45it/s]\u001b[A\n",
            " 50% 2240/4436 [00:38<00:38, 57.22it/s]\u001b[A\n",
            " 51% 2256/4436 [00:39<00:37, 57.41it/s]\u001b[A\n",
            " 51% 2272/4436 [00:39<00:37, 57.14it/s]\u001b[A\n",
            " 52% 2288/4436 [00:39<00:37, 57.34it/s]\u001b[A\n",
            " 52% 2304/4436 [00:39<00:37, 57.31it/s]\u001b[A\n",
            " 52% 2320/4436 [00:40<00:36, 57.38it/s]\u001b[A\n",
            " 53% 2336/4436 [00:40<00:36, 57.36it/s]\u001b[A\n",
            " 53% 2352/4436 [00:40<00:36, 57.07it/s]\u001b[A\n",
            " 53% 2368/4436 [00:41<00:36, 57.40it/s]\u001b[A\n",
            " 54% 2384/4436 [00:41<00:35, 57.37it/s]\u001b[A\n",
            " 54% 2400/4436 [00:41<00:35, 57.34it/s]\u001b[A\n",
            " 54% 2416/4436 [00:41<00:35, 57.32it/s]\u001b[A\n",
            " 55% 2432/4436 [00:42<00:35, 57.18it/s]\u001b[A\n",
            " 55% 2448/4436 [00:42<00:34, 57.36it/s]\u001b[A\n",
            " 56% 2464/4436 [00:42<00:34, 57.34it/s]\u001b[A\n",
            " 56% 2480/4436 [00:43<00:34, 57.37it/s]\u001b[A\n",
            " 56% 2496/4436 [00:43<00:33, 57.37it/s]\u001b[A\n",
            " 57% 2512/4436 [00:43<00:33, 57.38it/s]\u001b[A\n",
            " 57% 2528/4436 [00:43<00:33, 57.33it/s]\u001b[A\n",
            " 57% 2544/4436 [00:44<00:32, 57.36it/s]\u001b[A\n",
            " 58% 2560/4436 [00:44<00:32, 57.38it/s]\u001b[A\n",
            " 58% 2576/4436 [00:44<00:32, 57.00it/s]\u001b[A\n",
            " 58% 2592/4436 [00:44<00:32, 57.49it/s]\u001b[A\n",
            " 59% 2608/4436 [00:45<00:31, 57.45it/s]\u001b[A\n",
            " 59% 2624/4436 [00:45<00:31, 57.46it/s]\u001b[A\n",
            " 60% 2640/4436 [00:45<00:31, 57.32it/s]\u001b[A\n",
            " 60% 2656/4436 [00:46<00:31, 57.41it/s]\u001b[A\n",
            " 60% 2672/4436 [00:46<00:30, 57.27it/s]\u001b[A\n",
            " 61% 2688/4436 [00:46<00:30, 57.19it/s]\u001b[A\n",
            " 61% 2704/4436 [00:46<00:30, 57.23it/s]\u001b[A\n",
            " 61% 2720/4436 [00:47<00:29, 57.27it/s]\u001b[A\n",
            " 62% 2736/4436 [00:47<00:29, 57.16it/s]\u001b[A\n",
            " 62% 2752/4436 [00:47<00:29, 57.19it/s]\u001b[A\n",
            " 62% 2768/4436 [00:48<00:29, 57.40it/s]\u001b[A\n",
            " 63% 2784/4436 [00:48<00:28, 57.32it/s]\u001b[A\n",
            " 63% 2800/4436 [00:48<00:28, 56.96it/s]\u001b[A\n",
            " 63% 2816/4436 [00:48<00:28, 57.36it/s]\u001b[A\n",
            " 64% 2832/4436 [00:49<00:27, 57.34it/s]\u001b[A\n",
            " 64% 2848/4436 [00:49<00:27, 57.32it/s]\u001b[A\n",
            " 65% 2864/4436 [00:49<00:27, 57.26it/s]\u001b[A\n",
            " 65% 2880/4436 [00:50<00:27, 57.32it/s]\u001b[A\n",
            " 65% 2896/4436 [00:50<00:26, 57.30it/s]\u001b[A\n",
            " 66% 2912/4436 [00:50<00:26, 57.33it/s]\u001b[A\n",
            " 66% 2928/4436 [00:50<00:26, 57.26it/s]\u001b[A\n",
            " 66% 2944/4436 [00:51<00:26, 57.11it/s]\u001b[A\n",
            " 67% 2960/4436 [00:51<00:25, 57.07it/s]\u001b[A\n",
            " 67% 2976/4436 [00:51<00:25, 57.44it/s]\u001b[A\n",
            " 67% 2992/4436 [00:51<00:25, 57.38it/s]\u001b[A\n",
            " 68% 3008/4436 [00:52<00:24, 57.20it/s]\u001b[A\n",
            " 68% 3024/4436 [00:52<00:24, 57.22it/s]\u001b[A\n",
            " 69% 3040/4436 [00:52<00:24, 57.44it/s]\u001b[A\n",
            " 69% 3056/4436 [00:53<00:24, 57.39it/s]\u001b[A\n",
            " 69% 3072/4436 [00:53<00:23, 57.37it/s]\u001b[A\n",
            " 70% 3088/4436 [00:53<00:23, 57.27it/s]\u001b[A\n",
            " 70% 3104/4436 [00:53<00:23, 57.35it/s]\u001b[A\n",
            " 70% 3120/4436 [00:54<00:22, 57.36it/s]\u001b[A\n",
            " 71% 3136/4436 [00:54<00:22, 57.35it/s]\u001b[A\n",
            " 71% 3152/4436 [00:54<00:22, 57.16it/s]\u001b[A\n",
            " 71% 3168/4436 [00:55<00:22, 57.05it/s]\u001b[A\n",
            " 72% 3184/4436 [00:55<00:21, 57.46it/s]\u001b[A\n",
            " 72% 3200/4436 [00:55<00:21, 57.07it/s]\u001b[A\n",
            " 72% 3216/4436 [00:55<00:21, 57.50it/s]\u001b[A\n",
            " 73% 3232/4436 [00:56<00:20, 57.40it/s]\u001b[A\n",
            " 73% 3248/4436 [00:56<00:20, 57.37it/s]\u001b[A\n",
            " 74% 3264/4436 [00:56<00:20, 57.37it/s]\u001b[A\n",
            " 74% 3280/4436 [00:56<00:20, 57.42it/s]\u001b[A\n",
            " 74% 3296/4436 [00:57<00:19, 57.38it/s]\u001b[A\n",
            " 75% 3312/4436 [00:57<00:19, 57.15it/s]\u001b[A\n",
            " 75% 3328/4436 [00:57<00:19, 57.41it/s]\u001b[A\n",
            " 75% 3344/4436 [00:58<00:19, 57.29it/s]\u001b[A\n",
            " 76% 3360/4436 [00:58<00:18, 57.41it/s]\u001b[A\n",
            " 76% 3376/4436 [00:58<00:18, 57.32it/s]\u001b[A\n",
            " 76% 3392/4436 [00:58<00:18, 57.21it/s]\u001b[A\n",
            " 77% 3408/4436 [00:59<00:17, 57.38it/s]\u001b[A\n",
            " 77% 3424/4436 [00:59<00:17, 57.35it/s]\u001b[A\n",
            " 78% 3440/4436 [00:59<00:17, 57.30it/s]\u001b[A\n",
            " 78% 3456/4436 [01:00<00:17, 57.27it/s]\u001b[A\n",
            " 78% 3472/4436 [01:00<00:16, 57.37it/s]\u001b[A\n",
            " 79% 3488/4436 [01:00<00:16, 57.30it/s]\u001b[A\n",
            " 79% 3504/4436 [01:00<00:16, 57.35it/s]\u001b[A\n",
            " 79% 3520/4436 [01:01<00:15, 57.32it/s]\u001b[A\n",
            " 80% 3536/4436 [01:01<00:15, 57.15it/s]\u001b[A\n",
            " 80% 3552/4436 [01:01<00:15, 57.39it/s]\u001b[A\n",
            " 80% 3568/4436 [01:02<00:15, 57.21it/s]\u001b[A\n",
            " 81% 3584/4436 [01:02<00:14, 57.39it/s]\u001b[A\n",
            " 81% 3600/4436 [01:02<00:14, 57.40it/s]\u001b[A\n",
            " 82% 3616/4436 [01:02<00:14, 57.36it/s]\u001b[A\n",
            " 82% 3632/4436 [01:03<00:14, 57.38it/s]\u001b[A\n",
            " 82% 3648/4436 [01:03<00:13, 57.36it/s]\u001b[A\n",
            " 83% 3664/4436 [01:03<00:13, 57.05it/s]\u001b[A\n",
            " 83% 3680/4436 [01:03<00:13, 57.09it/s]\u001b[A\n",
            " 83% 3696/4436 [01:04<00:12, 57.54it/s]\u001b[A\n",
            " 84% 3712/4436 [01:04<00:12, 57.43it/s]\u001b[A\n",
            " 84% 3728/4436 [01:04<00:12, 57.43it/s]\u001b[A\n",
            " 84% 3744/4436 [01:05<00:12, 57.39it/s]\u001b[A\n",
            " 85% 3760/4436 [01:05<00:11, 57.24it/s]\u001b[A\n",
            " 85% 3776/4436 [01:05<00:11, 57.43it/s]\u001b[A\n",
            " 85% 3792/4436 [01:05<00:11, 57.30it/s]\u001b[A\n",
            " 86% 3808/4436 [01:06<00:10, 57.41it/s]\u001b[A\n",
            " 86% 3824/4436 [01:06<00:10, 57.24it/s]\u001b[A\n",
            " 87% 3840/4436 [01:06<00:10, 57.06it/s]\u001b[A\n",
            " 87% 3856/4436 [01:07<00:10, 57.40it/s]\u001b[A\n",
            " 87% 3872/4436 [01:07<00:09, 57.41it/s]\u001b[A\n",
            " 88% 3888/4436 [01:07<00:09, 57.38it/s]\u001b[A\n",
            " 88% 3904/4436 [01:07<00:09, 57.17it/s]\u001b[A\n",
            " 88% 3920/4436 [01:08<00:08, 57.42it/s]\u001b[A\n",
            " 89% 3936/4436 [01:08<00:08, 57.25it/s]\u001b[A\n",
            " 89% 3952/4436 [01:08<00:08, 57.37it/s]\u001b[A\n",
            " 89% 3968/4436 [01:08<00:08, 57.33it/s]\u001b[A\n",
            " 90% 3984/4436 [01:09<00:07, 57.22it/s]\u001b[A\n",
            " 90% 4000/4436 [01:09<00:07, 57.33it/s]\u001b[A\n",
            " 91% 4016/4436 [01:09<00:07, 57.40it/s]\u001b[A\n",
            " 91% 4032/4436 [01:10<00:07, 57.34it/s]\u001b[A\n",
            " 91% 4048/4436 [01:10<00:06, 57.16it/s]\u001b[A\n",
            " 92% 4064/4436 [01:10<00:06, 57.43it/s]\u001b[A\n",
            " 92% 4080/4436 [01:10<00:06, 57.39it/s]\u001b[A\n",
            " 92% 4096/4436 [01:11<00:05, 57.39it/s]\u001b[A\n",
            " 93% 4112/4436 [01:11<00:05, 57.34it/s]\u001b[A\n",
            " 93% 4128/4436 [01:11<00:05, 57.02it/s]\u001b[A\n",
            " 93% 4144/4436 [01:12<00:05, 57.42it/s]\u001b[A\n",
            " 94% 4160/4436 [01:12<00:04, 57.44it/s]\u001b[A\n",
            " 94% 4176/4436 [01:12<00:04, 57.41it/s]\u001b[A\n",
            " 94% 4192/4436 [01:12<00:04, 57.18it/s]\u001b[A\n",
            " 95% 4208/4436 [01:13<00:03, 57.54it/s]\u001b[A\n",
            " 95% 4224/4436 [01:13<00:03, 57.42it/s]\u001b[A\n",
            " 96% 4240/4436 [01:13<00:03, 57.55it/s]\u001b[A\n",
            " 96% 4256/4436 [01:14<00:03, 57.56it/s]\u001b[A\n",
            " 96% 4272/4436 [01:14<00:02, 57.53it/s]\u001b[A\n",
            " 97% 4288/4436 [01:14<00:02, 57.47it/s]\u001b[A\n",
            " 97% 4304/4436 [01:14<00:02, 57.27it/s]\u001b[A\n",
            " 97% 4320/4436 [01:15<00:02, 57.36it/s]\u001b[A\n",
            " 98% 4336/4436 [01:15<00:01, 57.07it/s]\u001b[A\n",
            " 98% 4352/4436 [01:15<00:01, 57.52it/s]\u001b[A\n",
            " 98% 4368/4436 [01:15<00:01, 57.44it/s]\u001b[A\n",
            " 99% 4384/4436 [01:16<00:00, 57.56it/s]\u001b[A\n",
            " 99% 4400/4436 [01:16<00:00, 57.52it/s]\u001b[A\n",
            "100% 4416/4436 [01:16<00:00, 57.52it/s]\u001b[A\n",
            "100% 4436/4436 [01:17<00:00, 57.35it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 321/4306 [00:00<00:01, 3204.84it/s]\u001b[A\n",
            " 15% 659/4306 [00:00<00:01, 3302.69it/s]\u001b[A\n",
            " 24% 1018/4306 [00:00<00:00, 3429.72it/s]\u001b[A\n",
            " 32% 1361/4306 [00:00<00:00, 3398.40it/s]\u001b[A\n",
            " 40% 1701/4306 [00:00<00:00, 3239.57it/s]\u001b[A\n",
            " 47% 2027/4306 [00:00<00:00, 3235.30it/s]\u001b[A\n",
            " 55% 2375/4306 [00:00<00:00, 3312.92it/s]\u001b[A\n",
            " 63% 2726/4306 [00:00<00:00, 3372.10it/s]\u001b[A\n",
            " 71% 3065/4306 [00:00<00:00, 3375.32it/s]\u001b[A\n",
            " 80% 3433/4306 [00:01<00:00, 3468.21it/s]\u001b[A\n",
            " 88% 3781/4306 [00:01<00:00, 3469.55it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 3364.00it/s]\n",
            "[10.05.21 01:56:04] Visualizing in TensorBoard...\n",
            "[10.05.21 01:56:04] Eval F1: 07.23, EM: 00.39\n",
            "100% 16020/16020 [17:34<00:00, 15.19it/s, NLL=0.621, epoch=0]\n",
            "[10.05.21 02:12:20] Epoch: 1\n",
            "100% 16020/16020 [16:10<00:00, 16.51it/s, NLL=0.55, epoch=1]\n",
            "[10.05.21 02:28:32] Epoch: 2\n",
            "100% 16020/16020 [16:10<00:00, 16.51it/s, NLL=0.344, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R670PnCCYva"
      },
      "source": [
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_resnet_hypercolumn_before_intermediate \\\n",
        "    --train-datasets squad02 \\\n",
        "    --eval-datasets race02 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLRVlgT5DmrH"
      },
      "source": [
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_resnet_hypercolumn_before_intermediate \\\n",
        "    --train-datasets squad03 \\\n",
        "    --eval-datasets race03 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkr-D68XD94N"
      },
      "source": [
        "!cp -r /content/robustqa /content/drive/MyDrive/robustqa_change_transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEzg-6rO_iBg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgB_Qggt_h3w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFGczuyMgDwA"
      },
      "source": [
        "# CNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sayztdJSUqDu",
        "outputId": "112ed612-c47b-4e51-be8d-3cf4c1e416af"
      },
      "source": [
        "# CNN + Hypercolumn\n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name experiment \\\n",
        "    --train-datasets squad_experiment \\\n",
        "    --eval-datasets race_experiment \\\n",
        "    --eval-every 50 \\\n",
        "    --batch-size 4 \\\n",
        "    --num-epochs 1 \\\n",
        "    --save-dir delete/ \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'qa_outputs.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[10.04.21 00:22:32] Args: {\n",
            "    \"batch_size\": 4,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race_experiment\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 50,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 1,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"experiment\",\n",
            "    \"save_dir\": \"delete/experiment-02\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad_experiment\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.04.21 00:22:32] Preparing Training Data...\n",
            "100% 40/40 [00:00<00:00, 21667.59it/s]\n",
            "Preprocessing not completely accurate for 1/40 instances\n",
            "[10.04.21 00:22:32] Preparing Validation Data...\n",
            "100% 18/18 [00:00<00:00, 23763.76it/s]\n",
            "[10.04.21 00:22:35] Epoch: 0\n",
            "  0% 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.04.21 00:22:35] Evaluating at step 0...\n",
            " 10% 4/40 [00:00<00:02, 12.89it/s, NLL=5.9, epoch=0]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "100% 18/18 [00:00<00:00, 55.99it/s]\n",
            "\n",
            "100% 18/18 [00:00<00:00, 2295.87it/s]\n",
            "[10.04.21 00:22:35] Visualizing in TensorBoard...\n",
            "[10.04.21 00:22:35] Eval F1: 00.00, EM: 00.00\n",
            "100% 40/40 [00:08<00:00,  4.89it/s, NLL=5.07, epoch=0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4la4NIX6Nadz"
      },
      "source": [
        "!rm -r /content/robustqa/delete/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEnJXy2vNV7J"
      },
      "source": [
        "!rm -r /content/robustqa/check/squad_cnn_hypercolumn_k5-01/\n",
        "!rm -r /content/robustqa/check/squad_cnn_hypercolumn_k5-02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xwyiq2bU-GM",
        "outputId": "5f6bfa9a-acb5-4d63-fae4-3d4622cb9d00"
      },
      "source": [
        "# CNN + Hypercolumn\n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_cnn_hypercolumn_k5 \\\n",
        "    --train-datasets squad01 \\\n",
        "    --eval-datasets race01 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --save-dir check/ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'qa_outputs.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'qa_outputs.weight', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[10.06.21 06:15:58] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_cnn_hypercolumn_k5\",\n",
            "    \"save_dir\": \"check/squad_cnn_hypercolumn_k5-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.06.21 06:15:58] Preparing Training Data...\n",
            "100% 16020/16020 [00:00<00:00, 19262.54it/s]\n",
            "Preprocessing not completely accurate for 150/16020 instances\n",
            "[10.06.21 06:16:03] Preparing Validation Data...\n",
            "100% 4436/4436 [00:00<00:00, 20267.08it/s]\n",
            "[10.06.21 06:16:17] Epoch: 0\n",
            "  0% 0/16020 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.06.21 06:16:18] Evaluating at step 0...\n",
            "  0% 16/16020 [00:01<16:06, 16.56it/s, NLL=5.97, epoch=0]\n",
            "  0% 0/4436 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4436 [00:00<00:21, 202.07it/s]\u001b[A\n",
            "  1% 64/4436 [00:00<00:31, 139.55it/s]\u001b[A\n",
            "  2% 80/4436 [00:00<00:32, 133.68it/s]\u001b[A\n",
            "  2% 96/4436 [00:00<00:33, 127.75it/s]\u001b[A\n",
            "  3% 112/4436 [00:00<00:35, 123.17it/s]\u001b[A\n",
            "  3% 128/4436 [00:00<00:35, 121.28it/s]\u001b[A\n",
            "  3% 144/4436 [00:01<00:36, 119.03it/s]\u001b[A\n",
            "  4% 160/4436 [00:01<00:36, 117.80it/s]\u001b[A\n",
            "  4% 176/4436 [00:01<00:36, 117.31it/s]\u001b[A\n",
            "  4% 192/4436 [00:01<00:36, 116.76it/s]\u001b[A\n",
            "  5% 208/4436 [00:01<00:36, 116.88it/s]\u001b[A\n",
            "  5% 224/4436 [00:01<00:36, 116.77it/s]\u001b[A\n",
            "  5% 240/4436 [00:01<00:36, 116.36it/s]\u001b[A\n",
            "  6% 256/4436 [00:02<00:35, 116.14it/s]\u001b[A\n",
            "  6% 272/4436 [00:02<00:36, 115.35it/s]\u001b[A\n",
            "  6% 288/4436 [00:02<00:36, 114.82it/s]\u001b[A\n",
            "  7% 304/4436 [00:02<00:35, 115.32it/s]\u001b[A\n",
            "  7% 320/4436 [00:02<00:35, 115.48it/s]\u001b[A\n",
            "  8% 336/4436 [00:02<00:35, 116.30it/s]\u001b[A\n",
            "  8% 352/4436 [00:02<00:35, 115.61it/s]\u001b[A\n",
            "  8% 368/4436 [00:03<00:35, 115.37it/s]\u001b[A\n",
            "  9% 384/4436 [00:03<00:34, 116.00it/s]\u001b[A\n",
            "  9% 400/4436 [00:03<00:34, 116.08it/s]\u001b[A\n",
            "  9% 416/4436 [00:03<00:34, 115.64it/s]\u001b[A\n",
            " 10% 432/4436 [00:03<00:34, 115.87it/s]\u001b[A\n",
            " 10% 448/4436 [00:03<00:34, 115.34it/s]\u001b[A\n",
            " 10% 464/4436 [00:03<00:34, 115.86it/s]\u001b[A\n",
            " 11% 480/4436 [00:04<00:34, 115.81it/s]\u001b[A\n",
            " 11% 496/4436 [00:04<00:34, 115.62it/s]\u001b[A\n",
            " 12% 512/4436 [00:04<00:34, 114.85it/s]\u001b[A\n",
            " 12% 528/4436 [00:04<00:33, 115.82it/s]\u001b[A\n",
            " 12% 544/4436 [00:04<00:33, 115.42it/s]\u001b[A\n",
            " 13% 560/4436 [00:04<00:33, 115.80it/s]\u001b[A\n",
            " 13% 576/4436 [00:04<00:33, 115.78it/s]\u001b[A\n",
            " 13% 592/4436 [00:04<00:33, 115.75it/s]\u001b[A\n",
            " 14% 608/4436 [00:05<00:33, 115.42it/s]\u001b[A\n",
            " 14% 624/4436 [00:05<00:32, 115.77it/s]\u001b[A\n",
            " 14% 640/4436 [00:05<00:33, 114.83it/s]\u001b[A\n",
            " 15% 656/4436 [00:05<00:32, 115.13it/s]\u001b[A\n",
            " 15% 672/4436 [00:05<00:32, 115.52it/s]\u001b[A\n",
            " 16% 688/4436 [00:05<00:32, 115.85it/s]\u001b[A\n",
            " 16% 704/4436 [00:05<00:32, 115.84it/s]\u001b[A\n",
            " 16% 720/4436 [00:06<00:32, 115.71it/s]\u001b[A\n",
            " 17% 736/4436 [00:06<00:32, 114.85it/s]\u001b[A\n",
            " 17% 752/4436 [00:06<00:32, 114.55it/s]\u001b[A\n",
            " 17% 768/4436 [00:06<00:31, 116.22it/s]\u001b[A\n",
            " 18% 784/4436 [00:06<00:31, 116.06it/s]\u001b[A\n",
            " 18% 800/4436 [00:06<00:31, 115.84it/s]\u001b[A\n",
            " 18% 816/4436 [00:06<00:31, 115.84it/s]\u001b[A\n",
            " 19% 832/4436 [00:07<00:31, 115.67it/s]\u001b[A\n",
            " 19% 848/4436 [00:07<00:31, 115.71it/s]\u001b[A\n",
            " 19% 864/4436 [00:07<00:30, 115.77it/s]\u001b[A\n",
            " 20% 880/4436 [00:07<00:30, 115.06it/s]\u001b[A\n",
            " 20% 896/4436 [00:07<00:30, 115.19it/s]\u001b[A\n",
            " 21% 912/4436 [00:07<00:30, 115.14it/s]\u001b[A\n",
            " 21% 928/4436 [00:07<00:30, 115.69it/s]\u001b[A\n",
            " 21% 944/4436 [00:08<00:30, 116.22it/s]\u001b[A\n",
            " 22% 960/4436 [00:08<00:29, 115.94it/s]\u001b[A\n",
            " 22% 976/4436 [00:08<00:30, 115.04it/s]\u001b[A\n",
            " 22% 992/4436 [00:08<00:29, 115.22it/s]\u001b[A\n",
            " 23% 1008/4436 [00:08<00:29, 116.06it/s]\u001b[A\n",
            " 23% 1024/4436 [00:08<00:29, 116.02it/s]\u001b[A\n",
            " 23% 1040/4436 [00:08<00:29, 115.98it/s]\u001b[A\n",
            " 24% 1056/4436 [00:09<00:29, 114.52it/s]\u001b[A\n",
            " 24% 1072/4436 [00:09<00:29, 115.99it/s]\u001b[A\n",
            " 25% 1088/4436 [00:09<00:28, 115.99it/s]\u001b[A\n",
            " 25% 1104/4436 [00:09<00:29, 114.58it/s]\u001b[A\n",
            " 25% 1120/4436 [00:09<00:28, 116.11it/s]\u001b[A\n",
            " 26% 1136/4436 [00:09<00:28, 115.38it/s]\u001b[A\n",
            " 26% 1152/4436 [00:09<00:28, 116.03it/s]\u001b[A\n",
            " 26% 1168/4436 [00:09<00:28, 115.35it/s]\u001b[A\n",
            " 27% 1184/4436 [00:10<00:28, 115.56it/s]\u001b[A\n",
            " 27% 1200/4436 [00:10<00:28, 115.13it/s]\u001b[A\n",
            " 27% 1216/4436 [00:10<00:27, 115.25it/s]\u001b[A\n",
            " 28% 1232/4436 [00:10<00:27, 116.17it/s]\u001b[A\n",
            " 28% 1248/4436 [00:10<00:27, 115.52it/s]\u001b[A\n",
            " 28% 1264/4436 [00:10<00:27, 116.11it/s]\u001b[A\n",
            " 29% 1280/4436 [00:10<00:27, 115.29it/s]\u001b[A\n",
            " 29% 1296/4436 [00:11<00:27, 115.82it/s]\u001b[A\n",
            " 30% 1312/4436 [00:11<00:27, 115.06it/s]\u001b[A\n",
            " 30% 1328/4436 [00:11<00:26, 115.85it/s]\u001b[A\n",
            " 30% 1344/4436 [00:11<00:26, 115.94it/s]\u001b[A\n",
            " 31% 1360/4436 [00:11<00:26, 115.44it/s]\u001b[A\n",
            " 31% 1376/4436 [00:11<00:26, 115.50it/s]\u001b[A\n",
            " 31% 1392/4436 [00:11<00:26, 115.82it/s]\u001b[A\n",
            " 32% 1408/4436 [00:12<00:26, 115.41it/s]\u001b[A\n",
            " 32% 1424/4436 [00:12<00:26, 114.82it/s]\u001b[A\n",
            " 32% 1440/4436 [00:12<00:26, 114.68it/s]\u001b[A\n",
            " 33% 1456/4436 [00:12<00:25, 116.27it/s]\u001b[A\n",
            " 33% 1472/4436 [00:12<00:25, 115.23it/s]\u001b[A\n",
            " 34% 1488/4436 [00:12<00:25, 115.35it/s]\u001b[A\n",
            " 34% 1504/4436 [00:12<00:25, 116.19it/s]\u001b[A\n",
            " 34% 1520/4436 [00:13<00:25, 115.45it/s]\u001b[A\n",
            " 35% 1536/4436 [00:13<00:24, 116.10it/s]\u001b[A\n",
            " 35% 1552/4436 [00:13<00:24, 115.37it/s]\u001b[A\n",
            " 35% 1568/4436 [00:13<00:24, 116.02it/s]\u001b[A\n",
            " 36% 1584/4436 [00:13<00:24, 114.60it/s]\u001b[A\n",
            "  0% 16/16020 [00:14<16:06, 16.56it/s, NLL=5.97, epoch=0]\n",
            " 36% 1616/4436 [00:13<00:24, 115.14it/s]\u001b[A\n",
            " 37% 1632/4436 [00:14<00:24, 115.24it/s]\u001b[A\n",
            " 37% 1648/4436 [00:14<00:24, 115.71it/s]\u001b[A\n",
            " 38% 1664/4436 [00:14<00:23, 116.35it/s]\u001b[A\n",
            " 38% 1680/4436 [00:14<00:23, 116.08it/s]\u001b[A\n",
            " 38% 1696/4436 [00:14<00:23, 115.17it/s]\u001b[A\n",
            " 39% 1712/4436 [00:14<00:23, 115.19it/s]\u001b[A\n",
            " 39% 1728/4436 [00:14<00:23, 115.32it/s]\u001b[A\n",
            " 39% 1744/4436 [00:14<00:23, 115.00it/s]\u001b[A\n",
            " 40% 1760/4436 [00:15<00:23, 116.11it/s]\u001b[A\n",
            " 40% 1776/4436 [00:15<00:22, 116.09it/s]\u001b[A\n",
            " 40% 1792/4436 [00:15<00:22, 115.07it/s]\u001b[A\n",
            " 41% 1808/4436 [00:15<00:22, 116.05it/s]\u001b[A\n",
            " 41% 1824/4436 [00:15<00:22, 115.38it/s]\u001b[A\n",
            " 41% 1840/4436 [00:15<00:22, 114.55it/s]\u001b[A\n",
            " 42% 1856/4436 [00:15<00:22, 115.69it/s]\u001b[A\n",
            " 42% 1872/4436 [00:16<00:22, 116.15it/s]\u001b[A\n",
            " 43% 1888/4436 [00:16<00:21, 116.04it/s]\u001b[A\n",
            " 43% 1904/4436 [00:16<00:22, 114.54it/s]\u001b[A\n",
            " 43% 1920/4436 [00:16<00:21, 116.08it/s]\u001b[A\n",
            " 44% 1936/4436 [00:16<00:21, 116.13it/s]\u001b[A\n",
            " 44% 1952/4436 [00:16<00:21, 116.06it/s]\u001b[A\n",
            " 44% 1968/4436 [00:16<00:21, 115.84it/s]\u001b[A\n",
            " 45% 1984/4436 [00:17<00:21, 115.10it/s]\u001b[A\n",
            " 45% 2000/4436 [00:17<00:21, 115.79it/s]\u001b[A\n",
            " 45% 2016/4436 [00:17<00:20, 115.68it/s]\u001b[A\n",
            " 46% 2032/4436 [00:17<00:20, 115.38it/s]\u001b[A\n",
            " 46% 2048/4436 [00:17<00:20, 115.92it/s]\u001b[A\n",
            " 47% 2064/4436 [00:17<00:20, 115.79it/s]\u001b[A\n",
            " 47% 2080/4436 [00:17<00:20, 115.86it/s]\u001b[A\n",
            " 47% 2096/4436 [00:18<00:20, 115.66it/s]\u001b[A\n",
            " 48% 2112/4436 [00:18<00:20, 115.83it/s]\u001b[A\n",
            " 48% 2128/4436 [00:18<00:19, 115.85it/s]\u001b[A\n",
            " 48% 2144/4436 [00:18<00:19, 115.76it/s]\u001b[A\n",
            " 49% 2160/4436 [00:18<00:19, 115.67it/s]\u001b[A\n",
            " 49% 2176/4436 [00:18<00:19, 115.72it/s]\u001b[A\n",
            " 49% 2192/4436 [00:18<00:19, 115.61it/s]\u001b[A\n",
            " 50% 2208/4436 [00:18<00:19, 115.71it/s]\u001b[A\n",
            " 50% 2224/4436 [00:19<00:19, 115.34it/s]\u001b[A\n",
            " 50% 2240/4436 [00:19<00:19, 115.55it/s]\u001b[A\n",
            " 51% 2256/4436 [00:19<00:18, 115.79it/s]\u001b[A\n",
            " 51% 2272/4436 [00:19<00:18, 114.62it/s]\u001b[A\n",
            " 52% 2288/4436 [00:19<00:18, 115.33it/s]\u001b[A\n",
            " 52% 2304/4436 [00:19<00:18, 116.04it/s]\u001b[A\n",
            " 52% 2320/4436 [00:19<00:18, 115.62it/s]\u001b[A\n",
            " 53% 2336/4436 [00:20<00:18, 114.65it/s]\u001b[A\n",
            " 53% 2352/4436 [00:20<00:18, 115.17it/s]\u001b[A\n",
            " 53% 2368/4436 [00:20<00:18, 114.76it/s]\u001b[A\n",
            " 54% 2384/4436 [00:20<00:17, 115.39it/s]\u001b[A\n",
            " 54% 2400/4436 [00:20<00:17, 115.87it/s]\u001b[A\n",
            " 54% 2416/4436 [00:20<00:17, 115.95it/s]\u001b[A\n",
            " 55% 2432/4436 [00:20<00:17, 116.31it/s]\u001b[A\n",
            " 55% 2448/4436 [00:21<00:17, 115.90it/s]\u001b[A\n",
            " 56% 2464/4436 [00:21<00:17, 115.98it/s]\u001b[A\n",
            " 56% 2480/4436 [00:21<00:17, 114.52it/s]\u001b[A\n",
            " 56% 2496/4436 [00:21<00:16, 115.63it/s]\u001b[A\n",
            " 57% 2512/4436 [00:21<00:16, 116.05it/s]\u001b[A\n",
            " 57% 2528/4436 [00:21<00:16, 115.87it/s]\u001b[A\n",
            " 57% 2544/4436 [00:21<00:16, 115.33it/s]\u001b[A\n",
            " 58% 2560/4436 [00:22<00:16, 114.39it/s]\u001b[A\n",
            " 58% 2576/4436 [00:22<00:16, 116.22it/s]\u001b[A\n",
            " 58% 2592/4436 [00:22<00:15, 116.05it/s]\u001b[A\n",
            " 59% 2608/4436 [00:22<00:15, 115.42it/s]\u001b[A\n",
            " 59% 2624/4436 [00:22<00:15, 115.98it/s]\u001b[A\n",
            " 60% 2640/4436 [00:22<00:15, 115.24it/s]\u001b[A\n",
            " 60% 2656/4436 [00:22<00:15, 115.20it/s]\u001b[A\n",
            " 60% 2672/4436 [00:22<00:15, 115.37it/s]\u001b[A\n",
            " 61% 2688/4436 [00:23<00:15, 116.18it/s]\u001b[A\n",
            " 61% 2704/4436 [00:23<00:14, 115.92it/s]\u001b[A\n",
            " 61% 2720/4436 [00:23<00:14, 115.39it/s]\u001b[A\n",
            " 62% 2736/4436 [00:23<00:14, 115.26it/s]\u001b[A\n",
            " 62% 2752/4436 [00:23<00:14, 115.65it/s]\u001b[A\n",
            " 62% 2768/4436 [00:23<00:14, 115.29it/s]\u001b[A\n",
            " 63% 2784/4436 [00:23<00:14, 115.94it/s]\u001b[A\n",
            " 63% 2800/4436 [00:24<00:14, 115.40it/s]\u001b[A\n",
            " 63% 2816/4436 [00:24<00:14, 115.54it/s]\u001b[A\n",
            " 64% 2832/4436 [00:24<00:13, 114.90it/s]\u001b[A\n",
            " 64% 2848/4436 [00:24<00:13, 115.93it/s]\u001b[A\n",
            " 65% 2864/4436 [00:24<00:13, 115.36it/s]\u001b[A\n",
            " 65% 2880/4436 [00:24<00:13, 115.98it/s]\u001b[A\n",
            " 65% 2896/4436 [00:24<00:13, 115.32it/s]\u001b[A\n",
            " 66% 2912/4436 [00:25<00:13, 115.02it/s]\u001b[A\n",
            " 66% 2928/4436 [00:25<00:13, 115.52it/s]\u001b[A\n",
            " 66% 2944/4436 [00:25<00:12, 115.50it/s]\u001b[A\n",
            " 67% 2960/4436 [00:25<00:12, 114.63it/s]\u001b[A\n",
            " 67% 2976/4436 [00:25<00:12, 115.23it/s]\u001b[A\n",
            " 67% 2992/4436 [00:25<00:12, 115.74it/s]\u001b[A\n",
            " 68% 3008/4436 [00:25<00:12, 115.17it/s]\u001b[A\n",
            " 68% 3024/4436 [00:26<00:12, 115.88it/s]\u001b[A\n",
            " 69% 3040/4436 [00:26<00:12, 115.45it/s]\u001b[A\n",
            " 69% 3056/4436 [00:26<00:11, 115.77it/s]\u001b[A\n",
            " 69% 3072/4436 [00:26<00:11, 115.10it/s]\u001b[A\n",
            " 70% 3088/4436 [00:26<00:11, 115.04it/s]\u001b[A\n",
            " 70% 3104/4436 [00:26<00:11, 115.29it/s]\u001b[A\n",
            " 70% 3120/4436 [00:26<00:11, 115.85it/s]\u001b[A\n",
            " 71% 3136/4436 [00:27<00:11, 115.67it/s]\u001b[A\n",
            " 71% 3152/4436 [00:27<00:11, 115.83it/s]\u001b[A\n",
            " 71% 3168/4436 [00:27<00:10, 115.89it/s]\u001b[A\n",
            " 72% 3184/4436 [00:27<00:10, 115.94it/s]\u001b[A\n",
            " 72% 3200/4436 [00:27<00:10, 116.34it/s]\u001b[A\n",
            " 72% 3216/4436 [00:27<00:10, 116.17it/s]\u001b[A\n",
            " 73% 3232/4436 [00:27<00:10, 116.10it/s]\u001b[A\n",
            " 73% 3248/4436 [00:27<00:10, 115.43it/s]\u001b[A\n",
            " 74% 3264/4436 [00:28<00:10, 115.78it/s]\u001b[A\n",
            " 74% 3280/4436 [00:28<00:09, 116.08it/s]\u001b[A\n",
            " 74% 3296/4436 [00:28<00:09, 115.94it/s]\u001b[A\n",
            " 75% 3312/4436 [00:28<00:09, 115.00it/s]\u001b[A\n",
            " 75% 3328/4436 [00:28<00:09, 115.23it/s]\u001b[A\n",
            " 75% 3344/4436 [00:28<00:09, 115.34it/s]\u001b[A\n",
            " 76% 3360/4436 [00:28<00:09, 115.41it/s]\u001b[A\n",
            " 76% 3376/4436 [00:29<00:09, 115.53it/s]\u001b[A\n",
            " 76% 3392/4436 [00:29<00:09, 115.21it/s]\u001b[A\n",
            " 77% 3408/4436 [00:29<00:08, 115.55it/s]\u001b[A\n",
            " 77% 3424/4436 [00:29<00:08, 115.49it/s]\u001b[A\n",
            " 78% 3440/4436 [00:29<00:08, 115.45it/s]\u001b[A\n",
            " 78% 3456/4436 [00:29<00:08, 115.34it/s]\u001b[A\n",
            " 78% 3472/4436 [00:29<00:08, 115.47it/s]\u001b[A\n",
            " 79% 3488/4436 [00:30<00:08, 115.90it/s]\u001b[A\n",
            " 79% 3504/4436 [00:30<00:08, 115.98it/s]\u001b[A\n",
            " 79% 3520/4436 [00:30<00:07, 115.68it/s]\u001b[A\n",
            " 80% 3536/4436 [00:30<00:07, 114.07it/s]\u001b[A\n",
            " 80% 3552/4436 [00:30<00:07, 116.31it/s]\u001b[A\n",
            " 80% 3568/4436 [00:30<00:07, 116.14it/s]\u001b[A\n",
            " 81% 3584/4436 [00:30<00:07, 115.02it/s]\u001b[A\n",
            " 81% 3600/4436 [00:31<00:07, 115.46it/s]\u001b[A\n",
            " 82% 3616/4436 [00:31<00:07, 115.16it/s]\u001b[A\n",
            " 82% 3632/4436 [00:31<00:06, 115.57it/s]\u001b[A\n",
            " 82% 3648/4436 [00:31<00:06, 115.68it/s]\u001b[A\n",
            " 83% 3664/4436 [00:31<00:06, 116.23it/s]\u001b[A\n",
            " 83% 3680/4436 [00:31<00:06, 115.57it/s]\u001b[A\n",
            " 83% 3696/4436 [00:31<00:06, 116.11it/s]\u001b[A\n",
            " 84% 3712/4436 [00:31<00:06, 115.88it/s]\u001b[A\n",
            " 84% 3728/4436 [00:32<00:06, 115.00it/s]\u001b[A\n",
            " 84% 3744/4436 [00:32<00:06, 114.94it/s]\u001b[A\n",
            " 85% 3760/4436 [00:32<00:05, 115.63it/s]\u001b[A\n",
            " 85% 3776/4436 [00:32<00:05, 115.90it/s]\u001b[A\n",
            " 85% 3792/4436 [00:32<00:05, 114.99it/s]\u001b[A\n",
            " 86% 3808/4436 [00:32<00:05, 115.31it/s]\u001b[A\n",
            " 86% 3824/4436 [00:32<00:05, 114.67it/s]\u001b[A\n",
            " 87% 3840/4436 [00:33<00:05, 113.79it/s]\u001b[A\n",
            " 87% 3856/4436 [00:33<00:05, 115.90it/s]\u001b[A\n",
            " 87% 3872/4436 [00:33<00:04, 116.03it/s]\u001b[A\n",
            " 88% 3888/4436 [00:33<00:04, 116.74it/s]\u001b[A\n",
            " 88% 3904/4436 [00:33<00:04, 116.28it/s]\u001b[A\n",
            " 88% 3920/4436 [00:33<00:04, 116.24it/s]\u001b[A\n",
            " 89% 3936/4436 [00:33<00:04, 115.25it/s]\u001b[A\n",
            " 89% 3952/4436 [00:34<00:04, 116.06it/s]\u001b[A\n",
            " 89% 3968/4436 [00:34<00:04, 115.62it/s]\u001b[A\n",
            " 90% 3984/4436 [00:34<00:03, 115.83it/s]\u001b[A\n",
            " 90% 4000/4436 [00:34<00:03, 115.76it/s]\u001b[A\n",
            " 91% 4016/4436 [00:34<00:03, 114.88it/s]\u001b[A\n",
            " 91% 4032/4436 [00:34<00:03, 115.82it/s]\u001b[A\n",
            " 91% 4048/4436 [00:34<00:03, 115.61it/s]\u001b[A\n",
            " 92% 4064/4436 [00:35<00:03, 115.73it/s]\u001b[A\n",
            " 92% 4080/4436 [00:35<00:03, 115.32it/s]\u001b[A\n",
            " 92% 4096/4436 [00:35<00:02, 115.77it/s]\u001b[A\n",
            " 93% 4112/4436 [00:35<00:02, 115.54it/s]\u001b[A\n",
            " 93% 4128/4436 [00:35<00:02, 115.64it/s]\u001b[A\n",
            " 93% 4144/4436 [00:35<00:02, 115.22it/s]\u001b[A\n",
            " 94% 4160/4436 [00:35<00:02, 115.17it/s]\u001b[A\n",
            " 94% 4176/4436 [00:36<00:02, 115.81it/s]\u001b[A\n",
            " 94% 4192/4436 [00:36<00:02, 115.13it/s]\u001b[A\n",
            " 95% 4208/4436 [00:36<00:01, 115.33it/s]\u001b[A\n",
            " 95% 4224/4436 [00:36<00:01, 115.50it/s]\u001b[A\n",
            " 96% 4240/4436 [00:36<00:01, 114.90it/s]\u001b[A\n",
            " 96% 4256/4436 [00:36<00:01, 115.40it/s]\u001b[A\n",
            " 96% 4272/4436 [00:36<00:01, 114.48it/s]\u001b[A\n",
            " 97% 4288/4436 [00:36<00:01, 114.87it/s]\u001b[A\n",
            " 97% 4304/4436 [00:37<00:01, 115.33it/s]\u001b[A\n",
            " 97% 4320/4436 [00:37<00:00, 116.02it/s]\u001b[A\n",
            " 98% 4336/4436 [00:37<00:00, 114.65it/s]\u001b[A\n",
            " 98% 4352/4436 [00:37<00:00, 115.54it/s]\u001b[A\n",
            " 98% 4368/4436 [00:37<00:00, 114.72it/s]\u001b[A\n",
            " 99% 4384/4436 [00:37<00:00, 116.20it/s]\u001b[A\n",
            " 99% 4400/4436 [00:37<00:00, 114.61it/s]\u001b[A\n",
            "100% 4416/4436 [00:38<00:00, 116.21it/s]\u001b[A\n",
            "100% 4436/4436 [00:38<00:00, 115.60it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 288/4306 [00:00<00:01, 2876.44it/s]\u001b[A\n",
            " 14% 584/4306 [00:00<00:01, 2920.09it/s]\u001b[A\n",
            " 21% 909/4306 [00:00<00:01, 3069.87it/s]\u001b[A\n",
            " 29% 1260/4306 [00:00<00:00, 3240.28it/s]\u001b[A\n",
            " 37% 1602/4306 [00:00<00:00, 3304.48it/s]\u001b[A\n",
            " 45% 1952/4306 [00:00<00:00, 3369.26it/s]\u001b[A\n",
            " 53% 2294/4306 [00:00<00:00, 3385.02it/s]\u001b[A\n",
            " 61% 2646/4306 [00:00<00:00, 3424.36it/s]\u001b[A\n",
            " 69% 2989/4306 [00:00<00:00, 3424.92it/s]\u001b[A\n",
            " 78% 3352/4306 [00:01<00:00, 3487.37it/s]\u001b[A\n",
            " 86% 3720/4306 [00:01<00:00, 3546.00it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 3359.06it/s]\n",
            "[10.06.21 06:16:59] Visualizing in TensorBoard...\n",
            "[10.06.21 06:16:59] Eval F1: 07.33, EM: 00.42\n",
            "100% 16020/16020 [09:47<00:00, 27.28it/s, NLL=0.946, epoch=0]\n",
            "[10.06.21 06:26:06] Epoch: 1\n",
            "100% 16020/16020 [09:00<00:00, 29.67it/s, NLL=0.604, epoch=1]\n",
            "[10.06.21 06:35:07] Epoch: 2\n",
            "100% 16020/16020 [09:00<00:00, 29.62it/s, NLL=0.787, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VWgwTpSV-YM",
        "outputId": "f590a9b9-b2c9-4fcc-e026-2d3256410b3c"
      },
      "source": [
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_cnn_hypercolumn_k5 \\\n",
        "    --train-datasets squad02 \\\n",
        "    --eval-datasets race02 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES \\\n",
        "    --save-dir check/ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "[10.06.21 06:44:18] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race02\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_cnn_hypercolumn_k5\",\n",
            "    \"save_dir\": \"check/squad_cnn_hypercolumn_k5-02\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad02\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.06.21 06:44:18] Preparing Training Data...\n",
            "100% 17496/17496 [00:00<00:00, 21103.05it/s]\n",
            "Preprocessing not completely accurate for 208/17496 instances\n",
            "[10.06.21 06:44:23] Preparing Validation Data...\n",
            "100% 2977/2977 [00:00<00:00, 21643.77it/s]\n",
            "[10.06.21 06:44:27] Epoch: 0\n",
            "  0% 0/17496 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.06.21 06:44:28] Evaluating at step 0...\n",
            "  0% 16/17496 [00:00<10:20, 28.15it/s, NLL=1.41, epoch=0]\n",
            "  0% 0/2977 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/2977 [00:00<00:14, 208.39it/s]\u001b[A\n",
            "  2% 64/2977 [00:00<00:20, 140.39it/s]\u001b[A\n",
            "  3% 80/2977 [00:00<00:21, 132.84it/s]\u001b[A\n",
            "  3% 96/2977 [00:00<00:22, 127.48it/s]\u001b[A\n",
            "  4% 112/2977 [00:00<00:23, 123.79it/s]\u001b[A\n",
            "  4% 128/2977 [00:00<00:23, 121.13it/s]\u001b[A\n",
            "  5% 144/2977 [00:01<00:23, 119.40it/s]\u001b[A\n",
            "  5% 160/2977 [00:01<00:23, 118.62it/s]\u001b[A\n",
            "  6% 176/2977 [00:01<00:23, 117.26it/s]\u001b[A\n",
            "  6% 192/2977 [00:01<00:23, 117.22it/s]\u001b[A\n",
            "  7% 208/2977 [00:01<00:23, 116.69it/s]\u001b[A\n",
            "  8% 224/2977 [00:01<00:23, 116.43it/s]\u001b[A\n",
            "  8% 240/2977 [00:01<00:23, 115.84it/s]\u001b[A\n",
            "  9% 256/2977 [00:02<00:23, 116.10it/s]\u001b[A\n",
            "  9% 272/2977 [00:02<00:23, 115.91it/s]\u001b[A\n",
            " 10% 288/2977 [00:02<00:23, 115.90it/s]\u001b[A\n",
            " 10% 304/2977 [00:02<00:23, 115.96it/s]\u001b[A\n",
            " 11% 320/2977 [00:02<00:22, 115.71it/s]\u001b[A\n",
            " 11% 336/2977 [00:02<00:22, 115.94it/s]\u001b[A\n",
            " 12% 352/2977 [00:02<00:22, 114.92it/s]\u001b[A\n",
            " 12% 368/2977 [00:03<00:22, 116.12it/s]\u001b[A\n",
            " 13% 384/2977 [00:03<00:22, 115.97it/s]\u001b[A\n",
            " 13% 400/2977 [00:03<00:22, 115.81it/s]\u001b[A\n",
            " 14% 416/2977 [00:03<00:22, 115.07it/s]\u001b[A\n",
            " 15% 432/2977 [00:03<00:21, 115.82it/s]\u001b[A\n",
            " 15% 448/2977 [00:03<00:21, 116.17it/s]\u001b[A\n",
            " 16% 464/2977 [00:03<00:21, 116.07it/s]\u001b[A\n",
            " 16% 480/2977 [00:04<00:21, 115.96it/s]\u001b[A\n",
            " 17% 496/2977 [00:04<00:21, 115.78it/s]\u001b[A\n",
            " 17% 512/2977 [00:04<00:21, 115.64it/s]\u001b[A\n",
            " 18% 528/2977 [00:04<00:21, 115.61it/s]\u001b[A\n",
            " 18% 544/2977 [00:04<00:21, 115.72it/s]\u001b[A\n",
            " 19% 560/2977 [00:04<00:20, 115.80it/s]\u001b[A\n",
            " 19% 576/2977 [00:04<00:20, 115.53it/s]\u001b[A\n",
            " 20% 592/2977 [00:04<00:20, 115.83it/s]\u001b[A\n",
            " 20% 608/2977 [00:05<00:20, 115.86it/s]\u001b[A\n",
            " 21% 624/2977 [00:05<00:20, 115.91it/s]\u001b[A\n",
            " 21% 640/2977 [00:05<00:20, 115.75it/s]\u001b[A\n",
            " 22% 656/2977 [00:05<00:20, 115.74it/s]\u001b[A\n",
            " 23% 672/2977 [00:05<00:19, 115.67it/s]\u001b[A\n",
            " 23% 688/2977 [00:05<00:19, 115.86it/s]\u001b[A\n",
            " 24% 704/2977 [00:05<00:19, 115.33it/s]\u001b[A\n",
            " 24% 720/2977 [00:06<00:19, 115.71it/s]\u001b[A\n",
            " 25% 736/2977 [00:06<00:19, 115.83it/s]\u001b[A\n",
            " 25% 752/2977 [00:06<00:19, 115.79it/s]\u001b[A\n",
            " 26% 768/2977 [00:06<00:19, 115.53it/s]\u001b[A\n",
            " 26% 784/2977 [00:06<00:18, 115.78it/s]\u001b[A\n",
            " 27% 800/2977 [00:06<00:18, 115.76it/s]\u001b[A\n",
            " 27% 816/2977 [00:06<00:18, 115.68it/s]\u001b[A\n",
            " 28% 832/2977 [00:07<00:18, 115.87it/s]\u001b[A\n",
            " 28% 848/2977 [00:07<00:18, 115.77it/s]\u001b[A\n",
            " 29% 864/2977 [00:07<00:18, 115.81it/s]\u001b[A\n",
            " 30% 880/2977 [00:07<00:18, 115.66it/s]\u001b[A\n",
            " 30% 896/2977 [00:07<00:18, 115.37it/s]\u001b[A\n",
            " 31% 912/2977 [00:07<00:17, 115.79it/s]\u001b[A\n",
            " 31% 928/2977 [00:07<00:17, 115.62it/s]\u001b[A\n",
            " 32% 944/2977 [00:08<00:17, 115.70it/s]\u001b[A\n",
            " 32% 960/2977 [00:08<00:17, 115.90it/s]\u001b[A\n",
            " 33% 976/2977 [00:08<00:17, 115.56it/s]\u001b[A\n",
            " 33% 992/2977 [00:08<00:17, 114.84it/s]\u001b[A\n",
            " 34% 1008/2977 [00:08<00:16, 115.97it/s]\u001b[A\n",
            " 34% 1024/2977 [00:08<00:16, 115.92it/s]\u001b[A\n",
            " 35% 1040/2977 [00:08<00:16, 115.91it/s]\u001b[A\n",
            " 35% 1056/2977 [00:09<00:16, 115.54it/s]\u001b[A\n",
            " 36% 1072/2977 [00:09<00:16, 115.70it/s]\u001b[A\n",
            " 37% 1088/2977 [00:09<00:16, 115.80it/s]\u001b[A\n",
            " 37% 1104/2977 [00:09<00:16, 115.74it/s]\u001b[A\n",
            " 38% 1120/2977 [00:09<00:16, 115.90it/s]\u001b[A\n",
            " 38% 1136/2977 [00:09<00:15, 115.56it/s]\u001b[A\n",
            " 39% 1152/2977 [00:09<00:15, 115.71it/s]\u001b[A\n",
            " 39% 1168/2977 [00:09<00:15, 115.81it/s]\u001b[A\n",
            " 40% 1184/2977 [00:10<00:15, 115.77it/s]\u001b[A\n",
            " 40% 1200/2977 [00:10<00:15, 115.80it/s]\u001b[A\n",
            " 41% 1216/2977 [00:10<00:15, 115.65it/s]\u001b[A\n",
            " 41% 1232/2977 [00:10<00:15, 115.74it/s]\u001b[A\n",
            " 42% 1248/2977 [00:10<00:14, 115.71it/s]\u001b[A\n",
            " 42% 1264/2977 [00:10<00:14, 115.80it/s]\u001b[A\n",
            " 43% 1280/2977 [00:10<00:14, 115.78it/s]\u001b[A\n",
            " 44% 1296/2977 [00:11<00:14, 115.53it/s]\u001b[A\n",
            " 44% 1312/2977 [00:11<00:14, 115.18it/s]\u001b[A\n",
            " 45% 1328/2977 [00:11<00:14, 115.96it/s]\u001b[A\n",
            " 45% 1344/2977 [00:11<00:14, 115.95it/s]\u001b[A\n",
            " 46% 1360/2977 [00:11<00:13, 115.72it/s]\u001b[A\n",
            " 46% 1376/2977 [00:11<00:13, 114.91it/s]\u001b[A\n",
            " 47% 1392/2977 [00:11<00:13, 115.75it/s]\u001b[A\n",
            " 47% 1408/2977 [00:12<00:13, 115.79it/s]\u001b[A\n",
            " 48% 1424/2977 [00:12<00:13, 115.74it/s]\u001b[A\n",
            " 48% 1440/2977 [00:12<00:13, 115.39it/s]\u001b[A\n",
            " 49% 1456/2977 [00:12<00:13, 115.78it/s]\u001b[A\n",
            " 49% 1472/2977 [00:12<00:13, 115.06it/s]\u001b[A\n",
            " 50% 1488/2977 [00:12<00:12, 115.04it/s]\u001b[A\n",
            " 51% 1504/2977 [00:12<00:12, 115.47it/s]\u001b[A\n",
            " 51% 1520/2977 [00:13<00:12, 116.15it/s]\u001b[A\n",
            " 52% 1536/2977 [00:13<00:12, 116.13it/s]\u001b[A\n",
            " 52% 1552/2977 [00:13<00:12, 116.03it/s]\u001b[A\n",
            " 53% 1568/2977 [00:13<00:12, 115.65it/s]\u001b[A\n",
            " 53% 1584/2977 [00:13<00:12, 115.88it/s]\u001b[A\n",
            " 54% 1600/2977 [00:13<00:11, 115.60it/s]\u001b[A\n",
            "  0% 16/17496 [00:14<10:20, 28.15it/s, NLL=1.41, epoch=0]\n",
            " 55% 1632/2977 [00:13<00:11, 115.82it/s]\u001b[A\n",
            " 55% 1648/2977 [00:14<00:11, 115.69it/s]\u001b[A\n",
            " 56% 1664/2977 [00:14<00:11, 115.87it/s]\u001b[A\n",
            " 56% 1680/2977 [00:14<00:11, 115.88it/s]\u001b[A\n",
            " 57% 1696/2977 [00:14<00:11, 115.63it/s]\u001b[A\n",
            " 58% 1712/2977 [00:14<00:10, 115.90it/s]\u001b[A\n",
            " 58% 1728/2977 [00:14<00:10, 115.76it/s]\u001b[A\n",
            " 59% 1744/2977 [00:14<00:10, 115.85it/s]\u001b[A\n",
            " 59% 1760/2977 [00:15<00:10, 115.88it/s]\u001b[A\n",
            " 60% 1776/2977 [00:15<00:10, 115.86it/s]\u001b[A\n",
            " 60% 1792/2977 [00:15<00:10, 115.84it/s]\u001b[A\n",
            " 61% 1808/2977 [00:15<00:10, 115.48it/s]\u001b[A\n",
            " 61% 1824/2977 [00:15<00:09, 115.55it/s]\u001b[A\n",
            " 62% 1840/2977 [00:15<00:09, 115.77it/s]\u001b[A\n",
            " 62% 1856/2977 [00:15<00:09, 115.77it/s]\u001b[A\n",
            " 63% 1872/2977 [00:16<00:09, 115.68it/s]\u001b[A\n",
            " 63% 1888/2977 [00:16<00:09, 115.90it/s]\u001b[A\n",
            " 64% 1904/2977 [00:16<00:09, 115.70it/s]\u001b[A\n",
            " 64% 1920/2977 [00:16<00:09, 115.67it/s]\u001b[A\n",
            " 65% 1936/2977 [00:16<00:09, 115.63it/s]\u001b[A\n",
            " 66% 1952/2977 [00:16<00:08, 114.63it/s]\u001b[A\n",
            " 66% 1968/2977 [00:16<00:08, 115.21it/s]\u001b[A\n",
            " 67% 1984/2977 [00:17<00:08, 115.96it/s]\u001b[A\n",
            " 67% 2000/2977 [00:17<00:08, 114.97it/s]\u001b[A\n",
            " 68% 2016/2977 [00:17<00:08, 115.31it/s]\u001b[A\n",
            " 68% 2032/2977 [00:17<00:08, 116.04it/s]\u001b[A\n",
            " 69% 2048/2977 [00:17<00:08, 115.96it/s]\u001b[A\n",
            " 69% 2064/2977 [00:17<00:07, 115.93it/s]\u001b[A\n",
            " 70% 2080/2977 [00:17<00:07, 115.77it/s]\u001b[A\n",
            " 70% 2096/2977 [00:17<00:07, 115.82it/s]\u001b[A\n",
            " 71% 2112/2977 [00:18<00:07, 114.41it/s]\u001b[A\n",
            " 71% 2128/2977 [00:18<00:07, 115.19it/s]\u001b[A\n",
            " 72% 2144/2977 [00:18<00:07, 116.09it/s]\u001b[A\n",
            " 73% 2160/2977 [00:18<00:07, 115.57it/s]\u001b[A\n",
            " 73% 2176/2977 [00:18<00:06, 115.87it/s]\u001b[A\n",
            " 74% 2192/2977 [00:18<00:06, 115.83it/s]\u001b[A\n",
            " 74% 2208/2977 [00:18<00:06, 115.67it/s]\u001b[A\n",
            " 75% 2224/2977 [00:19<00:06, 115.33it/s]\u001b[A\n",
            " 75% 2240/2977 [00:19<00:06, 116.20it/s]\u001b[A\n",
            " 76% 2256/2977 [00:19<00:06, 115.14it/s]\u001b[A\n",
            " 76% 2272/2977 [00:19<00:06, 115.31it/s]\u001b[A\n",
            " 77% 2288/2977 [00:19<00:05, 115.47it/s]\u001b[A\n",
            " 77% 2304/2977 [00:19<00:05, 115.41it/s]\u001b[A\n",
            " 78% 2320/2977 [00:19<00:05, 115.69it/s]\u001b[A\n",
            " 78% 2336/2977 [00:20<00:05, 116.25it/s]\u001b[A\n",
            " 79% 2352/2977 [00:20<00:05, 116.17it/s]\u001b[A\n",
            " 80% 2368/2977 [00:20<00:05, 116.01it/s]\u001b[A\n",
            " 80% 2384/2977 [00:20<00:05, 115.73it/s]\u001b[A\n",
            " 81% 2400/2977 [00:20<00:05, 115.05it/s]\u001b[A\n",
            " 81% 2416/2977 [00:20<00:04, 115.20it/s]\u001b[A\n",
            " 82% 2432/2977 [00:20<00:04, 115.37it/s]\u001b[A\n",
            " 82% 2448/2977 [00:21<00:04, 116.35it/s]\u001b[A\n",
            " 83% 2464/2977 [00:21<00:04, 115.27it/s]\u001b[A\n",
            " 83% 2480/2977 [00:21<00:04, 116.21it/s]\u001b[A\n",
            " 84% 2496/2977 [00:21<00:04, 115.29it/s]\u001b[A\n",
            " 84% 2512/2977 [00:21<00:04, 115.27it/s]\u001b[A\n",
            " 85% 2528/2977 [00:21<00:03, 115.29it/s]\u001b[A\n",
            " 85% 2544/2977 [00:21<00:03, 115.25it/s]\u001b[A\n",
            " 86% 2560/2977 [00:22<00:03, 115.64it/s]\u001b[A\n",
            " 87% 2576/2977 [00:22<00:03, 115.58it/s]\u001b[A\n",
            " 87% 2592/2977 [00:22<00:03, 116.32it/s]\u001b[A\n",
            " 88% 2608/2977 [00:22<00:03, 115.63it/s]\u001b[A\n",
            " 88% 2624/2977 [00:22<00:03, 115.54it/s]\u001b[A\n",
            " 89% 2640/2977 [00:22<00:02, 115.48it/s]\u001b[A\n",
            " 89% 2656/2977 [00:22<00:02, 115.51it/s]\u001b[A\n",
            " 90% 2672/2977 [00:22<00:02, 115.77it/s]\u001b[A\n",
            " 90% 2688/2977 [00:23<00:02, 116.39it/s]\u001b[A\n",
            " 91% 2704/2977 [00:23<00:02, 116.11it/s]\u001b[A\n",
            " 91% 2720/2977 [00:23<00:02, 115.18it/s]\u001b[A\n",
            " 92% 2736/2977 [00:23<00:02, 115.90it/s]\u001b[A\n",
            " 92% 2752/2977 [00:23<00:01, 115.93it/s]\u001b[A\n",
            " 93% 2768/2977 [00:23<00:01, 115.81it/s]\u001b[A\n",
            " 94% 2784/2977 [00:23<00:01, 115.68it/s]\u001b[A\n",
            " 94% 2800/2977 [00:24<00:01, 115.77it/s]\u001b[A\n",
            " 95% 2816/2977 [00:24<00:01, 115.83it/s]\u001b[A\n",
            " 95% 2832/2977 [00:24<00:01, 114.94it/s]\u001b[A\n",
            " 96% 2848/2977 [00:24<00:01, 115.15it/s]\u001b[A\n",
            " 96% 2864/2977 [00:24<00:00, 115.21it/s]\u001b[A\n",
            " 97% 2880/2977 [00:24<00:00, 115.41it/s]\u001b[A\n",
            " 97% 2896/2977 [00:24<00:00, 115.37it/s]\u001b[A\n",
            " 98% 2912/2977 [00:25<00:00, 115.18it/s]\u001b[A\n",
            " 98% 2928/2977 [00:25<00:00, 115.42it/s]\u001b[A\n",
            " 99% 2944/2977 [00:25<00:00, 115.50it/s]\u001b[A\n",
            " 99% 2960/2977 [00:25<00:00, 116.43it/s]\u001b[A\n",
            "100% 2977/2977 [00:25<00:00, 115.65it/s]\n",
            "\n",
            "  0% 0/2929 [00:00<?, ?it/s]\u001b[A\n",
            "  8% 246/2929 [00:00<00:01, 2457.76it/s]\u001b[A\n",
            " 17% 492/2929 [00:00<00:00, 2457.47it/s]\u001b[A\n",
            " 26% 752/2929 [00:00<00:00, 2519.60it/s]\u001b[A\n",
            " 34% 1005/2929 [00:00<00:00, 2521.96it/s]\u001b[A\n",
            " 43% 1258/2929 [00:00<00:00, 2453.28it/s]\u001b[A\n",
            " 51% 1504/2929 [00:00<00:00, 2428.17it/s]\u001b[A\n",
            " 60% 1767/2929 [00:00<00:00, 2491.31it/s]\u001b[A\n",
            " 69% 2026/2929 [00:00<00:00, 2519.57it/s]\u001b[A\n",
            " 78% 2282/2929 [00:00<00:00, 2530.89it/s]\u001b[A\n",
            " 87% 2549/2929 [00:01<00:00, 2573.44it/s]\u001b[A\n",
            "100% 2929/2929 [00:01<00:00, 2506.20it/s]\n",
            "[10.06.21 06:44:55] Visualizing in TensorBoard...\n",
            "[10.06.21 06:44:55] Eval F1: 84.39, EM: 71.53\n",
            "100% 17496/17496 [10:23<00:00, 28.07it/s, NLL=0.847, epoch=0]\n",
            "[10.06.21 06:54:52] Epoch: 1\n",
            "100% 17496/17496 [09:50<00:00, 29.64it/s, NLL=0.214, epoch=1]\n",
            "[10.06.21 07:04:43] Epoch: 2\n",
            "100% 17496/17496 [09:50<00:00, 29.65it/s, NLL=0.346, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FwsLZQGWDgM",
        "outputId": "af9d0870-0128-4b13-acad-4b92805998d8"
      },
      "source": [
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name squad_cnn_hypercolumn_k5 \\\n",
        "    --train-datasets squad03 \\\n",
        "    --eval-datasets race03 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES \\\n",
        "    --save-dir check/ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n",
            "[10.06.21 07:14:43] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race03\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"squad_cnn_hypercolumn_k5\",\n",
            "    \"save_dir\": \"check/squad_cnn_hypercolumn_k5-03\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad03\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.06.21 07:14:43] Preparing Training Data...\n",
            "100% 17047/17047 [00:00<00:00, 21386.48it/s]\n",
            "Preprocessing not completely accurate for 198/17047 instances\n",
            "[10.06.21 07:14:48] Preparing Validation Data...\n",
            "100% 3377/3377 [00:00<00:00, 21998.49it/s]\n",
            "[10.06.21 07:14:52] Epoch: 0\n",
            "  0% 0/17047 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.06.21 07:14:53] Evaluating at step 0...\n",
            "  0% 16/17047 [00:00<10:10, 27.91it/s, NLL=2.1, epoch=0]\n",
            "  0% 0/3377 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/3377 [00:00<00:15, 209.57it/s]\u001b[A\n",
            "  2% 64/3377 [00:00<00:23, 141.99it/s]\u001b[A\n",
            "  2% 80/3377 [00:00<00:24, 133.01it/s]\u001b[A\n",
            "  3% 96/3377 [00:00<00:25, 127.13it/s]\u001b[A\n",
            "  3% 112/3377 [00:00<00:26, 123.71it/s]\u001b[A\n",
            "  4% 128/3377 [00:00<00:26, 121.25it/s]\u001b[A\n",
            "  4% 144/3377 [00:01<00:27, 119.41it/s]\u001b[A\n",
            "  5% 160/3377 [00:01<00:27, 118.46it/s]\u001b[A\n",
            "  5% 176/3377 [00:01<00:27, 117.61it/s]\u001b[A\n",
            "  6% 192/3377 [00:01<00:27, 117.09it/s]\u001b[A\n",
            "  6% 208/3377 [00:01<00:27, 116.68it/s]\u001b[A\n",
            "  7% 224/3377 [00:01<00:27, 116.30it/s]\u001b[A\n",
            "  7% 240/3377 [00:01<00:26, 116.46it/s]\u001b[A\n",
            "  8% 256/3377 [00:02<00:26, 116.22it/s]\u001b[A\n",
            "  8% 272/3377 [00:02<00:26, 116.16it/s]\u001b[A\n",
            "  9% 288/3377 [00:02<00:26, 116.06it/s]\u001b[A\n",
            "  9% 304/3377 [00:02<00:26, 115.91it/s]\u001b[A\n",
            "  9% 320/3377 [00:02<00:26, 115.97it/s]\u001b[A\n",
            " 10% 336/3377 [00:02<00:26, 115.09it/s]\u001b[A\n",
            " 10% 352/3377 [00:02<00:26, 114.96it/s]\u001b[A\n",
            " 11% 368/3377 [00:03<00:26, 115.43it/s]\u001b[A\n",
            " 11% 384/3377 [00:03<00:25, 115.80it/s]\u001b[A\n",
            " 12% 400/3377 [00:03<00:25, 115.78it/s]\u001b[A\n",
            " 12% 416/3377 [00:03<00:25, 115.77it/s]\u001b[A\n",
            " 13% 432/3377 [00:03<00:25, 116.25it/s]\u001b[A\n",
            " 13% 448/3377 [00:03<00:25, 115.44it/s]\u001b[A\n",
            " 14% 464/3377 [00:03<00:25, 115.26it/s]\u001b[A\n",
            " 14% 480/3377 [00:04<00:25, 115.12it/s]\u001b[A\n",
            " 15% 496/3377 [00:04<00:24, 115.51it/s]\u001b[A\n",
            " 15% 512/3377 [00:04<00:24, 115.52it/s]\u001b[A\n",
            " 16% 528/3377 [00:04<00:24, 116.44it/s]\u001b[A\n",
            " 16% 544/3377 [00:04<00:24, 115.58it/s]\u001b[A\n",
            " 17% 560/3377 [00:04<00:24, 116.40it/s]\u001b[A\n",
            " 17% 576/3377 [00:04<00:24, 116.21it/s]\u001b[A\n",
            " 18% 592/3377 [00:04<00:23, 116.12it/s]\u001b[A\n",
            " 18% 608/3377 [00:05<00:23, 116.09it/s]\u001b[A\n",
            " 18% 624/3377 [00:05<00:23, 115.90it/s]\u001b[A\n",
            " 19% 640/3377 [00:05<00:23, 115.79it/s]\u001b[A\n",
            " 19% 656/3377 [00:05<00:23, 115.86it/s]\u001b[A\n",
            " 20% 672/3377 [00:05<00:23, 115.86it/s]\u001b[A\n",
            " 20% 688/3377 [00:05<00:23, 115.84it/s]\u001b[A\n",
            " 21% 704/3377 [00:05<00:23, 115.67it/s]\u001b[A\n",
            " 21% 720/3377 [00:06<00:22, 115.69it/s]\u001b[A\n",
            " 22% 736/3377 [00:06<00:22, 115.50it/s]\u001b[A\n",
            " 22% 752/3377 [00:06<00:22, 115.81it/s]\u001b[A\n",
            " 23% 768/3377 [00:06<00:22, 115.89it/s]\u001b[A\n",
            " 23% 784/3377 [00:06<00:22, 115.72it/s]\u001b[A\n",
            " 24% 800/3377 [00:06<00:22, 115.12it/s]\u001b[A\n",
            " 24% 816/3377 [00:06<00:22, 115.21it/s]\u001b[A\n",
            " 25% 832/3377 [00:07<00:22, 115.30it/s]\u001b[A\n",
            " 25% 848/3377 [00:07<00:21, 115.59it/s]\u001b[A\n",
            " 26% 864/3377 [00:07<00:21, 115.37it/s]\u001b[A\n",
            " 26% 880/3377 [00:07<00:21, 115.61it/s]\u001b[A\n",
            " 27% 896/3377 [00:07<00:21, 116.50it/s]\u001b[A\n",
            " 27% 912/3377 [00:07<00:21, 116.13it/s]\u001b[A\n",
            " 27% 928/3377 [00:07<00:21, 116.07it/s]\u001b[A\n",
            " 28% 944/3377 [00:08<00:20, 116.06it/s]\u001b[A\n",
            " 28% 960/3377 [00:08<00:20, 115.76it/s]\u001b[A\n",
            " 29% 976/3377 [00:08<00:20, 115.85it/s]\u001b[A\n",
            " 29% 992/3377 [00:08<00:20, 115.71it/s]\u001b[A\n",
            " 30% 1008/3377 [00:08<00:20, 114.93it/s]\u001b[A\n",
            " 30% 1024/3377 [00:08<00:20, 115.98it/s]\u001b[A\n",
            " 31% 1040/3377 [00:08<00:20, 115.95it/s]\u001b[A\n",
            " 31% 1056/3377 [00:08<00:20, 115.92it/s]\u001b[A\n",
            " 32% 1072/3377 [00:09<00:19, 115.87it/s]\u001b[A\n",
            " 32% 1088/3377 [00:09<00:19, 115.84it/s]\u001b[A\n",
            " 33% 1104/3377 [00:09<00:19, 115.72it/s]\u001b[A\n",
            " 33% 1120/3377 [00:09<00:19, 115.75it/s]\u001b[A\n",
            " 34% 1136/3377 [00:09<00:19, 115.95it/s]\u001b[A\n",
            " 34% 1152/3377 [00:09<00:19, 115.82it/s]\u001b[A\n",
            " 35% 1168/3377 [00:09<00:19, 115.81it/s]\u001b[A\n",
            " 35% 1184/3377 [00:10<00:18, 115.76it/s]\u001b[A\n",
            " 36% 1200/3377 [00:10<00:18, 115.90it/s]\u001b[A\n",
            " 36% 1216/3377 [00:10<00:18, 115.52it/s]\u001b[A\n",
            " 36% 1232/3377 [00:10<00:18, 115.52it/s]\u001b[A\n",
            " 37% 1248/3377 [00:10<00:18, 114.82it/s]\u001b[A\n",
            " 37% 1264/3377 [00:10<00:18, 115.85it/s]\u001b[A\n",
            " 38% 1280/3377 [00:10<00:18, 115.94it/s]\u001b[A\n",
            " 38% 1296/3377 [00:11<00:17, 115.80it/s]\u001b[A\n",
            " 39% 1312/3377 [00:11<00:17, 115.85it/s]\u001b[A\n",
            " 39% 1328/3377 [00:11<00:17, 116.04it/s]\u001b[A\n",
            " 40% 1344/3377 [00:11<00:17, 115.86it/s]\u001b[A\n",
            " 40% 1360/3377 [00:11<00:17, 115.01it/s]\u001b[A\n",
            " 41% 1376/3377 [00:11<00:17, 115.86it/s]\u001b[A\n",
            " 41% 1392/3377 [00:11<00:17, 115.72it/s]\u001b[A\n",
            " 42% 1408/3377 [00:12<00:17, 115.74it/s]\u001b[A\n",
            " 42% 1424/3377 [00:12<00:16, 115.74it/s]\u001b[A\n",
            " 43% 1440/3377 [00:12<00:16, 115.72it/s]\u001b[A\n",
            " 43% 1456/3377 [00:12<00:16, 115.11it/s]\u001b[A\n",
            " 44% 1472/3377 [00:12<00:16, 115.66it/s]\u001b[A\n",
            " 44% 1488/3377 [00:12<00:16, 115.91it/s]\u001b[A\n",
            " 45% 1504/3377 [00:12<00:16, 115.14it/s]\u001b[A\n",
            " 45% 1520/3377 [00:13<00:16, 116.02it/s]\u001b[A\n",
            " 45% 1536/3377 [00:13<00:15, 115.85it/s]\u001b[A\n",
            " 46% 1552/3377 [00:13<00:15, 115.97it/s]\u001b[A\n",
            " 46% 1568/3377 [00:13<00:15, 115.85it/s]\u001b[A\n",
            " 47% 1584/3377 [00:13<00:15, 115.86it/s]\u001b[A\n",
            " 47% 1600/3377 [00:13<00:15, 115.10it/s]\u001b[A\n",
            "  0% 16/17047 [00:14<10:10, 27.91it/s, NLL=2.1, epoch=0]\n",
            " 48% 1632/3377 [00:13<00:15, 115.37it/s]\u001b[A\n",
            " 49% 1648/3377 [00:14<00:14, 115.53it/s]\u001b[A\n",
            " 49% 1664/3377 [00:14<00:14, 115.67it/s]\u001b[A\n",
            " 50% 1680/3377 [00:14<00:14, 115.63it/s]\u001b[A\n",
            " 50% 1696/3377 [00:14<00:14, 115.06it/s]\u001b[A\n",
            " 51% 1712/3377 [00:14<00:14, 116.46it/s]\u001b[A\n",
            " 51% 1728/3377 [00:14<00:14, 116.28it/s]\u001b[A\n",
            " 52% 1744/3377 [00:14<00:14, 116.17it/s]\u001b[A\n",
            " 52% 1760/3377 [00:15<00:14, 115.34it/s]\u001b[A\n",
            " 53% 1776/3377 [00:15<00:13, 115.48it/s]\u001b[A\n",
            " 53% 1792/3377 [00:15<00:13, 115.62it/s]\u001b[A\n",
            " 54% 1808/3377 [00:15<00:13, 116.22it/s]\u001b[A\n",
            " 54% 1824/3377 [00:15<00:13, 116.22it/s]\u001b[A\n",
            " 54% 1840/3377 [00:15<00:13, 115.25it/s]\u001b[A\n",
            " 55% 1856/3377 [00:15<00:13, 115.49it/s]\u001b[A\n",
            " 55% 1872/3377 [00:16<00:12, 116.33it/s]\u001b[A\n",
            " 56% 1888/3377 [00:16<00:12, 116.18it/s]\u001b[A\n",
            " 56% 1904/3377 [00:16<00:12, 116.00it/s]\u001b[A\n",
            " 57% 1920/3377 [00:16<00:12, 115.85it/s]\u001b[A\n",
            " 57% 1936/3377 [00:16<00:12, 115.14it/s]\u001b[A\n",
            " 58% 1952/3377 [00:16<00:12, 115.10it/s]\u001b[A\n",
            " 58% 1968/3377 [00:16<00:12, 115.14it/s]\u001b[A\n",
            " 59% 1984/3377 [00:17<00:11, 116.29it/s]\u001b[A\n",
            " 59% 2000/3377 [00:17<00:11, 116.27it/s]\u001b[A\n",
            " 60% 2016/3377 [00:17<00:11, 116.12it/s]\u001b[A\n",
            " 60% 2032/3377 [00:17<00:11, 116.10it/s]\u001b[A\n",
            " 61% 2048/3377 [00:17<00:11, 115.88it/s]\u001b[A\n",
            " 61% 2064/3377 [00:17<00:11, 115.78it/s]\u001b[A\n",
            " 62% 2080/3377 [00:17<00:11, 115.76it/s]\u001b[A\n",
            " 62% 2096/3377 [00:17<00:11, 115.84it/s]\u001b[A\n",
            " 63% 2112/3377 [00:18<00:10, 115.74it/s]\u001b[A\n",
            " 63% 2128/3377 [00:18<00:10, 115.74it/s]\u001b[A\n",
            " 63% 2144/3377 [00:18<00:10, 115.68it/s]\u001b[A\n",
            " 64% 2160/3377 [00:18<00:10, 115.13it/s]\u001b[A\n",
            " 64% 2176/3377 [00:18<00:10, 116.16it/s]\u001b[A\n",
            " 65% 2192/3377 [00:18<00:10, 116.04it/s]\u001b[A\n",
            " 65% 2208/3377 [00:18<00:10, 115.96it/s]\u001b[A\n",
            " 66% 2224/3377 [00:19<00:09, 115.99it/s]\u001b[A\n",
            " 66% 2240/3377 [00:19<00:09, 115.87it/s]\u001b[A\n",
            " 67% 2256/3377 [00:19<00:09, 115.82it/s]\u001b[A\n",
            " 67% 2272/3377 [00:19<00:09, 115.77it/s]\u001b[A\n",
            " 68% 2288/3377 [00:19<00:09, 115.93it/s]\u001b[A\n",
            " 68% 2304/3377 [00:19<00:09, 115.82it/s]\u001b[A\n",
            " 69% 2320/3377 [00:19<00:09, 115.81it/s]\u001b[A\n",
            " 69% 2336/3377 [00:20<00:08, 115.85it/s]\u001b[A\n",
            " 70% 2352/3377 [00:20<00:08, 115.73it/s]\u001b[A\n",
            " 70% 2368/3377 [00:20<00:08, 115.70it/s]\u001b[A\n",
            " 71% 2384/3377 [00:20<00:08, 115.83it/s]\u001b[A\n",
            " 71% 2400/3377 [00:20<00:08, 115.45it/s]\u001b[A\n",
            " 72% 2416/3377 [00:20<00:08, 115.88it/s]\u001b[A\n",
            " 72% 2432/3377 [00:20<00:08, 115.90it/s]\u001b[A\n",
            " 72% 2448/3377 [00:21<00:08, 115.90it/s]\u001b[A\n",
            " 73% 2464/3377 [00:21<00:07, 115.84it/s]\u001b[A\n",
            " 73% 2480/3377 [00:21<00:07, 115.86it/s]\u001b[A\n",
            " 74% 2496/3377 [00:21<00:07, 115.83it/s]\u001b[A\n",
            " 74% 2512/3377 [00:21<00:07, 115.81it/s]\u001b[A\n",
            " 75% 2528/3377 [00:21<00:07, 115.00it/s]\u001b[A\n",
            " 75% 2544/3377 [00:21<00:07, 115.30it/s]\u001b[A\n",
            " 76% 2560/3377 [00:21<00:07, 115.51it/s]\u001b[A\n",
            " 76% 2576/3377 [00:22<00:06, 116.34it/s]\u001b[A\n",
            " 77% 2592/3377 [00:22<00:06, 115.90it/s]\u001b[A\n",
            " 77% 2608/3377 [00:22<00:06, 116.08it/s]\u001b[A\n",
            " 78% 2624/3377 [00:22<00:06, 115.99it/s]\u001b[A\n",
            " 78% 2640/3377 [00:22<00:06, 115.25it/s]\u001b[A\n",
            " 79% 2656/3377 [00:22<00:06, 116.05it/s]\u001b[A\n",
            " 79% 2672/3377 [00:22<00:06, 115.89it/s]\u001b[A\n",
            " 80% 2688/3377 [00:23<00:05, 115.90it/s]\u001b[A\n",
            " 80% 2704/3377 [00:23<00:05, 115.92it/s]\u001b[A\n",
            " 81% 2720/3377 [00:23<00:05, 115.46it/s]\u001b[A\n",
            " 81% 2736/3377 [00:23<00:05, 115.89it/s]\u001b[A\n",
            " 81% 2752/3377 [00:23<00:05, 115.17it/s]\u001b[A\n",
            " 82% 2768/3377 [00:23<00:05, 116.03it/s]\u001b[A\n",
            " 82% 2784/3377 [00:23<00:05, 115.92it/s]\u001b[A\n",
            " 83% 2800/3377 [00:24<00:04, 115.89it/s]\u001b[A\n",
            " 83% 2816/3377 [00:24<00:04, 115.90it/s]\u001b[A\n",
            " 84% 2832/3377 [00:24<00:04, 115.85it/s]\u001b[A\n",
            " 84% 2848/3377 [00:24<00:04, 115.82it/s]\u001b[A\n",
            " 85% 2864/3377 [00:24<00:04, 115.73it/s]\u001b[A\n",
            " 85% 2880/3377 [00:24<00:04, 115.86it/s]\u001b[A\n",
            " 86% 2896/3377 [00:24<00:04, 115.07it/s]\u001b[A\n",
            " 86% 2912/3377 [00:25<00:04, 115.24it/s]\u001b[A\n",
            " 87% 2928/3377 [00:25<00:03, 115.36it/s]\u001b[A\n",
            " 87% 2944/3377 [00:25<00:03, 115.56it/s]\u001b[A\n",
            " 88% 2960/3377 [00:25<00:03, 115.46it/s]\u001b[A\n",
            " 88% 2976/3377 [00:25<00:03, 116.28it/s]\u001b[A\n",
            " 89% 2992/3377 [00:25<00:03, 115.34it/s]\u001b[A\n",
            " 89% 3008/3377 [00:25<00:03, 115.53it/s]\u001b[A\n",
            " 90% 3024/3377 [00:26<00:03, 115.59it/s]\u001b[A\n",
            " 90% 3040/3377 [00:26<00:02, 115.54it/s]\u001b[A\n",
            " 90% 3056/3377 [00:26<00:02, 116.31it/s]\u001b[A\n",
            " 91% 3072/3377 [00:26<00:02, 116.21it/s]\u001b[A\n",
            " 91% 3088/3377 [00:26<00:02, 115.15it/s]\u001b[A\n",
            " 92% 3104/3377 [00:26<00:02, 115.56it/s]\u001b[A\n",
            " 92% 3120/3377 [00:26<00:02, 115.50it/s]\u001b[A\n",
            " 93% 3136/3377 [00:26<00:02, 115.64it/s]\u001b[A\n",
            " 93% 3152/3377 [00:27<00:01, 115.61it/s]\u001b[A\n",
            " 94% 3168/3377 [00:27<00:01, 115.68it/s]\u001b[A\n",
            " 94% 3184/3377 [00:27<00:01, 115.56it/s]\u001b[A\n",
            " 95% 3200/3377 [00:27<00:01, 116.49it/s]\u001b[A\n",
            " 95% 3216/3377 [00:27<00:01, 115.44it/s]\u001b[A\n",
            " 96% 3232/3377 [00:27<00:01, 115.39it/s]\u001b[A\n",
            " 96% 3248/3377 [00:27<00:01, 115.56it/s]\u001b[A\n",
            " 97% 3264/3377 [00:28<00:00, 115.55it/s]\u001b[A\n",
            " 97% 3280/3377 [00:28<00:00, 115.65it/s]\u001b[A\n",
            " 98% 3296/3377 [00:28<00:00, 116.36it/s]\u001b[A\n",
            " 98% 3312/3377 [00:28<00:00, 115.50it/s]\u001b[A\n",
            " 99% 3328/3377 [00:28<00:00, 115.54it/s]\u001b[A\n",
            " 99% 3344/3377 [00:28<00:00, 115.55it/s]\u001b[A\n",
            " 99% 3360/3377 [00:28<00:00, 115.54it/s]\u001b[A\n",
            "100% 3377/3377 [00:29<00:00, 115.71it/s]\n",
            "\n",
            "  0% 0/3335 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 245/3335 [00:00<00:01, 2443.52it/s]\u001b[A\n",
            " 15% 491/3335 [00:00<00:01, 2452.60it/s]\u001b[A\n",
            " 22% 737/3335 [00:00<00:01, 2443.65it/s]\u001b[A\n",
            " 29% 982/3335 [00:00<00:00, 2425.47it/s]\u001b[A\n",
            " 37% 1234/3335 [00:00<00:00, 2456.49it/s]\u001b[A\n",
            " 44% 1480/3335 [00:00<00:00, 2431.52it/s]\u001b[A\n",
            " 52% 1724/3335 [00:00<00:00, 2415.19it/s]\u001b[A\n",
            " 59% 1973/3335 [00:00<00:00, 2437.27it/s]\u001b[A\n",
            " 67% 2231/3335 [00:00<00:00, 2480.01it/s]\u001b[A\n",
            " 74% 2484/3335 [00:01<00:00, 2493.14it/s]\u001b[A\n",
            " 82% 2734/3335 [00:01<00:00, 2488.36it/s]\u001b[A\n",
            " 89% 2983/3335 [00:01<00:00, 2486.32it/s]\u001b[A\n",
            "100% 3335/3335 [00:01<00:00, 2479.30it/s]\n",
            "[10.06.21 07:15:24] Visualizing in TensorBoard...\n",
            "[10.06.21 07:15:24] Eval F1: 83.06, EM: 69.66\n",
            "100% 17047/17047 [10:12<00:00, 27.84it/s, NLL=1.12, epoch=0]\n",
            "[10.06.21 07:25:06] Epoch: 1\n",
            "100% 17047/17047 [09:34<00:00, 29.65it/s, NLL=0.628, epoch=1]\n",
            "[10.06.21 07:34:42] Epoch: 2\n",
            "100% 17047/17047 [09:34<00:00, 29.65it/s, NLL=0.726, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxX-Co4mWMSb"
      },
      "source": [
        "!cp -r /content/robustqa /content/drive/MyDrive/robustqa_change_transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8ekOrA-IoPh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-P94czNIpG_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msvjx7fCaO2d"
      },
      "source": [
        "#Conv Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdtbUqeEW9ua",
        "outputId": "796a4d22-b3d6-468f-9cf9-0becb44a80be"
      },
      "source": [
        "import torch\n",
        "\n",
        "a = torch.zeros(4, 8, 16)\n",
        "b = torch.ones(4, 8, 16)\n",
        "\n",
        "c = torch.cat((a, b), dim=-1).transpose(1, 2).contiguous()\n",
        "# c = c.unsqueeze(1)\n",
        "print(c.shape)\n",
        "print(c[0])\n",
        "\n",
        "conv = torch.nn.Conv1d(in_channels=32, out_channels=2, kernel_size=3, padding='same')\n",
        "outp = conv(c)\n",
        "print(outp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 32, 8])\n",
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "torch.Size([4, 2, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-101CykMFtfl",
        "outputId": "fe808f2e-be8f-4e19-8215-4733cac343f1"
      },
      "source": [
        "outp = outp.transpose(1, 2).contiguous()\n",
        "print(outp.shape)\n",
        "print(outp[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 2, 8])\n",
            "tensor([[-0.0870, -0.0549, -0.0549, -0.0549, -0.0549, -0.0549, -0.0549,  0.1070],\n",
            "        [-0.1452, -0.2878, -0.2878, -0.2878, -0.2878, -0.2878, -0.2878,  0.0312]],\n",
            "       grad_fn=<SelectBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJf_7MCyrHps"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dekkRCFZrN1U"
      },
      "source": [
        "#UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOG8jRlU18qt",
        "outputId": "bd3dbb9b-3b44-4cc5-bd8f-b7b77de7deb3"
      },
      "source": [
        "%%writefile /content/robustqa/transformers/models/roberta/modeling_roberta.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model. \"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from ...activations import ACT2FN, gelu\n",
        "from ...file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from ...utils import logging\n",
        "from .configuration_roberta import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        # End copy\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
        "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # if self.config.hyper_column:\n",
        "        # print(\"embedding = \" + str(embeddings.shape))\n",
        "        # hyper_columns = torch.zeros(embeddings.shape)\n",
        "        hyper_columns = torch.clone(embeddings) # [batch, 384, 768]\n",
        "        hyper_columns.fill_(0.0)\n",
        "        embeddings = torch.cat((embeddings, hyper_columns), dim=1) # [batch, 768, 768]\n",
        "        # print(\"embedding after = \" + str(embeddings.shape))\n",
        "        return embeddings\n",
        "\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n",
        "        \"\"\"\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n",
        "\n",
        "        Args:\n",
        "            inputs_embeds: torch.Tensor\n",
        "\n",
        "        Returns: torch.Tensor\n",
        "        \"\"\"\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        sequence_length = input_shape[1]\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n",
        "        )\n",
        "        return position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        # mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        # hidden_states = mixed_states[0]\n",
        "        # hyper_columns = mixed_states[1]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        # print(\"self attention = \" + str(len(outputs))) # len = 1\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print(\"self ouput = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"attention hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"attention hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        # print(\"attention outputs = \" + str(len(outputs)))\n",
        "        # print(outputs[0].shape)\n",
        "        outputs = (torch.cat((outputs[0], hyper_columns), dim=1),)\n",
        "        # print(\"ret outputs = \" + str(len(outputs)))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(hidden_states.shape)\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"intermediate hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"intermediate hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        # hidden_states.shape = [batch, 384, 3072]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=-1)\n",
        "        # print(\"intermediate after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print(\"input_tensor = \" + str(input_tensor.shape))\n",
        "        # print(\"hidden_states = \" + str(hidden_states.shape))\n",
        "        mixed_states = torch.split(hidden_states, 3072, dim=-1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"output hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"output hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print(hidden_states.shape)\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        # input_tensor.shape = [batch, 384, 768]\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor.split(384, dim=1)[0]) #problem!!\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=1)\n",
        "        # print(\"roberta output after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaHyperColumn(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 2)\n",
        "        # self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        ## identical hyper columns\n",
        "        # mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        # hidden_states = mixed_states[0]\n",
        "        # hyper_columns = mixed_states[0]\n",
        "        # hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "        # return hidden_states\n",
        "        # print(len(hidden_states))\n",
        "        # print(hidden_states[0].shape)\n",
        "        mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"hyper columns hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"hyper column hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hyper_columns = self.dense(hidden_states)\n",
        "        # print(hyper_columns.shape)\n",
        "        # hyper_columns = nn.functional.interpolate(hyper_columns, 768)\n",
        "        \n",
        "        # hyper_columns = hyper_columns.unsqueeze(1)\n",
        "        # hyper_columns = self.interp(hyper_columns, size=(384, 768), mode='bicubic')\n",
        "        # hyper_columns = hyper_columns.squeeze(1)\n",
        "        \n",
        "        hyper_columns = self.interp(hyper_columns, size=768, mode='linear')\n",
        "        \n",
        "        # print(hyper_columns.shape)\n",
        "        hyper_columns = self.dropout(hyper_columns)\n",
        "        # hyper_columns = self.LayerNorm(hyper_columns)\n",
        "\n",
        "        hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "        self.hyper_columns = RobertaHyperColumn(config)\n",
        "        # self.linear = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # if isinstance(config.hidden_act, str):\n",
        "        #     self.resnet_act_fn = ACT2FN[config.hidden_act]\n",
        "        # else:\n",
        "        #     self.resnet_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # print(\"roberta layer hidden states = \" + str(hidden_states.shape))\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            assert hasattr(\n",
        "                self, \"crossattention\"\n",
        "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        \n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        outputs = self.hyper_columns(outputs)\n",
        "        # Implement Residual Net\n",
        "        # mixed_tensor = torch.split(outputs[0], 384, dim=1)\n",
        "        # outputs_tensor = mixed_tensor[0]\n",
        "        # hyper_columns_tensor = mixed_tensor[1]\n",
        "        # # mixed_attention = torch.split(attention_output, 384, dim=1)\n",
        "        # # identity = mixed_attention[0]\n",
        "        # identity = torch.split(attention_output, 384, dim=1)[0]\n",
        "        # outputs_tensor = torch.add(outputs_tensor, identity)\n",
        "        # # outputs_tensor = torch.cat((outputs_tensor, outputs_tensor_add), dim=-1)\n",
        "        # # outputs_tensor = self.linear(outputs_tensor)\n",
        "        # # outputs_tensor = self.resnet_act_fn(outputs_tensor)\n",
        "        # outputs_tensor = torch.cat((outputs_tensor, hyper_columns_tensor), dim=1)\n",
        "        # outputs = (outputs_tensor,)\n",
        "\n",
        "        # print(\"roberta layer outputs = \" + str(len(outputs)) + \" \" + str(outputs[0].shape))\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        # print(\"roberta encoder hidden states = \" + str(hidden_states.shape))\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        all_hyper_columns = ()\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            # print(\"roberta encoder after hidden states = \" + str(hidden_states.shape))\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            # if (i + 1) % 4 == 0:\n",
        "            hyper_column = torch.split(hidden_states, 384, dim=1)[1]\n",
        "            # print(\"encoder hyper column shape = \" + str(hyper_column.shape))\n",
        "            all_hyper_columns = all_hyper_columns + (hyper_column,)\n",
        "\n",
        "        # hidden_states = torch.split(hidden_states, 384, dim=1)[0]\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        hidden_states = torch.split(hidden_states, 384, dim=1)[0] # [batch, 384, 768]\n",
        "        # print(\"before = \" + str(hidden_states.shape))\n",
        "        to_add = (hidden_states,) + all_hyper_columns # len = 13\n",
        "        # print(\"len to add = \" + str(len(to_add)))\n",
        "        hidden_states = torch.cat(to_add, dim=1)\n",
        "        # print(\"after = \" + str(hidden_states.shape))\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(RobertaPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "\n",
        "    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", ROBERTA_START_DOCSTRING\n",
        ")\n",
        "class RobertaForCausalLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "            >>> import torch\n",
        "\n",
        "            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "            >>> config.is_decoder = True\n",
        "            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n",
        "\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dense(features)\n",
        "        x = gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # project back to size of vocabulary with bias\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _tie_weights(self):\n",
        "        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n",
        "        self.bias = self.decoder.bias\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
        "    pooled output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            position_ids=flat_position_ids,\n",
        "            token_type_ids=flat_token_type_ids,\n",
        "            attention_mask=flat_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=flat_inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.in_channels_init = config.hidden_size * config.num_hidden_layers\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, padding=0)\n",
        "        self.act = nn.Tanh()\n",
        "        self.down_sample_01 = nn.Conv1d(in_channels=self.in_channels_init,\n",
        "                                        out_channels=self.in_channels_init//16,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_02 = nn.Conv1d(in_channels=self.in_channels_init//16,\n",
        "                                        out_channels=self.in_channels_init//32,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_03 = nn.Conv1d(in_channels=self.in_channels_init//32,\n",
        "                                        out_channels=self.in_channels_init//64,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_04 = nn.Conv1d(in_channels=self.in_channels_init//64,\n",
        "                                        out_channels=self.in_channels_init//128,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.up_sample_04 = nn.ConvTranspose1d(in_channels=self.in_channels_init//128,\n",
        "                                               out_channels=self.in_channels_init//64,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_03 = nn.ConvTranspose1d(in_channels=self.in_channels_init//64,\n",
        "                                               out_channels=self.in_channels_init//32,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_02 = nn.ConvTranspose1d(in_channels=self.in_channels_init//32,\n",
        "                                               out_channels=self.in_channels_init//16,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_01 = nn.ConvTranspose1d(in_channels=self.in_channels_init//16,\n",
        "                                               out_channels=self.in_channels_init//8,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # identity1 = x\n",
        "        x = self.down_sample_01(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity2 = x\n",
        "        x = self.down_sample_02(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity3 = x\n",
        "        x = self.down_sample_03(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity4 = x\n",
        "        x = self.down_sample_04(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        #up\n",
        "        # y = x\n",
        "        y = self.up_sample_04(x)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity4), dim=-1)\n",
        "        y = self.up_sample_03(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity3), dim=-1)\n",
        "        y = self.up_sample_02(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity2), dim=-1)\n",
        "        y = self.up_sample_01(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity1), dim=-1)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        # self.unet = UNet(config)\n",
        "        # self.qa_outputs = nn.Conv1d(in_channels=config.hidden_size * config.num_hidden_layers // 8, out_channels=2,\n",
        "        #                             kernel_size=5, padding='same')\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size * config.num_hidden_layers, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # print(len(outputs))\n",
        "        # print(\"outputs shape = \" + str(outputs[0].shape))\n",
        "        sequence_output = torch.split(outputs[0], 384, dim=1)[0]\n",
        "        # print(sequence_output.shape)\n",
        "        hyper_columns = torch.split(outputs[0], 384, dim=1)[1:]\n",
        "        # print(\"qa sequence = \" + str(sequence_output.shape))\n",
        "        # print(\"qa hyper columns len = \" + str(len(hyper_columns)))\n",
        "        hyper_columns_tensor = torch.cat(hyper_columns, dim=-1)\n",
        "\n",
        "        sequence_output = hyper_columns_tensor\n",
        "        # print(sequence_output.shape)\n",
        "        \n",
        "        # sequence_output = torch.cat((sequence_output, hyper_columns_tensor), dim=-1)\n",
        "\n",
        "        # CNN\n",
        "        # sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        # logits = self.qa_outputs(sequence_output)\n",
        "        # logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        # UNet\n",
        "        # sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        # unet_output = self.unet(sequence_output)\n",
        "        # unet_output = unet_output.transpose(1, 2).contiguous()\n",
        "        # logits = self.qa_outputs(unet_output)\n",
        "        # logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
        "    \"\"\"\n",
        "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
        "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
        "\n",
        "    Args:\n",
        "        x: torch.Tensor x:\n",
        "\n",
        "    Returns: torch.Tensor\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
        "    return incremental_indices.long() + padding_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/robustqa/transformers/models/roberta/modeling_roberta.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2HNFx4pg3Jj"
      },
      "source": [
        "#Runner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_P1KdNIwKjav",
        "outputId": "e27adf9b-cde3-461f-ee1b-458594fc253e"
      },
      "source": [
        "# Pure Hypercolumn\n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name experiment \\\n",
        "    --train-datasets squad_experiment \\\n",
        "    --eval-datasets race_experiment \\\n",
        "    --eval-every 50 \\\n",
        "    --batch-size 4 \\\n",
        "    --num-epochs 1 \\\n",
        "    --save-dir delete/ \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/robustqa\n",
            "Downloading: 100% 878k/878k [00:00<00:00, 2.11MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 1.07MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 3.12MB/s]\n",
            "Downloading: 100% 481/481 [00:00<00:00, 409kB/s]\n",
            "Downloading: 100% 478M/478M [00:08<00:00, 59.1MB/s]\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'qa_outputs.weight', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[10.20.21 08:17:24] Args: {\n",
            "    \"batch_size\": 4,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race_experiment\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 50,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 1,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"experiment\",\n",
            "    \"save_dir\": \"delete/experiment-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad_experiment\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.20.21 08:17:24] Preparing Training Data...\n",
            "100% 40/40 [00:00<00:00, 21255.82it/s]\n",
            "Preprocessing not completely accurate for 1/40 instances\n",
            "[10.20.21 08:17:24] Preparing Validation Data...\n",
            "100% 18/18 [00:00<00:00, 22185.56it/s]\n",
            "[10.20.21 08:17:37] Epoch: 0\n",
            "  0% 0/40 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.20.21 08:17:37] Evaluating at step 0...\n",
            " 10% 4/40 [00:00<00:02, 12.19it/s, NLL=5.91, epoch=0]\n",
            "  0% 0/18 [00:00<?, ?it/s]\u001b[A\n",
            "100% 18/18 [00:00<00:00, 110.13it/s]\n",
            "\n",
            "100% 18/18 [00:00<00:00, 2908.67it/s]\n",
            "[10.20.21 08:17:38] Visualizing in TensorBoard...\n",
            "[10.20.21 08:17:38] Eval F1: 04.16, EM: 00.00\n",
            "100% 40/40 [00:04<00:00,  8.43it/s, NLL=4.78, epoch=0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBJmu2VO1teQ"
      },
      "source": [
        "!rm -r /content/robustqa/delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHxufMN2-73P"
      },
      "source": [
        "!rm -r /content/robustqa/check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0b39aeroY--",
        "outputId": "b8223b7d-deb6-4b49-d800-75ec8d3840f5"
      },
      "source": [
        "# Pure Hypercolumn\n",
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name pure_hyper_column \\\n",
        "    --train-datasets squad01 \\\n",
        "    --eval-datasets race01 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --save-dir check/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'qa_outputs.weight', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.bias', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'qa_outputs.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[10.20.21 08:18:41] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race01\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"pure_hyper_column\",\n",
            "    \"save_dir\": \"check/pure_hyper_column-01\",\n",
            "    \"saved\": \"NO\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad01\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.20.21 08:18:41] Preparing Training Data...\n",
            "100% 16020/16020 [00:00<00:00, 21178.72it/s]\n",
            "Preprocessing not completely accurate for 150/16020 instances\n",
            "[10.20.21 08:18:46] Preparing Validation Data...\n",
            "100% 4436/4436 [00:00<00:00, 22689.41it/s]\n",
            "[10.20.21 08:18:51] Epoch: 0\n",
            "  0% 0/16020 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.20.21 08:18:52] Evaluating at step 0...\n",
            "  0% 16/16020 [00:00<09:45, 27.32it/s, NLL=5.95, epoch=0]\n",
            "  0% 0/4436 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/4436 [00:00<00:20, 210.88it/s]\u001b[A\n",
            "  1% 64/4436 [00:00<00:29, 146.18it/s]\u001b[A\n",
            "  2% 80/4436 [00:00<00:31, 137.52it/s]\u001b[A\n",
            "  2% 96/4436 [00:00<00:33, 131.13it/s]\u001b[A\n",
            "  3% 112/4436 [00:00<00:33, 128.91it/s]\u001b[A\n",
            "  3% 128/4436 [00:00<00:34, 125.64it/s]\u001b[A\n",
            "  3% 144/4436 [00:01<00:34, 124.80it/s]\u001b[A\n",
            "  4% 160/4436 [00:01<00:34, 123.93it/s]\u001b[A\n",
            "  4% 176/4436 [00:01<00:34, 122.45it/s]\u001b[A\n",
            "  4% 192/4436 [00:01<00:34, 121.95it/s]\u001b[A\n",
            "  5% 208/4436 [00:01<00:34, 120.90it/s]\u001b[A\n",
            "  5% 224/4436 [00:01<00:34, 121.25it/s]\u001b[A\n",
            "  5% 240/4436 [00:01<00:35, 119.64it/s]\u001b[A\n",
            "  6% 256/4436 [00:02<00:34, 119.86it/s]\u001b[A\n",
            "  6% 272/4436 [00:02<00:34, 119.82it/s]\u001b[A\n",
            "  6% 288/4436 [00:02<00:34, 120.48it/s]\u001b[A\n",
            "  7% 304/4436 [00:02<00:34, 120.20it/s]\u001b[A\n",
            "  7% 320/4436 [00:02<00:34, 119.65it/s]\u001b[A\n",
            "  8% 336/4436 [00:02<00:34, 120.24it/s]\u001b[A\n",
            "  8% 352/4436 [00:02<00:33, 120.27it/s]\u001b[A\n",
            "  8% 368/4436 [00:02<00:33, 120.68it/s]\u001b[A\n",
            "  9% 384/4436 [00:03<00:33, 120.82it/s]\u001b[A\n",
            "  9% 400/4436 [00:03<00:33, 120.75it/s]\u001b[A\n",
            "  9% 416/4436 [00:03<00:33, 119.39it/s]\u001b[A\n",
            " 10% 432/4436 [00:03<00:33, 119.77it/s]\u001b[A\n",
            " 10% 448/4436 [00:03<00:33, 119.78it/s]\u001b[A\n",
            " 10% 464/4436 [00:03<00:33, 120.25it/s]\u001b[A\n",
            " 11% 480/4436 [00:03<00:32, 121.48it/s]\u001b[A\n",
            " 11% 496/4436 [00:04<00:32, 121.25it/s]\u001b[A\n",
            " 12% 512/4436 [00:04<00:32, 120.77it/s]\u001b[A\n",
            " 12% 528/4436 [00:04<00:32, 120.88it/s]\u001b[A\n",
            " 12% 544/4436 [00:04<00:32, 121.09it/s]\u001b[A\n",
            " 13% 560/4436 [00:04<00:32, 119.66it/s]\u001b[A\n",
            " 13% 576/4436 [00:04<00:31, 121.54it/s]\u001b[A\n",
            " 13% 592/4436 [00:04<00:31, 121.45it/s]\u001b[A\n",
            " 14% 608/4436 [00:04<00:31, 121.18it/s]\u001b[A\n",
            " 14% 624/4436 [00:05<00:31, 120.70it/s]\u001b[A\n",
            " 14% 640/4436 [00:05<00:31, 121.31it/s]\u001b[A\n",
            " 15% 656/4436 [00:05<00:31, 121.01it/s]\u001b[A\n",
            " 15% 672/4436 [00:05<00:31, 120.79it/s]\u001b[A\n",
            " 16% 688/4436 [00:05<00:31, 120.73it/s]\u001b[A\n",
            " 16% 704/4436 [00:05<00:30, 120.99it/s]\u001b[A\n",
            " 16% 720/4436 [00:05<00:30, 121.11it/s]\u001b[A\n",
            " 17% 736/4436 [00:05<00:30, 120.88it/s]\u001b[A\n",
            " 17% 752/4436 [00:06<00:30, 120.76it/s]\u001b[A\n",
            " 17% 768/4436 [00:06<00:30, 120.69it/s]\u001b[A\n",
            " 18% 784/4436 [00:06<00:30, 120.38it/s]\u001b[A\n",
            " 18% 800/4436 [00:06<00:30, 120.09it/s]\u001b[A\n",
            " 18% 816/4436 [00:06<00:30, 120.22it/s]\u001b[A\n",
            " 19% 832/4436 [00:06<00:30, 119.85it/s]\u001b[A\n",
            " 19% 848/4436 [00:06<00:29, 119.71it/s]\u001b[A\n",
            " 19% 864/4436 [00:07<00:29, 119.35it/s]\u001b[A\n",
            " 20% 880/4436 [00:07<00:29, 119.78it/s]\u001b[A\n",
            " 20% 896/4436 [00:07<00:29, 119.99it/s]\u001b[A\n",
            " 21% 912/4436 [00:07<00:29, 120.10it/s]\u001b[A\n",
            " 21% 928/4436 [00:07<00:29, 119.91it/s]\u001b[A\n",
            " 21% 944/4436 [00:07<00:29, 119.89it/s]\u001b[A\n",
            " 22% 960/4436 [00:07<00:29, 119.60it/s]\u001b[A\n",
            " 22% 976/4436 [00:07<00:28, 120.08it/s]\u001b[A\n",
            " 22% 992/4436 [00:08<00:28, 119.96it/s]\u001b[A\n",
            " 23% 1008/4436 [00:08<00:28, 120.29it/s]\u001b[A\n",
            " 23% 1024/4436 [00:08<00:28, 119.32it/s]\u001b[A\n",
            " 23% 1040/4436 [00:08<00:28, 120.43it/s]\u001b[A\n",
            " 24% 1056/4436 [00:08<00:28, 120.16it/s]\u001b[A\n",
            " 24% 1072/4436 [00:08<00:28, 120.12it/s]\u001b[A\n",
            " 25% 1088/4436 [00:08<00:27, 119.93it/s]\u001b[A\n",
            " 25% 1104/4436 [00:09<00:27, 120.15it/s]\u001b[A\n",
            " 25% 1120/4436 [00:09<00:27, 119.89it/s]\u001b[A\n",
            " 26% 1136/4436 [00:09<00:27, 120.36it/s]\u001b[A\n",
            " 26% 1152/4436 [00:09<00:27, 119.11it/s]\u001b[A\n",
            " 26% 1168/4436 [00:09<00:27, 119.07it/s]\u001b[A\n",
            " 27% 1184/4436 [00:09<00:27, 120.16it/s]\u001b[A\n",
            " 27% 1200/4436 [00:09<00:26, 120.91it/s]\u001b[A\n",
            " 27% 1216/4436 [00:09<00:26, 119.71it/s]\u001b[A\n",
            " 28% 1232/4436 [00:10<00:26, 119.68it/s]\u001b[A\n",
            " 28% 1248/4436 [00:10<00:26, 120.42it/s]\u001b[A\n",
            " 28% 1264/4436 [00:10<00:26, 120.35it/s]\u001b[A\n",
            " 29% 1280/4436 [00:10<00:25, 121.43it/s]\u001b[A\n",
            " 29% 1296/4436 [00:10<00:25, 121.25it/s]\u001b[A\n",
            " 30% 1312/4436 [00:10<00:25, 121.00it/s]\u001b[A\n",
            " 30% 1328/4436 [00:10<00:25, 120.97it/s]\u001b[A\n",
            " 30% 1344/4436 [00:11<00:25, 120.69it/s]\u001b[A\n",
            " 31% 1360/4436 [00:11<00:25, 120.88it/s]\u001b[A\n",
            " 31% 1376/4436 [00:11<00:25, 120.85it/s]\u001b[A\n",
            " 31% 1392/4436 [00:11<00:25, 120.38it/s]\u001b[A\n",
            " 32% 1408/4436 [00:11<00:25, 120.13it/s]\u001b[A\n",
            " 32% 1424/4436 [00:11<00:25, 120.30it/s]\u001b[A\n",
            " 32% 1440/4436 [00:11<00:24, 119.93it/s]\u001b[A\n",
            " 33% 1456/4436 [00:11<00:24, 119.94it/s]\u001b[A\n",
            " 33% 1472/4436 [00:12<00:24, 120.16it/s]\u001b[A\n",
            " 34% 1488/4436 [00:12<00:24, 120.30it/s]\u001b[A\n",
            " 34% 1504/4436 [00:12<00:24, 120.11it/s]\u001b[A\n",
            " 34% 1520/4436 [00:12<00:24, 119.26it/s]\u001b[A\n",
            " 35% 1536/4436 [00:12<00:24, 119.84it/s]\u001b[A\n",
            " 35% 1552/4436 [00:12<00:24, 120.14it/s]\u001b[A\n",
            " 35% 1568/4436 [00:12<00:23, 121.04it/s]\u001b[A\n",
            "  0% 16/16020 [00:13<09:45, 27.32it/s, NLL=5.95, epoch=0]\n",
            " 36% 1600/4436 [00:13<00:23, 120.65it/s]\u001b[A\n",
            " 36% 1616/4436 [00:13<00:23, 120.56it/s]\u001b[A\n",
            " 37% 1632/4436 [00:13<00:23, 119.74it/s]\u001b[A\n",
            " 37% 1648/4436 [00:13<00:22, 121.42it/s]\u001b[A\n",
            " 38% 1664/4436 [00:13<00:22, 121.00it/s]\u001b[A\n",
            " 38% 1680/4436 [00:13<00:22, 120.49it/s]\u001b[A\n",
            " 38% 1696/4436 [00:13<00:22, 120.73it/s]\u001b[A\n",
            " 39% 1712/4436 [00:14<00:22, 120.83it/s]\u001b[A\n",
            " 39% 1728/4436 [00:14<00:22, 120.68it/s]\u001b[A\n",
            " 39% 1744/4436 [00:14<00:22, 120.46it/s]\u001b[A\n",
            " 40% 1760/4436 [00:14<00:22, 119.63it/s]\u001b[A\n",
            " 40% 1776/4436 [00:14<00:22, 120.41it/s]\u001b[A\n",
            " 40% 1792/4436 [00:14<00:22, 119.60it/s]\u001b[A\n",
            " 41% 1808/4436 [00:14<00:21, 119.66it/s]\u001b[A\n",
            " 41% 1824/4436 [00:15<00:21, 120.07it/s]\u001b[A\n",
            " 41% 1840/4436 [00:15<00:21, 119.28it/s]\u001b[A\n",
            " 42% 1856/4436 [00:15<00:21, 120.29it/s]\u001b[A\n",
            " 42% 1872/4436 [00:15<00:21, 119.90it/s]\u001b[A\n",
            " 43% 1888/4436 [00:15<00:21, 120.25it/s]\u001b[A\n",
            " 43% 1904/4436 [00:15<00:21, 120.49it/s]\u001b[A\n",
            " 43% 1920/4436 [00:15<00:20, 121.67it/s]\u001b[A\n",
            " 44% 1936/4436 [00:15<00:20, 121.27it/s]\u001b[A\n",
            " 44% 1952/4436 [00:16<00:20, 121.28it/s]\u001b[A\n",
            " 44% 1968/4436 [00:16<00:20, 121.39it/s]\u001b[A\n",
            " 45% 1984/4436 [00:16<00:20, 120.79it/s]\u001b[A\n",
            " 45% 2000/4436 [00:16<00:20, 121.09it/s]\u001b[A\n",
            " 45% 2016/4436 [00:16<00:20, 120.33it/s]\u001b[A\n",
            " 46% 2032/4436 [00:16<00:20, 119.41it/s]\u001b[A\n",
            " 46% 2048/4436 [00:16<00:19, 120.75it/s]\u001b[A\n",
            " 47% 2064/4436 [00:17<00:19, 120.53it/s]\u001b[A\n",
            " 47% 2080/4436 [00:17<00:19, 120.66it/s]\u001b[A\n",
            " 47% 2096/4436 [00:17<00:19, 121.15it/s]\u001b[A\n",
            " 48% 2112/4436 [00:17<00:19, 120.61it/s]\u001b[A\n",
            " 48% 2128/4436 [00:17<00:19, 120.80it/s]\u001b[A\n",
            " 48% 2144/4436 [00:17<00:18, 120.93it/s]\u001b[A\n",
            " 49% 2160/4436 [00:17<00:18, 120.74it/s]\u001b[A\n",
            " 49% 2176/4436 [00:17<00:18, 120.01it/s]\u001b[A\n",
            " 49% 2192/4436 [00:18<00:18, 120.03it/s]\u001b[A\n",
            " 50% 2208/4436 [00:18<00:18, 119.61it/s]\u001b[A\n",
            " 50% 2224/4436 [00:18<00:18, 119.66it/s]\u001b[A\n",
            " 50% 2240/4436 [00:18<00:18, 118.62it/s]\u001b[A\n",
            " 51% 2256/4436 [00:18<00:18, 120.71it/s]\u001b[A\n",
            " 51% 2272/4436 [00:18<00:17, 120.44it/s]\u001b[A\n",
            " 52% 2288/4436 [00:18<00:17, 120.57it/s]\u001b[A\n",
            " 52% 2304/4436 [00:19<00:17, 120.05it/s]\u001b[A\n",
            " 52% 2320/4436 [00:19<00:17, 120.06it/s]\u001b[A\n",
            " 53% 2336/4436 [00:19<00:17, 120.33it/s]\u001b[A\n",
            " 53% 2352/4436 [00:19<00:17, 120.52it/s]\u001b[A\n",
            " 53% 2368/4436 [00:19<00:17, 120.36it/s]\u001b[A\n",
            " 54% 2384/4436 [00:19<00:17, 120.57it/s]\u001b[A\n",
            " 54% 2400/4436 [00:19<00:16, 120.69it/s]\u001b[A\n",
            " 54% 2416/4436 [00:19<00:16, 120.60it/s]\u001b[A\n",
            " 55% 2432/4436 [00:20<00:16, 119.94it/s]\u001b[A\n",
            " 55% 2448/4436 [00:20<00:16, 119.54it/s]\u001b[A\n",
            " 56% 2464/4436 [00:20<00:16, 120.25it/s]\u001b[A\n",
            " 56% 2480/4436 [00:20<00:16, 119.82it/s]\u001b[A\n",
            " 56% 2496/4436 [00:20<00:16, 120.09it/s]\u001b[A\n",
            " 57% 2512/4436 [00:20<00:16, 118.84it/s]\u001b[A\n",
            " 57% 2528/4436 [00:20<00:16, 119.15it/s]\u001b[A\n",
            " 57% 2544/4436 [00:21<00:15, 119.00it/s]\u001b[A\n",
            " 58% 2560/4436 [00:21<00:15, 119.18it/s]\u001b[A\n",
            " 58% 2576/4436 [00:21<00:15, 119.56it/s]\u001b[A\n",
            " 58% 2592/4436 [00:21<00:15, 119.10it/s]\u001b[A\n",
            " 59% 2608/4436 [00:21<00:15, 118.92it/s]\u001b[A\n",
            " 59% 2624/4436 [00:21<00:15, 119.76it/s]\u001b[A\n",
            " 60% 2640/4436 [00:21<00:14, 119.96it/s]\u001b[A\n",
            " 60% 2656/4436 [00:21<00:14, 119.97it/s]\u001b[A\n",
            " 60% 2672/4436 [00:22<00:14, 119.99it/s]\u001b[A\n",
            " 61% 2688/4436 [00:22<00:14, 119.93it/s]\u001b[A\n",
            " 61% 2704/4436 [00:22<00:14, 120.21it/s]\u001b[A\n",
            " 61% 2720/4436 [00:22<00:14, 120.11it/s]\u001b[A\n",
            " 62% 2736/4436 [00:22<00:14, 119.12it/s]\u001b[A\n",
            " 62% 2752/4436 [00:22<00:13, 120.36it/s]\u001b[A\n",
            " 62% 2768/4436 [00:22<00:13, 120.27it/s]\u001b[A\n",
            " 63% 2784/4436 [00:23<00:13, 119.67it/s]\u001b[A\n",
            " 63% 2800/4436 [00:23<00:13, 120.31it/s]\u001b[A\n",
            " 63% 2816/4436 [00:23<00:13, 120.49it/s]\u001b[A\n",
            " 64% 2832/4436 [00:23<00:13, 120.60it/s]\u001b[A\n",
            " 64% 2848/4436 [00:23<00:13, 119.85it/s]\u001b[A\n",
            " 65% 2864/4436 [00:23<00:13, 120.40it/s]\u001b[A\n",
            " 65% 2880/4436 [00:23<00:12, 120.42it/s]\u001b[A\n",
            " 65% 2896/4436 [00:23<00:12, 120.19it/s]\u001b[A\n",
            " 66% 2912/4436 [00:24<00:12, 120.40it/s]\u001b[A\n",
            " 66% 2928/4436 [00:24<00:12, 119.84it/s]\u001b[A\n",
            " 66% 2944/4436 [00:24<00:12, 119.96it/s]\u001b[A\n",
            " 67% 2960/4436 [00:24<00:12, 120.10it/s]\u001b[A\n",
            " 67% 2976/4436 [00:24<00:12, 120.16it/s]\u001b[A\n",
            " 67% 2992/4436 [00:24<00:12, 118.76it/s]\u001b[A\n",
            " 68% 3008/4436 [00:24<00:11, 120.25it/s]\u001b[A\n",
            " 68% 3024/4436 [00:25<00:11, 120.60it/s]\u001b[A\n",
            " 69% 3040/4436 [00:25<00:11, 120.23it/s]\u001b[A\n",
            " 69% 3056/4436 [00:25<00:11, 120.07it/s]\u001b[A\n",
            " 69% 3072/4436 [00:25<00:11, 120.02it/s]\u001b[A\n",
            " 70% 3088/4436 [00:25<00:11, 120.30it/s]\u001b[A\n",
            " 70% 3104/4436 [00:25<00:11, 119.85it/s]\u001b[A\n",
            " 70% 3120/4436 [00:25<00:11, 119.16it/s]\u001b[A\n",
            " 71% 3136/4436 [00:25<00:10, 119.61it/s]\u001b[A\n",
            " 71% 3152/4436 [00:26<00:10, 119.91it/s]\u001b[A\n",
            " 71% 3168/4436 [00:26<00:10, 119.38it/s]\u001b[A\n",
            " 72% 3184/4436 [00:26<00:10, 119.80it/s]\u001b[A\n",
            " 72% 3200/4436 [00:26<00:10, 119.43it/s]\u001b[A\n",
            " 72% 3216/4436 [00:26<00:10, 119.65it/s]\u001b[A\n",
            " 73% 3232/4436 [00:26<00:10, 119.86it/s]\u001b[A\n",
            " 73% 3248/4436 [00:26<00:09, 119.02it/s]\u001b[A\n",
            " 74% 3264/4436 [00:27<00:09, 119.73it/s]\u001b[A\n",
            " 74% 3280/4436 [00:27<00:09, 119.60it/s]\u001b[A\n",
            " 74% 3296/4436 [00:27<00:09, 119.07it/s]\u001b[A\n",
            " 75% 3312/4436 [00:27<00:09, 119.13it/s]\u001b[A\n",
            " 75% 3328/4436 [00:27<00:09, 119.30it/s]\u001b[A\n",
            " 75% 3344/4436 [00:27<00:09, 119.31it/s]\u001b[A\n",
            " 76% 3360/4436 [00:27<00:09, 119.54it/s]\u001b[A\n",
            " 76% 3376/4436 [00:27<00:08, 119.76it/s]\u001b[A\n",
            " 76% 3392/4436 [00:28<00:08, 120.31it/s]\u001b[A\n",
            " 77% 3408/4436 [00:28<00:08, 119.87it/s]\u001b[A\n",
            " 77% 3424/4436 [00:28<00:08, 119.85it/s]\u001b[A\n",
            " 78% 3440/4436 [00:28<00:08, 119.96it/s]\u001b[A\n",
            " 78% 3456/4436 [00:28<00:08, 120.20it/s]\u001b[A\n",
            " 78% 3472/4436 [00:28<00:08, 118.54it/s]\u001b[A\n",
            " 79% 3488/4436 [00:28<00:08, 118.44it/s]\u001b[A\n",
            " 79% 3504/4436 [00:29<00:07, 119.46it/s]\u001b[A\n",
            " 79% 3520/4436 [00:29<00:07, 119.13it/s]\u001b[A\n",
            " 80% 3536/4436 [00:29<00:07, 118.91it/s]\u001b[A\n",
            " 80% 3552/4436 [00:29<00:07, 119.35it/s]\u001b[A\n",
            " 80% 3568/4436 [00:29<00:07, 119.53it/s]\u001b[A\n",
            " 81% 3584/4436 [00:29<00:07, 120.46it/s]\u001b[A\n",
            " 81% 3600/4436 [00:29<00:06, 120.30it/s]\u001b[A\n",
            " 82% 3616/4436 [00:29<00:06, 120.49it/s]\u001b[A\n",
            " 82% 3632/4436 [00:30<00:06, 119.42it/s]\u001b[A\n",
            " 82% 3648/4436 [00:30<00:06, 119.70it/s]\u001b[A\n",
            " 83% 3664/4436 [00:30<00:06, 119.90it/s]\u001b[A\n",
            " 83% 3680/4436 [00:30<00:06, 119.58it/s]\u001b[A\n",
            " 83% 3696/4436 [00:30<00:06, 119.70it/s]\u001b[A\n",
            " 84% 3712/4436 [00:30<00:06, 119.74it/s]\u001b[A\n",
            " 84% 3728/4436 [00:30<00:05, 119.45it/s]\u001b[A\n",
            " 84% 3744/4436 [00:31<00:05, 119.91it/s]\u001b[A\n",
            " 85% 3760/4436 [00:31<00:05, 119.23it/s]\u001b[A\n",
            " 85% 3776/4436 [00:31<00:05, 119.77it/s]\u001b[A\n",
            " 85% 3792/4436 [00:31<00:05, 119.32it/s]\u001b[A\n",
            " 86% 3808/4436 [00:31<00:05, 119.29it/s]\u001b[A\n",
            " 86% 3824/4436 [00:31<00:05, 119.25it/s]\u001b[A\n",
            " 87% 3840/4436 [00:31<00:04, 119.92it/s]\u001b[A\n",
            " 87% 3856/4436 [00:31<00:04, 119.78it/s]\u001b[A\n",
            " 87% 3872/4436 [00:32<00:04, 119.34it/s]\u001b[A\n",
            " 88% 3888/4436 [00:32<00:04, 119.56it/s]\u001b[A\n",
            " 88% 3904/4436 [00:32<00:04, 119.65it/s]\u001b[A\n",
            " 88% 3920/4436 [00:32<00:04, 119.74it/s]\u001b[A\n",
            " 89% 3936/4436 [00:32<00:04, 119.84it/s]\u001b[A\n",
            " 89% 3952/4436 [00:32<00:04, 119.68it/s]\u001b[A\n",
            " 89% 3968/4436 [00:32<00:03, 119.60it/s]\u001b[A\n",
            " 90% 3984/4436 [00:33<00:03, 119.95it/s]\u001b[A\n",
            " 90% 4000/4436 [00:33<00:03, 120.24it/s]\u001b[A\n",
            " 91% 4016/4436 [00:33<00:03, 119.74it/s]\u001b[A\n",
            " 91% 4032/4436 [00:33<00:03, 119.77it/s]\u001b[A\n",
            " 91% 4048/4436 [00:33<00:03, 120.17it/s]\u001b[A\n",
            " 92% 4064/4436 [00:33<00:03, 119.42it/s]\u001b[A\n",
            " 92% 4080/4436 [00:33<00:02, 119.49it/s]\u001b[A\n",
            " 92% 4096/4436 [00:33<00:02, 119.66it/s]\u001b[A\n",
            " 93% 4112/4436 [00:34<00:02, 119.77it/s]\u001b[A\n",
            " 93% 4128/4436 [00:34<00:02, 119.79it/s]\u001b[A\n",
            " 93% 4144/4436 [00:34<00:02, 119.95it/s]\u001b[A\n",
            " 94% 4160/4436 [00:34<00:02, 119.51it/s]\u001b[A\n",
            " 94% 4176/4436 [00:34<00:02, 119.25it/s]\u001b[A\n",
            " 94% 4192/4436 [00:34<00:02, 118.85it/s]\u001b[A\n",
            " 95% 4208/4436 [00:34<00:01, 118.36it/s]\u001b[A\n",
            " 95% 4224/4436 [00:35<00:01, 118.26it/s]\u001b[A\n",
            " 96% 4240/4436 [00:35<00:01, 117.90it/s]\u001b[A\n",
            " 96% 4256/4436 [00:35<00:01, 117.91it/s]\u001b[A\n",
            " 96% 4272/4436 [00:35<00:01, 118.09it/s]\u001b[A\n",
            " 97% 4288/4436 [00:35<00:01, 117.68it/s]\u001b[A\n",
            " 97% 4304/4436 [00:35<00:01, 117.24it/s]\u001b[A\n",
            " 97% 4320/4436 [00:35<00:00, 118.93it/s]\u001b[A\n",
            " 98% 4336/4436 [00:36<00:00, 119.11it/s]\u001b[A\n",
            " 98% 4352/4436 [00:36<00:00, 118.70it/s]\u001b[A\n",
            " 98% 4368/4436 [00:36<00:00, 118.47it/s]\u001b[A\n",
            " 99% 4384/4436 [00:36<00:00, 118.06it/s]\u001b[A\n",
            " 99% 4400/4436 [00:36<00:00, 117.82it/s]\u001b[A\n",
            "100% 4416/4436 [00:36<00:00, 118.52it/s]\u001b[A\n",
            "100% 4436/4436 [00:36<00:00, 120.00it/s]\n",
            "\n",
            "  0% 0/4306 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 301/4306 [00:00<00:01, 3003.96it/s]\u001b[A\n",
            " 14% 602/4306 [00:00<00:01, 2960.54it/s]\u001b[A\n",
            " 21% 916/4306 [00:00<00:01, 3039.36it/s]\u001b[A\n",
            " 28% 1221/4306 [00:00<00:01, 3008.22it/s]\u001b[A\n",
            " 35% 1522/4306 [00:00<00:00, 2985.79it/s]\u001b[A\n",
            " 43% 1840/4306 [00:00<00:00, 3049.81it/s]\u001b[A\n",
            " 50% 2156/4306 [00:00<00:00, 3084.77it/s]\u001b[A\n",
            " 57% 2474/4306 [00:00<00:00, 3113.46it/s]\u001b[A\n",
            " 65% 2790/4306 [00:00<00:00, 3127.38it/s]\u001b[A\n",
            " 72% 3103/4306 [00:01<00:00, 3113.90it/s]\u001b[A\n",
            " 79% 3415/4306 [00:01<00:00, 3101.00it/s]\u001b[A\n",
            " 87% 3726/4306 [00:01<00:00, 3094.97it/s]\u001b[A\n",
            "100% 4306/4306 [00:01<00:00, 3035.07it/s]\n",
            "[10.20.21 08:19:31] Visualizing in TensorBoard...\n",
            "[10.20.21 08:19:31] Eval F1: 07.42, EM: 00.00\n",
            "100% 16020/16020 [09:28<00:00, 28.18it/s, NLL=0.628, epoch=0]\n",
            "[10.20.21 08:28:21] Epoch: 1\n",
            "100% 16020/16020 [08:45<00:00, 30.48it/s, NLL=0.731, epoch=1]\n",
            "[10.20.21 08:37:08] Epoch: 2\n",
            "100% 16020/16020 [08:45<00:00, 30.50it/s, NLL=0.0879, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKTYcVSOoelE",
        "outputId": "cb55c56e-28e8-4c4b-a355-98d40321ade6"
      },
      "source": [
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name pure_hyper_column \\\n",
        "    --train-datasets squad02 \\\n",
        "    --eval-datasets race02 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES \\\n",
        "    --save-dir check/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/robustqa\n",
            "[10.20.21 08:46:03] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race02\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"pure_hyper_column\",\n",
            "    \"save_dir\": \"check/pure_hyper_column-02\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad02\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.20.21 08:46:03] Preparing Training Data...\n",
            "100% 17496/17496 [00:00<00:00, 20814.45it/s]\n",
            "Preprocessing not completely accurate for 208/17496 instances\n",
            "[10.20.21 08:46:08] Preparing Validation Data...\n",
            "100% 2977/2977 [00:00<00:00, 21856.04it/s]\n",
            "[10.20.21 08:46:13] Epoch: 0\n",
            "  0% 0/17496 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.20.21 08:46:14] Evaluating at step 0...\n",
            "  0% 16/17496 [00:00<10:03, 28.98it/s, NLL=0.682, epoch=0]\n",
            "  0% 0/2977 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/2977 [00:00<00:14, 209.10it/s]\u001b[A\n",
            "  2% 64/2977 [00:00<00:20, 145.52it/s]\u001b[A\n",
            "  3% 80/2977 [00:00<00:21, 136.21it/s]\u001b[A\n",
            "  3% 96/2977 [00:00<00:21, 131.57it/s]\u001b[A\n",
            "  4% 112/2977 [00:00<00:22, 126.91it/s]\u001b[A\n",
            "  4% 128/2977 [00:00<00:22, 124.61it/s]\u001b[A\n",
            "  5% 144/2977 [00:01<00:22, 123.41it/s]\u001b[A\n",
            "  5% 160/2977 [00:01<00:23, 122.32it/s]\u001b[A\n",
            "  6% 176/2977 [00:01<00:23, 121.72it/s]\u001b[A\n",
            "  6% 192/2977 [00:01<00:23, 120.48it/s]\u001b[A\n",
            "  7% 208/2977 [00:01<00:22, 120.45it/s]\u001b[A\n",
            "  8% 224/2977 [00:01<00:22, 120.09it/s]\u001b[A\n",
            "  8% 240/2977 [00:01<00:22, 119.45it/s]\u001b[A\n",
            "  9% 256/2977 [00:02<00:22, 119.36it/s]\u001b[A\n",
            "  9% 272/2977 [00:02<00:22, 119.51it/s]\u001b[A\n",
            " 10% 288/2977 [00:02<00:22, 119.40it/s]\u001b[A\n",
            " 10% 304/2977 [00:02<00:22, 119.65it/s]\u001b[A\n",
            " 11% 320/2977 [00:02<00:22, 119.13it/s]\u001b[A\n",
            " 11% 336/2977 [00:02<00:22, 118.93it/s]\u001b[A\n",
            " 12% 352/2977 [00:02<00:21, 119.66it/s]\u001b[A\n",
            " 12% 368/2977 [00:02<00:21, 119.45it/s]\u001b[A\n",
            " 13% 384/2977 [00:03<00:21, 119.00it/s]\u001b[A\n",
            " 13% 400/2977 [00:03<00:21, 119.00it/s]\u001b[A\n",
            " 14% 416/2977 [00:03<00:21, 118.97it/s]\u001b[A\n",
            " 15% 432/2977 [00:03<00:21, 119.44it/s]\u001b[A\n",
            " 15% 448/2977 [00:03<00:21, 119.55it/s]\u001b[A\n",
            " 16% 464/2977 [00:03<00:20, 119.72it/s]\u001b[A\n",
            " 16% 480/2977 [00:03<00:20, 119.21it/s]\u001b[A\n",
            " 17% 496/2977 [00:04<00:20, 118.85it/s]\u001b[A\n",
            " 17% 512/2977 [00:04<00:20, 119.32it/s]\u001b[A\n",
            " 18% 528/2977 [00:04<00:20, 119.57it/s]\u001b[A\n",
            " 18% 544/2977 [00:04<00:20, 119.67it/s]\u001b[A\n",
            " 19% 560/2977 [00:04<00:20, 119.45it/s]\u001b[A\n",
            " 19% 576/2977 [00:04<00:20, 119.01it/s]\u001b[A\n",
            " 20% 592/2977 [00:04<00:20, 117.98it/s]\u001b[A\n",
            " 20% 608/2977 [00:04<00:19, 119.55it/s]\u001b[A\n",
            " 21% 624/2977 [00:05<00:19, 119.63it/s]\u001b[A\n",
            " 21% 640/2977 [00:05<00:19, 119.79it/s]\u001b[A\n",
            " 22% 656/2977 [00:05<00:19, 119.59it/s]\u001b[A\n",
            " 23% 672/2977 [00:05<00:19, 119.34it/s]\u001b[A\n",
            " 23% 688/2977 [00:05<00:19, 119.20it/s]\u001b[A\n",
            " 24% 704/2977 [00:05<00:19, 117.72it/s]\u001b[A\n",
            " 24% 720/2977 [00:05<00:18, 119.49it/s]\u001b[A\n",
            " 25% 736/2977 [00:06<00:18, 119.43it/s]\u001b[A\n",
            " 25% 752/2977 [00:06<00:18, 119.69it/s]\u001b[A\n",
            " 26% 768/2977 [00:06<00:18, 119.40it/s]\u001b[A\n",
            " 26% 784/2977 [00:06<00:18, 119.60it/s]\u001b[A\n",
            " 27% 800/2977 [00:06<00:18, 119.89it/s]\u001b[A\n",
            " 27% 816/2977 [00:06<00:18, 117.96it/s]\u001b[A\n",
            " 28% 832/2977 [00:06<00:18, 118.33it/s]\u001b[A\n",
            " 28% 848/2977 [00:06<00:17, 120.15it/s]\u001b[A\n",
            " 29% 864/2977 [00:07<00:17, 120.03it/s]\u001b[A\n",
            " 30% 880/2977 [00:07<00:17, 119.61it/s]\u001b[A\n",
            " 30% 896/2977 [00:07<00:17, 119.69it/s]\u001b[A\n",
            " 31% 912/2977 [00:07<00:17, 119.76it/s]\u001b[A\n",
            " 31% 928/2977 [00:07<00:17, 118.08it/s]\u001b[A\n",
            " 32% 944/2977 [00:07<00:17, 118.85it/s]\u001b[A\n",
            " 32% 960/2977 [00:07<00:16, 119.66it/s]\u001b[A\n",
            " 33% 976/2977 [00:08<00:16, 119.86it/s]\u001b[A\n",
            " 33% 992/2977 [00:08<00:16, 119.40it/s]\u001b[A\n",
            " 34% 1008/2977 [00:08<00:16, 119.64it/s]\u001b[A\n",
            " 34% 1024/2977 [00:08<00:16, 119.61it/s]\u001b[A\n",
            " 35% 1040/2977 [00:08<00:16, 119.81it/s]\u001b[A\n",
            " 35% 1056/2977 [00:08<00:16, 119.56it/s]\u001b[A\n",
            " 36% 1072/2977 [00:08<00:15, 119.71it/s]\u001b[A\n",
            " 37% 1088/2977 [00:09<00:15, 118.75it/s]\u001b[A\n",
            " 37% 1104/2977 [00:09<00:15, 118.96it/s]\u001b[A\n",
            " 38% 1120/2977 [00:09<00:15, 119.31it/s]\u001b[A\n",
            " 38% 1136/2977 [00:09<00:15, 119.45it/s]\u001b[A\n",
            " 39% 1152/2977 [00:09<00:15, 119.26it/s]\u001b[A\n",
            " 39% 1168/2977 [00:09<00:15, 119.31it/s]\u001b[A\n",
            " 40% 1184/2977 [00:09<00:15, 119.07it/s]\u001b[A\n",
            " 40% 1200/2977 [00:09<00:14, 119.02it/s]\u001b[A\n",
            " 41% 1216/2977 [00:10<00:14, 118.62it/s]\u001b[A\n",
            " 41% 1232/2977 [00:10<00:14, 118.81it/s]\u001b[A\n",
            " 42% 1248/2977 [00:10<00:14, 118.73it/s]\u001b[A\n",
            " 42% 1264/2977 [00:10<00:14, 118.73it/s]\u001b[A\n",
            " 43% 1280/2977 [00:10<00:14, 118.64it/s]\u001b[A\n",
            " 44% 1296/2977 [00:10<00:14, 118.19it/s]\u001b[A\n",
            " 44% 1312/2977 [00:10<00:13, 119.01it/s]\u001b[A\n",
            " 45% 1328/2977 [00:11<00:13, 118.48it/s]\u001b[A\n",
            " 45% 1344/2977 [00:11<00:13, 118.83it/s]\u001b[A\n",
            " 46% 1360/2977 [00:11<00:13, 119.13it/s]\u001b[A\n",
            " 46% 1376/2977 [00:11<00:13, 119.23it/s]\u001b[A\n",
            " 47% 1392/2977 [00:11<00:13, 118.79it/s]\u001b[A\n",
            " 47% 1408/2977 [00:11<00:13, 118.48it/s]\u001b[A\n",
            " 48% 1424/2977 [00:11<00:13, 118.91it/s]\u001b[A\n",
            " 48% 1440/2977 [00:11<00:12, 118.93it/s]\u001b[A\n",
            " 49% 1456/2977 [00:12<00:12, 118.32it/s]\u001b[A\n",
            " 49% 1472/2977 [00:12<00:12, 118.15it/s]\u001b[A\n",
            " 50% 1488/2977 [00:12<00:12, 118.81it/s]\u001b[A\n",
            " 51% 1504/2977 [00:12<00:12, 118.77it/s]\u001b[A\n",
            " 51% 1520/2977 [00:12<00:12, 118.70it/s]\u001b[A\n",
            " 52% 1536/2977 [00:12<00:12, 118.57it/s]\u001b[A\n",
            " 52% 1552/2977 [00:12<00:11, 119.62it/s]\u001b[A\n",
            " 53% 1568/2977 [00:13<00:11, 119.87it/s]\u001b[A\n",
            " 53% 1584/2977 [00:13<00:11, 119.39it/s]\u001b[A\n",
            "  0% 16/17496 [00:13<10:03, 28.98it/s, NLL=0.682, epoch=0]\n",
            " 54% 1616/2977 [00:13<00:11, 119.59it/s]\u001b[A\n",
            " 55% 1632/2977 [00:13<00:11, 119.33it/s]\u001b[A\n",
            " 55% 1648/2977 [00:13<00:11, 119.22it/s]\u001b[A\n",
            " 56% 1664/2977 [00:13<00:10, 119.61it/s]\u001b[A\n",
            " 56% 1680/2977 [00:13<00:10, 119.31it/s]\u001b[A\n",
            " 57% 1696/2977 [00:14<00:10, 119.28it/s]\u001b[A\n",
            " 58% 1712/2977 [00:14<00:10, 119.11it/s]\u001b[A\n",
            " 58% 1728/2977 [00:14<00:10, 119.29it/s]\u001b[A\n",
            " 59% 1744/2977 [00:14<00:10, 119.62it/s]\u001b[A\n",
            " 59% 1760/2977 [00:14<00:10, 119.08it/s]\u001b[A\n",
            " 60% 1776/2977 [00:14<00:10, 118.89it/s]\u001b[A\n",
            " 60% 1792/2977 [00:14<00:09, 118.99it/s]\u001b[A\n",
            " 61% 1808/2977 [00:15<00:09, 118.97it/s]\u001b[A\n",
            " 61% 1824/2977 [00:15<00:09, 118.42it/s]\u001b[A\n",
            " 62% 1840/2977 [00:15<00:09, 119.01it/s]\u001b[A\n",
            " 62% 1856/2977 [00:15<00:09, 117.12it/s]\u001b[A\n",
            " 63% 1872/2977 [00:15<00:09, 118.57it/s]\u001b[A\n",
            " 63% 1888/2977 [00:15<00:09, 119.49it/s]\u001b[A\n",
            " 64% 1904/2977 [00:15<00:08, 119.43it/s]\u001b[A\n",
            " 64% 1920/2977 [00:15<00:08, 119.43it/s]\u001b[A\n",
            " 65% 1936/2977 [00:16<00:08, 119.65it/s]\u001b[A\n",
            " 66% 1952/2977 [00:16<00:08, 119.05it/s]\u001b[A\n",
            " 66% 1968/2977 [00:16<00:08, 119.26it/s]\u001b[A\n",
            " 67% 1984/2977 [00:16<00:08, 119.00it/s]\u001b[A\n",
            " 67% 2000/2977 [00:16<00:08, 118.98it/s]\u001b[A\n",
            " 68% 2016/2977 [00:16<00:08, 118.97it/s]\u001b[A\n",
            " 68% 2032/2977 [00:16<00:07, 118.91it/s]\u001b[A\n",
            " 69% 2048/2977 [00:17<00:07, 119.11it/s]\u001b[A\n",
            " 69% 2064/2977 [00:17<00:07, 119.24it/s]\u001b[A\n",
            " 70% 2080/2977 [00:17<00:07, 119.04it/s]\u001b[A\n",
            " 70% 2096/2977 [00:17<00:07, 118.52it/s]\u001b[A\n",
            " 71% 2112/2977 [00:17<00:07, 118.57it/s]\u001b[A\n",
            " 71% 2128/2977 [00:17<00:07, 118.95it/s]\u001b[A\n",
            " 72% 2144/2977 [00:17<00:07, 118.91it/s]\u001b[A\n",
            " 73% 2160/2977 [00:18<00:06, 118.13it/s]\u001b[A\n",
            " 73% 2176/2977 [00:18<00:06, 118.16it/s]\u001b[A\n",
            " 74% 2192/2977 [00:18<00:06, 118.78it/s]\u001b[A\n",
            " 74% 2208/2977 [00:18<00:06, 119.13it/s]\u001b[A\n",
            " 75% 2224/2977 [00:18<00:06, 119.30it/s]\u001b[A\n",
            " 75% 2240/2977 [00:18<00:06, 119.57it/s]\u001b[A\n",
            " 76% 2256/2977 [00:18<00:06, 119.93it/s]\u001b[A\n",
            " 76% 2272/2977 [00:18<00:05, 119.74it/s]\u001b[A\n",
            " 77% 2288/2977 [00:19<00:05, 119.50it/s]\u001b[A\n",
            " 77% 2304/2977 [00:19<00:05, 119.63it/s]\u001b[A\n",
            " 78% 2320/2977 [00:19<00:05, 119.40it/s]\u001b[A\n",
            " 78% 2336/2977 [00:19<00:05, 119.15it/s]\u001b[A\n",
            " 79% 2352/2977 [00:19<00:05, 119.09it/s]\u001b[A\n",
            " 80% 2368/2977 [00:19<00:05, 118.14it/s]\u001b[A\n",
            " 80% 2384/2977 [00:19<00:04, 119.61it/s]\u001b[A\n",
            " 81% 2400/2977 [00:20<00:04, 119.49it/s]\u001b[A\n",
            " 81% 2416/2977 [00:20<00:04, 119.49it/s]\u001b[A\n",
            " 82% 2432/2977 [00:20<00:04, 119.15it/s]\u001b[A\n",
            " 82% 2448/2977 [00:20<00:04, 119.02it/s]\u001b[A\n",
            " 83% 2464/2977 [00:20<00:04, 118.95it/s]\u001b[A\n",
            " 83% 2480/2977 [00:20<00:04, 118.69it/s]\u001b[A\n",
            " 84% 2496/2977 [00:20<00:04, 117.27it/s]\u001b[A\n",
            " 84% 2512/2977 [00:20<00:03, 117.87it/s]\u001b[A\n",
            " 85% 2528/2977 [00:21<00:03, 118.89it/s]\u001b[A\n",
            " 85% 2544/2977 [00:21<00:03, 118.68it/s]\u001b[A\n",
            " 86% 2560/2977 [00:21<00:03, 119.12it/s]\u001b[A\n",
            " 87% 2576/2977 [00:21<00:03, 118.87it/s]\u001b[A\n",
            " 87% 2592/2977 [00:21<00:03, 119.07it/s]\u001b[A\n",
            " 88% 2608/2977 [00:21<00:03, 118.43it/s]\u001b[A\n",
            " 88% 2624/2977 [00:21<00:02, 119.09it/s]\u001b[A\n",
            " 89% 2640/2977 [00:22<00:02, 119.43it/s]\u001b[A\n",
            " 89% 2656/2977 [00:22<00:02, 119.55it/s]\u001b[A\n",
            " 90% 2672/2977 [00:22<00:02, 119.06it/s]\u001b[A\n",
            " 90% 2688/2977 [00:22<00:02, 119.30it/s]\u001b[A\n",
            " 91% 2704/2977 [00:22<00:02, 119.72it/s]\u001b[A\n",
            " 91% 2720/2977 [00:22<00:02, 119.31it/s]\u001b[A\n",
            " 92% 2736/2977 [00:22<00:02, 118.37it/s]\u001b[A\n",
            " 92% 2752/2977 [00:22<00:01, 118.48it/s]\u001b[A\n",
            " 93% 2768/2977 [00:23<00:01, 118.75it/s]\u001b[A\n",
            " 94% 2784/2977 [00:23<00:01, 118.44it/s]\u001b[A\n",
            " 94% 2800/2977 [00:23<00:01, 118.97it/s]\u001b[A\n",
            " 95% 2816/2977 [00:23<00:01, 119.20it/s]\u001b[A\n",
            " 95% 2832/2977 [00:23<00:01, 119.28it/s]\u001b[A\n",
            " 96% 2848/2977 [00:23<00:01, 118.79it/s]\u001b[A\n",
            " 96% 2864/2977 [00:23<00:00, 120.30it/s]\u001b[A\n",
            " 97% 2880/2977 [00:24<00:00, 120.14it/s]\u001b[A\n",
            " 97% 2896/2977 [00:24<00:00, 119.59it/s]\u001b[A\n",
            " 98% 2912/2977 [00:24<00:00, 119.80it/s]\u001b[A\n",
            " 98% 2928/2977 [00:24<00:00, 119.66it/s]\u001b[A\n",
            " 99% 2944/2977 [00:24<00:00, 119.45it/s]\u001b[A\n",
            " 99% 2960/2977 [00:24<00:00, 118.89it/s]\u001b[A\n",
            "100% 2977/2977 [00:24<00:00, 119.09it/s]\n",
            "\n",
            "  0% 0/2929 [00:00<?, ?it/s]\u001b[A\n",
            "  9% 261/2929 [00:00<00:01, 2604.55it/s]\u001b[A\n",
            " 18% 522/2929 [00:00<00:00, 2538.76it/s]\u001b[A\n",
            " 27% 787/2929 [00:00<00:00, 2587.18it/s]\u001b[A\n",
            " 36% 1046/2929 [00:00<00:00, 2546.98it/s]\u001b[A\n",
            " 45% 1306/2929 [00:00<00:00, 2564.06it/s]\u001b[A\n",
            " 54% 1571/2929 [00:00<00:00, 2590.39it/s]\u001b[A\n",
            " 63% 1844/2929 [00:00<00:00, 2634.67it/s]\u001b[A\n",
            " 72% 2108/2929 [00:00<00:00, 2606.02it/s]\u001b[A\n",
            " 81% 2369/2929 [00:00<00:00, 2546.68it/s]\u001b[A\n",
            " 90% 2624/2929 [00:01<00:00, 2528.54it/s]\u001b[A\n",
            "100% 2929/2929 [00:01<00:00, 2567.48it/s]\n",
            "[10.20.21 08:46:40] Visualizing in TensorBoard...\n",
            "[10.20.21 08:46:40] Eval F1: 83.28, EM: 70.37\n",
            "100% 17496/17496 [10:04<00:00, 28.93it/s, NLL=1.78, epoch=0]\n",
            "[10.20.21 08:56:19] Epoch: 1\n",
            "100% 17496/17496 [09:34<00:00, 30.46it/s, NLL=0.855, epoch=1]\n",
            "[10.20.21 09:05:55] Epoch: 2\n",
            "100% 17496/17496 [09:34<00:00, 30.46it/s, NLL=0.264, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSmog4Zwofpu",
        "outputId": "96cf696c-2761-4d4c-97a3-cd59619c0f71"
      },
      "source": [
        "%cd /content/robustqa/\n",
        "!python3 train.py \\\n",
        "    --do-train \\\n",
        "    --recompute-features \\\n",
        "    --run-name pure_hyper_column \\\n",
        "    --train-datasets squad03 \\\n",
        "    --eval-datasets race03 \\\n",
        "    --eval-every 500 \\\n",
        "    --batch-size 16 \\\n",
        "    --num-epochs 3 \\\n",
        "    --saved YES \\\n",
        "    --save-dir check/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/robustqa\n",
            "[10.20.21 09:15:39] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race03\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": true,\n",
            "    \"run_name\": \"pure_hyper_column\",\n",
            "    \"save_dir\": \"check/pure_hyper_column-03\",\n",
            "    \"saved\": \"YES\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad03\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[10.20.21 09:15:39] Preparing Training Data...\n",
            "100% 17047/17047 [00:00<00:00, 21064.46it/s]\n",
            "Preprocessing not completely accurate for 198/17047 instances\n",
            "[10.20.21 09:15:44] Preparing Validation Data...\n",
            "100% 3377/3377 [00:00<00:00, 21487.58it/s]\n",
            "[10.20.21 09:15:49] Epoch: 0\n",
            "  0% 0/17047 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "[10.20.21 09:15:49] Evaluating at step 0...\n",
            "  0% 16/17047 [00:00<09:49, 28.87it/s, NLL=1.43, epoch=0]\n",
            "  0% 0/3377 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 32/3377 [00:00<00:15, 212.36it/s]\u001b[A\n",
            "  2% 64/3377 [00:00<00:22, 145.42it/s]\u001b[A\n",
            "  2% 80/3377 [00:00<00:24, 136.91it/s]\u001b[A\n",
            "  3% 96/3377 [00:00<00:24, 131.41it/s]\u001b[A\n",
            "  3% 112/3377 [00:00<00:25, 127.16it/s]\u001b[A\n",
            "  4% 128/3377 [00:00<00:26, 124.43it/s]\u001b[A\n",
            "  4% 144/3377 [00:01<00:26, 122.66it/s]\u001b[A\n",
            "  5% 160/3377 [00:01<00:26, 121.37it/s]\u001b[A\n",
            "  5% 176/3377 [00:01<00:26, 120.64it/s]\u001b[A\n",
            "  6% 192/3377 [00:01<00:26, 120.37it/s]\u001b[A\n",
            "  6% 208/3377 [00:01<00:26, 120.63it/s]\u001b[A\n",
            "  7% 224/3377 [00:01<00:26, 120.27it/s]\u001b[A\n",
            "  7% 240/3377 [00:01<00:26, 119.53it/s]\u001b[A\n",
            "  8% 256/3377 [00:02<00:25, 120.81it/s]\u001b[A\n",
            "  8% 272/3377 [00:02<00:25, 120.33it/s]\u001b[A\n",
            "  9% 288/3377 [00:02<00:25, 120.13it/s]\u001b[A\n",
            "  9% 304/3377 [00:02<00:25, 120.08it/s]\u001b[A\n",
            "  9% 320/3377 [00:02<00:25, 120.30it/s]\u001b[A\n",
            " 10% 336/3377 [00:02<00:25, 120.01it/s]\u001b[A\n",
            " 10% 352/3377 [00:02<00:25, 118.90it/s]\u001b[A\n",
            " 11% 368/3377 [00:02<00:25, 118.94it/s]\u001b[A\n",
            " 11% 384/3377 [00:03<00:24, 120.22it/s]\u001b[A\n",
            " 12% 400/3377 [00:03<00:24, 120.04it/s]\u001b[A\n",
            " 12% 416/3377 [00:03<00:24, 120.27it/s]\u001b[A\n",
            " 13% 432/3377 [00:03<00:24, 119.85it/s]\u001b[A\n",
            " 13% 448/3377 [00:03<00:24, 119.65it/s]\u001b[A\n",
            " 14% 464/3377 [00:03<00:24, 119.71it/s]\u001b[A\n",
            " 14% 480/3377 [00:03<00:24, 119.76it/s]\u001b[A\n",
            " 15% 496/3377 [00:04<00:24, 119.57it/s]\u001b[A\n",
            " 15% 512/3377 [00:04<00:23, 119.76it/s]\u001b[A\n",
            " 16% 528/3377 [00:04<00:23, 119.74it/s]\u001b[A\n",
            " 16% 544/3377 [00:04<00:23, 119.54it/s]\u001b[A\n",
            " 17% 560/3377 [00:04<00:23, 119.13it/s]\u001b[A\n",
            " 17% 576/3377 [00:04<00:23, 119.25it/s]\u001b[A\n",
            " 18% 592/3377 [00:04<00:23, 119.23it/s]\u001b[A\n",
            " 18% 608/3377 [00:04<00:23, 119.35it/s]\u001b[A\n",
            " 18% 624/3377 [00:05<00:23, 118.92it/s]\u001b[A\n",
            " 19% 640/3377 [00:05<00:23, 118.46it/s]\u001b[A\n",
            " 19% 656/3377 [00:05<00:22, 119.05it/s]\u001b[A\n",
            " 20% 672/3377 [00:05<00:22, 119.34it/s]\u001b[A\n",
            " 20% 688/3377 [00:05<00:22, 119.07it/s]\u001b[A\n",
            " 21% 704/3377 [00:05<00:22, 119.50it/s]\u001b[A\n",
            " 21% 720/3377 [00:05<00:22, 119.39it/s]\u001b[A\n",
            " 22% 736/3377 [00:06<00:22, 119.38it/s]\u001b[A\n",
            " 22% 752/3377 [00:06<00:21, 119.49it/s]\u001b[A\n",
            " 23% 768/3377 [00:06<00:21, 119.70it/s]\u001b[A\n",
            " 23% 784/3377 [00:06<00:21, 119.31it/s]\u001b[A\n",
            " 24% 800/3377 [00:06<00:21, 119.41it/s]\u001b[A\n",
            " 24% 816/3377 [00:06<00:21, 119.16it/s]\u001b[A\n",
            " 25% 832/3377 [00:06<00:21, 118.04it/s]\u001b[A\n",
            " 25% 848/3377 [00:06<00:21, 119.07it/s]\u001b[A\n",
            " 26% 864/3377 [00:07<00:21, 118.95it/s]\u001b[A\n",
            " 26% 880/3377 [00:07<00:20, 119.36it/s]\u001b[A\n",
            " 27% 896/3377 [00:07<00:20, 119.11it/s]\u001b[A\n",
            " 27% 912/3377 [00:07<00:20, 119.31it/s]\u001b[A\n",
            " 27% 928/3377 [00:07<00:20, 119.45it/s]\u001b[A\n",
            " 28% 944/3377 [00:07<00:20, 119.35it/s]\u001b[A\n",
            " 28% 960/3377 [00:07<00:20, 119.04it/s]\u001b[A\n",
            " 29% 976/3377 [00:08<00:20, 119.10it/s]\u001b[A\n",
            " 29% 992/3377 [00:08<00:19, 119.55it/s]\u001b[A\n",
            " 30% 1008/3377 [00:08<00:19, 118.74it/s]\u001b[A\n",
            " 30% 1024/3377 [00:08<00:19, 118.78it/s]\u001b[A\n",
            " 31% 1040/3377 [00:08<00:19, 118.29it/s]\u001b[A\n",
            " 31% 1056/3377 [00:08<00:19, 119.14it/s]\u001b[A\n",
            " 32% 1072/3377 [00:08<00:19, 118.76it/s]\u001b[A\n",
            " 32% 1088/3377 [00:09<00:19, 118.63it/s]\u001b[A\n",
            " 33% 1104/3377 [00:09<00:19, 118.38it/s]\u001b[A\n",
            " 33% 1120/3377 [00:09<00:19, 118.45it/s]\u001b[A\n",
            " 34% 1136/3377 [00:09<00:18, 118.97it/s]\u001b[A\n",
            " 34% 1152/3377 [00:09<00:18, 118.97it/s]\u001b[A\n",
            " 35% 1168/3377 [00:09<00:18, 119.33it/s]\u001b[A\n",
            " 35% 1184/3377 [00:09<00:18, 117.82it/s]\u001b[A\n",
            " 36% 1200/3377 [00:09<00:18, 118.36it/s]\u001b[A\n",
            " 36% 1216/3377 [00:10<00:18, 119.96it/s]\u001b[A\n",
            " 36% 1232/3377 [00:10<00:17, 119.77it/s]\u001b[A\n",
            " 37% 1248/3377 [00:10<00:17, 119.57it/s]\u001b[A\n",
            " 37% 1264/3377 [00:10<00:17, 119.55it/s]\u001b[A\n",
            " 38% 1280/3377 [00:10<00:17, 119.57it/s]\u001b[A\n",
            " 38% 1296/3377 [00:10<00:17, 118.54it/s]\u001b[A\n",
            " 39% 1312/3377 [00:10<00:17, 120.15it/s]\u001b[A\n",
            " 39% 1328/3377 [00:11<00:17, 119.80it/s]\u001b[A\n",
            " 40% 1344/3377 [00:11<00:17, 119.31it/s]\u001b[A\n",
            " 40% 1360/3377 [00:11<00:16, 119.55it/s]\u001b[A\n",
            " 41% 1376/3377 [00:11<00:16, 119.54it/s]\u001b[A\n",
            " 41% 1392/3377 [00:11<00:16, 119.45it/s]\u001b[A\n",
            " 42% 1408/3377 [00:11<00:16, 119.32it/s]\u001b[A\n",
            " 42% 1424/3377 [00:11<00:16, 119.22it/s]\u001b[A\n",
            " 43% 1440/3377 [00:11<00:16, 118.34it/s]\u001b[A\n",
            " 43% 1456/3377 [00:12<00:16, 119.48it/s]\u001b[A\n",
            " 44% 1472/3377 [00:12<00:15, 119.78it/s]\u001b[A\n",
            " 44% 1488/3377 [00:12<00:15, 119.98it/s]\u001b[A\n",
            " 45% 1504/3377 [00:12<00:15, 119.73it/s]\u001b[A\n",
            " 45% 1520/3377 [00:12<00:15, 119.65it/s]\u001b[A\n",
            " 45% 1536/3377 [00:12<00:15, 119.29it/s]\u001b[A\n",
            " 46% 1552/3377 [00:12<00:15, 119.32it/s]\u001b[A\n",
            " 46% 1568/3377 [00:13<00:15, 119.45it/s]\u001b[A\n",
            "  0% 16/17047 [00:13<09:49, 28.87it/s, NLL=1.43, epoch=0]\n",
            " 47% 1600/3377 [00:13<00:14, 118.93it/s]\u001b[A\n",
            " 48% 1616/3377 [00:13<00:14, 119.14it/s]\u001b[A\n",
            " 48% 1632/3377 [00:13<00:14, 119.50it/s]\u001b[A\n",
            " 49% 1648/3377 [00:13<00:14, 119.23it/s]\u001b[A\n",
            " 49% 1664/3377 [00:13<00:14, 119.14it/s]\u001b[A\n",
            " 50% 1680/3377 [00:13<00:14, 118.28it/s]\u001b[A\n",
            " 50% 1696/3377 [00:14<00:14, 119.30it/s]\u001b[A\n",
            " 51% 1712/3377 [00:14<00:13, 119.46it/s]\u001b[A\n",
            " 51% 1728/3377 [00:14<00:13, 119.64it/s]\u001b[A\n",
            " 52% 1744/3377 [00:14<00:13, 119.36it/s]\u001b[A\n",
            " 52% 1760/3377 [00:14<00:13, 119.92it/s]\u001b[A\n",
            " 53% 1776/3377 [00:14<00:13, 119.42it/s]\u001b[A\n",
            " 53% 1792/3377 [00:14<00:13, 119.71it/s]\u001b[A\n",
            " 54% 1808/3377 [00:15<00:13, 119.06it/s]\u001b[A\n",
            " 54% 1824/3377 [00:15<00:12, 119.65it/s]\u001b[A\n",
            " 54% 1840/3377 [00:15<00:12, 119.28it/s]\u001b[A\n",
            " 55% 1856/3377 [00:15<00:12, 119.07it/s]\u001b[A\n",
            " 55% 1872/3377 [00:15<00:12, 119.24it/s]\u001b[A\n",
            " 56% 1888/3377 [00:15<00:12, 119.32it/s]\u001b[A\n",
            " 56% 1904/3377 [00:15<00:12, 118.72it/s]\u001b[A\n",
            " 57% 1920/3377 [00:15<00:12, 119.46it/s]\u001b[A\n",
            " 57% 1936/3377 [00:16<00:12, 119.20it/s]\u001b[A\n",
            " 58% 1952/3377 [00:16<00:11, 119.00it/s]\u001b[A\n",
            " 58% 1968/3377 [00:16<00:11, 118.93it/s]\u001b[A\n",
            " 59% 1984/3377 [00:16<00:11, 118.97it/s]\u001b[A\n",
            " 59% 2000/3377 [00:16<00:11, 118.80it/s]\u001b[A\n",
            " 60% 2016/3377 [00:16<00:11, 119.58it/s]\u001b[A\n",
            " 60% 2032/3377 [00:16<00:11, 118.59it/s]\u001b[A\n",
            " 61% 2048/3377 [00:17<00:11, 119.15it/s]\u001b[A\n",
            " 61% 2064/3377 [00:17<00:10, 119.43it/s]\u001b[A\n",
            " 62% 2080/3377 [00:17<00:10, 119.57it/s]\u001b[A\n",
            " 62% 2096/3377 [00:17<00:10, 119.56it/s]\u001b[A\n",
            " 63% 2112/3377 [00:17<00:10, 119.41it/s]\u001b[A\n",
            " 63% 2128/3377 [00:17<00:10, 119.05it/s]\u001b[A\n",
            " 63% 2144/3377 [00:17<00:10, 119.40it/s]\u001b[A\n",
            " 64% 2160/3377 [00:17<00:10, 118.90it/s]\u001b[A\n",
            " 64% 2176/3377 [00:18<00:10, 119.04it/s]\u001b[A\n",
            " 65% 2192/3377 [00:18<00:09, 119.08it/s]\u001b[A\n",
            " 65% 2208/3377 [00:18<00:09, 119.55it/s]\u001b[A\n",
            " 66% 2224/3377 [00:18<00:09, 118.48it/s]\u001b[A\n",
            " 66% 2240/3377 [00:18<00:09, 119.06it/s]\u001b[A\n",
            " 67% 2256/3377 [00:18<00:09, 119.03it/s]\u001b[A\n",
            " 67% 2272/3377 [00:18<00:09, 118.36it/s]\u001b[A\n",
            " 68% 2288/3377 [00:19<00:09, 119.23it/s]\u001b[A\n",
            " 68% 2304/3377 [00:19<00:08, 119.26it/s]\u001b[A\n",
            " 69% 2320/3377 [00:19<00:08, 119.41it/s]\u001b[A\n",
            " 69% 2336/3377 [00:19<00:08, 119.29it/s]\u001b[A\n",
            " 70% 2352/3377 [00:19<00:08, 119.37it/s]\u001b[A\n",
            " 70% 2368/3377 [00:19<00:08, 119.63it/s]\u001b[A\n",
            " 71% 2384/3377 [00:19<00:08, 118.64it/s]\u001b[A\n",
            " 71% 2400/3377 [00:20<00:08, 119.51it/s]\u001b[A\n",
            " 72% 2416/3377 [00:20<00:08, 119.51it/s]\u001b[A\n",
            " 72% 2432/3377 [00:20<00:07, 119.11it/s]\u001b[A\n",
            " 72% 2448/3377 [00:20<00:07, 119.12it/s]\u001b[A\n",
            " 73% 2464/3377 [00:20<00:07, 118.80it/s]\u001b[A\n",
            " 73% 2480/3377 [00:20<00:07, 119.40it/s]\u001b[A\n",
            " 74% 2496/3377 [00:20<00:07, 119.43it/s]\u001b[A\n",
            " 74% 2512/3377 [00:20<00:07, 118.97it/s]\u001b[A\n",
            " 75% 2528/3377 [00:21<00:07, 118.42it/s]\u001b[A\n",
            " 75% 2544/3377 [00:21<00:07, 118.64it/s]\u001b[A\n",
            " 76% 2560/3377 [00:21<00:06, 119.15it/s]\u001b[A\n",
            " 76% 2576/3377 [00:21<00:06, 118.46it/s]\u001b[A\n",
            " 77% 2592/3377 [00:21<00:06, 119.37it/s]\u001b[A\n",
            " 77% 2608/3377 [00:21<00:06, 118.77it/s]\u001b[A\n",
            " 78% 2624/3377 [00:21<00:06, 120.09it/s]\u001b[A\n",
            " 78% 2640/3377 [00:22<00:06, 119.70it/s]\u001b[A\n",
            " 79% 2656/3377 [00:22<00:06, 119.67it/s]\u001b[A\n",
            " 79% 2672/3377 [00:22<00:05, 119.06it/s]\u001b[A\n",
            " 80% 2688/3377 [00:22<00:05, 119.26it/s]\u001b[A\n",
            " 80% 2704/3377 [00:22<00:05, 119.47it/s]\u001b[A\n",
            " 81% 2720/3377 [00:22<00:05, 119.55it/s]\u001b[A\n",
            " 81% 2736/3377 [00:22<00:05, 119.47it/s]\u001b[A\n",
            " 81% 2752/3377 [00:22<00:05, 119.30it/s]\u001b[A\n",
            " 82% 2768/3377 [00:23<00:05, 119.07it/s]\u001b[A\n",
            " 82% 2784/3377 [00:23<00:04, 119.07it/s]\u001b[A\n",
            " 83% 2800/3377 [00:23<00:04, 119.42it/s]\u001b[A\n",
            " 83% 2816/3377 [00:23<00:04, 119.03it/s]\u001b[A\n",
            " 84% 2832/3377 [00:23<00:04, 119.07it/s]\u001b[A\n",
            " 84% 2848/3377 [00:23<00:04, 119.29it/s]\u001b[A\n",
            " 85% 2864/3377 [00:23<00:04, 119.05it/s]\u001b[A\n",
            " 85% 2880/3377 [00:24<00:04, 119.08it/s]\u001b[A\n",
            " 86% 2896/3377 [00:24<00:04, 118.77it/s]\u001b[A\n",
            " 86% 2912/3377 [00:24<00:03, 119.22it/s]\u001b[A\n",
            " 87% 2928/3377 [00:24<00:03, 119.27it/s]\u001b[A\n",
            " 87% 2944/3377 [00:24<00:03, 119.45it/s]\u001b[A\n",
            " 88% 2960/3377 [00:24<00:03, 119.43it/s]\u001b[A\n",
            " 88% 2976/3377 [00:24<00:03, 119.39it/s]\u001b[A\n",
            " 89% 2992/3377 [00:24<00:03, 118.36it/s]\u001b[A\n",
            " 89% 3008/3377 [00:25<00:03, 119.07it/s]\u001b[A\n",
            " 90% 3024/3377 [00:25<00:02, 119.04it/s]\u001b[A\n",
            " 90% 3040/3377 [00:25<00:02, 118.92it/s]\u001b[A\n",
            " 90% 3056/3377 [00:25<00:02, 119.49it/s]\u001b[A\n",
            " 91% 3072/3377 [00:25<00:02, 119.35it/s]\u001b[A\n",
            " 91% 3088/3377 [00:25<00:02, 119.16it/s]\u001b[A\n",
            " 92% 3104/3377 [00:25<00:02, 119.24it/s]\u001b[A\n",
            " 92% 3120/3377 [00:26<00:02, 118.39it/s]\u001b[A\n",
            " 93% 3136/3377 [00:26<00:02, 119.24it/s]\u001b[A\n",
            " 93% 3152/3377 [00:26<00:01, 118.84it/s]\u001b[A\n",
            " 94% 3168/3377 [00:26<00:01, 119.63it/s]\u001b[A\n",
            " 94% 3184/3377 [00:26<00:01, 119.16it/s]\u001b[A\n",
            " 95% 3200/3377 [00:26<00:01, 119.10it/s]\u001b[A\n",
            " 95% 3216/3377 [00:26<00:01, 118.94it/s]\u001b[A\n",
            " 96% 3232/3377 [00:26<00:01, 119.25it/s]\u001b[A\n",
            " 96% 3248/3377 [00:27<00:01, 118.93it/s]\u001b[A\n",
            " 97% 3264/3377 [00:27<00:00, 118.86it/s]\u001b[A\n",
            " 97% 3280/3377 [00:27<00:00, 119.03it/s]\u001b[A\n",
            " 98% 3296/3377 [00:27<00:00, 118.78it/s]\u001b[A\n",
            " 98% 3312/3377 [00:27<00:00, 118.60it/s]\u001b[A\n",
            " 99% 3328/3377 [00:27<00:00, 118.90it/s]\u001b[A\n",
            " 99% 3344/3377 [00:27<00:00, 118.42it/s]\u001b[A\n",
            " 99% 3360/3377 [00:28<00:00, 117.90it/s]\u001b[A\n",
            "100% 3377/3377 [00:28<00:00, 119.18it/s]\n",
            "\n",
            "  0% 0/3335 [00:00<?, ?it/s]\u001b[A\n",
            "  7% 224/3335 [00:00<00:01, 2238.86it/s]\u001b[A\n",
            " 14% 454/3335 [00:00<00:01, 2272.40it/s]\u001b[A\n",
            " 21% 696/3335 [00:00<00:01, 2336.57it/s]\u001b[A\n",
            " 28% 938/3335 [00:00<00:01, 2367.32it/s]\u001b[A\n",
            " 36% 1186/3335 [00:00<00:00, 2407.69it/s]\u001b[A\n",
            " 43% 1427/3335 [00:00<00:00, 2404.61it/s]\u001b[A\n",
            " 50% 1677/3335 [00:00<00:00, 2431.95it/s]\u001b[A\n",
            " 58% 1921/3335 [00:00<00:00, 2422.85it/s]\u001b[A\n",
            " 65% 2167/3335 [00:00<00:00, 2430.93it/s]\u001b[A\n",
            " 72% 2415/3335 [00:01<00:00, 2444.50it/s]\u001b[A\n",
            " 80% 2660/3335 [00:01<00:00, 2427.74it/s]\u001b[A\n",
            " 87% 2903/3335 [00:01<00:00, 2400.63it/s]\u001b[A\n",
            "100% 3335/3335 [00:01<00:00, 2395.38it/s]\n",
            "[10.20.21 09:16:20] Visualizing in TensorBoard...\n",
            "[10.20.21 09:16:20] Eval F1: 83.04, EM: 68.97\n",
            "100% 17047/17047 [09:53<00:00, 28.74it/s, NLL=0.517, epoch=0]\n",
            "[10.20.21 09:25:43] Epoch: 1\n",
            "100% 17047/17047 [09:19<00:00, 30.46it/s, NLL=0.376, epoch=1]\n",
            "[10.20.21 09:35:04] Epoch: 2\n",
            "100% 17047/17047 [09:19<00:00, 30.48it/s, NLL=0.56, epoch=2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g1jvGXSGqLC",
        "outputId": "84eb0503-5cac-4d58-a5ba-a954b8702f49"
      },
      "source": [
        "!ls /content/robustqa/check"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best  pure_hyper_column-01  pure_hyper_column-02  pure_hyper_column-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN4D14_cGxxh"
      },
      "source": [
        "!mv /content/robustqa/check/pure_hyper_column-03/ /content/drive/MyDrive/robustqa_change_transformers/save/pure_hyper_column"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bOxi8dOHXcT",
        "outputId": "f2ba0b5f-8503-46cf-8568-524ef03aad0d"
      },
      "source": [
        "!ls /content/drive/MyDrive/robustqa_change_transformers/save"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eval_results\t\t   squad_hypercolumn_in_resnet\n",
            "original\t\t   squad_hypercolumn_out_of_resnet\n",
            "pure_hyper_column\t   squad_resnet_hypercolumn\n",
            "squad_cnn_hypercolumn_k3   squad_resnet_hypercolumn_before_intermediate-01\n",
            "squad_cnn_hypercolumn_k5   squad_resnet_hypercolumn_linear\n",
            "squad_cnn_hypercolumn_k7   squad_resnet_hypercolumn_no_act\n",
            "squad_cnn_hypercolumn_k9   squad_unet_hc_qa_linear\n",
            "squad_complete_12_bicubic  squad_unet_hc_qa_linear_10epc\n",
            "squad_complete_12_linear   squad_unet_hypercolumn\n",
            "squad_complete_13_bicubic  squad_unet_no_hc_qa_linear\n",
            "squad_complete_13_layers   squad_unet_no_hc_qa_linear_10epc\n",
            "squad_complete_13_linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiXO2cGMHgBc"
      },
      "source": [
        "!rm -r /content/robustqa/check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTMOJsYLHsHN",
        "outputId": "3e9f18a4-d98e-4b32-8a9e-02250a0ba320"
      },
      "source": [
        "!ls /content/robustqa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "args.py\t\t     environment.yml  README.md      transformers\n",
            "convert_to_squad.py  eval\t      split_data.py  util.py\n",
            "datasets\t     __pycache__      train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EguamCVXo84W"
      },
      "source": [
        "!cp -r /content/robustqa /content/drive/MyDrive/robustqa_change_transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lwKkPQNoti2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxiP06dDrjaI"
      },
      "source": [
        "#RoBERTa-CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zs2I-2hUo8Yp",
        "outputId": "63e7b590-b606-4c51-dc76-2a7847925b21"
      },
      "source": [
        "%%writefile /content/robustqa/transformers/models/roberta/modeling_roberta.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model. \"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from ...activations import ACT2FN, gelu\n",
        "from ...file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from ...utils import logging\n",
        "from .configuration_roberta import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        # End copy\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
        "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # if self.config.hyper_column:\n",
        "        # print(\"embedding = \" + str(embeddings.shape))\n",
        "        # hyper_columns = torch.zeros(embeddings.shape)\n",
        "        hyper_columns = torch.clone(embeddings) # [batch, 384, 768]\n",
        "        hyper_columns.fill_(0.0)\n",
        "        embeddings = torch.cat((embeddings, hyper_columns), dim=1) # [batch, 768, 768]\n",
        "        # print(\"embedding after = \" + str(embeddings.shape))\n",
        "        return embeddings\n",
        "\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n",
        "        \"\"\"\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n",
        "\n",
        "        Args:\n",
        "            inputs_embeds: torch.Tensor\n",
        "\n",
        "        Returns: torch.Tensor\n",
        "        \"\"\"\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        sequence_length = input_shape[1]\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n",
        "        )\n",
        "        return position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        # mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        # hidden_states = mixed_states[0]\n",
        "        # hyper_columns = mixed_states[1]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        # print(\"self attention = \" + str(len(outputs))) # len = 1\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print(\"self ouput = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"attention hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"attention hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        # print(\"attention outputs = \" + str(len(outputs)))\n",
        "        # print(outputs[0].shape)\n",
        "        outputs = (torch.cat((outputs[0], hyper_columns), dim=1),)\n",
        "        # print(\"ret outputs = \" + str(len(outputs)))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(hidden_states.shape)\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"intermediate hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"intermediate hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        # hidden_states.shape = [batch, 384, 3072]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=-1)\n",
        "        # print(\"intermediate after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print(\"input_tensor = \" + str(input_tensor.shape))\n",
        "        # print(\"hidden_states = \" + str(hidden_states.shape))\n",
        "        mixed_states = torch.split(hidden_states, 3072, dim=-1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"output hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"output hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print(hidden_states.shape)\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        # input_tensor.shape = [batch, 384, 768]\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor.split(384, dim=1)[0]) #problem!!\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=1)\n",
        "        # print(\"roberta output after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaHyperColumn(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 2)\n",
        "        # self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(len(hidden_states))\n",
        "        # print(hidden_states[0].shape)\n",
        "        mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"hyper columns hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"hyper column hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hyper_columns = self.dense(hidden_states)\n",
        "        # print(hyper_columns.shape)\n",
        "        # hyper_columns = nn.functional.interpolate(hyper_columns, 768)\n",
        "        \n",
        "        # hyper_columns = hyper_columns.unsqueeze(1)\n",
        "        # hyper_columns = self.interp(hyper_columns, size=(384, 768), mode='bicubic')\n",
        "        # hyper_columns = hyper_columns.squeeze(1)\n",
        "        \n",
        "        hyper_columns = self.interp(hyper_columns, size=768, mode='linear')\n",
        "        \n",
        "        # print(hyper_columns.shape)\n",
        "        hyper_columns = self.dropout(hyper_columns)\n",
        "        # hyper_columns = self.LayerNorm(hyper_columns)\n",
        "\n",
        "        hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "        self.hyper_columns = RobertaHyperColumn(config)\n",
        "        # self.linear = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # if isinstance(config.hidden_act, str):\n",
        "        #     self.resnet_act_fn = ACT2FN[config.hidden_act]\n",
        "        # else:\n",
        "        #     self.resnet_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # print(\"roberta layer hidden states = \" + str(hidden_states.shape))\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            assert hasattr(\n",
        "                self, \"crossattention\"\n",
        "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        \n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        outputs = self.hyper_columns(outputs)\n",
        "        # Implement Residual Net\n",
        "        # mixed_tensor = torch.split(outputs[0], 384, dim=1)\n",
        "        # outputs_tensor = mixed_tensor[0]\n",
        "        # hyper_columns_tensor = mixed_tensor[1]\n",
        "        # # mixed_attention = torch.split(attention_output, 384, dim=1)\n",
        "        # # identity = mixed_attention[0]\n",
        "        # identity = torch.split(attention_output, 384, dim=1)[0]\n",
        "        # outputs_tensor = torch.add(outputs_tensor, identity)\n",
        "        # # outputs_tensor = torch.cat((outputs_tensor, outputs_tensor_add), dim=-1)\n",
        "        # # outputs_tensor = self.linear(outputs_tensor)\n",
        "        # # outputs_tensor = self.resnet_act_fn(outputs_tensor)\n",
        "        # outputs_tensor = torch.cat((outputs_tensor, hyper_columns_tensor), dim=1)\n",
        "        # outputs = (outputs_tensor,)\n",
        "\n",
        "        # print(\"roberta layer outputs = \" + str(len(outputs)) + \" \" + str(outputs[0].shape))\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        # print(\"roberta encoder hidden states = \" + str(hidden_states.shape))\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        all_hyper_columns = ()\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            # print(\"roberta encoder after hidden states = \" + str(hidden_states.shape))\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            # if (i + 1) % 4 == 0:\n",
        "            hyper_column = torch.split(hidden_states, 384, dim=1)[1]\n",
        "            # print(\"encoder hyper column shape = \" + str(hyper_column.shape))\n",
        "            all_hyper_columns = all_hyper_columns + (hyper_column,)\n",
        "\n",
        "        # hidden_states = torch.split(hidden_states, 384, dim=1)[0]\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        hidden_states = torch.split(hidden_states, 384, dim=1)[0] # [batch, 384, 768]\n",
        "        # print(\"before = \" + str(hidden_states.shape))\n",
        "        to_add = (hidden_states,) + all_hyper_columns # len = 13\n",
        "        # print(\"len to add = \" + str(len(to_add)))\n",
        "        hidden_states = torch.cat(to_add, dim=1)\n",
        "        # print(\"after = \" + str(hidden_states.shape))\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(RobertaPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "\n",
        "    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", ROBERTA_START_DOCSTRING\n",
        ")\n",
        "class RobertaForCausalLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "            >>> import torch\n",
        "\n",
        "            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "            >>> config.is_decoder = True\n",
        "            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n",
        "\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dense(features)\n",
        "        x = gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # project back to size of vocabulary with bias\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _tie_weights(self):\n",
        "        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n",
        "        self.bias = self.decoder.bias\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
        "    pooled output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            position_ids=flat_position_ids,\n",
        "            token_type_ids=flat_token_type_ids,\n",
        "            attention_mask=flat_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=flat_inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.in_channels_init = config.hidden_size * config.num_hidden_layers\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, padding=0)\n",
        "        self.act = nn.Tanh()\n",
        "        self.down_sample_01 = nn.Conv1d(in_channels=self.in_channels_init,\n",
        "                                        out_channels=self.in_channels_init//16,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_02 = nn.Conv1d(in_channels=self.in_channels_init//16,\n",
        "                                        out_channels=self.in_channels_init//32,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_03 = nn.Conv1d(in_channels=self.in_channels_init//32,\n",
        "                                        out_channels=self.in_channels_init//64,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_04 = nn.Conv1d(in_channels=self.in_channels_init//64,\n",
        "                                        out_channels=self.in_channels_init//128,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.up_sample_04 = nn.ConvTranspose1d(in_channels=self.in_channels_init//128,\n",
        "                                               out_channels=self.in_channels_init//64,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_03 = nn.ConvTranspose1d(in_channels=self.in_channels_init//64,\n",
        "                                               out_channels=self.in_channels_init//32,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_02 = nn.ConvTranspose1d(in_channels=self.in_channels_init//32,\n",
        "                                               out_channels=self.in_channels_init//16,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_01 = nn.ConvTranspose1d(in_channels=self.in_channels_init//16,\n",
        "                                               out_channels=self.in_channels_init//8,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # identity1 = x\n",
        "        x = self.down_sample_01(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity2 = x\n",
        "        x = self.down_sample_02(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity3 = x\n",
        "        x = self.down_sample_03(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity4 = x\n",
        "        x = self.down_sample_04(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        #up\n",
        "        # y = x\n",
        "        y = self.up_sample_04(x)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity4), dim=-1)\n",
        "        y = self.up_sample_03(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity3), dim=-1)\n",
        "        y = self.up_sample_02(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity2), dim=-1)\n",
        "        y = self.up_sample_01(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity1), dim=-1)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.unet = UNet(config)\n",
        "        # self.qa_outputs = nn.Conv1d(in_channels=config.hidden_size * config.num_hidden_layers // 8, out_channels=2,\n",
        "        #                             kernel_size=5, padding='same')\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size * config.num_hidden_layers // 8, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # print(len(outputs))\n",
        "        # print(\"outputs shape = \" + str(outputs[0].shape))\n",
        "        sequence_output = torch.split(outputs[0], 384, dim=1)[0]\n",
        "        # print(sequence_output.shape)\n",
        "        hyper_columns = torch.split(outputs[0], 384, dim=1)[1:]\n",
        "        # print(\"qa sequence = \" + str(sequence_output.shape))\n",
        "        # print(\"qa hyper columns len = \" + str(len(hyper_columns)))\n",
        "        hyper_columns_tensor = torch.cat(hyper_columns, dim=-1)\n",
        "\n",
        "        sequence_output = hyper_columns_tensor\n",
        "        # print(sequence_output.shape)\n",
        "        \n",
        "        # sequence_output = torch.cat((sequence_output, hyper_columns_tensor), dim=-1)\n",
        "\n",
        "        # CNN\n",
        "        # sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        # logits = self.qa_outputs(sequence_output)\n",
        "        # logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        # UNet\n",
        "        sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        unet_output = self.unet(sequence_output)\n",
        "        unet_output = unet_output.transpose(1, 2).contiguous()\n",
        "        logits = self.qa_outputs(unet_output)\n",
        "        # logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
        "    \"\"\"\n",
        "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
        "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
        "\n",
        "    Args:\n",
        "        x: torch.Tensor x:\n",
        "\n",
        "    Returns: torch.Tensor\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
        "    return incremental_indices.long() + padding_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting /content/robustqa/transformers/models/roberta/modeling_roberta.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVJs1bFVo7gu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bwwdMXTrsOX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSdz5NWyrs9p"
      },
      "source": [
        "#Utility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeFkPZHaKxdM",
        "outputId": "81789a5f-6428-46ba-ab31-810193391f0e"
      },
      "source": [
        "%cd /content/drive/MyDrive/robustqa_change_transformers/robustqa/check"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/robustqa_change_transformers/robustqa/check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHXHNLyYmifP"
      },
      "source": [
        "!mv squad_unet_hypercolumn-03/ ../save/squad_unet_hypercolumn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6ZwE1bemb7i",
        "outputId": "1f0e53be-8966-4450-ef94-66ddf2cf59b6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best\t\t\t   squad_unet_hypercolumn-02\n",
            "squad_unet_hypercolumn-01  squad_unet_hypercolumn-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK2qntIbTHMe",
        "outputId": "88d3e3b8-9584-4a16-c141-c5f00fd9b150"
      },
      "source": [
        "%cd /content/robustqa/check"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa/check\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dh8viR0srl8",
        "outputId": "e552eb8b-6ca7-4592-e456-74fb410f59e7"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best\t\t\t    squad_unet_hc_qa_linear-02\n",
            "squad_unet_hc_qa_linear-01  squad_unet_hc_qa_linear-03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTKvyyrMsse6"
      },
      "source": [
        "!rm -r best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oylbRUIssvcB"
      },
      "source": [
        "!rm -r squad_unet_hc_qa_linear-02"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHGzLi3ctWsJ"
      },
      "source": [
        "!mv squad_unet_hc_qa_linear-03/ ../save/squad_unet_hc_qa_linear"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHMlu8kNtdeP",
        "outputId": "99ade6b0-cfa1-402d-d6ac-98acdf442a10"
      },
      "source": [
        "%cd /content/robustqa"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/robustqa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uH8z9VzFtk_O"
      },
      "source": [
        "!rm -r check"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EisDAp0YtnTe",
        "outputId": "8dadd0e2-148b-42d3-81c6-94ecce95e0fe"
      },
      "source": [
        "%cd /content/drive/MyDrive/robustqa_change_transformers/save"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/robustqa_change_transformers/save\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeFtO5oMbpmN",
        "outputId": "9092f97c-8159-40d5-e2a1-6abae5791d79"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_results\t\t   squad_hypercolumn_in_resnet\n",
            "squad_cnn_hypercolumn_k3   squad_hypercolumn_out_of_resnet\n",
            "squad_cnn_hypercolumn_k5   squad_resnet_hypercolumn\n",
            "squad_cnn_hypercolumn_k7   squad_resnet_hypercolumn_before_intermediate-01\n",
            "squad_cnn_hypercolumn_k9   squad_resnet_hypercolumn_linear\n",
            "squad_complete_12_bicubic  squad_resnet_hypercolumn_no_act\n",
            "squad_complete_12_linear   squad_unet_hc_qa_linear\n",
            "squad_complete_13_bicubic  squad_unet_hc_qa_linear_10epc\n",
            "squad_complete_13_layers   squad_unet_hypercolumn\n",
            "squad_complete_13_linear   squad_unet_no_hc_qa_linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpHWTS4Xcfs_"
      },
      "source": [
        "!mv /content/drive/MyDrive/robustqa_change_transformers/save/squad_unet_hc_qa_linear_10epc/ /content/drive/MyDrive/robustqa_change_transformers/save/squad_unet_no_hc_qa_linear_10epc/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V6cGxctdmTU",
        "outputId": "39a8b056-c79e-438d-ec8e-e677d3dc0893"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "eval_results\t\t   squad_hypercolumn_in_resnet\n",
            "squad_cnn_hypercolumn_k3   squad_hypercolumn_out_of_resnet\n",
            "squad_cnn_hypercolumn_k5   squad_resnet_hypercolumn\n",
            "squad_cnn_hypercolumn_k7   squad_resnet_hypercolumn_before_intermediate-01\n",
            "squad_cnn_hypercolumn_k9   squad_resnet_hypercolumn_linear\n",
            "squad_complete_12_bicubic  squad_resnet_hypercolumn_no_act\n",
            "squad_complete_12_linear   squad_unet_hc_qa_linear\n",
            "squad_complete_13_bicubic  squad_unet_hypercolumn\n",
            "squad_complete_13_layers   squad_unet_no_hc_qa_linear\n",
            "squad_complete_13_linear   squad_unet_no_hc_qa_linear_10epc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC0UEHJYdzVQ"
      },
      "source": [
        "!mv robustqa/save/ ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZPIthahd7NU",
        "outputId": "7825cd04-b7d0-41fb-9a46-09a92eb992e3"
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Fn5kwObeAG_"
      },
      "source": [
        "!rm -r robustqa/robustqa_original/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNa3qOBteF_g"
      },
      "source": [
        "!rm -r robustqa/save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCmiI_9oeLR3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpcTl2pHKFWn"
      },
      "source": [
        "#BertCNN (Probably Wrong)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohB3ng5TGOOR"
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model. \"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from ...activations import ACT2FN, gelu\n",
        "from ...file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from ...utils import logging\n",
        "from .configuration_roberta import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        # End copy\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
        "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        # if self.config.hyper_column:\n",
        "        # print(\"embedding = \" + str(embeddings.shape))\n",
        "        # hyper_columns = torch.zeros(embeddings.shape)\n",
        "        hyper_columns = torch.clone(embeddings) # [batch, 384, 768]\n",
        "        hyper_columns.fill_(0.0)\n",
        "        embeddings = torch.cat((embeddings, hyper_columns), dim=1) # [batch, 768, 768]\n",
        "        # print(\"embedding after = \" + str(embeddings.shape))\n",
        "        return embeddings\n",
        "\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n",
        "        \"\"\"\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n",
        "\n",
        "        Args:\n",
        "            inputs_embeds: torch.Tensor\n",
        "\n",
        "        Returns: torch.Tensor\n",
        "        \"\"\"\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        sequence_length = input_shape[1]\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n",
        "        )\n",
        "        return position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        # mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        # hidden_states = mixed_states[0]\n",
        "        # hyper_columns = mixed_states[1]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        # print(\"self attention = \" + str(len(outputs))) # len = 1\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        # print(\"self ouput = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # if self.config.hyper_column:\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"attention hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"attention hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        # print(\"attention outputs = \" + str(len(outputs)))\n",
        "        # print(outputs[0].shape)\n",
        "        outputs = (torch.cat((outputs[0], hyper_columns), dim=1),)\n",
        "        # print(\"ret outputs = \" + str(len(outputs)))\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # print(hidden_states.shape)\n",
        "        mixed_states = torch.split(hidden_states, 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"intermediate hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"intermediate hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        # hidden_states.shape = [batch, 384, 3072]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=-1)\n",
        "        # print(\"intermediate after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        # print(\"input_tensor = \" + str(input_tensor.shape))\n",
        "        # print(\"hidden_states = \" + str(hidden_states.shape))\n",
        "        mixed_states = torch.split(hidden_states, 3072, dim=-1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"output hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"output hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        # print(hidden_states.shape)\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        # input_tensor.shape = [batch, 384, 768]\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor.split(384, dim=1)[0]) #problem!!\n",
        "        # hidden_states.shape = [batch, 384, 768]\n",
        "        hidden_states = torch.cat((hidden_states, hyper_columns), dim=1)\n",
        "        # print(\"roberta output after = \" + str(hidden_states.shape))\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class RobertaHyperColumn(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, 2)\n",
        "        # self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.interp = nn.functional.interpolate\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # identical hyper columns\n",
        "        mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[0]\n",
        "        hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "        return hidden_states\n",
        "        # print(len(hidden_states))\n",
        "        # print(hidden_states[0].shape)\n",
        "        mixed_states = torch.split(hidden_states[0], 384, dim=1)\n",
        "        hidden_states = mixed_states[0]\n",
        "        hyper_columns = mixed_states[1]\n",
        "        # print(\"hyper columns hidden states = \" + str(hidden_states.shape))\n",
        "        # print(\"hyper column hyper columns = \" + str(hyper_columns.shape))\n",
        "\n",
        "        hyper_columns = self.dense(hidden_states)\n",
        "        # print(hyper_columns.shape)\n",
        "        # hyper_columns = nn.functional.interpolate(hyper_columns, 768)\n",
        "        \n",
        "        # hyper_columns = hyper_columns.unsqueeze(1)\n",
        "        # hyper_columns = self.interp(hyper_columns, size=(384, 768), mode='bicubic')\n",
        "        # hyper_columns = hyper_columns.squeeze(1)\n",
        "        \n",
        "        hyper_columns = self.interp(hyper_columns, size=768, mode='linear')\n",
        "        \n",
        "        # print(hyper_columns.shape)\n",
        "        hyper_columns = self.dropout(hyper_columns)\n",
        "        # hyper_columns = self.LayerNorm(hyper_columns)\n",
        "\n",
        "        hidden_states = (torch.cat((hidden_states, hyper_columns), dim=1),)\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            assert self.is_decoder, f\"{self} should be used as a decoder model if cross attention is added\"\n",
        "            self.crossattention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "        self.hyper_columns = RobertaHyperColumn(config)\n",
        "        # self.linear = nn.Linear(config.hidden_size * 2, config.hidden_size)\n",
        "        # if isinstance(config.hidden_act, str):\n",
        "        #     self.resnet_act_fn = ACT2FN[config.hidden_act]\n",
        "        # else:\n",
        "        #     self.resnet_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # print(\"roberta layer hidden states = \" + str(hidden_states.shape))\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            assert hasattr(\n",
        "                self, \"crossattention\"\n",
        "            ), f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        \n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        outputs = self.hyper_columns(outputs)\n",
        "        # Implement Residual Net\n",
        "        # mixed_tensor = torch.split(outputs[0], 384, dim=1)\n",
        "        # outputs_tensor = mixed_tensor[0]\n",
        "        # hyper_columns_tensor = mixed_tensor[1]\n",
        "        # # mixed_attention = torch.split(attention_output, 384, dim=1)\n",
        "        # # identity = mixed_attention[0]\n",
        "        # identity = torch.split(attention_output, 384, dim=1)[0]\n",
        "        # outputs_tensor = torch.add(outputs_tensor, identity)\n",
        "        # # outputs_tensor = torch.cat((outputs_tensor, outputs_tensor_add), dim=-1)\n",
        "        # # outputs_tensor = self.linear(outputs_tensor)\n",
        "        # # outputs_tensor = self.resnet_act_fn(outputs_tensor)\n",
        "        # outputs_tensor = torch.cat((outputs_tensor, hyper_columns_tensor), dim=1)\n",
        "        # outputs = (outputs_tensor,)\n",
        "\n",
        "        # print(\"roberta layer outputs = \" + str(len(outputs)) + \" \" + str(outputs[0].shape))\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        # print(\"roberta encoder hidden states = \" + str(hidden_states.shape))\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "        all_hyper_columns = ()\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            # print(\"roberta encoder after hidden states = \" + str(hidden_states.shape))\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "            # if (i + 1) % 4 == 0:\n",
        "            hyper_column = torch.split(hidden_states, 384, dim=1)[1]\n",
        "            # print(\"encoder hyper column shape = \" + str(hyper_column.shape))\n",
        "            all_hyper_columns = all_hyper_columns + (hyper_column,)\n",
        "\n",
        "        # hidden_states = torch.split(hidden_states, 384, dim=1)[0]\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        hidden_states = torch.split(hidden_states, 384, dim=1)[0] # [batch, 384, 768]\n",
        "        # print(\"before = \" + str(hidden_states.shape))\n",
        "        to_add = (hidden_states,) + all_hyper_columns # len = 13\n",
        "        # print(\"len to add = \" + str(len(to_add)))\n",
        "        hidden_states = torch.cat(to_add, dim=1)\n",
        "        # print(\"after = \" + str(hidden_states.shape))\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(RobertaPreTrainedModel):\n",
        "    \"\"\"\n",
        "\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "\n",
        "    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", ROBERTA_START_DOCSTRING\n",
        ")\n",
        "class RobertaForCausalLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        Example::\n",
        "\n",
        "            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "            >>> import torch\n",
        "\n",
        "            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "            >>> config.is_decoder = True\n",
        "            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n",
        "\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dense(features)\n",
        "        x = gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # project back to size of vocabulary with bias\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _tie_weights(self):\n",
        "        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n",
        "        self.bias = self.decoder.bias\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
        "    pooled output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            position_ids=flat_position_ids,\n",
        "            token_type_ids=flat_token_type_ids,\n",
        "            attention_mask=flat_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=flat_inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.in_channels_init = config.hidden_size * config.num_hidden_layers\n",
        "        self.maxpool = nn.MaxPool1d(kernel_size=2, padding=0)\n",
        "        self.act = nn.Tanh()\n",
        "        self.down_sample_01 = nn.Conv1d(in_channels=self.in_channels_init,\n",
        "                                        out_channels=self.in_channels_init//16,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_02 = nn.Conv1d(in_channels=self.in_channels_init//16,\n",
        "                                        out_channels=self.in_channels_init//32,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_03 = nn.Conv1d(in_channels=self.in_channels_init//32,\n",
        "                                        out_channels=self.in_channels_init//64,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.down_sample_04 = nn.Conv1d(in_channels=self.in_channels_init//64,\n",
        "                                        out_channels=self.in_channels_init//128,\n",
        "                                        kernel_size=5, padding='same')\n",
        "        self.up_sample_04 = nn.ConvTranspose1d(in_channels=self.in_channels_init//128,\n",
        "                                               out_channels=self.in_channels_init//64,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_03 = nn.ConvTranspose1d(in_channels=self.in_channels_init//64,\n",
        "                                               out_channels=self.in_channels_init//32,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_02 = nn.ConvTranspose1d(in_channels=self.in_channels_init//32,\n",
        "                                               out_channels=self.in_channels_init//16,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        self.up_sample_01 = nn.ConvTranspose1d(in_channels=self.in_channels_init//16,\n",
        "                                               out_channels=self.in_channels_init//8,\n",
        "                                               kernel_size=2, stride=2, padding=0)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # identity1 = x\n",
        "        x = self.down_sample_01(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity2 = x\n",
        "        x = self.down_sample_02(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity3 = x\n",
        "        x = self.down_sample_03(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        # identity4 = x\n",
        "        x = self.down_sample_04(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool(x)\n",
        "        #up\n",
        "        # y = x\n",
        "        y = self.up_sample_04(x)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity4), dim=-1)\n",
        "        y = self.up_sample_03(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity3), dim=-1)\n",
        "        y = self.up_sample_02(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity2), dim=-1)\n",
        "        y = self.up_sample_01(y)\n",
        "        # y = self.act(y)\n",
        "        # y = torch.cat((y, identity1), dim=-1)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class BertCNN(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.filter_sizes = [1, 2, 3, 4, 5]\n",
        "        self.config = config\n",
        "        # self.conv1 = nn.Conv2d(in_channels=config.hidden_size, out_channels=32, kernel_size=3, padding='same')\n",
        "        # self.conv2 = nn.Conv2d(in_channels=config.hidden_size, out_channels=32*2, kernel_size=3, padding='same')\n",
        "        # self.conv3 = nn.Conv2d(in_channels=config.hidden_size, out_channels=32*3, kernel_size=3, padding='same')\n",
        "        # self.conv4 = nn.Conv2d(in_channels=config.hidden_size, out_channels=32*4, kernel_size=3, padding='same')\n",
        "        self.conv1 = nn.Conv2d(12, 384, (384, 1))\n",
        "        self.conv2 = nn.Conv2d(12, 384, (384, 2))\n",
        "        self.conv3 = nn.Conv2d(12, 384, (384, 3))\n",
        "        self.conv4 = nn.Conv2d(12, 384, (384, 4))\n",
        "        self.act = nn.ReLU()\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d((384, 64))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size = x.shape[0]\n",
        "        \n",
        "        x1 = self.conv1(x)\n",
        "        x1 = self.act(x1)\n",
        "        x1 = self.maxpool(x1)\n",
        "        \n",
        "        x2 = self.conv2(x)\n",
        "        x2 = self.act(x2)\n",
        "        x2 = self.maxpool(x2)\n",
        "        \n",
        "        x3 = self.conv3(x)\n",
        "        x3 = self.act(x3)\n",
        "        x3 = self.maxpool(x3)\n",
        "        \n",
        "        x4 = self.conv4(x)\n",
        "        x4 = self.act(x4)\n",
        "        x4 = self.maxpool(x4)\n",
        "        # x4 = x4.transpose(1, 2)\n",
        "        print(x1.shape)\n",
        "        print(x2.shape)\n",
        "        print(x3.shape)\n",
        "        print(x4.shape)\n",
        "        ret = torch.cat((x1, x2, x3, x4), dim=-1)\n",
        "        # ret = ret.permute((0, 2, 1, 3)).contiguous()\n",
        "        print(ret.shape)\n",
        "        return ret\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        # self.unet = UNet(config)\n",
        "        # self.qa_outputs = nn.Conv1d(in_channels=config.hidden_size * config.num_hidden_layers // 8, out_channels=2,\n",
        "        #                             kernel_size=5, padding='same')\n",
        "        self.bert_cnn = BertCNN(config)\n",
        "        self.dense = nn.Linear(32 * config.num_hidden_layers, config.num_labels)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size * config.num_hidden_layers // 8, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        tokenizer_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # print(len(outputs))\n",
        "        # print(\"outputs shape = \" + str(outputs[0].shape))\n",
        "        sequence_output = torch.split(outputs[0], 384, dim=1)[0]\n",
        "        # print(sequence_output.shape)\n",
        "        hyper_columns = torch.split(outputs[0], 384, dim=1)[1:]\n",
        "        # print(\"qa sequence = \" + str(sequence_output.shape))\n",
        "        # print(\"qa hyper columns len = \" + str(len(hyper_columns)))\n",
        "        hyper_columns_tensor = torch.cat(hyper_columns, dim=-1)\n",
        "\n",
        "        sequence_output = hyper_columns_tensor\n",
        "        # print(sequence_output.shape)\n",
        "        \n",
        "        # sequence_output = torch.cat((sequence_output, hyper_columns_tensor), dim=-1)\n",
        "\n",
        "        # CNN\n",
        "        # sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        # logits = self.qa_outputs(sequence_output)\n",
        "        # logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        # UNet\n",
        "        # sequence_output = sequence_output.transpose(1, 2).contiguous()\n",
        "        # unet_output = self.unet(sequence_output)\n",
        "        # unet_output = unet_output.transpose(1, 2).contiguous()\n",
        "        # logits = self.qa_outputs(unet_output)\n",
        "        # logits = logits.transpose(1, 2).contiguous()\n",
        "\n",
        "        # BertCNN\n",
        "        bert_inp = torch.stack(hyper_columns, dim=1)\n",
        "        # bert_inp = bert_inp.permute(0, 2, 1, 3).contiguous()\n",
        "        print(bert_inp.shape)\n",
        "        bert_cnn = self.bert_cnn(bert_inp)\n",
        "        print(bert_cnn.shape)\n",
        "        flat = torch.flatten(bert_cnn, start_dim=2)\n",
        "        print(flat.shape)\n",
        "        logits = self.dense(flat)\n",
        "        print(\"logits = \")\n",
        "        print(logits.shape)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
        "    \"\"\"\n",
        "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
        "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
        "\n",
        "    Args:\n",
        "        x: torch.Tensor x:\n",
        "\n",
        "    Returns: torch.Tensor\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
        "    return incremental_indices.long() + padding_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV19iEXWLHsR",
        "outputId": "a07c9d99-026a-45f5-b32d-8c30833dd9b1"
      },
      "source": [
        "import torch\n",
        "\n",
        "inp = torch.rand(4, 12, 384, 768)\n",
        "f = torch.nn.AdaptiveMaxPool2d(384)\n",
        "outp = f(inp)\n",
        "print(outp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 12, 384, 384])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucllGwUuPJCL"
      },
      "source": [
        "outp = outp.squeeze(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs2AF8RHPMCP",
        "outputId": "9f1fc066-f153-40b3-be16-801eb5a7e73d"
      },
      "source": [
        "outp.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 12, 384, 384])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBEp_1507KhK",
        "outputId": "d95d5a22-e84b-47a2-86c6-ff159b0612b1"
      },
      "source": [
        "import torch\n",
        "inp = torch.rand(4, 12, 384, 768)\n",
        "f = torch.nn.Conv2d(12, 32, kernel_size=(1, 385))\n",
        "outp = f(inp)\n",
        "print(outp.shape)\n",
        "outp = torch.nn.AdaptiveMaxPool2d((384, 2))(outp)\n",
        "print(outp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 32, 384, 384])\n",
            "torch.Size([4, 32, 384, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1MgBlJ9J2lB"
      },
      "source": [
        "#Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmgF92qeKFFz"
      },
      "source": [
        "%pycat /content/robustqa/train.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88-_Jy66bTwT"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "# from transformers import DistilBertTokenizerFast\n",
        "# from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import RobertaTokenizerFast\n",
        "from transformers import RobertaForQuestionAnswering\n",
        "\n",
        "from transformers import AdamW\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def prepare_eval_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "    # corresponding example_id and we will store the offset mappings.\n",
        "    tokenized_examples[\"id\"] = []\n",
        "    for i in tqdm(range(len(tokenized_examples[\"input_ids\"]))):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"id\"].append(dataset_dict[\"id\"][sample_index])\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def prepare_train_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "    tokenized_examples['id'] = []\n",
        "    inaccurate = 0\n",
        "    for i, offsets in enumerate(tqdm(offset_mapping)):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = dataset_dict['answer'][sample_index]\n",
        "        # Start/end character index of the answer in the text.\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "            # assertion to check if this checks out\n",
        "            context = dataset_dict['context'][sample_index]\n",
        "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
        "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
        "            if context[offset_st : offset_en] != answer['text'][0]:\n",
        "                inaccurate += 1\n",
        "\n",
        "    total = len(tokenized_examples['id'])\n",
        "    print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
        "    #TODO: cache this if possible\n",
        "    # cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
        "    # if os.path.exists(cache_path) and not args.recompute_features:\n",
        "    #     tokenized_examples = util.load_pickle(cache_path)\n",
        "    # else:\n",
        "    if split=='train':\n",
        "        tokenized_examples = prepare_train_data(dataset_dict, tokenizer)\n",
        "    else:\n",
        "        tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
        "    # util.save_pickle(tokenized_examples, cache_path)\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use a logger, use tensorboard\n",
        "class Trainer():\n",
        "    def __init__(self, args, log, best_dir, load_dir):\n",
        "        self.lr = args.lr\n",
        "        self.num_epochs = args.num_epochs\n",
        "        self.device = args.device\n",
        "        self.eval_every = args.eval_every\n",
        "        self.path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        self.num_visuals = args.num_visuals\n",
        "        self.save_dir = args.save_dir\n",
        "        self.log = log\n",
        "        self.best_dir = best_dir\n",
        "        self.load_dir = load_dir\n",
        "        self.visualize_predictions = args.visualize_predictions\n",
        "        if not os.path.exists(self.path):\n",
        "            os.makedirs(self.path)\n",
        "\n",
        "    def save_model(self, checkpoint, model, mode):\n",
        "        if mode =='normal':\n",
        "            model.save_pretrained(self.path)\n",
        "            # torch.save(checkpoint, self.path)\n",
        "        if mode == 'best':\n",
        "            torch.save(checkpoint, self.best_dir+'/best_checkpoint.pt')\n",
        "\n",
        "    def load_model(self, checkpoint_fpath, model, optim, best_scores):\n",
        "        checkpoint = torch.load(checkpoint_fpath)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optim.load_state_dict(checkpoint['optim'])\n",
        "        best_scores = checkpoint['best_scores']\n",
        "        return model, optim, best_scores\n",
        "\n",
        "    def evaluate(self, model, data_loader, data_dict, return_preds=False, split='validation'):\n",
        "        device = self.device\n",
        "\n",
        "        model.eval()\n",
        "        pred_dict = {}\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        with torch.no_grad(), \\\n",
        "                tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
        "            for batch in data_loader:\n",
        "                # Setup for forward\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                batch_size = len(input_ids)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                # Forward\n",
        "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "                # TODO: compute loss\n",
        "\n",
        "                all_start_logits.append(start_logits)\n",
        "                all_end_logits.append(end_logits)\n",
        "                progress_bar.update(batch_size)\n",
        "\n",
        "        # Get F1 and EM scores\n",
        "        start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
        "        end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
        "        preds = util.postprocess_qa_predictions(data_dict,\n",
        "                                                 data_loader.dataset.encodings,\n",
        "                                                 (start_logits, end_logits))\n",
        "        if split == 'validation':\n",
        "            results = util.eval_dicts(data_dict, preds)\n",
        "            results_list = [('F1', results['F1']),\n",
        "                            ('EM', results['EM'])]\n",
        "        else:\n",
        "            results_list = [('F1', -1.0),\n",
        "                            ('EM', -1.0)]\n",
        "        results = OrderedDict(results_list)\n",
        "        if return_preds:\n",
        "            return preds, results\n",
        "        return results\n",
        "\n",
        "    def train(self, model, train_dataloader, eval_dataloader, val_dict, saved=False):\n",
        "        device = self.device\n",
        "        model.to(device)\n",
        "        optim = AdamW(model.parameters(), lr=self.lr)\n",
        "        global_idx = 0\n",
        "        best_scores = curr_score = {'F1': -1.0, 'EM': -1.0}\n",
        "        tbx = SummaryWriter(self.save_dir)\n",
        "\n",
        "        if saved:\n",
        "            model, optim, best_scores = self.load_model(self.best_dir+'/best_checkpoint.pt', model, optim, best_scores)\n",
        "            model, optim, curr_scores = self.load_model(self.load_dir, model, optim, curr_scores)\n",
        "        \n",
        "        for epoch_num in range(self.num_epochs):\n",
        "            self.log.info(f'Epoch: {epoch_num}')\n",
        "            with torch.enable_grad(), tqdm(total=len(train_dataloader.dataset)) as progress_bar:\n",
        "                for batch in train_dataloader:\n",
        "                    optim.zero_grad()\n",
        "                    model.train()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    start_positions = batch['start_positions'].to(device)\n",
        "                    end_positions = batch['end_positions'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                                    start_positions=start_positions,\n",
        "                                    end_positions=end_positions)\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    progress_bar.update(len(input_ids))\n",
        "                    progress_bar.set_postfix(epoch=epoch_num, NLL=loss.item())\n",
        "                    tbx.add_scalar('train/NLL', loss.item(), global_idx)\n",
        "                    if (global_idx % self.eval_every) == 0:\n",
        "                        self.log.info(f'Evaluating at step {global_idx}...')\n",
        "                        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
        "                        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
        "                        self.log.info('Visualizing in TensorBoard...')\n",
        "                        for k, v in curr_score.items():\n",
        "                            tbx.add_scalar(f'val/{k}', v, global_idx)\n",
        "                        self.log.info(f'Eval {results_str}')\n",
        "                        if self.visualize_predictions:\n",
        "                            util.visualize(tbx,\n",
        "                                           pred_dict=preds,\n",
        "                                           gold_dict=val_dict,\n",
        "                                           step=global_idx,\n",
        "                                           split='val',\n",
        "                                           num_visuals=self.num_visuals)\n",
        "                        if curr_score['F1'] >= best_scores['F1']:\n",
        "                            best_scores = curr_score\n",
        "                            checkpoint = {'state_dict': model.state_dict(),\n",
        "                                          'optim': optim.state_dict(),\n",
        "                                          'best_scores': best_scores}\n",
        "                            self.save_model(checkpoint, model, 'best')\n",
        "                        global_idx += 1\n",
        "            best_scores = curr_score\n",
        "            checkpoint = {'state_dict': model.state_dict(),\n",
        "                          'optim': optim.state_dict(),\n",
        "                          'best_scores': best_scores} \n",
        "            self.save_model(checkpoint, model, 'normal')\n",
        "            self.load_dir = self.path\n",
        "        return best_scores\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    data_encodings = read_and_process(args, tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
        "    return util.QADataset(data_encodings, train=(split_name=='train')), dataset_dict\n",
        "\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    args = get_train_test_args()\n",
        "    best_dir = args.save_dir + 'best'\n",
        "    util.set_seed(args.seed)\n",
        "    load_dir = ''\n",
        "    # model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "    # tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "    \n",
        "    tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "    if args.saved == 'YES':\n",
        "        checkpoint_dir = ''\n",
        "        for uid in range(1, 100):\n",
        "            save_dir = os.path.join(args.save_dir, f'{args.run_name}-{uid:02d}')\n",
        "            if os.path.exists(save_dir):\n",
        "                checkpoint_dir = save_dir\n",
        "            else:\n",
        "                break    \n",
        "        load_dir = os.path.join(checkpoint_dir, 'checkpoint')\n",
        "        model = RobertaForQuestionAnswering.from_pretrained(load_dir)\n",
        "        # model = AutoModel.from_config(checkpoint_path + '/config.json')\n",
        "    else:\n",
        "        model = RobertaForQuestionAnswering.from_pretrained('roberta-base')\n",
        "\n",
        "    if args.do_train:\n",
        "        if not os.path.exists(args.save_dir):\n",
        "            os.makedirs(args.save_dir)\n",
        "        if not os.path.exists(best_dir):\n",
        "            os.makedirs(best_dir)\n",
        "        args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "        log = util.get_logger(args.save_dir, 'log_train')\n",
        "        log.info(f'Args: {json.dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "        log.info(\"Preparing Training Data...\")\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        trainer = Trainer(args, log, best_dir, load_dir)\n",
        "        train_dataset, _ = get_dataset(args, args.train_datasets, args.train_dir, tokenizer, 'train')\n",
        "        log.info(\"Preparing Validation Data...\")\n",
        "        val_dataset, val_dict = get_dataset(args, args.train_datasets, args.val_dir, tokenizer, 'val')\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=RandomSampler(train_dataset))\n",
        "        val_loader = DataLoader(val_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=SequentialSampler(val_dataset))\n",
        "        best_scores = trainer.train(model, train_loader, val_loader, val_dict)\n",
        "    if args.do_eval:\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        split_name = 'test' if 'test' in args.eval_dir else 'validation'\n",
        "        log = util.get_logger(args.save_dir, f'log_{split_name}')\n",
        "        trainer = Trainer(args, log, best_dir, load_dir)\n",
        "        checkpoint_path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        # model = DistilBertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "        model = RobertaForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "        model.to(args.device)\n",
        "        eval_dataset, eval_dict = get_dataset(args, args.eval_datasets, args.eval_dir, tokenizer, split_name)\n",
        "        eval_loader = DataLoader(eval_dataset,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 sampler=SequentialSampler(eval_dataset))\n",
        "        eval_preds, eval_scores = trainer.evaluate(model, eval_loader,\n",
        "                                                   eval_dict, return_preds=True,\n",
        "                                                   split=split_name)\n",
        "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in eval_scores.items())\n",
        "        log.info(f'Eval {results_str}')\n",
        "        # Write submission file\n",
        "        sub_path = os.path.join(args.save_dir, split_name + '_' + args.sub_file)\n",
        "        log.info(f'Writing submission file to {sub_path}...')\n",
        "        with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
        "            csv_writer = csv.writer(csv_fh, delimiter=',')\n",
        "            csv_writer.writerow(['Id', 'Predicted'])\n",
        "            for uuid in sorted(eval_preds):\n",
        "                csv_writer.writerow([uuid, eval_preds[uuid]])\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DdqXxmbnq_R"
      },
      "source": [
        "!rm -r /content/robustqa/save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J52jdEo_nGjr"
      },
      "source": [
        "!mkdir /content/robustqa/save\n",
        "!cp -r /content/drive/MyDrive/robustqa_change_transformers/save/squad_complete_12_linear/ /content/robustqa/save"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFpp7wqysRNU",
        "outputId": "c3d55d96-d172-4046-db0d-6e4d0c991aea"
      },
      "source": [
        "!mv -v /content/robustqa/save/squad_complete_12_linear/* /content/robustqa/save/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "renamed '/content/robustqa/save/squad_complete_12_linear/checkpoint' -> '/content/robustqa/save/checkpoint'\n",
            "renamed '/content/robustqa/save/squad_complete_12_linear/events.out.tfevents.1632551545.956a4f35651a' -> '/content/robustqa/save/events.out.tfevents.1632551545.956a4f35651a'\n",
            "renamed '/content/robustqa/save/squad_complete_12_linear/log_train.txt' -> '/content/robustqa/save/log_train.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LriK-uWOl8et",
        "outputId": "2ca51ff6-46cc-4445-faf5-ab403c272b4d"
      },
      "source": [
        "#Evaluate Modified\n",
        "%cd /content/robustqa\n",
        "!python3 train.py \\\n",
        "    --do-eval \\\n",
        "    --eval-dir ./datasets/indomain_val/ \\\n",
        "    --eval-datasets squad"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/robustqa\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'roberta.encoder.layer.3.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.weight', 'roberta.encoder.layer.9.hyper_columns.dense.weight', 'roberta.encoder.layer.4.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.weight', 'roberta.encoder.layer.1.hyper_columns.dense.bias', 'roberta.encoder.layer.3.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.weight', 'qa_outputs.bias', 'roberta.encoder.layer.5.hyper_columns.dense.weight', 'roberta.encoder.layer.8.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.weight', 'roberta.encoder.layer.2.hyper_columns.dense.bias', 'roberta.encoder.layer.9.hyper_columns.dense.bias', 'roberta.encoder.layer.2.hyper_columns.dense.weight', 'roberta.encoder.layer.5.hyper_columns.dense.bias', 'roberta.encoder.layer.7.hyper_columns.dense.bias', 'roberta.encoder.layer.1.hyper_columns.dense.weight', 'roberta.encoder.layer.0.hyper_columns.dense.bias', 'roberta.encoder.layer.6.hyper_columns.dense.bias', 'roberta.encoder.layer.11.hyper_columns.dense.bias', 'roberta.encoder.layer.10.hyper_columns.dense.weight', 'roberta.encoder.layer.10.hyper_columns.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100% 10790/10790 [00:00<00:00, 23346.55it/s]\n",
            "  0% 0/10790 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3613: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "100% 10790/10790 [01:30<00:00, 119.25it/s]\n",
            "100% 10570/10570 [00:04<00:00, 2259.36it/s]\n",
            "[10.20.21 09:52:29] Eval F1: 84.24, EM: 71.06\n",
            "[10.20.21 09:52:29] Writing submission file to save/validation_...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfKN1drlFqHd"
      },
      "source": [
        "!mv /content/robustqa/save/log_validation.txt /content/drive/MyDrive/robustqa_change_transformers/save/eval_results/pure_hc_log_validation.txt\n",
        "!mv /content/robustqa/save/validation_ /content/drive/MyDrive/robustqa_change_transformers/save/eval_results/pure_hc_validation_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkEuVylGxfcG",
        "outputId": "f3857084-7663-4382-b02f-dedf869ba4fc"
      },
      "source": [
        "%%writefile /content/robustqa/transformers/models/roberta/modeling_roberta.py\n",
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch RoBERTa model. \"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from packaging import version\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from ...activations import ACT2FN, gelu\n",
        "from ...file_utils import (\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "from ...modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from ...modeling_utils import (\n",
        "    PreTrainedModel,\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from ...utils import logging\n",
        "from .configuration_roberta import RobertaConfig\n",
        "\n",
        "\n",
        "logger = logging.get_logger(__name__)\n",
        "\n",
        "_CHECKPOINT_FOR_DOC = \"roberta-base\"\n",
        "_CONFIG_FOR_DOC = \"RobertaConfig\"\n",
        "_TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n",
        "\n",
        "ROBERTA_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"roberta-base\",\n",
        "    \"roberta-large\",\n",
        "    \"roberta-large-mnli\",\n",
        "    \"distilroberta-base\",\n",
        "    \"roberta-base-openai-detector\",\n",
        "    \"roberta-large-openai-detector\",\n",
        "    # See all RoBERTa models at https://huggingface.co/models?filter=roberta\n",
        "]\n",
        "\n",
        "\n",
        "class RobertaEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if version.parse(torch.__version__) > version.parse(\"1.6.0\"):\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long, device=self.position_ids.device),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "        # End copy\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
        "    ):\n",
        "        if position_ids is None:\n",
        "            if input_ids is not None:\n",
        "                # Create the position ids from the input token ids. Any padded tokens remain padded.\n",
        "                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n",
        "            else:\n",
        "                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n",
        "\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n",
        "        \"\"\"\n",
        "        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n",
        "        Args:\n",
        "            inputs_embeds: torch.Tensor\n",
        "        Returns: torch.Tensor\n",
        "        \"\"\"\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "        sequence_length = input_shape[1]\n",
        "\n",
        "        position_ids = torch.arange(\n",
        "            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n",
        "        )\n",
        "        return position_ids.unsqueeze(0).expand(input_shape)\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n",
        "class RobertaSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n",
        "class RobertaSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta\n",
        "class RobertaAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.self = RobertaSelfAttention(config)\n",
        "        self.output = RobertaSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n",
        "class RobertaIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertOutput\n",
        "class RobertaOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n",
        "class RobertaLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = RobertaAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            if not self.is_decoder:\n",
        "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
        "            self.crossattention = RobertaAttention(config)\n",
        "        self.intermediate = RobertaIntermediate(config)\n",
        "        self.output = RobertaOutput(config)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_value=None,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n",
        "class RobertaEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=False,\n",
        "        output_hidden_states=False,\n",
        "        return_dict=True,\n",
        "    ):\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warning(\n",
        "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "# Copied from transformers.models.bert.modeling_bert.BertPooler\n",
        "class RobertaPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "class RobertaPreTrainedModel(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    config_class = RobertaConfig\n",
        "    base_model_prefix = \"roberta\"\n",
        "    supports_gradient_checkpointing = True\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, RobertaEncoder):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "    def update_keys_to_ignore(self, config, del_keys_to_ignore):\n",
        "        \"\"\"Remove some keys from ignore list\"\"\"\n",
        "        if not config.tie_word_embeddings:\n",
        "            # must make a new list, or the class variable gets modified!\n",
        "            self._keys_to_ignore_on_save = [k for k in self._keys_to_ignore_on_save if k not in del_keys_to_ignore]\n",
        "            self._keys_to_ignore_on_load_missing = [\n",
        "                k for k in self._keys_to_ignore_on_load_missing if k not in del_keys_to_ignore\n",
        "            ]\n",
        "\n",
        "\n",
        "ROBERTA_START_DOCSTRING = r\"\"\"\n",
        "    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic\n",
        "    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,\n",
        "    pruning heads etc.)\n",
        "    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__\n",
        "    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to\n",
        "    general usage and behavior.\n",
        "    Parameters:\n",
        "        config (:class:`~transformers.RobertaConfig`): Model configuration class with all the parameters of the\n",
        "            model. Initializing with a config file does not load the weights associated with the model, only the\n",
        "            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model\n",
        "            weights.\n",
        "\"\"\"\n",
        "\n",
        "ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n",
        "    Args:\n",
        "        input_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`):\n",
        "            Indices of input sequence tokens in the vocabulary.\n",
        "            Indices can be obtained using :class:`~transformers.RobertaTokenizer`. See\n",
        "            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for\n",
        "            details.\n",
        "            `What are input IDs? <../glossary.html#input-ids>`__\n",
        "        attention_mask (:obj:`torch.FloatTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "            `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "        token_type_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,\n",
        "            1]``:\n",
        "            - 0 corresponds to a `sentence A` token,\n",
        "            - 1 corresponds to a `sentence B` token.\n",
        "            `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
        "        position_ids (:obj:`torch.LongTensor` of shape :obj:`({0})`, `optional`):\n",
        "            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,\n",
        "            config.max_position_embeddings - 1]``.\n",
        "            `What are position IDs? <../glossary.html#position-ids>`_\n",
        "        head_mask (:obj:`torch.FloatTensor` of shape :obj:`(num_heads,)` or :obj:`(num_layers, num_heads)`, `optional`):\n",
        "            Mask to nullify selected heads of the self-attention modules. Mask values selected in ``[0, 1]``:\n",
        "            - 1 indicates the head is **not masked**,\n",
        "            - 0 indicates the head is **masked**.\n",
        "        inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`({0}, hidden_size)`, `optional`):\n",
        "            Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded representation.\n",
        "            This is useful if you want more control over how to convert :obj:`input_ids` indices into associated\n",
        "            vectors than the model's internal embedding lookup matrix.\n",
        "        output_attentions (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned\n",
        "            tensors for more detail.\n",
        "        output_hidden_states (:obj:`bool`, `optional`):\n",
        "            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for\n",
        "            more detail.\n",
        "        return_dict (:obj:`bool`, `optional`):\n",
        "            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"The bare RoBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaModel(RobertaPreTrainedModel):\n",
        "    \"\"\"\n",
        "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
        "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
        "    all you need`_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n",
        "    Kaiser and Illia Polosukhin.\n",
        "    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration\n",
        "    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`\n",
        "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
        "    input to the forward pass.\n",
        "    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762\n",
        "    \"\"\"\n",
        "\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.__init__ with Bert->Roberta\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = RobertaEmbeddings(config)\n",
        "        self.encoder = RobertaEncoder(config)\n",
        "\n",
        "        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\"\n",
        "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
        "        class PreTrainedModel\n",
        "        \"\"\"\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    # Copied from transformers.models.bert.modeling_bert.BertModel.forward\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, device)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"RoBERTa Model with a `language modeling` head on top for CLM fine-tuning. \"\"\", ROBERTA_START_DOCSTRING\n",
        ")\n",
        "class RobertaForCausalLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if not config.is_decoder:\n",
        "            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        past_key_values=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
        "            the model is configured as a decoder.\n",
        "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
        "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
        "            - 1 for tokens that are **not masked**,\n",
        "            - 0 for tokens that are **masked**.\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
        "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
        "            ignored (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
        "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
        "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
        "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
        "        use_cache (:obj:`bool`, `optional`):\n",
        "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
        "            decoding (see :obj:`past_key_values`).\n",
        "        Returns:\n",
        "        Example::\n",
        "            >>> from transformers import RobertaTokenizer, RobertaForCausalLM, RobertaConfig\n",
        "            >>> import torch\n",
        "            >>> tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            >>> config = RobertaConfig.from_pretrained(\"roberta-base\")\n",
        "            >>> config.is_decoder = True\n",
        "            >>> model = RobertaForCausalLM.from_pretrained('roberta-base', config=config)\n",
        "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
        "            >>> outputs = model(**inputs)\n",
        "            >>> prediction_logits = outputs.logits\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        if labels is not None:\n",
        "            use_cache = False\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        lm_loss = None\n",
        "        if labels is not None:\n",
        "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
        "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
        "            labels = labels[:, 1:].contiguous()\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
        "\n",
        "        return CausalLMOutputWithCrossAttentions(\n",
        "            loss=lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
        "        input_shape = input_ids.shape\n",
        "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
        "        if attention_mask is None:\n",
        "            attention_mask = input_ids.new_ones(input_shape)\n",
        "\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            input_ids = input_ids[:, -1:]\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"past_key_values\": past}\n",
        "\n",
        "    def _reorder_cache(self, past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
        "        return reordered_past\n",
        "\n",
        "\n",
        "@add_start_docstrings(\"\"\"RoBERTa Model with a `language modeling` head on top. \"\"\", ROBERTA_START_DOCSTRING)\n",
        "class RobertaForMaskedLM(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_save = [r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"lm_head.decoder.weight\", r\"lm_head.decoder.bias\"]\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        if config.is_decoder:\n",
        "            logger.warning(\n",
        "                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n",
        "                \"bi-directional self-attention.\"\n",
        "            )\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.lm_head = RobertaLMHead(config)\n",
        "\n",
        "        # The LM head weights require special treatment only when they are tied with the word embeddings\n",
        "        self.update_keys_to_ignore(config, [\"lm_head.decoder.weight\"])\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head.decoder\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head.decoder = new_embeddings\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MaskedLMOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "        mask=\"<mask>\",\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n",
        "            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n",
        "        kwargs (:obj:`Dict[str, any]`, optional, defaults to `{}`):\n",
        "            Used to hide legacy arguments that have been deprecated.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_attention_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.lm_head(sequence_output)\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (prediction_scores,) + outputs[2:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return MaskedLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=prediction_scores,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaLMHead(nn.Module):\n",
        "    \"\"\"Roberta Head for masked language modeling.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "\n",
        "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
        "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
        "        self.decoder.bias = self.bias\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dense(features)\n",
        "        x = gelu(x)\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # project back to size of vocabulary with bias\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def _tie_weights(self):\n",
        "        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n",
        "        self.bias = self.decoder.bias\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n",
        "    pooled output) e.g. for GLUE tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForSequenceClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.classifier = RobertaClassificationHead(config)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=SequenceClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n",
        "            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n",
        "            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
        "    softmax) e.g. for RocStories/SWAG tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForMultipleChoice(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.roberta = RobertaModel(config)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, 1)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=MultipleChoiceModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        token_type_ids=None,\n",
        "        attention_mask=None,\n",
        "        labels=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n",
        "            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n",
        "            :obj:`input_ids` above)\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n",
        "\n",
        "        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n",
        "        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n",
        "        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n",
        "        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            flat_input_ids,\n",
        "            position_ids=flat_position_ids,\n",
        "            token_type_ids=flat_token_type_ids,\n",
        "            attention_mask=flat_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=flat_inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = logits.view(-1, num_choices)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(reshaped_logits, labels)\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (reshaped_logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return MultipleChoiceModelOutput(\n",
        "            loss=loss,\n",
        "            logits=reshaped_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n",
        "    Named-Entity-Recognition (NER) tasks.\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=TokenClassifierOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n",
        "            1]``.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            # Only keep active parts of the loss\n",
        "            if attention_mask is not None:\n",
        "                active_loss = attention_mask.view(-1) == 1\n",
        "                active_logits = logits.view(-1, self.num_labels)\n",
        "                active_labels = torch.where(\n",
        "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
        "                )\n",
        "                loss = loss_fct(active_logits, active_labels)\n",
        "            else:\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return TokenClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class RobertaClassificationHead(nn.Module):\n",
        "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        classifier_dropout = (\n",
        "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
        "        )\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
        "        x = self.dropout(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.tanh(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "@add_start_docstrings(\n",
        "    \"\"\"\n",
        "    Roberta Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n",
        "    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n",
        "    \"\"\",\n",
        "    ROBERTA_START_DOCSTRING,\n",
        ")\n",
        "class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n",
        "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n",
        "    @add_code_sample_docstrings(\n",
        "        processor_class=_TOKENIZER_FOR_DOC,\n",
        "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
        "        output_type=QuestionAnsweringModelOutput,\n",
        "        config_class=_CONFIG_FOR_DOC,\n",
        "    )\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n",
        "            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n",
        "            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n",
        "            sequence are not taken into account for computing the loss.\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        outputs = self.roberta(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (start_logits, end_logits) + outputs[2:]\n",
        "            return ((total_loss,) + output) if total_loss is not None else output\n",
        "\n",
        "        return QuestionAnsweringModelOutput(\n",
        "            loss=total_loss,\n",
        "            start_logits=start_logits,\n",
        "            end_logits=end_logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n",
        "    \"\"\"\n",
        "    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n",
        "    are ignored. This is modified from fairseq's `utils.make_positions`.\n",
        "    Args:\n",
        "        x: torch.Tensor x:\n",
        "    Returns: torch.Tensor\n",
        "    \"\"\"\n",
        "    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n",
        "    mask = input_ids.ne(padding_idx).int()\n",
        "    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n",
        "    return incremental_indices.long() + padding_idx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/robustqa/transformers/models/roberta/modeling_roberta.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYUG0ju3PRlT",
        "outputId": "12c51605-f011-4f63-8310-f78300ec2db2"
      },
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 86912, done.\u001b[K\n",
            "remote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 86912 (delta 5), reused 7 (delta 0), pack-reused 86896\u001b[K\n",
            "Receiving objects: 100% (86912/86912), 69.98 MiB | 20.86 MiB/s, done.\n",
            "Resolving deltas: 100% (62532/62532), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNcGRpNSQVEh"
      },
      "source": [
        "!rm -r /content/robustqa/transformers/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7cogla0QkKT"
      },
      "source": [
        "!mv /content/transformers/ /content/robustqa/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxB-t3qlQswu"
      },
      "source": [
        "!mv /content/robustqa/transformers/src/transformers/ /content/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3lZBCdnQ9Un"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}